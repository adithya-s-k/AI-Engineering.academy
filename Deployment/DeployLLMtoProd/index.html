<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/Deployment/DeployLLMtoProd/ rel=canonical><link href=../ rel=prev><link href=../Quantization/AWQ_Quantization/ rel=next><link rel=icon href=../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.0"><title>LLMs to Prod - AI Engineering Academy</title><link rel=stylesheet href=../../assets/stylesheets/main.618322db.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
  posthog.init('phc_OL7nUCVeKtVJe8eHSKGs8zPTQAyr0hm8opAPFdFlkBz', {
      api_host: 'https://us.i.posthog.com',
      person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
  })
</script><meta property=og:type content=website><meta property=og:title content="LLMs to Prod - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/Deployment/DeployLLMtoProd.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/Deployment/DeployLLMtoProd/ property=og:url><meta property=twitter:card content=summary_large_image><meta property=twitter:title content="LLMs to Prod - AI Engineering Academy"><meta property=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta property=twitter:image content=https://aiengineering.academy/assets/images/social/Deployment/DeployLLMtoProd.png></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LLMs to Prod </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../RAG/ class=md-tabs__link> RAG </a> </li> <li class=md-tabs__item> <a href=../../LLM/ class=md-tabs__link> LLM </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../AIBreakDown/TRM/ class=md-tabs__link> AI BreakDown </a> </li> <li class=md-tabs__item> <a href=../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../LLM/ class=md-nav__link> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> Deployment </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Deployment </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> LLMs to Prod </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> Quantization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> Quantization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Quantization/AWQ_Quantization/ class=md-nav__link> <span class=md-ellipsis> AWQ </span> </a> </li> <li class=md-nav__item> <a href=../Quantization/GGUF_Quantization/ class=md-nav__link> <span class=md-ellipsis> GGUF </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../AIBreakDown/TRM/ class=md-nav__link> <span class=md-ellipsis> AI BreakDown </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/Deployment/DeployLLMtoProd.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/Deployment/DeployLLMtoProd.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1>LLMs to Prod</h1> <div><p>Deploying Large Language Models (LLMs) into production involves choosing the right tools and considering costs. Here’s a breakdown for comparing TGI, vLLM, and SGlang, deploying Llama 3.2 70B (likely Llama 3.1 70B), scaling on Kubernetes, and deciding between hosting your own or using third-party APIs.</p> <p><strong>Comparing TGI, vLLM, and SGlang</strong></p> <ul> <li><strong>TGI</strong> (Text Generation Inference) from Hugging Face is great for integrating with their ecosystem, supporting models like Llama, with tensor parallelism for performance.</li> <li><strong>vLLM</strong>, developed at UC Berkeley, offers high throughput with Paged Attention, ideal for fast inference on GPUs.</li> <li><strong>SGlang</strong>, by LMSYS Org, focuses on efficiency with low-latency serving, suitable for real-time applications.</li> </ul> <p>Each can deploy Llama 3.1 70B, requiring multiple GPUs (e.g., 8 with 40 GB each). TGI is user-friendly for Hugging Face users, vLLM excels in speed, and SGlang offers scalability.</p> <p><strong>Deploying Llama 3.2 70B</strong></p> <p>Given Llama 3.2 models are 1B, 3B, 11B, and 90B, "Llama 3.2 70B" likely means Llama 3.1 70B. Deployment steps:</p> <ul> <li><strong>TGI</strong>: Use Hugging Face’s launcher, containerize, and distribute across GPUs.</li> <li><strong>vLLM</strong>: Leverage Paged Attention, deploy via container images, and scale with Kubernetes.</li> <li><strong>SGlang</strong>: Use its runtime, optimize for GPUs, and deploy on clusters.</li> </ul> <p><strong>Deploying on Kubernetes at Scale</strong></p> <p>All three can be containerized and deployed on Kubernetes:</p> <ul> <li>Use Kubernetes for scaling, monitoring, and managing resources across nodes.</li> <li>TGI has Kubernetes deployment examples <a href=https://huggingface.co/docs/transformers/en/llm_tgi_deployment>Hugging Face TGI</a>.</li> <li>vLLM and SGlang also support containerization, with scaling via Kubernetes pods.</li> </ul> <p><strong>Price Comparison and Hosting vs. Third-Party APIs</strong></p> <ul> <li><strong>Hosting Your Own</strong>: Costs include GPU hardware (e.g., AWS A100 at ~<span class=arithmatex>\(3/hour each, needing 8 for Llama 3.1 70B, ~\)</span>24/hour) and maintenance. For high volume, it’s cheaper long-term.</li> <li><strong>Third-Party APIs</strong>: OpenAI charges per token (e.g., $0.01/1000 tokens input, $0.03/1000 output). For low volume, it’s easier and cost-effective.</li> <li>Research suggests hosting is better for privacy and high usage, while APIs suit quick setups.</li> </ul> <p>This guide helps decide based on your needs, with Kubernetes offering scalability for all options.</p> <hr> <p><strong>Survey Note: Comprehensive Analysis of Deploying LLMs into Production</strong></p> <p>Deploying Large Language Models (LLMs) into production is a complex task, requiring careful selection of tools, infrastructure, and cost management. This analysis compares Text Generation Inference (TGI), vLLM, and SGlang for deploying models like Llama 3.2 70B (likely Llama 3.1 70B given current offerings), discusses deployment on Kubernetes at scale, provides a rough price comparison, and evaluates hosting versus using third-party APIs. The analysis is based on current research and documentation as of February 24, 2025.</p> <p><strong>Introduction to LLM Deployment</strong></p> <p>Deploying LLMs in production involves setting up the model to handle real-world requests efficiently, managing resources, ensuring reliability, and maintaining performance. Given the computational demands of LLMs, specialized frameworks are essential for optimizing inference speed and memory usage. This survey explores three prominent tools: TGI, vLLM, and SGlang, focusing on their capabilities for deploying a large model like Llama 3.1 70B, scaling on Kubernetes, and cost implications.</p> <p><strong>Overview of Deployment Tools</strong></p> <ol> <li><strong>Text Generation Inference (TGI)</strong>:</li> <li>TGI, developed by Hugging Face, is a toolkit designed for deploying and serving Large Language Models (LLMs). It supports various open-source models, including Llama, Falcon, StarCoder, BLOOM, and GPT-NeoX, making it versatile for production environments.</li> <li>Key features include tensor parallelism for faster inference across multiple GPUs, optimized transformers code using Flash Attention and Paged Attention, and a simple API for compatibility with Hugging Face models. It is already in use by organizations like IBM and Grammarly, indicating robust production readiness.</li> <li>TGI offers distributed tracing via Open Telemetry and Prometheus metrics for monitoring, enhancing operational visibility.</li> <li><strong>vLLM</strong>:</li> <li>vLLM, originating from the Sky Computing Lab at UC Berkeley, is an open-source library for fast LLM inference and serving, with over 200k monthly downloads and an Apache 2.0 license. It is designed for high-throughput and memory-efficient serving, leveraging PagedAttention and continuous batching.</li> <li>It supports distributed inference across multiple GPUs, with quantizations like GPTQ, AWQ, INT4, INT8, and FP8, and optimized CUDA kernels including FlashAttention and FlashInfer. vLLM is particularly noted for up to 24x higher throughput compared to Hugging Face Transformers without model changes.</li> <li>It is suitable for applications requiring parallel processing and streaming output, with integration capabilities for platforms like SageMaker and LangChain.</li> <li><strong>SGlang</strong>:</li> <li>SGlang, developed by LMSYS Org, is a fast serving framework for LLMs and vision-language models, focusing on efficient execution of complex language model programs. It offers a flexible frontend language for programming LLM applications, including chained generation calls, advanced prompting, and multi-modal inputs.</li> <li>It supports a wide range of generative models (Llama, Gemma, Mistral, QWen, DeepSeek, LLaVA, etc.), embedding models, and reward models, with easy extensibility. SGlang introduces optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding, achieving up to 6.4x higher throughput compared to state-of-the-art systems.</li> <li>Backed by an active community and supported by industry players like NVIDIA and xAI, SGlang is designed for scalability and low-latency inference, suitable for real-time applications.</li> </ol> <p><strong>Deploying Llama 3.2 70B: Clarification and Approach</strong></p> <p>The user query mentions "Llama 3.2 70B," but current documentation as of February 2025 indicates Llama 3.2 models are available in sizes 1B, 3B, 11B, and 90B, with multimodal capabilities for 11B and 90B, and no explicit 70B version <a href=https://huggingface.co/meta-llama>Meta Llama</a>. Given this, it is likely the user intended Llama 3.1 70B, which is part of the Llama 3.1 collection with sizes ranging from 8B to 405B, released in July 2024 <a href=https://ai.meta.com/blog/meta-llama-3/ >Meta AI Blog</a>. This analysis will proceed with Llama 3.1 70B as the target model.</p> <ul> <li><strong>System Requirements</strong>:</li> <li>Llama 3.1 70B, with 70 billion parameters, requires significant computational resources. In float16 precision, each parameter consumes 2 bytes, totaling approximately 140 GB for model weights. During inference, additional memory is needed for the key-value (KV) cache, potentially requiring 5-10x more memory for high-throughput scenarios.</li> <li>Deployment typically requires multiple GPUs, with recommendations including 8 GPUs with at least 40 GB VRAM each (e.g., NVIDIA A100 or H100). For example, deploying on AWS might use inf2.48xlarge instances with 12 Inferentia2 accelerators, or cloud instances like g5.48xlarge for EC2 <a href=https://www.philschmid.de/inferentia2-llama3-70b>Deploy Llama 3 70B on AWS Inferentia2</a>.</li> </ul> <p><strong>Comparison of TGI, vLLM, and SGlang for Llama 3.1 70B</strong></p> <p>To compare these frameworks, we evaluate performance, ease of use, and specific support for deploying Llama 3.1 70B:</p> <ul> <li><strong>Performance</strong>:</li> <li>TGI leverages tensor parallelism and optimized attention mechanisms, achieving high-performance text generation. Benchmarks suggest it handles Llama 2 70B with 8 GPUs of 40 GB each, suitable for production but may have higher latency for very large models <a href=https://www.ideas2it.com/blogs/deploying-llm-powered-applications-in-production-using-tgi>Deploying LLM Powered Applications with HuggingFace TGI</a>.</li> <li>vLLM, with PagedAttention and continuous batching, offers up to 24x higher throughput than Hugging Face Transformers, making it ideal for high-throughput scenarios. For Llama 3.1 70B, it supports CPU offloading on NVIDIA GH200 instances, expanding available memory <a href=https://docs.lambdalabs.com/public-cloud/on-demand/serving-llama-31-vllm-gh200/ >Serving Llama 3.1 8B and 70B using vLLM on an NVIDIA GH200 instance</a>.</li> <li>SGlang achieves up to 6.4x higher throughput compared to vLLM and TensorRT-LLM on tasks like agent control and JSON decoding, with optimizations like RadixAttention for KV cache reuse <a href=https://arxiv.org/html/2312.07104v2>SGLang: Efficient Execution of Structured Language Model Programs</a>. It is designed for low-latency, real-time applications.</li> <li><strong>Ease of Use</strong>:</li> <li>TGI offers a simple launcher and integration with Hugging Face Hub, making it user-friendly for developers familiar with the ecosystem. However, distributed setup may require additional configuration.</li> <li>vLLM is noted for its ease of deployment, with pre-configured environments on platforms like AWS and support for Hugging Face models, requiring minimal setup for inference <a href=https://medium.com/@55_learning/deploy-large-language-model-llm-with-vllm-on-k8s-6378be632b54>Deploy Large Language Model (LLM) with vLLM on K8s</a>.</li> <li>SGlang provides an intuitive interface for programming LLM applications, but being relatively new, it may have fewer community resources compared to TGI and vLLM <a href=https://github.com/sgl-project/sglang>SGLang GitHub</a>.</li> <li><strong>Specific Support for Llama 3.1 70B</strong>:</li> <li>All three frameworks support Llama models, with TGI explicitly mentioned for Llama 2 and 3 deployments <a href=https://www.philschmid.de/inferentia2-llama-70b-inference>Deploy Llama 2 70B on AWS Inferentia2 with Hugging Face Optimum</a>. For Llama 3.1 70B, TGI’s tensor parallelism is effective.</li> <li>vLLM has detailed guides for deploying Llama 3.1 70B, including on NVIDIA GH200 with CPU offloading, making it suitable for memory-constrained environments <a href=https://infohub.delltechnologies.com/en-au/p/run-llama-3-on-dell-poweredge-xe9680-and-amd-mi300x-with-vllm/ >Run Llama 3 on Dell PowerEdge XE9680 and AMD MI300x with vLLM</a>.</li> <li>SGlang supports Llama 3.1 70B, with benchmarks showing superior throughput compared to vLLM and TensorRT-LLM, particularly for complex tasks <a href=https://lmsys.org/blog/2024-07-25-sglang-llama3/ >Achieving Faster Open-Source Llama3 Serving with SGLang Runtime</a>.</li> </ul> <p><strong>Deploying Llama 3.1 70B with Each Framework</strong></p> <p>Given the user’s mention of Llama 3.2 70B, current documentation (as of February 24, 2025) shows Llama 3.2 models are 1B, 3B, 11B, and 90B, with no 70B <a href=https://huggingface.co/meta-llama>Meta Llama</a>. It’s likely they meant Llama 3.1 70B, part of the Llama 3.1 collection released in July 2024 <a href=https://ai.meta.com/blog/meta-llama-3/ >Meta AI Blog</a>. Here’s how to deploy it:</p> <p><strong>TGI Deployment</strong></p> <ol> <li><strong>Prerequisites</strong>: Install TGI, ensure 8 GPUs with 40 GB VRAM each.</li> <li><strong>Steps</strong>:</li> <li>Download the model: huggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct.</li> <li>Start the server: tgi-server --model-id meta-llama/Meta-Llama-3-70B-Instruct --num-shards 8.</li> </ol> <p><strong>vLLM Deployment</strong></p> <ol> <li><strong>Prerequisites</strong>: Install vLLM, ensure GPU compatibility.</li> <li><strong>Steps</strong>:</li> <li>Download the model weights.</li> <li>Start the server: vllm --model meta-llama/Meta-Llama-3-70B-Instruct --tensor-parallel-degree 8.</li> </ol> <p><strong>SGlang Deployment</strong></p> <ol> <li><strong>Prerequisites</strong>: Install SGlang, ensure GPU support.</li> <li><strong>Steps</strong>:</li> <li>Load the model: pip install sglang[srt].</li> <li>Start the server: sglang serve meta-llama/Meta-Llama-3-70B-Instruct.</li> </ol> <p><strong>Deploying on Kubernetes at Scale</strong></p> <p>All three can be containerized and deployed on Kubernetes for scalability:</p> <p><strong>TGI on Kubernetes</strong></p> <ul> <li>Create a cluster with GPU support using gcloud container clusters create.</li> <li>Define a Deployment:</li> </ul> <p>yaml</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=nt>metadata</span><span class=p>:</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">tgi-server</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=nt>spec</span><span class=p>:</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=w>  </span><span class=nt>selector</span><span class=p>:</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">tgi-server</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=w>  </span><span class=nt>template</span><span class=p>:</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=w>      </span><span class=nt>labels</span><span class=p>:</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">tgi-server</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=w>    </span><span class=nt>spec</span><span class=p>:</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a><span class=w>      </span><span class=nt>containers</span><span class=p>:</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a><span class=w>        </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">tgi-server</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=w>          </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">huggingface/tgi-server</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a><span class=w>          </span><span class=nt>args</span><span class=p>:</span>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a><span class=w>            </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">--model-id=meta-llama/Meta-Llama-3-70B-Instruct</span>
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a><span class=w>            </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">--num-shards=8</span>
</span><span id=__span-0-21><a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a><span class=w>          </span><span class=nt>resources</span><span class=p>:</span>
</span><span id=__span-0-22><a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a><span class=w>            </span><span class=nt>limits</span><span class=p>:</span>
</span><span id=__span-0-23><a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a><span class=w>              </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
</span></code></pre></div> <ul> <li>Apply with kubectl apply -f deployment.yaml.</li> </ul> <p><strong>vLLM on Kubernetes</strong></p> <ul> <li>Create a cluster with GPU support.</li> <li>Define a Deployment:</li> </ul> <p>yaml</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=nt>metadata</span><span class=p>:</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">vllm-server</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=nt>spec</span><span class=p>:</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=w>  </span><span class=nt>selector</span><span class=p>:</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">vllm-server</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=w>  </span><span class=nt>template</span><span class=p>:</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a><span class=w>      </span><span class=nt>labels</span><span class=p>:</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">vllm-server</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a><span class=w>    </span><span class=nt>spec</span><span class=p>:</span>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a><span class=w>      </span><span class=nt>containers</span><span class=p>:</span>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a><span class=w>        </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">vllm-server</span>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a><span class=w>          </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">vllm/vllm-openai</span>
</span><span id=__span-1-18><a id=__codelineno-1-18 name=__codelineno-1-18 href=#__codelineno-1-18></a><span class=w>          </span><span class=nt>args</span><span class=p>:</span>
</span><span id=__span-1-19><a id=__codelineno-1-19 name=__codelineno-1-19 href=#__codelineno-1-19></a><span class=w>            </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">--model=meta-llama/Meta-Llama-3-70B-Instruct</span>
</span><span id=__span-1-20><a id=__codelineno-1-20 name=__codelineno-1-20 href=#__codelineno-1-20></a><span class=w>            </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">--tensor-parallel-degree=8</span>
</span><span id=__span-1-21><a id=__codelineno-1-21 name=__codelineno-1-21 href=#__codelineno-1-21></a><span class=w>          </span><span class=nt>resources</span><span class=p>:</span>
</span><span id=__span-1-22><a id=__codelineno-1-22 name=__codelineno-1-22 href=#__codelineno-1-22></a><span class=w>            </span><span class=nt>limits</span><span class=p>:</span>
</span><span id=__span-1-23><a id=__codelineno-1-23 name=__codelineno-1-23 href=#__codelineno-1-23></a><span class=w>              </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
</span></code></pre></div> <ul> <li>Apply with kubectl apply -f deployment.yaml.</li> </ul> <p><strong>SGlang on Kubernetes</strong></p> <ul> <li>Create a cluster with GPU support.</li> <li>Define a Deployment:</li> </ul> <p>yaml</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=nt>metadata</span><span class=p>:</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">sglang-server</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=nt>spec</span><span class=p>:</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=w>  </span><span class=nt>selector</span><span class=p>:</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">sglang-server</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=w>  </span><span class=nt>template</span><span class=p>:</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=w>      </span><span class=nt>labels</span><span class=p>:</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">sglang-server</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a><span class=w>    </span><span class=nt>spec</span><span class=p>:</span>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a><span class=w>      </span><span class=nt>containers</span><span class=p>:</span>
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a><span class=w>        </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">sglang-server</span>
</span><span id=__span-2-17><a id=__codelineno-2-17 name=__codelineno-2-17 href=#__codelineno-2-17></a><span class=w>          </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">sglang/sglang-runtime</span>
</span><span id=__span-2-18><a id=__codelineno-2-18 name=__codelineno-2-18 href=#__codelineno-2-18></a><span class=w>          </span><span class=nt>args</span><span class=p>:</span>
</span><span id=__span-2-19><a id=__codelineno-2-19 name=__codelineno-2-19 href=#__codelineno-2-19></a><span class=w>            </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">serve</span>
</span><span id=__span-2-20><a id=__codelineno-2-20 name=__codelineno-2-20 href=#__codelineno-2-20></a><span class=w>            </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">meta-llama/Meta-Llama-3-70B-Instruct</span>
</span><span id=__span-2-21><a id=__codelineno-2-21 name=__codelineno-2-21 href=#__codelineno-2-21></a><span class=w>          </span><span class=nt>resources</span><span class=p>:</span>
</span><span id=__span-2-22><a id=__codelineno-2-22 name=__codelineno-2-22 href=#__codelineno-2-22></a><span class=w>            </span><span class=nt>limits</span><span class=p>:</span>
</span><span id=__span-2-23><a id=__codelineno-2-23 name=__codelineno-2-23 href=#__codelineno-2-23></a><span class=w>              </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
</span></code></pre></div> <ul> <li>Apply with kubectl apply -f deployment.yaml.</li> </ul> <p><strong>Deploying on Kubernetes at Scale</strong></p> <p>The user query mentions "Kuberntees," likely a typo for "Kubernetes," a standard container orchestration system for managing applications at scale. All three frameworks can be deployed on Kubernetes, leveraging containerization for scalability:</p> <ul> <li><strong>TGI on Kubernetes</strong>:</li> <li>Hugging Face provides examples for deploying TGI on Kubernetes, using container images and managing with Kubernetes pods. For Llama 3.1 70B, distribute across multiple nodes with tensor parallelism, ensuring sufficient GPU resources <a href=https://huggingface.co/docs/google-cloud/en/examples/gke-tgi-deployment>Deploy Meta Llama 3 8B with TGI DLC on GKE</a>.</li> <li>Scaling involves adjusting pod replicas based on load, with monitoring via Kubernetes metrics and Open Telemetry for distributed tracing.</li> <li><strong>vLLM on Kubernetes</strong>:</li> <li>vLLM supports deployment on Kubernetes, with guides for using GPU operators and scaling on platforms like Azure Kubernetes Service (AKS). For Llama 3.1 70B, use container images and configure for distributed inference, leveraging PagedAttention for efficiency <a href=https://medium.com/@55_learning/deploy-large-language-model-llm-with-vllm-on-k8s-6378be632b54>Deploy Large Language Model (LLM) with vLLM on K8s</a>.</li> <li>Scaling can be automated with Horizontal Pod Autoscaler, monitoring GPU utilization and throughput.</li> <li><strong>SGlang on Kubernetes</strong>:</li> <li>SGlang can be containerized and deployed on Kubernetes, with its runtime optimized for GPU utilization. For Llama 3.1 70B, configure for high throughput, using Kubernetes for dynamic workload distribution <a href=https://rocm.blogs.amd.com/artificial-intelligence/sglang/README.html>SGLang: Fast Serving Framework for Large Language and Vision-Language Models on AMD Instinct GPUs</a>.</li> <li>Scaling involves managing pod distribution across nodes, ensuring low-latency inference for real-time applications.</li> </ul> <p><strong>Rough Price Comparison and Hosting vs. Third-Party APIs</strong></p> <p>To compare the cost of hosting your own LLM versus using a third-party API, consider the following factors:</p> <ul> <li><strong>Hosting Your Own LLM</strong>:</li> <li><strong>Hardware Costs</strong>: Deploying Llama 3.1 70B requires significant GPU resources, typically 8 GPUs with 40 GB VRAM each (e.g., NVIDIA A100 or H100). On AWS, an A100 instance might cost approximately <span class=arithmatex>\(3 per hour each, totaling ~\)</span>24/hour for 8 GPUs, or $17,280/month for continuous operation <a href=https://abhinand05.medium.com/self-hosting-llama-3-1-70b-or-any-70b-llm-affordably-2bd323d72f8d>Self-Hosting LLaMA 3.1 70B (or any ~70B LLM) Affordably</a>. On-premises, initial hardware costs (e.g., $25,000 per H100 GPU) and power consumption add to the expense.</li> <li><strong>Operational Costs</strong>: Include maintenance, cooling, and power, which can be significant for on-premises setups. Cloud hosting reduces some operational overhead but increases hourly costs.</li> <li><strong>Break-even Analysis</strong>: For high-volume usage (e.g., 1000+ requests/day), hosting can be cost-effective long-term, especially with optimizations like quantization reducing memory needs.</li> <li><strong>Third-Party APIs (e.g., OpenAI)</strong>:</li> <li>OpenAI’s pricing is token-based, with GPT-4o at $0.01 per 1000 input tokens and $0.03 per 1000 output tokens as of February 2025 <a href=https://openai.com/pricing>OpenAI Pricing</a>. For Llama 3.1 70B, API providers like Replicate or Groq offer competitive rates, with some at $0.88 per 1M tokens blended <a href=https://artificialanalysis.ai/models/llama-3-instruct-70b>Llama 3 70B - Intelligence, Performance &amp; Price Analysis</a>.</li> <li>For low to medium usage (e.g., 100-1000 requests/day), APIs are more cost-effective, with costs below $100/month for small volumes, but scaling up (e.g., 2000 requests/day) can reach $2000/month, making hosting more viable <a href=https://sawerakhadium567.medium.com/is-hosting-your-own-llm-cheaper-than-openai-8a9a4dc76c6a>Is Hosting Your Own LLM Cheaper than OpenAI?</a>.</li> <li><strong>Comparison Table(these are rough calculations)</strong>:</li> </ul> <table> <thead> <tr> <th><strong>Aspect</strong></th> <th><strong>Hosting Your Own (Llama 3.1 70B)</strong></th> <th><strong>Third-Party API (e.g., OpenAI)</strong></th> </tr> </thead> <tbody> <tr> <td>Initial Cost</td> <td>High (Hardware ~$200,000 for 8 H100s)</td> <td>Low (No hardware needed)</td> </tr> <tr> <td>Monthly Cost (Low Use)</td> <td>~$17,280 (Cloud, continuous)</td> <td>~$100 (1000 requests/day)</td> </tr> <tr> <td>Monthly Cost (High Use)</td> <td>~$17,280 (Fixed)</td> <td>~$2000 (2000 requests/day)</td> </tr> <tr> <td>Privacy</td> <td>High (On-premises control)</td> <td>Low (Data sent to provider)</td> </tr> <tr> <td>Scalability</td> <td>High (Kubernetes, custom scaling)</td> <td>Medium (API limits)</td> </tr> <tr> <td>Ease of Deployment</td> <td>Medium (Setup complexity)</td> <td>High (Quick integration)</td> </tr> </tbody> </table> <ul> <li><strong>Decision Factors</strong>:</li> <li>Host your own LLM for high data privacy needs (e.g., finance, healthcare), large-scale usage where API costs escalate, and when customization is critical. It’s also suitable for long-term cost savings at high volumes.</li> <li>Use third-party APIs for quick deployment, low to medium usage, and when infrastructure management is a burden. They offer ease of use but may compromise on privacy and cost at scale.</li> </ul> <p><strong>Conclusion</strong></p> <p>Research suggests that TGI, vLLM, and SGlang are viable for deploying LLMs like Llama 3.1 70B, each with unique strengths: TGI for ecosystem integration, vLLM for speed, and SGlang for efficiency. Deploying on Kubernetes at scale is feasible for all, with containerization and scaling options. The evidence leans toward hosting your own LLM being cost-effective for high-volume use, while third-party APIs suit lower volumes. An unexpected detail is that Llama 3.2 70B may not exist, likely referring to Llama 3.1 70B, highlighting the importance of model version clarity. Choose based on your usage volume, privacy needs, and infrastructure capabilities.</p> <p><strong>Key Citations</strong></p> <ul> <li><a href=https://www.ideas2it.com/blogs/deploying-llm-powered-applications-in-production-using-tgi>Deploying LLM Powered Applications with HuggingFace TGI</a></li> <li><a href=https://github.com/vllm-project/vllm>vLLM GitHub: A high-throughput and memory-efficient inference and serving engine for LLMs</a></li> <li><a href=https://github.com/sgl-project/sglang>SGLang GitHub: SGLang is a fast serving framework for large language models and vision language models</a></li> <li><a href=https://huggingface.co/meta-llama>Meta Llama: Llama 3.2 Vision and other models</a></li> <li><a href=https://ai.meta.com/blog/meta-llama-3/ >Meta AI Blog: Meta Llama 3</a></li> <li><a href=https://www.philschmid.de/inferentia2-llama-70b-inference>Deploy Llama 2 70B on AWS Inferentia2 with Hugging Face Optimum</a></li> <li><a href=https://docs.lambdalabs.com/public-cloud/on-demand/serving-llama-31-vllm-gh200/ >Serving Llama 3.1 8B and 70B using vLLM on an NVIDIA GH200 instance</a></li> <li><a href=https://arxiv.org/html/2312.07104v2>SGLang: Efficient Execution of Structured Language Model Programs</a></li> <li><a href=https://medium.com/@55_learning/deploy-large-language-model-llm-with-vllm-on-k8s-6378be632b54>Deploy Large Language Model (LLM) with vLLM on K8s</a></li> <li><a href=https://huggingface.co/docs/google-cloud/en/examples/gke-tgi-deployment>Deploy Meta Llama 3 8B with TGI DLC on GKE</a></li> <li><a href=https://abhinand05.medium.com/self-hosting-llama-3-1-70b-or-any-70b-llm-affordably-2bd323d72f8d>Self-Hosting LLaMA 3.1 70B (or any ~70B LLM) Affordably</a></li> <li><a href=https://sawerakhadium567.medium.com/is-hosting-your-own-llm-cheaper-than-openai-8a9a4dc76c6a>Is Hosting Your Own LLM Cheaper than OpenAI?</a></li> <li><a href=https://artificialanalysis.ai/models/llama-3-instruct-70b>Llama 3 70B - Intelligence, Performance &amp; Price Analysis</a></li> <li><a href=https://openai.com/pricing>OpenAI Pricing: API pricing details</a></li> </ul></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 27, 2025 17:29:26 UTC">November 27, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 27, 2025 17:29:26 UTC">November 27, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../ class="md-footer__link md-footer__link--prev" aria-label="Previous: Introduction"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Introduction </div> </div> </a> <a href=../Quantization/AWQ_Quantization/ class="md-footer__link md-footer__link--next" aria-label="Next: AWQ"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> AWQ </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.e71a0d61.min.js></script> <script src=../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>