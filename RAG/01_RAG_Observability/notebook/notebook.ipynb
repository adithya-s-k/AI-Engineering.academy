{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h2>RAG Observability - Arize Phoenix Setup</h2>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h3 ><a href=\"https://aiengineering.academy/\" target=\"_blank\">AI Engineering.academy</a></h3>\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "<a href=\"https://aiengineering.academy/\" target=\"_blank\">\n",
    "<img src=\"https://raw.githubusercontent.com/adithya-s-k/AI-Engineering.academy/main/assets/banner.png\" alt=\"Ai Engineering. Academy\" width=\"50%\">\n",
    "</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "[![GitHub Stars](https://img.shields.io/github/stars/adithya-s-k/AI-Engineering.academy?style=social)](https://github.com/adithya-s-k/AI-Engineering.academy/stargazers)\n",
    "[![GitHub Forks](https://img.shields.io/github/forks/adithya-s-k/AI-Engineering.academy?style=social)](https://github.com/adithya-s-k/AI-Engineering.academy/network/members)\n",
    "[![GitHub Issues](https://img.shields.io/github/issues/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/issues)\n",
    "[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/pulls)\n",
    "[![License](https://img.shields.io/github/license/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/blob/main/LICENSE)\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to this notebook, where we explore the setup and observation of a Retrieval-Augmented Generation (RAG) pipeline using Llama Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Getting Started](#getting-started)\n",
    "3. [Usage](#usage)\n",
    "4. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This guide provides a comprehensive walkthrough for configuring the necessary tools and libraries, including embedding models and vector store indexing, to enable efficient document retrieval and query processing. We‚Äôll cover everything from installation and setup to querying and retrieving relevant information, equipping you with the knowledge to harness the power of RAG pipelines for advanced search capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To get started with this notebook, you'll need to have a basic understanding of Python and some familiarity with machine learning concepts. Don't worry if you're new to some of these ideas ‚Äì we'll guide you through each step!\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.7+\n",
    "- Jupyter Notebook or JupyterLab\n",
    "- Basic knowledge of Python and machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Install required packages\n",
    "\n",
    "To get started with setting up Arize Phoenix, you'll need to install the required packages.\n",
    "\n",
    "Arize Phoenix is a comprehensive tool designed for observability and monitoring in machine learning and AI systems. It provides functionalities for tracking and analyzing various aspects of machine learning models and data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix\n",
    "!pip install openinference-instrumentation-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands will install:\n",
    "\n",
    "- `arize-phoenix`: A tool for observability in machine learning workflows.\n",
    "- `openinference-instrumentation-openai`: A package to instrument OpenAI models with observability tools like Arize Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setting up Arize Phoenix\n",
    "\n",
    "There are 3 ways to do this:\n",
    "\n",
    "Read more [here.](https://docs.arize.com/phoenix/quickstart)\n",
    "\n",
    "- Command Line\n",
    "\n",
    "  ```bash\n",
    "  python3 -m phoenix.server.main serve\n",
    "  ```\n",
    "\n",
    "- Docker\n",
    "\n",
    "  Launch the phoenix docker image using:\n",
    "\n",
    "  ```bash\n",
    "  docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest\n",
    "  ```\n",
    "\n",
    "  This will expose the Phoenix UI and REST API on localhost:6006 and exposes the gRPC endpoint for spans on localhost:4317.\n",
    "\n",
    "- Notebook\n",
    "\n",
    "  ```python\n",
    "  import phoenix as px\n",
    "  px.launch_app()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Import Required Libraries and Configure the Environment\n",
    "\n",
    "Before proceeding with data processing and evaluation, import the necessary libraries and set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import phoenix as px\n",
    "\n",
    "# Allows concurrent evaluations in notebook environments\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set display options for pandas DataFrames to show more content\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `getpass`: A utility for securely capturing password input.\n",
    "  `nest_asyncio`: Allows the usage of asyncio within Jupyter notebooks.\n",
    "\n",
    "- `pandas` (`pd`): A powerful data manipulation library for Python.\n",
    "\n",
    "- `tqdm`: Provides progress bars for loops, useful for tracking the progress of data processing.\n",
    "\n",
    "- `phoenix` (`px`): The phoenix library is part of Arize's observability tools. It provides an interactive UI for exploring data and monitoring machine learning models.\n",
    "\n",
    "Configure `nest_asyncio` to allow concurrent evaluations in notebook environments and set the maximum column width for pandas DataFrames to ensure better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Launch the Phoenix App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x243f0fb3650>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import phoenix as px\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function initializes and launches the Phoenix app, which opens in a new tab in your default web browser. It provides an interactive interface for exploring datasets, visualizing model performance, and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 View the Phoenix App Session\n",
    "\n",
    "Once the Phoenix app is launched, you can use the session object to interact with the app directly in the notebook. Run the following code to launch the Phoenix app and view it in the current session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch and view the Phoenix app session\n",
    "(session := px.launch_app()).view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line launches the Phoenix app and assigns the session to a variable named session, with the `view()` method allowing you to display the Phoenix app directly within the notebook interface, providing a more integrated experience without switching between the browser and the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Set Up the Endpoint for Traces\n",
    "\n",
    "To send traces to the Phoenix app for analysis and observability, define the endpoint URL where the Phoenix app is listening for incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://127.0.0.1:6006/v1/traces\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `endpoint` variable stores the URL of the Phoenix app's endpoint that listens for incoming traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trace Open AI\n",
    "\n",
    "For more integration, [read.](https://docs.arize.com/phoenix/tracing/integrations-tracing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install and Import the OpenAI Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openai`: The Python client library for OpenAI's API. It enables you to make requests to OpenAI's models, including GPT-3 and GPT-4, for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve API key from environment variable or prompt user if not set\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "# Set the API key for the OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Store the API key in environment variables for future use\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieve API Key: The code first attempts to get the API key from an environment variable (OPENAI_API_KEY). If the key is not found, it prompts the user to enter it securely using getpass.\n",
    "\n",
    "- Set API Key: The retrieved or provided API key is then set for the openai client library.\n",
    "\n",
    "- Store API Key: Finally, the API key is stored in the environment variables to ensure it is available for future use within the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set Up OpenTelemetry for Tracing\n",
    "\n",
    "To enable tracing for your OpenAI interactions, configure OpenTelemetry with the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "# Set up the Tracer Provider\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "\n",
    "# Define the OTLP Span Exporter with the endpoint\n",
    "span_exporter = OTLPSpanExporter(endpoint)\n",
    "\n",
    "# Set up the Span Processor to process and export spans\n",
    "span_processor = SimpleSpanProcessor(span_exporter)\n",
    "\n",
    "# Add the Span Processor to the Tracer Provider\n",
    "tracer_provider.add_span_processor(span_processor)\n",
    "\n",
    "# Set the global Tracer Provider\n",
    "trace_api.set_tracer_provider(tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenTelemetry Libraries**\n",
    "\n",
    "In the provided code, several OpenTelemetry libraries are used to set up tracing. Here's an overview of each:\n",
    "\n",
    "- `opentelemetry`:\n",
    "\n",
    "  \\***\\*Purpose\\*\\***: The core library for OpenTelemetry, providing APIs for tracing and metrics.\n",
    "\n",
    "  **Usage**: It includes the trace module, which is used to create and manage traces.\n",
    "\n",
    "- `opentelemetry.exporter.otlp.proto.http.trace_exporter`:\n",
    "\n",
    "  **Purpose**: Provides the OTLP (OpenTelemetry Protocol) exporter for traces using HTTP.\n",
    "\n",
    "  **Usage**: The `OTLPSpanExporter` class in this module sends trace data to an OTLP-compatible backend. This exporter is configured with an endpoint where trace data will be sent.\n",
    "\n",
    "- `opentelemetry.sdk.trace`:\n",
    "\n",
    "  **Purpose**: Contains the SDK implementations for tracing, including the `TracerProvider`.\n",
    "\n",
    "  **Usage**:\n",
    "\n",
    "  - `TracerProvider`: Manages Tracer instances and is responsible for exporting spans (units of work) collected during tracing.\n",
    "\n",
    "  - `SimpleSpanProcessor`: A span processor that exports spans synchronously, used to process and send trace data to the exporter.\n",
    "\n",
    "- `opentelemetry.sdk.trace.export`:\n",
    "\n",
    "  **Purpose**: Provides classes for exporting trace data.\n",
    "\n",
    "  **Usage**:\n",
    "\n",
    "  - `SimpleSpanProcessor`: Processes spans and exports them using the specified exporter. It ensures that spans are sent to the backend for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Instrument OpenAI with OpenInference\n",
    "\n",
    "To integrate OpenTelemetry with OpenAI and enable tracing for OpenAI model interactions, use the `OpenAIInstrumentor` from the `openinference` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "# Instantiate and apply instrumentation for OpenAI\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `OpenAIInstrumentor`: A class from the openinference library designed to instrument OpenAI's API calls, enabling tracing and observability.\n",
    "\n",
    "- `instrument()`: This method configures the OpenAI API client to automatically generate and send trace data to the OpenTelemetry backend. It integrates with the tracing setup you have configured, allowing you to monitor and analyze interactions with OpenAI's models.\n",
    "\n",
    "By running this code, you ensure that all OpenAI API calls are traced, allowing you to capture detailed insights into model usage and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Make a Request to OpenAI API\n",
    "\n",
    "To interact with OpenAI‚Äôs API and obtain a response, use the following code. This example demonstrates how to create a chat completion using the OpenAI API and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client instance\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Make a request to the OpenAI API for a chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n",
    ")\n",
    "\n",
    "# Print the content of the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `openai.OpenAI()`: Initializes an OpenAI client instance that can be used to interact with the OpenAI API.\n",
    "\n",
    "- `client.chat.completions.create()`: Sends a request to the OpenAI API to create a chat completion using the specified model.\n",
    "\n",
    "  - `model=\"gpt-4o\"`: Specifies the model to use for generating completions. Ensure the model name is correct and available in your OpenAI API account.\n",
    "\n",
    "  - `messages`: A list of message objects representing the conversation history. In this case, it includes a single message from the user asking to \"Write a haiku.\"\n",
    "\n",
    "`response.choices[0].message.content`: Extracts and prints the content of the completion response generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trace Llama index\n",
    "\n",
    "### 3.1 Install and Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-core\n",
    "# !pip install llama-index-llms-openai\n",
    "# !pip install openinference-instrumentation-llama-index==2.2.4\n",
    "# !pip install -U llama-index-callbacks-arize-phoenix\n",
    "# !pip install \"arize-phoenix[llama-index]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `llama-index`: Core package for Llama Index functionality.\n",
    "\n",
    "- `llama-index-core`: Provides core features and utilities for Llama Index.\n",
    "\n",
    "- `llama-index-llms-openai`: Integration package for Llama Index and OpenAI models.\n",
    "\n",
    "- `openinference-instrumentation-llama-index==2.2.4`: Provides instrumentation for tracing Llama Index interactions.\n",
    "\n",
    "- `llama-index-callbacks-arize-phoenix`: Callback integration for Arize Phoenix with Llama Index.\n",
    "\n",
    "- `arize-phoenix[llama-index]`: Extends Arize Phoenix to support Llama Index tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Retrieve the URL of the Active Phoenix Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the URL of the active Phoenix session\n",
    "px.active_session().url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accesses the current active session of the Phoenix app and retrieves its URL, allowing you to view or share the link to the Phoenix interface where you can monitor and analyze trace data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set Up Tracing for Llama Index\n",
    "\n",
    "To instrument Llama Index for tracing with OpenTelemetry, configure the tracer provider and integrate the Llama Index instrumentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "# Set up the Tracer Provider\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "\n",
    "# Add Span Processor to the Tracer Provider\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "\n",
    "# Instrument Llama Index with the Tracer Provider\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `LlamaIndexInstrumentor`: This class from openinference.instrumentation.llama_index instruments Llama Index to support tracing and observability.\n",
    "\n",
    "- `trace_sdk.TracerProvider()`: Initializes a new Tracer Provider responsible for creating and managing trace data.\n",
    "  OTLPSpanExporter(endpoint): Configures the OTLP exporter to send trace data to the specified endpoint.\n",
    "- `SimpleSpanProcessor`: Processes and exports spans synchronously.\n",
    "\n",
    "- `tracer_provider.add_span_processor`: Adds the span processor to the Tracer Provider.\n",
    "\n",
    "- `LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)`: Applies the instrumentation to Llama Index, using the provided Tracer Provider for tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Interact with Llama Index Using OpenAI\n",
    "\n",
    "To perform a completion request with Llama Index using an OpenAI model, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Make a completion request\n",
    "resp = llm.complete(\"Paul Graham is \")\n",
    "\n",
    "# Print the response\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `from llama_index.llms.openai import OpenAI`: Imports the OpenAI class from the llama_index package, allowing interaction with OpenAI models.\n",
    "\n",
    "- `OpenAI(model=\"gpt-4o-mini\")`: Initializes an instance of the OpenAI class with the specified model (e.g., gpt-4).\n",
    "\n",
    "- `llm.complete(...)`: Sends a prompt to the model to generate a completion based on the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Perform a Chat Interaction with Llama Index Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Define the chat messages\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "\n",
    "# Get the response from the model\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `OpenAI`: A class for interacting with OpenAI models.\n",
    "\n",
    "- `ChatMessage`: A class to format chat messages.\n",
    "\n",
    "- `OpenAI()`: Initializes an instance of the OpenAI class.\n",
    "\n",
    "- `ChatMessage`: Creates chat message objects with a specified role (e.g., \"system\", \"user\") and content.\n",
    "\n",
    "  - `role=\"system\"`: Defines the system message to set the context or personality of the model.\n",
    "  - `role=\"user\"`: Represents a user message in the chat.\n",
    "\n",
    "- `llm.chat(messages)`: Sends the defined messages to the model and retrieves the response.\n",
    "\n",
    "This code sets up a chat with an OpenAI model, specifying system and user messages to guide the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observe RAG Pipeline\n",
    "\n",
    "### 4.1 Setup an environment for observing a RAG piepline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-vector-stores-qdrant \n",
    "# !pip install llama-index-readers-file \n",
    "# !pip install llama-index-embeddings-fastembed \n",
    "# !pip install llama-index-llms-openai\n",
    "# !pip install -U qdrant_client fastembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `llama-index`: Core package for Llama Index functionality.\n",
    "\n",
    "- `llama-index-vector-stores-qdrant`: Integration for using Qdrant as a vector store with Llama Index.\n",
    "\n",
    "- `llama-index-readers-file`: Provides file reading capabilities for Llama Index.\n",
    "\n",
    "- `llama-index-embeddings-fastembed`: Adds FastEmbed support for generating embeddings with Llama Index.\n",
    "\n",
    "- `llama-index-llms-openai`: Integration for using OpenAI models with Llama Index.\n",
    "\n",
    "- `qdrant_client`: Client library for interacting with Qdrant, a vector search engine.\n",
    "\n",
    "- `fastembed`: Library for generating embeddings quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prepare RAG Pipeline with Embeddings and Document Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "# Settings.embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `VectorStoreIndex`: A class used to create and manage a vector store index. This index allows for efficient similarity search and retrieval based on vector representations of documents.\n",
    "\n",
    "- `SimpleDirectoryReader`: A class for loading documents from a specified directory. It reads and preprocesses files from the directory \"sample_data\" to be used in the indexing process.\n",
    "\n",
    "- `FastEmbedEmbedding`: Provides an embedding model for generating vector representations of text using the FastEmbed library. The model specified (\"BAAI/bge-base-en-v1.5\") is used to convert documents into embeddings, which are then used for similarity search within the vector store index.\n",
    "\n",
    "- `from llama_index.embeddings.openai import OpenAIEmbedding`:\n",
    "\n",
    "  `OpenAIEmbedding`: (Commented out) Provides an embedding model for generating vector representations using OpenAI‚Äôs embeddings. Uncomment this line if you wish to use OpenAI‚Äôs model instead of FastEmbed. This model can be configured with parameters like `embed_batch_size` for batch processing.\n",
    "\n",
    "- `Settings`: A configuration class used to set global settings for embedding models. By assigning the embed_model attribute, you specify which embedding model to use for the RAG pipeline.\n",
    "\n",
    "- `Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")`:\n",
    "  Configures the Settings class to use the FastEmbed model for generating embeddings. This step is crucial for defining how text data will be represented in the vector store.\n",
    "\n",
    "- `documents = SimpleDirectoryReader(\"data\").load_data()`:\n",
    "  Loads and preprocesses documents (in this case) from the \"data\" directory. Please ensure to tweak the directory name according to your project. The `load_data()` method reads all files in the specified directory and prepares them for indexing.\n",
    "\n",
    "- `index = VectorStoreIndex.from_documents(documents)`:\n",
    "  Creates a VectorStoreIndex from the preprocessed documents. This index allows for efficient querying and retrieval based on the vector representations generated by the embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Query the Vector Store Index\n",
    "\n",
    "Once the vector store index is set up, you can use it to perform queries and retrieve relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `as_query_engine()`: Converts the `VectorStoreIndex` into a query engine. This engine allows you to perform searches and retrieve information based on the vector representations of documents stored in the index.\n",
    "\n",
    "- `query()`: Executes a query against the vector store index. The query string \"What did the author do growing up?\" is used to search for relevant documents and retrieve information based on the context provided by the vector embeddings.\n",
    "\n",
    "Finally, the `response` containing the information retrieved from the vector store index, which is based on the query and the indexed documents is output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this guide, we have set up a Retrieval-Augmented Generation (RAG) pipeline using Llama Index and integrated it with various components to observe its functionality. We began by configuring and installing the necessary libraries, including Llama Index, OpenTelemetry, and various embedding models.\n",
    "\n",
    "We then proceeded to:\n",
    "\n",
    "- Initialize and configure the embedding models, using FastEmbed or OpenAI models as needed.\n",
    "- Load and index documents from a directory to prepare the data for querying.\n",
    "- Set up a query engine to perform searches and retrieve relevant information based on the indexed documents.\n",
    "\n",
    "By following these steps, you have successfully prepared a RAG pipeline capable of efficient document retrieval and query processing. This setup enables advanced search and information retrieval capabilities, leveraging the power of vector-based embeddings and indexing.\n",
    "\n",
    "Feel free to experiment with different configurations and queries to further explore the capabilities of the RAG pipeline. For any questions or additional customization, consult the documentation of the libraries used or seek further guidance.\n",
    "\n",
    "If you find this guide helpful, please consider giving us a star on GitHub! ‚≠ê\n",
    "\n",
    "[![GitHub stars](https://img.shields.io/github/stars/adithya-s-k/AI-Engineering.academy.svg?style=social&label=Star&maxAge=2482000)](https://github.com/adithya-s-k/AI-Engineering.academy)\n",
    "\n",
    "Thank you for following this guide, and happy querying!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
