{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI Engineering Academy","text":"<p>Welcome to AI Engineering Academy</p>"},{"location":"AIBreakDown/BACKUP/","title":"Tiny Recursive Models (TRM): When 7M Parameters Beat 671B","text":""},{"location":"AIBreakDown/BACKUP/#tiny-recursive-models-trm-when-7m-parameters-beat-671b","title":"Tiny Recursive Models (TRM): When 7M Parameters Beat 671B","text":"<p>When a tiny 7 million parameter model decisively beats massive 671 billion parameter models at reasoning tasks, it's time to rethink everything we thought we knew about AI scaling. Welcome to the world of Tiny Recursive Models (TRM), where less truly is more.</p> <p>The results speak for themselves: On Sudoku-Extreme, TRM achieves 87.4% accuracy while GPT-4, Claude 3.7, and DeepSeek R1 all score 0%. That's not a typo. Zero percent. On ARC-AGI-2, the newest and hardest abstract reasoning benchmark, TRM's 7.8% accuracy beats Gemini 2.5 Pro's 4.9% and O3-mini's 3.0%, while using less than 0.01% of the parameters.</p> <p>This isn't just an incremental improvement. It's a paradigm shift in how we think about building AI systems for reasoning tasks.</p>"},{"location":"AIBreakDown/BACKUP/#1-introduction-the-paradigm-shift","title":"1. Introduction: The Paradigm Shift","text":"<p>For years, we've been in an arms race for bigger models: - 2018: BERT (110M parameters) - 2019: GPT-2 (1.5B parameters) - 2020: GPT-3 (175B parameters) - 2023: GPT-4 (rumored 1.7T parameters)</p> <p>The assumption was simple: more parameters equals better reasoning. Throw enough compute at a problem, and you'll solve it.</p> <p>TRM challenges this assumption fundamentally. With just 7 million parameters, a model smaller than most image classifiers from 2015, it outperforms frontier language models on systematic reasoning tasks. The secret? Iteration, not just scale.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing","title":"What You're Seeing","text":"<p>The opening visualization introduces our journey through TRM's architecture using Sudoku as a teaching framework. Sudoku is perfect for understanding TRM because: - It requires both local constraints (cell-level rules) and global reasoning (grid-level consistency) - Solutions emerge through iterative refinement, not single-pass generation - Success is binary and verifiable (no ambiguity in correctness)</p> <p>These same properties appear in many reasoning tasks: mathematical problem-solving, logical deduction, constraint satisfaction, and even code generation.</p>"},{"location":"AIBreakDown/BACKUP/#why-this-matters-for-you","title":"Why This Matters for You","text":"<p>For Practitioners: You can now deploy powerful reasoning models on edge devices, mobile phones, and constrained environments. A 7M parameter model fits comfortably in less than 30MB of memory.</p> <p>For Researchers: TRM demonstrates that architectural innovation can be more important than raw scale. The techniques here generalize to other domains where data is scarce but reasoning is crucial.</p> <p>For the Field: This challenges the \"bigger is better\" narrative and suggests we've been leaving significant efficiency gains on the table by focusing primarily on scaling.</p>"},{"location":"AIBreakDown/BACKUP/#2-the-problem-why-llms-fail-at-systematic-reasoning","title":"2. The Problem: Why LLMs Fail at Systematic Reasoning","text":"<p>Let's start with a concrete example. Here's a hard Sudoku puzzle:</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_1","title":"What You're Seeing","text":"<p>This scene shows how a Sudoku puzzle maps to a machine learning problem. You're seeing: - A 9x9 grid with initial clues (given numbers) - Empty cells that must be filled (the reasoning challenge) - Constraints that must be satisfied simultaneously (row, column, box uniqueness) - The grid representation that neural networks process</p> <p>The visualization demonstrates tokenization, where each cell becomes a token (like words in language), and positional encoding, where grid location matters (like word order in sentences).</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive","title":"Technical Deep Dive","text":"<p>Why Traditional LLMs Struggle:</p> <p>Large language models like GPT-4 or Claude generate answers autoregressively, token by token, in a single forward pass. For a Sudoku puzzle, this means:</p> <ol> <li>No Error Correction: Once a token is generated, it's used for all subsequent predictions. A single error cascades through the entire solution.</li> <li>No Iterative Refinement: Humans solve Sudoku by making tentative guesses, checking constraints, and revising. LLMs don't naturally do this.</li> <li>Working Memory Limitations: Even with chain-of-thought prompting, LLMs must hold all intermediate reasoning in the context window, competing with the input puzzle for limited space.</li> </ol> <p>Here's the empirical evidence from hard Sudoku puzzles:</p> Model Parameters Accuracy DeepSeek R1 671B 0.0% Claude 3.7 8K Unknown 0.0% O3-mini-high Unknown 0.0% TRM-MLP 5M 87.4% <p>Input Representation:</p> <p>When we feed a Sudoku puzzle to a neural network, we need to represent it numerically:</p> <pre><code># Each cell is represented as an integer (0-9, plus special tokens)\n# For a 9x9 Sudoku:\ngrid = [\n    [5, 3, 0, 0, 7, 0, 0, 0, 0],  # Row 1: 0 means empty\n    [6, 0, 0, 1, 9, 5, 0, 0, 0],  # Row 2\n    # ... more rows\n]\n\n# Flattened to sequence: [5, 3, 0, 0, 7, 0, ...]\n# Length: 81 tokens for full 9x9 grid\n# Vocabulary size: 12 (0=pad, 1=EOS, 2-11=digits 0-9)\n</code></pre> <p>The grid is flattened into a sequence and each position becomes a token. For variable-sized grids (up to 30x30 for ARC-AGI), we use padding and special end-of-sequence tokens.</p> <p>Problem Complexity:</p> <p>Sudoku is NP-complete. The model must learn: - Row constraints: Each of 9 rows must contain digits 1-9 exactly once - Column constraints: Each of 9 columns must contain digits 1-9 exactly once - Box constraints: Each of 9 3x3 sub-grids must contain digits 1-9 exactly once - Logical deduction: Advanced techniques like naked pairs, hidden triples, X-wings</p> <p>This requires global reasoning. Changing one cell can invalidate choices made 20 steps earlier. Single-pass autoregressive generation simply cannot handle this effectively.</p> Key Concepts <ul> <li>Tokenization: Each cell is a discrete token (like words in NLP)</li> <li>Positional Information: Grid location is critical (encoded like position in a sentence)</li> <li>Hard Constraints: Rules that MUST be satisfied (unlike soft preferences in language)</li> <li>NP-Complete: No known polynomial-time algorithm; requires search/reasoning</li> </ul>"},{"location":"AIBreakDown/BACKUP/#3-from-hrm-to-trm-the-evolution","title":"3. From HRM to TRM: The Evolution","text":"<p>Before diving into TRM's architecture, we need to understand its predecessor: Hierarchical Reasoning Models (HRM).</p> <p>HRM was itself a breakthrough. Published earlier in 2024, it introduced two key ideas: 1. Recursive Reasoning: Process information through the same network multiple times, rather than a single forward pass 2. Deep Supervision: Provide training signal at multiple intermediate steps, not just the final output</p> <p>HRM achieved impressive results: 55% on Sudoku-Extreme (vs 0% for LLMs), 40.3% on ARC-AGI-1. But the architecture was complex: - Two separate networks (f_L and f_H) operating at different \"hierarchical frequencies\" - Theoretical justification based on neuroscience (brain oscillations, hierarchical processing) - Complex training with Q-learning for adaptive computation - Heavy mathematical dependencies (Implicit Function Theorem, fixed-point iterations)</p>"},{"location":"AIBreakDown/BACKUP/#the-trm-insight","title":"The TRM Insight","text":"<p>The TRM paper asked a simple question: What if most of HRM's complexity is unnecessary?</p> <p>The analysis revealed that HRM's gains came primarily from deep supervision (iterating on predictions), not from the hierarchical dual-network design. This led to radical simplification:</p> <p>HRM: Two networks (27M params) \u2192 TRM: One network (7M params) HRM: Complex fixed-point theory \u2192 TRM: Straightforward backpropagation HRM: 4-layer networks \u2192 TRM: 2-layer networks (surprisingly better!) HRM: Dual latent states \u2192 TRM: Clear separation (answer + reasoning)</p> <p>The result? Better performance with 75% fewer parameters and conceptually simpler architecture.</p>"},{"location":"AIBreakDown/BACKUP/#4-data-flow-how-information-moves-through-trm","title":"4. Data Flow: How Information Moves Through TRM","text":"<p>Before diving into the three-stream architecture, let's understand the overall data flow.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_2","title":"What You're Seeing","text":"<p>This visualization shows how information flows through TRM: - Input encoding: Sudoku grid \u2192 embedded vectors (dimension 256) - Transformer processing: Recursive passes through the same small network - Output decoding: Embedded vectors \u2192 predicted solution grid</p> <p>Notice how the same network (same weights) is applied multiple times. This is weight sharing, the key to TRM's parameter efficiency.</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_1","title":"Technical Deep Dive","text":"<p>The Complete Pipeline:</p> <pre><code># Simplified TRM forward pass\ndef trm_forward(question_grid, answer_grid, latent_reasoning):\n    # Phase 1: Embed inputs into vector space\n    x = embed_question(question_grid)        # [batch, 81, 256]\n    y = embed_answer(answer_grid)            # [batch, 81, 256]\n    z = initialize_latent(latent_reasoning)  # [batch, 32, 256]\n\n    # Phase 2: Recursive reasoning (8 steps)\n    # Only update z (reasoning), keep x (question) and y (answer) fixed\n    for step in range(8):\n        combined = concatenate([x, y, z])    # [batch, 194, 256]\n        output = transformer_block(combined)  # Same weights every time!\n        _, _, z = split(output, [81, 81, 32])\n\n    # Phase 3: Refinement (16 steps)\n    # Now update y (answer) using the refined z (reasoning)\n    for step in range(16):\n        combined = concatenate([x, y, z])\n        output = transformer_block(combined)  # Still same weights!\n        _, y, _ = split(output, [81, 81, 32])\n\n    # Phase 4: Decode to predictions\n    predictions = reverse_embed(y)  # [batch, 81, 12] logits\n    return predictions\n</code></pre> <p>Key Innovation: Weight Sharing</p> <p>Unlike a traditional transformer with different weights for each layer, TRM uses the same weights repeatedly. This: - Dramatically reduces parameters (2-layer network used 24 times = 48 effective layers) - Forces information compression (all reasoning must fit in the shared representation) - Acts as strong regularization (prevents overfitting on small datasets)</p> <p>The Three Phases:</p> <ol> <li>Embedding: Transform discrete tokens (0-11) into continuous vectors (256-dim)</li> <li>Recursive Processing: Apply the same transformer 24 times with different update patterns</li> <li>Decoding: Transform vectors back to discrete predictions</li> </ol> <p>This is fundamentally different from standard transformers, which have unique weights for each layer.</p> Architecture Details <ul> <li>Embedding dimension: 256 (vs 512-2048 in large LLMs)</li> <li>Transformer layers: 2 (vs 96 in GPT-4)</li> <li>Recursions: 8 + 16 = 24 effective passes</li> <li>Total effective depth: 2 layers \u00d7 24 recursions = 48 layers</li> <li>Parameters: ~7M total (vs 175B in GPT-3)</li> </ul>"},{"location":"AIBreakDown/BACKUP/#5-recurrence-vs-transformers-understanding-the-difference","title":"5. Recurrence vs Transformers: Understanding the Difference","text":"<p>To appreciate TRM's design, we need to understand what came before it.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_3","title":"What You're Seeing","text":"<p>Side-by-side comparison: - Left: Recurrent processing (RNN/LSTM) - sequential, cell by cell - Right: Transformer processing - parallel across all cells</p> <p>Traditional recurrent networks process information sequentially, maintaining a hidden state. Transformers process everything in parallel using attention.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_2","title":"Technical Deep Dive","text":"<p>Recurrent Neural Networks (RNNs):</p> <pre><code># RNN processes sequentially\nhidden = initial_state\nfor cell in grid:  # Must go one by one\n    hidden = rnn_cell(cell, hidden)\n    predictions.append(decode(hidden))\n</code></pre> <p>Problems with RNNs: - Sequential bottleneck: Can't parallelize across the grid - Vanishing gradients: Information from early cells gets diluted - Limited context: Hidden state size is fixed, limiting working memory - Training time: O(n) sequential steps, very slow on modern hardware</p> <p>Transformers:</p> <pre><code># Transformer processes in parallel\nembedded_grid = embed(grid)  # All cells at once\nattended = self_attention(embedded_grid)  # All-to-all connections\npredictions = decode(attended)  # Parallel predictions\n</code></pre> <p>Advantages of Transformers: - Parallelization: Process all 81 Sudoku cells simultaneously on GPU - Direct connections: Every cell can attend to every other cell (no gradient vanishing) - Flexible context: Attention weights dynamically determine what's relevant - Training speed: O(1) sequential operations (10-100x faster than RNNs)</p> <p>Performance Comparison:</p> Aspect RNN/LSTM Transformer TRM Parallelization Sequential only Fully parallel Fully parallel Context Window Fixed hidden size Full grid access Full grid + latent Training Speed Slow Fast Fast Memory O(n) O(n\u00b2) O(n\u00b2) but n is small Parameters Few Many Very few (reused) Gradient Flow Vanishing Direct paths Direct paths <p>Why TRM Uses Transformers:</p> <p>For reasoning tasks with rich interactions (like Sudoku constraints), transformers excel because: 1. Every cell needs information from many other cells simultaneously 2. The importance of different cells changes dynamically as the puzzle is solved 3. Parallel processing allows efficient training even with recursive application</p> Why This Matters <ul> <li>RNNs were the standard for sequences before 2017 but couldn't handle long-range dependencies</li> <li>Transformers revolutionized NLP by enabling parallel processing and better scaling</li> <li>TRM shows transformers work even better when recursively applied on small networks</li> </ul>"},{"location":"AIBreakDown/BACKUP/#6-the-three-streams-trms-core-architecture","title":"6. The Three Streams: TRM's Core Architecture","text":"<p>Here's where TRM gets really interesting. Instead of just processing input \u2192 output, TRM maintains three separate information streams.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_4","title":"What You're Seeing","text":"<p>This visualization shows TRM's internal representations: - Question stream (x): The input puzzle, fixed throughout processing - Answer stream (y): The working solution, progressively refined - Reasoning stream (z): Intermediate thoughts and constraints, helps improve y</p> <p>Think of it like working on a puzzle at your desk with three Post-it notes:</p> <ol> <li>Note 1 (x-stream): \"Original puzzle clues\" - you keep referring back to it but never change it</li> <li>Note 2 (y-stream): \"Current solution attempt\" - starts rough, gets better with each revision</li> <li>Note 3 (z-stream): \"Scratch work and logic\" - temporary deductions that help you improve the solution</li> </ol>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_3","title":"Technical Deep Dive","text":"<p>Why Three Streams?</p> <p>HRM called these \"hierarchical latent states\" with complex biological justification. TRM has a simpler explanation:</p> <p>x-stream (Question): This is your problem statement. It never changes during solving. The model can always look back at the original clues.</p> <pre><code># Question stream - fixed throughout\nx = embed_question([5, 3, 0, 0, 7, ...])  # Original puzzle\n# Shape: [batch_size, 81, 256]\n# Never updated during recursive processing\n</code></pre> <p>y-stream (Answer): This is your working solution. It starts as a guess (often initialized randomly or as a copy of input) and gets iteratively refined.</p> <pre><code># Answer stream - refined in Phase 2\ny = embed_initial_answer([0, 0, 0, ...])  # Start empty or random\n# Shape: [batch_size, 81, 256]\n# Updated during refinement phase to improve predictions\n</code></pre> <p>z-stream (Reasoning): This is your scratch space for intermediate logic. It doesn't directly correspond to a solution but helps compute one.</p> <pre><code># Reasoning stream - refined in Phase 1\nz = torch.randn(batch_size, 32, 256) * 0.02  # Start random\n# Shape: [batch_size, 32, 256]\n# 32 is arbitrary \"working memory\" size\n# Updated during reasoning phase to build understanding\n</code></pre> <p>The Interaction:</p> <p>All three streams are concatenated and processed together:</p> <pre><code>def forward_pass(x, y, z):\n    # Concatenate the three streams\n    combined = torch.cat([x, y, z], dim=1)  # [batch, 194, 256]\n    #                     81 + 81 + 32 = 194 tokens\n\n    # Process through transformer\n    # Every token can attend to every other token across all streams!\n    attended = multi_head_attention(combined)\n    processed = feed_forward(attended)\n\n    # Split back into three streams\n    x_new = processed[:, :81, :]      # Question (won't use update)\n    y_new = processed[:, 81:162, :]   # Answer (may use update)\n    z_new = processed[:, 162:, :]     # Reasoning (may use update)\n\n    return x_new, y_new, z_new\n</code></pre> <p>Why This Works:</p> <ol> <li>Cross-stream attention: The answer stream can look at both the question and the reasoning</li> <li>Separation of concerns: Reasoning logic (z) is separated from actual predictions (y)</li> <li>Memory efficiency: Only 32 reasoning tokens vs 81 answer tokens (less to store in working memory)</li> <li>Iterative refinement: Improve reasoning first, then use it to improve the answer</li> </ol> <p>The Update Schedule:</p> Phase Steps Updates Purpose Reasoning 8 z only Build understanding of the problem Refinement 16 y only Improve answer using understanding <p>This separation is crucial. You don't try to improve your answer until you've thought about the problem. Just like a human would:</p> <ol> <li>Read the puzzle (x)</li> <li>Think about constraints and deductions (update z 8 times)</li> <li>Fill in answers based on your reasoning (update y 16 times)</li> </ol> Why Not More Streams? <p>The paper tested this: - 1 stream: 71.9% accuracy - forces model to mix answer and reasoning - 2 streams (y + z): 87.4% accuracy - optimal separation - 7 streams (multi-scale z): 77.6% accuracy - unnecessary complexity</p> <p>Two streams (answer + reasoning) is the sweet spot.</p>"},{"location":"AIBreakDown/BACKUP/#7-recursive-improvement-the-24-pass-process","title":"7. Recursive Improvement: The 24-Pass Process","text":"<p>Now we get to the heart of TRM: how it uses recursive processing to progressively improve predictions.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_5","title":"What You're Seeing","text":"<p>This visualization shows TRM making multiple passes through the puzzle: - Passes 1-8: Building reasoning in the z-stream (understanding constraints) - Passes 9-24: Refining the answer in the y-stream (improving predictions) - Notice how difficult cells require more iterations, while easy cells stabilize quickly</p> <p>Each pass uses the same transformer weights, but the information in the streams evolves.</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_4","title":"Technical Deep Dive","text":"<p>Phase 1: Reasoning (8 iterations)</p> <pre><code># Phase 1: Build understanding\n# Update z-stream only, keep x and y fixed\nfor step in range(8):\n    x_new, y_new, z_new = forward_pass(x, y, z)\n\n    # Only keep updated z (reasoning)\n    z = z_new\n    # Discard x_new and y_new (don't update question or answer yet)\n\n    # What's happening in z:\n    # - Identifying constraint relationships\n    # - Building logical deduction chains\n    # - Preparing to update the answer\n</code></pre> <p>After 8 steps, z contains rich information about the problem structure, but y (the answer) hasn't changed yet.</p> <p>Phase 2: Refinement (16 iterations)</p> <pre><code># Phase 2: Improve answer\n# Update y-stream only, keep x and z fixed\nfor step in range(16):\n    x_new, y_new, z_new = forward_pass(x, y, z)\n\n    # Only keep updated y (answer)\n    y = y_new\n    # Discard x_new and z_new (question and reasoning are done)\n\n    # What's happening in y:\n    # - Filling in cells based on reasoning in z\n    # - Correcting mistakes from previous iterations\n    # - Increasing confidence in predictions\n</code></pre> <p>After 16 steps, y should contain a high-quality solution, informed by the reasoning in z.</p> <p>Why This Schedule?</p> <p>You might ask: why 8 reasoning steps and 16 refinement steps? The paper tested various combinations:</p> Configuration Reasoning Steps Refinement Steps Effective Depth Accuracy Small 2 4 12 73.7% Medium 6 10 32 84.2% Optimal 8 16 48 87.4% Large 12 24 72 85.8% <p>The optimal point balances: - Enough reasoning to build good understanding (8 steps) - Enough refinement to improve answers fully (16 steps) - Not too much that you overfit or waste compute (diminishing returns after 24 total)</p> <p>Complete Training Loop with Deep Supervision:</p> <p>Here's where it gets really interesting. TRM doesn't just apply these 24 passes once. It applies them up to 16 times during training, with supervision at each round:</p> <pre><code>def train_step(question_grid, true_answer_grid):\n    x = embed_question(question_grid)\n    y = embed_answer(question_grid)  # Start with question as initial guess\n    z = torch.randn(batch_size, 32, 256) * 0.02\n\n    total_loss = 0\n\n    # Deep supervision: train on multiple improvement cycles\n    for supervision_step in range(16):  # Up to 16 supervision steps\n        # Phase 1: Reasoning (8 steps)\n        for i in range(8):\n            x_new, y_new, z_new = forward_pass(x, y, z)\n            z = z_new\n\n        # Phase 2: Refinement (16 steps)\n        for i in range(16):\n            x_new, y_new, z_new = forward_pass(x, y, z)\n            y = y_new\n\n        # Compute loss after this improvement cycle\n        predictions = reverse_embed(y)\n        loss = cross_entropy(predictions, true_answer_grid)\n        total_loss += loss\n\n        # Detach to prevent backprop through all previous cycles\n        # Only backprop through the last cycle\n        y = y.detach()\n        z = z.detach()\n\n    # Backpropagate\n    total_loss.backward()\n    optimizer.step()\n</code></pre> <p>This gives TRM: - Effective depth: 2 layers \u00d7 24 recursions \u00d7 16 supervision = 768 effective layers! - Multiple chances: If the model doesn't get it right in one cycle, it has 15 more tries - Progressive improvement: Each supervision step sees the model's own previous attempt</p> <p>Convergence Patterns:</p> <p>From experiments: - Easy cells: Converge in ~2-4 passes (single digit possibility) - Medium cells: Converge in ~8-12 passes (simple logical deduction) - Hard cells: Need all 24 passes (complex constraint propagation)</p> Comparison to Human Solving <p>Humans solve Sudoku similarly: 1. Scan phase: Look for obvious cells (like z-stream reasoning) 2. Fill phase: Write in answers based on deductions (like y-stream refinement) 3. Iterate: If stuck, re-examine and refine (like multiple supervision cycles)</p> <p>TRM learns to mimic this without being explicitly programmed to do so!</p>"},{"location":"AIBreakDown/BACKUP/#8-transformer-architecture-the-building-blocks","title":"8. Transformer Architecture: The Building Blocks","text":"<p>Now let's look at what's inside those transformer blocks that get applied recursively.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_6","title":"What You're Seeing","text":"<p>Detailed visualization of: - Attention weights between cells (which cells influence which) - Multi-head attention (different heads specialize in different constraint types) - Information flow through layers</p> <p>Notice how attention patterns change across passes as the model refines its understanding.</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_5","title":"Technical Deep Dive","text":"<p>Multi-Head Self-Attention:</p> <p>This is the core of how transformers work. For each token, we compute:</p> <pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention allows the model to attend to different aspects\n    of the input simultaneously.\n\n    For Sudoku:\n    - Head 1 might focus on row constraints\n    - Head 2 might focus on column constraints\n    - Head 3 might focus on box constraints\n    - Head 4 might focus on more complex patterns\n    \"\"\"\n\n    def __init__(self, d_model=256, n_heads=4, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n\n        self.d_model = d_model      # 256\n        self.n_heads = n_heads      # 4\n        self.d_k = d_model // n_heads  # 64 per head\n\n        # Linear projections for Q, K, V\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n\n        # Output projection\n        self.out_proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # Step 1: Project to Q, K, V\n        q = self.q_proj(x)  # [batch, seq_len, 256]\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        # Step 2: Split into multiple heads\n        # Reshape to [batch, n_heads, seq_len, d_k]\n        q = q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        k = k.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Step 3: Compute attention scores\n        # scores[i,j] = how much should token i attend to token j?\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        # Shape: [batch, n_heads, seq_len, seq_len]\n\n        # Why divide by sqrt(d_k)?\n        # Without scaling, dot products can get very large\n        # \u2192 softmax becomes peaked \u2192 gradients vanish\n        # Scaling keeps values in a nice range for softmax\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Step 4: Apply softmax to get attention weights\n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Step 5: Apply attention to values\n        attn_output = torch.matmul(attn_weights, v)\n        # Shape: [batch, n_heads, seq_len, d_k]\n\n        # Step 6: Concatenate heads and project\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, seq_len, d_model)\n        output = self.out_proj(attn_output)\n\n        return output\n</code></pre> <p>Feed-Forward Network:</p> <p>After attention, each token is processed independently through a feed-forward network:</p> <pre><code>class FeedForward(nn.Module):\n    \"\"\"\n    Two-layer MLP with GELU activation.\n\n    Typical expansion factor is 4x:\n    256 \u2192 1024 \u2192 256\n\n    This adds non-linearity and allows the model to process\n    the attended information.\n    \"\"\"\n\n    def __init__(self, d_model=256, d_ff=1024, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Expand \u2192 Activate \u2192 Compress\n        return self.linear2(self.dropout(torch.nn.functional.gelu(self.linear1(x))))\n</code></pre> <p>Complete Transformer Block:</p> <pre><code>class TransformerBlock(nn.Module):\n    \"\"\"\n    A single transformer block combining:\n    1. Multi-head attention (tokens talk to each other)\n    2. Add &amp; Normalize (residual connection)\n    3. Feed-forward (process information)\n    4. Add &amp; Normalize (another residual)\n\n    TRM uses 2 of these blocks sequentially, then recurses.\n    \"\"\"\n\n    def __init__(self, d_model=256, n_heads=4, d_ff=1024, dropout=0.1, use_attention=True):\n        super().__init__()\n        self.use_attention = use_attention\n\n        if use_attention:\n            self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n            self.norm1 = nn.LayerNorm(d_model)\n\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Block 1: Self-attention with residual\n        if self.use_attention:\n            residual = x\n            x = self.attention(x, mask)\n            x = self.norm1(residual + self.dropout(x))\n\n        # Block 2: Feed-forward with residual\n        residual = x\n        x = self.ffn(x)\n        x = self.norm2(residual + self.dropout(x))\n\n        return x\n</code></pre> <p>Attention Patterns for Sudoku:</p> <p>Different attention heads learn different constraint types:</p> Head Focus Example Pattern 1 Row constraints Cell (4,5) attends strongly to all cells in row 4 2 Column constraints Cell (4,5) attends strongly to all cells in column 5 3 Box constraints Cell (4,5) attends strongly to cells in its 3x3 box 4 Complex logic Pairs, triples, and other Sudoku techniques <p>This emerges from training without explicit programming!</p> MLP-Only Variant (TRM-MLP) <p>Interestingly, for Sudoku, you can replace attention with a simple MLP:</p> <pre><code>class MLPMixer(nn.Module):\n    \"\"\"\n    For fixed, small sequence lengths, an MLP can work better than attention.\n    TRM-MLP achieves 87.4% on Sudoku vs 74.7% for TRM-Att.\n    \"\"\"\n\n    def __init__(self, seq_len=194, d_model=256):\n        super().__init__()\n        self.norm = nn.LayerNorm(d_model)\n        # Mix across sequence dimension\n        self.mix = nn.Linear(seq_len, seq_len)\n\n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        residual = x\n        x = self.norm(x)\n        x = x.transpose(1, 2)  # [batch, d_model, seq_len]\n        x = self.mix(x)\n        x = x.transpose(1, 2)  # [batch, seq_len, d_model]\n        return residual + x\n</code></pre> <p>For larger, variable-length tasks like ARC-AGI, attention works better.</p>"},{"location":"AIBreakDown/BACKUP/#9-deep-supervision-training-at-multiple-scales","title":"9. Deep Supervision: Training at Multiple Scales","text":"<p>One of TRM's key innovations is deep supervision, which provides training signal at multiple points during the recursive process.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_7","title":"What You're Seeing","text":"<p>Visualization of training signals at different depths: - Loss is computed after each supervision cycle (not just at the end) - Earlier cycles receive weaker signal (still learning basics) - Later cycles receive stronger signal (refining final answer) - Gradients flow through the network at multiple points</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_6","title":"Technical Deep Dive","text":"<p>Traditional Training:</p> <pre><code># Standard approach: loss only at the end\nx, y, z = embed_inputs(question, initial_answer)\nfor i in range(24):  # All 24 recursive passes\n    x, y, z = transformer_block(x, y, z)\npredictions = decode(y)\nloss = cross_entropy(predictions, ground_truth)\nloss.backward()  # Backprop through all 24 passes\n</code></pre> <p>Problem: Gradients must flow through 24 applications of the same network. Even with good architecture, this can lead to vanishing gradients.</p> <p>Deep Supervision Approach:</p> <pre><code># TRM approach: supervision at multiple points\ndef train_with_deep_supervision(question, ground_truth, n_sup=16):\n    x = embed_question(question)\n    y = embed_answer(question)  # Start with input as initial guess\n    z = torch.randn_like(latent_size)\n\n    total_loss = 0\n\n    for sup_step in range(n_sup):  # 16 supervision cycles\n        # Do one complete reasoning + refinement cycle\n        # Phase 1: Reasoning (8 steps)\n        for i in range(8):\n            with torch.no_grad() if sup_step &lt; n_sup - 1 else nullcontext():\n                x, y, z_new = forward_pass(x, y, z)\n                z = z_new\n\n        # Phase 2: Refinement (16 steps)\n        for i in range(16):\n            with torch.no_grad() if sup_step &lt; n_sup - 1 else nullcontext():\n                x, y_new, z = forward_pass(x, y, z)\n                y = y_new\n\n        # Compute loss at this supervision step\n        predictions = decode(y)\n        loss = cross_entropy(predictions, ground_truth)\n\n        # Weight early supervision less than later supervision\n        weight = (sup_step + 1) / n_sup  # 0.0625, 0.125, ..., 1.0\n        total_loss += weight * loss\n\n        # Detach for next cycle (prevent backprop through all history)\n        y = y.detach()\n        z = z.detach()\n\n    # Backpropagate total weighted loss\n    total_loss.backward()\n    optimizer.step()\n</code></pre> <p>Benefits of Deep Supervision:</p> <ol> <li>Better Gradient Flow: Each supervision cycle provides direct signal, preventing vanishing gradients</li> <li>Curriculum Learning: Early cycles learn basic patterns, later cycles learn refinement</li> <li>Regularization: Training on intermediate predictions prevents overfitting to final answers</li> <li>Interpretability: Can inspect what the model predicts at each stage</li> </ol> <p>Effective Depth:</p> <p>With deep supervision, TRM achieves: - 2 layers per block \u00d7 24 recursive applications \u00d7 16 supervision cycles = 768 effective layers</p> <p>This is deeper than GPT-4 (estimated ~96-128 layers) while using 0.001% of the parameters!</p> <p>Weighting Strategy:</p> <p>The paper uses increasing weights for later supervision steps:</p> Supervision Step Weight Rationale 1-4 0.0625-0.25 Still learning basic reasoning 5-8 0.3125-0.5 Starting to make good predictions 9-12 0.5625-0.75 Refinement phase 13-16 0.8125-1.0 Final answer quality <p>This ensures the model focuses most on getting the final answer right, while still learning from intermediate attempts.</p> Deep Supervision in Practice <ul> <li>Used in: ResNet, DenseNet, Vision Transformers, many modern architectures</li> <li>Particularly effective for: Very deep networks, small datasets, complex reasoning</li> <li>Can be removed at inference: Only need final prediction, not intermediate ones</li> <li>Alternative names: Intermediate supervision, auxiliary losses, multi-scale training</li> </ul>"},{"location":"AIBreakDown/BACKUP/#10-adaptive-computation-time-act-computing-smarter-not-harder","title":"10. Adaptive Computation Time (ACT): Computing Smarter, Not Harder","text":"<p>Not all problems require the same amount of thinking. TRM uses Adaptive Computation Time to dynamically adjust processing.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_8","title":"What You're Seeing","text":"<p>The model adapting its computation: - Easy cells (only one valid option): Halt after 2-3 supervision cycles - Medium cells (simple deduction): Use 8-12 cycles - Hard cells (complex constraints): Use all 16 cycles</p> <p>Color intensity shows which cells required more computation.</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_7","title":"Technical Deep Dive","text":"<p>The Problem:</p> <p>Training with 16 supervision cycles on every example is expensive. Many examples are easy and don't need all 16 cycles. Can we train on easy examples faster without losing quality on hard ones?</p> <p>HRM's Solution (Complex):</p> <p>HRM used Q-learning with two losses: 1. Halting loss: Learn when to stop 2. Continue loss: Learn benefit of continuing</p> <p>This required two forward passes per training step (one for each loss), doubling compute.</p> <p>TRM's Solution (Simpler):</p> <p>Just predict whether the current answer is correct:</p> <pre><code>class TRMWithACT(nn.Module):\n    def __init__(self, d_model=256):\n        super().__init__()\n        self.trm = TinyRecursiveModel(d_model)\n        # Single halting predictor\n        self.halt_head = nn.Linear(d_model, 1)\n\n    def forward(self, x, y, z, ground_truth=None):\n        # Normal TRM forward pass\n        predictions = self.trm(x, y, z)\n\n        # Predict: \"Is this answer correct?\"\n        # Average pool the answer stream\n        y_pooled = y.mean(dim=1)  # [batch, d_model]\n        halt_logit = self.halt_head(y_pooled)  # [batch, 1]\n\n        if ground_truth is not None:\n            # Training: learn to predict correctness\n            is_correct = (predictions.argmax(dim=-1) == ground_truth).float().mean()\n            halt_loss = nn.functional.binary_cross_entropy_with_logits(\n                halt_logit, is_correct.unsqueeze(0)\n            )\n            return predictions, halt_loss\n        else:\n            # Inference: use prediction to decide whether to halt\n            should_halt = torch.sigmoid(halt_logit) &gt; 0.5\n            return predictions, should_halt\n</code></pre> <p>Training with ACT:</p> <pre><code>def train_with_act(question, ground_truth, max_sup=16):\n    x, y, z = initialize_streams(question)\n    total_loss = 0\n\n    for sup_step in range(max_sup):\n        # Do one reasoning + refinement cycle\n        y, z = trm_cycle(x, y, z)\n\n        # Compute loss and halting prediction\n        predictions, halt_loss = model(x, y, z, ground_truth)\n        prediction_loss = cross_entropy(predictions, ground_truth)\n\n        total_loss += prediction_loss + halt_loss\n\n        # Check if we should halt (during training, to compute statistics)\n        should_halt = torch.sigmoid(model.halt_head(y.mean(dim=1))) &gt; 0.5\n\n        if should_halt and sup_step &gt;= 2:  # Minimum 2 cycles\n            break\n\n        y = y.detach()\n        z = z.detach()\n\n    return total_loss / (sup_step + 1)  # Average loss per cycle used\n</code></pre> <p>Benefits:</p> <ol> <li>Single Forward Pass: Unlike HRM's Q-learning (2 passes), TRM uses 1</li> <li>Simpler: Just binary classification (correct vs incorrect)</li> <li>Effective: On average, training uses ~6 supervision cycles instead of 16</li> <li>No Performance Loss: Test accuracy is the same (we still use all 16 at inference)</li> </ol> <p>Halting Statistics (Sudoku-Extreme):</p> Puzzle Difficulty Average Cycles Used Time Saved Easy 2.3 85% Medium 5.8 64% Hard 11.2 30% Extreme 15.1 6% <p>Comparison to HRM:</p> Aspect HRM ACT TRM ACT Improvement Forward passes 2 per step 1 per step 2x faster Loss functions 2 (halt + continue) 1 (halt only) Simpler Training time 100% 50% 2x faster Test accuracy 55% 87.4% 59% better When to Use ACT <p>Use ACT when: - Training data has mixed difficulty - Want faster training - Have compute budget constraints</p> <p>Don't use ACT when: - All examples are similar difficulty - Training time is not a concern - Want simplest possible implementation</p>"},{"location":"AIBreakDown/BACKUP/#11-the-complete-solving-process-end-to-end","title":"11. The Complete Solving Process: End-to-End","text":"<p>Let's see how all the pieces come together to solve a Sudoku puzzle.</p> <p></p>"},{"location":"AIBreakDown/BACKUP/#what-youre-seeing_9","title":"What You're Seeing","text":"<p>Complete end-to-end visualization: - Input: Partial Sudoku grid with clues - Processing: 24 recursive passes (8 reasoning + 16 refinement) - Attention: Patterns evolving as understanding builds - Predictions: Confidence increasing, errors being corrected - Output: Complete solved grid</p>"},{"location":"AIBreakDown/BACKUP/#technical-deep-dive_8","title":"Technical Deep Dive","text":"<p>Step-by-Step Execution:</p> <pre><code>def solve_sudoku(puzzle_grid):\n    \"\"\"\n    Complete inference pipeline for solving a Sudoku puzzle.\n\n    Args:\n        puzzle_grid: [9, 9] numpy array with 0 for empty cells\n\n    Returns:\n        solution_grid: [9, 9] numpy array with completed solution\n    \"\"\"\n    # Step 1: Preprocess\n    # Flatten to sequence and add special tokens\n    puzzle_flat = puzzle_grid.flatten()  # [81]\n    puzzle_tokens = puzzle_flat + 2  # Shift by 2 (0=pad, 1=EOS, 2+=digits)\n    puzzle_tensor = torch.tensor(puzzle_tokens).unsqueeze(0)  # [1, 81]\n\n    # Step 2: Embed\n    x = model.embed_tokens(puzzle_tensor)  # [1, 81, 256]\n    y = model.embed_tokens(puzzle_tensor)  # Start with input as guess\n    z = torch.randn(1, 32, 256) * 0.02  # Random reasoning state\n\n    # Step 3: Reasoning Phase (8 passes)\n    for i in range(8):\n        # Concatenate three streams\n        combined = torch.cat([x, y, z], dim=1)  # [1, 194, 256]\n\n        # Pass through 2-layer transformer\n        for transformer_block in model.transformer_blocks:\n            combined = transformer_block(combined)\n\n        # Split and update only z\n        x_new, y_new, z_new = torch.split(combined, [81, 81, 32], dim=1)\n        z = z_new\n\n    # Step 4: Refinement Phase (16 passes)\n    for i in range(16):\n        combined = torch.cat([x, y, z], dim=1)\n\n        for transformer_block in model.transformer_blocks:\n            combined = transformer_block(combined)\n\n        x_new, y_new, z_new = torch.split(combined, [81, 81, 32], dim=1)\n        y = y_new\n\n    # Step 5: Decode to predictions\n    logits = model.reverse_embedding(y)  # [1, 81, 12]\n    predictions = torch.argmax(logits, dim=-1)  # [1, 81]\n\n    # Step 6: Post-process\n    solution_tokens = predictions[0].cpu().numpy() - 2  # Shift back\n    solution_grid = solution_tokens.reshape(9, 9)\n\n    return solution_grid\n\n# Example usage\npuzzle = np.array([\n    [5, 3, 0, 0, 7, 0, 0, 0, 0],\n    [6, 0, 0, 1, 9, 5, 0, 0, 0],\n    [0, 9, 8, 0, 0, 0, 0, 6, 0],\n    # ... rest of puzzle\n])\n\nsolution = solve_sudoku(puzzle)\nprint(solution)\n</code></pre> <p>What Happens Inside:</p> <p>Passes 1-8 (Reasoning): - z learns constraint relationships - Identifies which cells constrain which - Builds logical deduction chains - Prepares information for filling cells</p> <p>Passes 9-24 (Refinement): - y gets updated with actual cell values - Early passes fill obvious cells - Middle passes apply logical deduction - Late passes correct errors and verify</p> <p>Success Metrics:</p> <p>On Sudoku-Extreme test set (423K puzzles): - Accuracy: 87.4% (TRM-MLP) or 74.7% (TRM-Att) - Inference time: ~15ms per puzzle on GPU - Invalid solutions: &lt;0.5% (most errors are unsolved, not rule-breaking)</p> <p>Why It Works:</p> <ol> <li>Iterative Refinement: Can fix errors from earlier passes</li> <li>Global Context: Attention allows every cell to influence every other</li> <li>Learned Strategies: Discovers Sudoku solving techniques from data</li> <li>Deep Supervision: Trained to improve answers progressively</li> </ol> Extending to ARC-AGI <p>The same architecture works on ARC-AGI with minimal changes:</p> <pre><code># Changes for ARC-AGI:\n# 1. Larger grids (up to 30x30 instead of 9x9)\n# 2. Multi-example tasks (2-3 examples shown before test)\n# 3. Task ID embedding (link examples from same task)\n# 4. More colors (10 instead of 9)\n\n# Everything else stays the same:\n# - Three streams (question, answer, reasoning)\n# - Recursive processing (8 + 16 passes)\n# - Deep supervision (16 cycles)\n# - Same 2-layer transformer\n</code></pre> <p>Results on ARC-AGI-1: 44.6% (vs 40.3% for HRM, 21% for direct prediction)</p>"},{"location":"AIBreakDown/BACKUP/#12-hrm-vs-trm-a-comprehensive-comparison","title":"12. HRM vs TRM: A Comprehensive Comparison","text":"<p>Let's directly compare TRM to its predecessor HRM across all dimensions.</p>"},{"location":"AIBreakDown/BACKUP/#architecture-comparison","title":"Architecture Comparison","text":"Aspect HRM TRM Impact Networks 2 separate (f_L, f_H) 1 unified 50% fewer weights Layers 4 per network 2 total 75% fewer layers Parameters 27M 5-7M 74-81% reduction Latent States z_L and z_H (hierarchical) y and z (answer + reasoning) Simpler interpretation Biological Justification Complex (brain oscillations) None needed Easier to understand Fixed-Point Theory Required (IFT) Not required Simpler training Gradient Computation 1-step approximation Full backprop More accurate ACT Mechanism Q-learning (2 passes) Binary BCE (1 pass) 2x faster"},{"location":"AIBreakDown/BACKUP/#performance-comparison","title":"Performance Comparison","text":"<p>Sudoku-Extreme:</p> Model Parameters Accuracy Direct prediction 27M 0.0% HRM 27M 55.0% TRM (n=2, T=2) 5M 73.7% TRM (n=3, T=3) 5M 87.4% <p>Maze-Hard:</p> Model Parameters Accuracy Direct prediction 27M 0.0% HRM 27M 74.5% TRM-Att 7M 85.3% <p>ARC-AGI-1:</p> Model Parameters Accuracy Direct prediction 27M 21.0% HRM 27M 40.3% TRM-Att 7M 44.6% <p>ARC-AGI-2:</p> Model Parameters Accuracy Gemini 2.5 Pro Unknown 4.9% HRM 27M 5.0% TRM-Att 7M 7.8%"},{"location":"AIBreakDown/BACKUP/#key-innovations-in-trm","title":"Key Innovations in TRM","text":"<p>1. No Fixed-Point Theorem Required</p> <p>HRM relied on the Implicit Function Theorem (IFT), assuming recursions converge to a fixed point z* where:</p> <pre><code>z_L* \u2248 f_L(z_L* + z_H + x)\nz_H* \u2248 f_H(z_L + z_H*)\n</code></pre> <p>TRM eliminates this by: - Defining a \"full recursion cycle\" (n steps of reasoning, then refinement) - Back-propagating through the complete cycle - No assumptions about convergence needed</p> <p>2. Simpler Latent State Interpretation</p> <p>HRM: \"z_L is low-level hierarchical reasoning, z_H is high-level hierarchical reasoning based on brain oscillations\"</p> <p>TRM: \"y is your current answer, z is your scratch work to improve it\"</p> <p>The TRM interpretation is: - Easier to understand - Doesn't require neuroscience background - Actually explains why 2 streams is optimal (not 1, not 3+)</p> <p>3. Single Network with Weight Sharing</p> <p>HRM used f_L and f_H with different weights. TRM showed you can use the same weights for both, determined by what inputs you provide:</p> <pre><code># HRM: Two networks\nz_L = f_L(z_L + z_H + x)  # Has x\nz_H = f_H(z_L + z_H)      # No x\n\n# TRM: One network, different inputs\nz = net(x, y, z)  # Has x \u2192 updates reasoning\ny = net(y, z)     # No x \u2192 updates answer\n</code></pre> <p>The network learns to behave differently based on input composition.</p> <p>4. Two Layers Beat Four Layers</p> <p>Surprisingly, TRM found that smaller networks generalize better on limited data:</p> Layers Parameters Sudoku Accuracy 4 10M 79.5% 3 7.5M 83.2% 2 5M 87.4% 1 2.5M 68.3% <p>Hypothesis: With small datasets, overfitting is the main enemy. Smaller networks with more recursions provide better regularization than large networks with fewer recursions.</p>"},{"location":"AIBreakDown/BACKUP/#training-time-comparison","title":"Training Time Comparison","text":"<p>TRM Training: - Time: 48 hours on 4\u00d7 H100 GPUs - Dataset: ~100K examples (with augmentation) - Iterations: ~1M optimization steps - Cost: ~$500-1000 in compute</p> <p>HRM Training: - Time: ~36 hours on 4\u00d7 H100 GPUs - Dataset: Same - Iterations: ~750K optimization steps - Cost: ~$400-800 in compute</p> <p>TRM trains slightly longer but achieves much better results. The parameter efficiency means inference is much cheaper.</p> Why Not Use HRM Anymore? <p>TRM is strictly better: - Simpler: Easier to understand and implement - Better: Higher accuracy across all benchmarks - Smaller: Fewer parameters means faster inference - Cleaner: No complex mathematical requirements - Same cost: Training time is similar</p> <p>Unless you specifically need the hierarchical interpretation for some reason, use TRM.</p>"},{"location":"AIBreakDown/BACKUP/#13-complete-pytorch-implementation","title":"13. Complete PyTorch Implementation","text":"<p>Now let's build TRM from scratch with complete, runnable PyTorch code.</p>"},{"location":"AIBreakDown/BACKUP/#multi-head-attention","title":"Multi-Head Attention","text":"<p>We already saw this earlier, but here's the complete implementation:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention mechanism.\n\n    Key insight: Different heads learn different relationships.\n    For Sudoku: row constraints, column constraints, box constraints, etc.\n    \"\"\"\n\n    def __init__(self, d_model=256, n_heads=4, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # Project and split into heads\n        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Attention scores\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Apply attention\n        attn_output = torch.matmul(attn_weights, v)\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, seq_len, d_model)\n\n        return self.out_proj(attn_output)\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#feed-forward-network","title":"Feed-Forward Network","text":"<pre><code>class FeedForward(nn.Module):\n    \"\"\"\n    Position-wise feed-forward network.\n    Applies the same transformation to each position independently.\n    \"\"\"\n\n    def __init__(self, d_model=256, d_ff=1024, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#transformer-block","title":"Transformer Block","text":"<pre><code>class TransformerBlock(nn.Module):\n    \"\"\"\n    Complete transformer block with attention and feed-forward.\n    Includes residual connections and layer normalization.\n    \"\"\"\n\n    def __init__(self, d_model=256, n_heads=4, d_ff=1024, dropout=0.1, use_attention=True):\n        super().__init__()\n        self.use_attention = use_attention\n\n        if use_attention:\n            self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n            self.norm1 = nn.LayerNorm(d_model)\n\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-attention block\n        if self.use_attention:\n            residual = x\n            x = self.attention(x, mask)\n            x = self.norm1(residual + self.dropout(x))\n\n        # Feed-forward block\n        residual = x\n        x = self.ffn(x)\n        x = self.norm2(residual + self.dropout(x))\n\n        return x\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#complete-trm-model","title":"Complete TRM Model","text":"<pre><code>class TRM(nn.Module):\n    \"\"\"\n    Tiny Recursive Model - Complete Implementation\n\n    Key components:\n    - Three streams: question (x), answer (y), reasoning (z)\n    - Recursive processing: same weights applied multiple times\n    - Two-phase updates: reasoning then refinement\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size=12,           # 0=pad, 1=EOS, 2-11=digits 0-9\n        d_model=256,             # Embedding dimension\n        n_heads=4,               # Attention heads\n        d_ff=1024,               # Feed-forward dimension\n        n_layers=2,              # Transformer layers (2 is optimal!)\n        max_seq_len=512,         # Maximum sequence length\n        dropout=0.1,\n        n_reasoning_steps=8,     # Phase 1: update z\n        n_refinement_steps=16,   # Phase 2: update y\n        use_attention=True,      # False for MLP-only variant\n        tie_embeddings=True      # Share input/output embeddings\n    ):\n        super().__init__()\n\n        self.d_model = d_model\n        self.n_reasoning_steps = n_reasoning_steps\n        self.n_refinement_steps = n_refinement_steps\n        self.use_attention = use_attention\n\n        # Embeddings\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n        self.embedding_dropout = nn.Dropout(dropout)\n\n        # Transformer blocks (reused recursively)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout, use_attention)\n            for _ in range(n_layers)\n        ])\n\n        # Output projection\n        self.reverse_embedding = nn.Linear(d_model, vocab_size, bias=False)\n\n        # Weight tying: use same weights for input and output embeddings\n        if tie_embeddings:\n            self.reverse_embedding.weight = self.token_embedding.weight\n\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with small random values.\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            elif isinstance(module, nn.LayerNorm):\n                nn.init.ones_(module.weight)\n                nn.init.zeros_(module.bias)\n\n    def embed_tokens(self, token_ids):\n        \"\"\"\n        Convert token IDs to embeddings with positional encoding.\n\n        Args:\n            token_ids: [batch, seq_len] integer tensor\n\n        Returns:\n            embeddings: [batch, seq_len, d_model] float tensor\n        \"\"\"\n        batch_size, seq_len = token_ids.shape\n\n        # Token embeddings\n        token_emb = self.token_embedding(token_ids)\n\n        # Positional embeddings\n        positions = torch.arange(seq_len, device=token_ids.device)\n        positions = positions.unsqueeze(0).expand(batch_size, -1)\n        pos_emb = self.position_embedding(positions)\n\n        # Combine\n        embeddings = self.embedding_dropout(token_emb + pos_emb)\n\n        return embeddings\n\n    def apply_transformer_blocks(self, x, mask=None):\n        \"\"\"Apply all transformer blocks sequentially.\"\"\"\n        for block in self.transformer_blocks:\n            x = block(x, mask)\n        return x\n\n    def forward_pass(self, x, y, z, mask=None):\n        \"\"\"\n        Single forward pass through the transformer.\n\n        This is the key: all three streams are concatenated,\n        processed together, then split back apart.\n\n        Args:\n            x: [batch, len_x, d_model] - question stream\n            y: [batch, len_y, d_model] - answer stream\n            z: [batch, len_z, d_model] - reasoning stream\n\n        Returns:\n            x_new, y_new, z_new: Updated streams\n        \"\"\"\n        len_x = x.size(1)\n        len_y = y.size(1)\n        len_z = z.size(1)\n\n        # Concatenate three streams\n        combined = torch.cat([x, y, z], dim=1)  # [batch, len_x+len_y+len_z, d_model]\n\n        # Process through transformer\n        combined = self.apply_transformer_blocks(combined, mask)\n\n        # Split back\n        x_new = combined[:, :len_x, :]\n        y_new = combined[:, len_x:len_x+len_y, :]\n        z_new = combined[:, len_x+len_y:, :]\n\n        return x_new, y_new, z_new\n\n    def recursive_reasoning(self, x, y, z, mask=None, return_trajectory=False):\n        \"\"\"\n        The heart of TRM: recursive reasoning.\n\n        Phase 1 (n_reasoning_steps): Update z (build understanding)\n        Phase 2 (n_refinement_steps): Update y (improve answer)\n\n        Args:\n            x: Question stream (fixed)\n            y: Answer stream (refined in phase 2)\n            z: Reasoning stream (refined in phase 1)\n\n        Returns:\n            y_final: Final answer predictions\n            trajectory: (optional) History of states\n        \"\"\"\n        trajectory = {'z_states': [], 'y_states': []} if return_trajectory else None\n\n        # Phase 1: Build reasoning (update z only)\n        for step in range(self.n_reasoning_steps):\n            x_new, y_new, z_new = self.forward_pass(x, y, z, mask)\n            z = z_new  # Only update z\n\n            if return_trajectory:\n                trajectory['z_states'].append(z.detach().clone())\n\n        # Phase 2: Refine answer (update y only)\n        for step in range(self.n_refinement_steps):\n            x_new, y_new, z_new = self.forward_pass(x, y, z, mask)\n            y = y_new  # Only update y\n\n            if return_trajectory:\n                trajectory['y_states'].append(y.detach().clone())\n\n        return (y, trajectory) if return_trajectory else y\n\n    def forward(self, question_ids, answer_ids=None, latent_len=32, mask=None):\n        \"\"\"\n        Complete forward pass.\n\n        Args:\n            question_ids: [batch, len_q] - input question as tokens\n            answer_ids: [batch, len_a] - target answer (for training)\n            latent_len: int - length of reasoning stream\n\n        Returns:\n            logits: [batch, len_a, vocab_size] - predicted answer\n        \"\"\"\n        batch_size = question_ids.size(0)\n        device = question_ids.device\n\n        # Embed question (x stream - fixed)\n        x = self.embed_tokens(question_ids)\n\n        # Embed or initialize answer (y stream - will be refined)\n        if answer_ids is not None:\n            y = self.embed_tokens(answer_ids)  # Training: start from target\n        else:\n            len_a = 81  # Default for Sudoku\n            y = torch.randn(batch_size, len_a, self.d_model, device=device) * 0.02\n\n        # Initialize reasoning (z stream - will be refined)\n        z = torch.randn(batch_size, latent_len, self.d_model, device=device) * 0.02\n\n        # Recursive reasoning\n        y_final = self.recursive_reasoning(x, y, z, mask)\n\n        # Decode to token predictions\n        logits = self.reverse_embedding(y_final)\n\n        return logits\n\n    def generate(self, question_ids, max_length=81, latent_len=32):\n        \"\"\"\n        Generate answer for a given question.\n\n        Args:\n            question_ids: [batch, len_q] - input question\n            max_length: int - maximum answer length\n            latent_len: int - reasoning stream length\n\n        Returns:\n            generated: [batch, max_length] - predicted tokens\n        \"\"\"\n        batch_size = question_ids.size(0)\n        device = question_ids.device\n\n        # Initialize empty answer\n        y_init = torch.zeros(batch_size, max_length, dtype=torch.long, device=device)\n\n        # Get predictions\n        logits = self.forward(question_ids, y_init, latent_len)\n\n        # Take argmax\n        generated = torch.argmax(logits, dim=-1)\n\n        return generated\n\n    def count_parameters(self):\n        \"\"\"Count total trainable parameters.\"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#model-variants","title":"Model Variants","text":"<pre><code>def create_trm_att(vocab_size=12, d_model=256, n_layers=2):\n    \"\"\"\n    Create TRM with attention (TRM-Att variant).\n\n    Best for: Maze, ARC-AGI (large, variable-length problems)\n    Parameters: ~7M\n    \"\"\"\n    return TRM(\n        vocab_size=vocab_size,\n        d_model=d_model,\n        n_heads=4,\n        d_ff=d_model * 4,\n        n_layers=n_layers,\n        n_reasoning_steps=8,\n        n_refinement_steps=16,\n        use_attention=True\n    )\n\ndef create_trm_mlp(vocab_size=12, d_model=256, n_layers=2):\n    \"\"\"\n    Create TRM without attention (TRM-MLP variant).\n\n    Best for: Sudoku (fixed-length, highly structured problems)\n    Parameters: ~5M\n    Achieves 87.4% on Sudoku-Extreme!\n    \"\"\"\n    return TRM(\n        vocab_size=vocab_size,\n        d_model=d_model,\n        n_heads=4,  # Not used, but kept for compatibility\n        d_ff=d_model * 4,\n        n_layers=n_layers,\n        n_reasoning_steps=8,\n        n_refinement_steps=16,\n        use_attention=False  # Key difference!\n    )\n\n# Example: Create models\nmodel_att = create_trm_att()\nmodel_mlp = create_trm_mlp()\n\nprint(f\"TRM-Att parameters: {model_att.count_parameters() / 1e6:.2f}M\")\nprint(f\"TRM-MLP parameters: {model_mlp.count_parameters() / 1e6:.2f}M\")\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#training-utilities","title":"Training Utilities","text":"<pre><code>class ExponentialMovingAverage:\n    \"\"\"\n    EMA for model weights - improves stability on small datasets.\n\n    Instead of using current weights directly, maintain a moving average.\n    This prevents sharp updates that might break good solutions.\n    \"\"\"\n\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n\n        # Initialize shadow weights\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def update(self):\n        \"\"\"Update shadow weights after each optimization step.\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data\n\n    def apply_shadow(self):\n        \"\"\"Replace model weights with shadow weights (for inference).\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data.clone()\n                param.data = self.shadow[name]\n\n    def restore(self):\n        \"\"\"Restore original weights (after inference).\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n\ndef get_lr_scheduler(optimizer, warmup_steps=1000, total_steps=100000):\n    \"\"\"\n    Learning rate scheduler with warmup and cosine decay.\n\n    Warmup: Gradually increase from 0 to target LR\n    Cosine: Smoothly decrease to near 0\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step &lt; warmup_steps:\n            # Linear warmup\n            return float(current_step) / float(max(1, warmup_steps))\n        # Cosine decay\n        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#training-loop-with-deep-supervision","title":"Training Loop with Deep Supervision","text":"<pre><code>def train_with_deep_supervision(\n    model,\n    train_loader,\n    optimizer,\n    scheduler,\n    ema,\n    device,\n    n_supervision=16,\n    max_grad_norm=1.0\n):\n    \"\"\"\n    Training loop with deep supervision and all the tricks.\n\n    Args:\n        model: TRM model\n        train_loader: DataLoader for training data\n        optimizer: Optimizer (e.g., AdamW)\n        scheduler: Learning rate scheduler\n        ema: Exponential moving average\n        device: torch.device\n        n_supervision: Number of supervision cycles (16 in paper)\n        max_grad_norm: Gradient clipping threshold\n\n    Returns:\n        average_loss: Loss for this epoch\n    \"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = 0\n\n    for question_ids, answer_ids in train_loader:\n        question_ids = question_ids.to(device)\n        answer_ids = answer_ids.to(device)\n\n        batch_size = question_ids.size(0)\n\n        # Initialize streams\n        x = model.embed_tokens(question_ids)\n        y = model.embed_tokens(question_ids)  # Start with input\n        z = torch.randn(batch_size, 32, model.d_model, device=device) * 0.02\n\n        optimizer.zero_grad()\n        cycle_loss = 0\n\n        # Deep supervision: multiple improvement cycles\n        for sup_step in range(n_supervision):\n            # Phase 1: Reasoning\n            for i in range(model.n_reasoning_steps):\n                x_new, y_new, z_new = model.forward_pass(x, y, z)\n                z = z_new\n\n            # Phase 2: Refinement\n            for i in range(model.n_refinement_steps):\n                x_new, y_new, z_new = model.forward_pass(x, y, z)\n                y = y_new\n\n            # Compute loss for this cycle\n            logits = model.reverse_embedding(y)\n            loss = F.cross_entropy(\n                logits.reshape(-1, logits.size(-1)),\n                answer_ids.reshape(-1),\n                ignore_index=0  # Ignore padding\n            )\n\n            # Weight: increase for later supervision steps\n            weight = (sup_step + 1) / n_supervision\n            cycle_loss += weight * loss\n\n            # Detach to prevent backprop through all history\n            y = y.detach()\n            z = z.detach()\n\n        # Backpropagate\n        cycle_loss.backward()\n\n        # Gradient clipping (crucial for stability!)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n        # Update weights\n        optimizer.step()\n        scheduler.step()\n\n        # Update EMA\n        ema.update()\n\n        total_loss += cycle_loss.item()\n        num_batches += 1\n\n    return total_loss / num_batches\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#complete-training-script","title":"Complete Training Script","text":"<pre><code>def main():\n    \"\"\"\n    Complete training script for TRM.\n    \"\"\"\n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Hyperparameters\n    vocab_size = 12\n    d_model = 256\n    n_layers = 2\n    batch_size = 32\n    learning_rate = 1e-4\n    num_epochs = 100\n\n    # Create model\n    model = create_trm_mlp(vocab_size, d_model, n_layers)\n    model = model.to(device)\n\n    print(f\"Parameters: {model.count_parameters() / 1e6:.2f}M\")\n\n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = get_lr_scheduler(optimizer, warmup_steps=1000)\n\n    # EMA for stability\n    ema = ExponentialMovingAverage(model, decay=0.999)\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training loop\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Train\n        train_loss = train_with_deep_supervision(\n            model, train_loader, optimizer, scheduler, ema, device\n        )\n\n        # Validate with EMA weights\n        ema.apply_shadow()\n        val_loss = evaluate(model, val_loader, criterion, device)\n        ema.restore()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n        print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n        # Save best model\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'ema_shadow': ema.shadow,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_loss': val_loss,\n            }, 'best_trm_model.pt')\n            print(\"  Saved new best model!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This is a complete, working implementation of TRM. You can run this code to train your own model!</p>"},{"location":"AIBreakDown/BACKUP/#14-getting-started-practical-setup-guide","title":"14. Getting Started: Practical Setup Guide","text":"<p>Let's get you set up to actually train and use TRM.</p>"},{"location":"AIBreakDown/BACKUP/#installation","title":"Installation","text":"<pre><code># Clone the TinyRecursiveModels repository\ngit clone https://github.com/alexjmartineau/TinyRecursiveModels.git\ncd TinyRecursiveModels\n\n# Create virtual environment\npython -m venv trm_env\nsource trm_env/bin/activate  # On Windows: trm_env\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Core dependencies:\n# - torch &gt;= 2.0.0\n# - numpy &gt;= 1.24.0\n# - tqdm (for progress bars)\n# - wandb (optional, for logging)\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#dataset-preparation","title":"Dataset Preparation","text":"<p>For Sudoku:</p> <pre><code>import numpy as np\nfrom puzzle_dataset import SudokuDataset\n\n# The repository includes Sudoku puzzle generators\n# Download pre-generated puzzles or generate your own\n\n# Training set: 1000 extreme Sudoku puzzles\n# Test set: 423,000 puzzles for evaluation\n\ndataset = SudokuDataset(\n    data_dir='data/sudoku',\n    split='train',\n    augmentations=True  # Apply dihedral transformations and recoloring\n)\n\n# Data augmentation for Sudoku:\n# - 72 dihedral transformations (rotations + flips + reflections)\n# - 9 color permutations\n# Total: 72 variants per puzzle\n\nprint(f\"Dataset size: {len(dataset)}\")\nprint(f\"With augmentation: {len(dataset) * 72}\")\n</code></pre> <p>For ARC-AGI:</p> <pre><code># Download ARC-AGI datasets\nwget https://github.com/fchollet/ARC-AGI/archive/refs/heads/master.zip\nunzip master.zip\n\n# The repository includes data loaders for ARC\n# Each task has:\n# - 2-3 training examples (input-output pairs)\n# - 1-2 test inputs (predict outputs)\n</code></pre> <pre><code>from dataset.arc_dataset import ARCDataset\n\narc_dataset = ARCDataset(\n    data_dir='ARC-AGI-master/data',\n    split='training',  # or 'evaluation'\n    augmentations=True,\n    max_augmentations=1000  # Up to 1000 augmented versions per task\n)\n\n# ARC augmentation:\n# - Color permutations (swap color mappings)\n# - Dihedral transformations (rotations and flips)\n# - Spatial translations (move grids within 30x30 space)\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#grid-representation","title":"Grid Representation","text":"<p>Understanding how grids are tokenized:</p> <pre><code>def tokenize_grid(grid, max_size=30):\n    \"\"\"\n    Convert a grid to token sequence.\n\n    Args:\n        grid: [H, W] numpy array with values 0-9\n        max_size: Maximum grid dimension (30 for ARC-AGI)\n\n    Returns:\n        tokens: [max_size * max_size] with padding\n    \"\"\"\n    H, W = grid.shape\n\n    # Flatten grid\n    flat = grid.flatten()  # [H*W]\n\n    # Add 2 to shift (0=pad, 1=EOS, 2-11=colors 0-9)\n    tokens = flat + 2\n\n    # Add EOS markers to delineate grid boundary\n    # Create padded grid with EOS on right and bottom\n    padded = np.zeros((max_size, max_size), dtype=np.int64)\n    padded[:H, :W] = tokens.reshape(H, W)\n    padded[H, :] = 1  # EOS marker on bottom\n    padded[:, W] = 1  # EOS marker on right\n\n    return padded.flatten()\n\n# Example: 2x2 grid\ngrid = np.array([[5, 3], [6, 0]])\ntokens = tokenize_grid(grid, max_size=4)\nprint(tokens)\n# Output: [7, 5, 1, 0, 8, 2, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n#         [5+2, 3+2, EOS, pad, 6+2, 0+2, EOS, pad, ...]\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#training-your-first-trm","title":"Training Your First TRM","text":"<pre><code># Train on Sudoku (fastest, good for testing)\npython pretrain.py \\\n    --dataset sudoku \\\n    --model_type mlp \\\n    --d_model 256 \\\n    --n_layers 2 \\\n    --n_reasoning_steps 8 \\\n    --n_refinement_steps 16 \\\n    --n_supervision 16 \\\n    --batch_size 32 \\\n    --learning_rate 1e-4 \\\n    --num_epochs 100 \\\n    --save_dir checkpoints/sudoku_trm\n\n# Train on ARC-AGI (slower, more challenging)\npython pretrain.py \\\n    --dataset arc \\\n    --model_type att \\\n    --d_model 256 \\\n    --n_layers 2 \\\n    --batch_size 16 \\\n    --learning_rate 5e-5 \\\n    --num_epochs 200 \\\n    --save_dir checkpoints/arc_trm\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#inference-and-evaluation","title":"Inference and Evaluation","text":"<pre><code># Load trained model\nmodel = TRM.load_from_checkpoint('checkpoints/sudoku_trm/best_model.pt')\nmodel.eval()\nmodel.to(device)\n\n# Solve a Sudoku puzzle\npuzzle = np.array([\n    [5, 3, 0, 0, 7, 0, 0, 0, 0],\n    [6, 0, 0, 1, 9, 5, 0, 0, 0],\n    [0, 9, 8, 0, 0, 0, 0, 6, 0],\n    [8, 0, 0, 0, 6, 0, 0, 0, 3],\n    [4, 0, 0, 8, 0, 3, 0, 0, 1],\n    [7, 0, 0, 0, 2, 0, 0, 0, 6],\n    [0, 6, 0, 0, 0, 0, 2, 8, 0],\n    [0, 0, 0, 4, 1, 9, 0, 0, 5],\n    [0, 0, 0, 0, 8, 0, 0, 7, 9]\n])\n\n# Tokenize\npuzzle_tokens = torch.tensor(puzzle.flatten() + 2).unsqueeze(0).to(device)\n\n# Generate solution\nwith torch.no_grad():\n    solution_tokens = model.generate(puzzle_tokens)\n\n# Decode\nsolution = (solution_tokens[0].cpu().numpy() - 2).reshape(9, 9)\n\nprint(\"Solution:\")\nprint(solution)\n\n# Verify (check all constraints)\ndef is_valid_sudoku(grid):\n    # Check rows\n    for row in grid:\n        if len(set(row)) != 9 or not set(row) == set(range(1, 10)):\n            return False\n    # Check columns\n    for col in grid.T:\n        if len(set(col)) != 9:\n            return False\n    # Check boxes\n    for i in range(0, 9, 3):\n        for j in range(0, 9, 3):\n            box = grid[i:i+3, j:j+3].flatten()\n            if len(set(box)) != 9:\n                return False\n    return True\n\nif is_valid_sudoku(solution):\n    print(\"Valid solution!\")\nelse:\n    print(\"Invalid solution\")\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#evaluation-on-test-set","title":"Evaluation on Test Set","text":"<pre><code># Evaluate on Sudoku test set (423K puzzles)\npython evaluators/eval_sudoku.py \\\n    --model_path checkpoints/sudoku_trm/best_model.pt \\\n    --test_data data/sudoku/test.pkl \\\n    --batch_size 64 \\\n    --output_file results/sudoku_results.json\n\n# Evaluate on ARC-AGI evaluation set\npython evaluators/eval_arc.py \\\n    --model_path checkpoints/arc_trm/best_model.pt \\\n    --data_dir ARC-AGI-master/data/evaluation \\\n    --output_file results/arc_results.json\n</code></pre> <p>Results will include: - Accuracy: % of puzzles solved correctly - Partial credit: % of cells filled correctly - Invalid rate: % of solutions that violate constraints - Inference time: Average time per puzzle</p>"},{"location":"AIBreakDown/BACKUP/#15-results-across-all-benchmarks","title":"15. Results Across All Benchmarks","text":"<p>Let's look at comprehensive results across all tasks.</p>"},{"location":"AIBreakDown/BACKUP/#sudoku-extreme","title":"Sudoku-Extreme","text":"Model Parameters Train Time Accuracy Direct prediction 27M 24h 0.0% HRM 27M 36h 55.0% TRM-MLP 5M 48h 87.4% TRM-Att 7M 48h 74.7% <p>Key findings: - MLP variant works better than attention for fixed-size structured tasks - 75% fewer parameters than HRM, 59% better accuracy - Still far from human performance (~99%) but unprecedented for neural networks</p>"},{"location":"AIBreakDown/BACKUP/#maze-hard","title":"Maze-Hard","text":"Model Parameters Accuracy Direct prediction 27M 0.0% HRM 27M 74.5% TRM-MLP 19M 0.0% TRM-Att 7M 85.3% <p>Key findings: - Attention is crucial for path-finding (non-local dependencies) - 14% improvement over HRM with fewer parameters</p>"},{"location":"AIBreakDown/BACKUP/#arc-agi-1-original-benchmark","title":"ARC-AGI-1 (Original Benchmark)","text":"Model Parameters Accuracy Direct prediction 27M 21.0% HRM 27M 40.3% TRM-Att 7M 44.6% TRM-MLP 19M 29.6% <p>Context - LLM baselines: - DeepSeek R1: 15.8% - Claude 3.7: 28.6% - O3-mini: 34.5% - Gemini 2.5 Pro: 37.0%</p> <p>TRM beats most LLMs with &lt;0.01% of parameters!</p>"},{"location":"AIBreakDown/BACKUP/#arc-agi-2-2024-competition","title":"ARC-AGI-2 (2024 Competition)","text":"Model Parameters Accuracy Claude 3.7 16K Unknown 0.7% DeepSeek R1 671B 1.3% O3-mini Unknown 3.0% Gemini 2.5 Pro 32K Unknown 4.9% HRM 27M 5.0% TRM-Att 7M 7.8% <p>Key findings: - ARC-AGI-2 is extremely hard (designed to resist current methods) - TRM achieves highest score among non-ensemble models - Current top leaderboard score: ~30% (Grok-4 with extensive test-time compute and ensembling)</p>"},{"location":"AIBreakDown/BACKUP/#parameter-efficiency","title":"Parameter Efficiency","text":"<p>Let's visualize the efficiency:</p> Model Parameters ARC-AGI-1 Acc Params per 1% Acc Gemini 2.5 Pro ~1T (estimated) 37.0% 27B O3-mini Unknown 34.5% ? HRM 27M 40.3% 670K TRM 7M 44.6% 157K <p>TRM achieves each percentage point of accuracy with 4.3x fewer parameters than HRM!</p>"},{"location":"AIBreakDown/BACKUP/#inference-speed","title":"Inference Speed","text":"<p>On a single NVIDIA A100 GPU:</p> Task Model Latency (ms) Throughput (puzzles/sec) Sudoku 9x9 TRM-MLP 12ms 83 Sudoku 9x9 HRM 25ms 40 ARC-AGI 30x30 TRM-Att 45ms 22 ARC-AGI 30x30 HRM 89ms 11 <p>TRM is 2x faster than HRM due to fewer parameters and simpler architecture.</p>"},{"location":"AIBreakDown/BACKUP/#scaling-analysis","title":"Scaling Analysis","text":"<p>How does performance scale with recursion depth?</p> Recursion Config Effective Depth Sudoku Acc Training Time n=1, T=1 6 63.2% Fast (20h) n=2, T=2 20 81.9% Medium (35h) n=3, T=3 42 87.4% Slow (48h) n=4, T=4 72 84.2% Very slow (72h) <p>Takeaway: There's a sweet spot around 40-50 effective layers. More doesn't always help (overfitting).</p>"},{"location":"AIBreakDown/BACKUP/#16-why-this-works-theoretical-insights","title":"16. Why This Works: Theoretical Insights","text":""},{"location":"AIBreakDown/BACKUP/#compression-and-regularization","title":"Compression and Regularization","text":"<p>The Chinchilla Argument:</p> <p>The Chinchilla scaling laws (from large language model research) suggest that for a given compute budget, there's an optimal model size. Too small: underfitting. Too large: needs more data.</p> <p>From the YouTube transcript, we can think of this as:</p> <pre><code>For fixed compute budget:\n- Small data \u2192 Optimal model size is SMALL\n- Large data \u2192 Optimal model size is LARGE\n\nARC-AGI has ~1000 training tasks\n\u2192 Optimal size is ~5-10M parameters\n\u2192 TRM (7M) is near optimal\n\u2192 LLMs (100B+) are massively overparameterized\n</code></pre> <p>Compression Forces Learning:</p> <p>When you force knowledge into fewer parameters: 1. Model must learn efficient representations (no room for memorization) 2. Recursive application means same weights handle multiple cases 3. This is strong regularization (like L2, dropout, but architectural)</p> <p>Comparison to Knowledge Distillation:</p> Technique Approach Result Knowledge Distillation Large teacher \u2192 Small student Student learns teacher's outputs TRM Small model + recursion Model learns efficient reasoning <p>TRM shows you might not need the large teacher at all for reasoning tasks!</p>"},{"location":"AIBreakDown/BACKUP/#iterative-refinement-vs-single-pass","title":"Iterative Refinement vs Single-Pass","text":"<p>How Humans Solve Sudoku:</p> <ol> <li>Scan for obvious cells (naked singles)</li> <li>Look for hidden singles</li> <li>Apply pair/triple techniques</li> <li>Try and verify</li> <li>Backtrack if needed</li> </ol> <p>This is inherently iterative.</p> <p>How LLMs Try to Solve Sudoku:</p> <ol> <li>Read puzzle</li> <li>Generate token 1</li> <li>Generate token 2 (conditioned on token 1)</li> <li>...</li> <li>Generate token 81</li> </ol> <p>Once token 1 is generated, it can't be changed. If wrong, solution fails.</p> <p>How TRM Solves Sudoku:</p> <ol> <li>Read puzzle (x stream)</li> <li>Think about constraints (update z 8 times)</li> <li>Propose solution (update y 16 times)</li> <li>Can revise earlier predictions in later passes!</li> </ol> <p>This matches human solving much better.</p>"},{"location":"AIBreakDown/BACKUP/#error-correction","title":"Error Correction","text":"<p>Key Insight: TRM can fix its own mistakes.</p> <pre><code># Pass 5: Model predicts cell (4,5) = 7\ny[4,5] = embed(7)\n\n# Pass 10: Model realizes this violates a constraint\n# Can change prediction!\ny[4,5] = embed(2)  # Corrected\n\n# LLM: Once 7 is generated, it's permanent\n</code></pre> <p>This error correction capability is crucial for reasoning tasks where mistakes are easy to make but also easy to verify.</p>"},{"location":"AIBreakDown/BACKUP/#deep-supervision-as-curriculum-learning","title":"Deep Supervision as Curriculum Learning","text":"<p>Deep supervision creates an implicit curriculum:</p> Supervision Cycle What Model Learns 1-4 Basic patterns (rows, columns, boxes) 5-8 Simple logical deductions 9-12 Complex constraint propagation 13-16 Verification and correction <p>Early cycles see more examples (due to ACT halting), so model learns basics thoroughly before advancing.</p>"},{"location":"AIBreakDown/BACKUP/#17-applications-and-extensions","title":"17. Applications and Extensions","text":""},{"location":"AIBreakDown/BACKUP/#beyond-puzzles","title":"Beyond Puzzles","text":"<p>TRM's architecture generalizes to many reasoning domains:</p> <p>Constraint Satisfaction Problems (CSP): - Graph coloring - Scheduling - Resource allocation - Configuration problems</p> <p>Mathematical Reasoning: - Equation solving - Proof verification - Symbolic integration - Geometric constructions</p> <p>Code Generation: - Program synthesis from examples - Bug fixing - Code optimization - Test generation</p> <p>Planning and Search: - Route planning - Game playing (Go, Chess with legal moves) - Robotic manipulation planning</p>"},{"location":"AIBreakDown/BACKUP/#modifications-for-other-domains","title":"Modifications for Other Domains","text":"<p>For Longer Sequences:</p> <pre><code># Increase latent reasoning capacity\nz = torch.randn(batch_size, 64, d_model)  # vs 32 for Sudoku\n\n# Use attention (not MLP)\nmodel = create_trm_att(use_attention=True)\n\n# More recursions for complex problems\nmodel.n_reasoning_steps = 12  # vs 8\nmodel.n_refinement_steps = 24  # vs 16\n</code></pre> <p>For Multi-Modal Tasks:</p> <pre><code># Add vision encoder\nclass MultiModalTRM(nn.Module):\n    def __init__(self):\n        self.vision_encoder = VisionTransformer(...)\n        self.trm = TRM(...)\n\n    def forward(self, image, text_query):\n        # Encode image\n        image_features = self.vision_encoder(image)\n\n        # Use as x-stream (question)\n        x = image_features\n\n        # Text query as initial y\n        y = self.trm.embed_tokens(text_query)\n\n        # Reasoning\n        z = torch.randn(...)\n\n        # Run TRM\n        y_final = self.trm.recursive_reasoning(x, y, z)\n\n        return self.trm.reverse_embedding(y_final)\n</code></pre>"},{"location":"AIBreakDown/BACKUP/#competition-considerations","title":"Competition Considerations","text":"<p>ARC-AGI Private Leaderboard Constraints:</p> <ul> <li>Time limit: 12 hours</li> <li>Hardware: 4\u00d7 NVIDIA L4 GPUs</li> <li>Compute: ~48 L4-hours total</li> </ul> <p>Current TRM: - Training: 48 hours on 4\u00d7 H100 GPUs - H100 vs L4: ~3x more powerful - Total compute: 48h \u00d7 4 H100s = 192 H100-hours = 576 L4-hours</p> <p>This is 12x more than allowed!</p> <p>Path to Competition Compliance:</p> <ol> <li>Better data efficiency: Train on fewer augmentations</li> <li>Knowledge distillation: Use larger model to teach smaller one</li> <li>Transfer learning: Pre-train on Concept ARC, fine-tune on competition tasks</li> <li>Efficient search: Test-time compute for multiple tries</li> <li>Architecture search: Find even smaller networks that work</li> </ol> <p>Current TRM is a research result demonstrating what's possible, not (yet) optimized for the competition constraints.</p>"},{"location":"AIBreakDown/BACKUP/#18-conclusion-and-future-directions","title":"18. Conclusion and Future Directions","text":""},{"location":"AIBreakDown/BACKUP/#the-paradigm-shift","title":"The Paradigm Shift","text":"<p>TRM represents a fundamental shift in how we think about AI for reasoning:</p> <p>Old paradigm: - Bigger models are better - Scale to hundreds of billions of parameters - Single-pass inference is sufficient - Pre-train on internet-scale data</p> <p>New paradigm (TRM): - Smaller models with recursion are better - 5-10 million parameters is enough - Iterative refinement beats single-pass - Train on small, high-quality datasets</p> <p>The Result: - 7M parameters outperform 671B parameters - Deployable on edge devices - Train in 2 days, not 2 months - Accessible to researchers without massive compute</p>"},{"location":"AIBreakDown/BACKUP/#open-questions","title":"Open Questions","text":"<p>1. Optimal Recursion Depth: - Current: 8 reasoning + 16 refinement steps - Is there a better schedule? Task-dependent? - Can we learn the schedule?</p> <p>2. Generative Extension: - TRM is currently supervised (predict correct answer) - Can we extend to generation (sample from distribution)? - Would enable uncertainty quantification</p> <p>3. Scaling to Larger Problems: - Current: Up to 30\u00d730 grids - Can TRM handle 100\u00d7100? Variable-size with better efficiency?</p> <p>4. Transfer Learning: - Can a model trained on Sudoku transfer to other logic puzzles? - What's the minimal fine-tuning needed?</p> <p>5. Theoretical Understanding: - Why does recursion help so much? - Can we prove convergence properties? - Connection to iterative algorithms in algorithms literature?</p>"},{"location":"AIBreakDown/BACKUP/#what-this-means-for-the-field","title":"What This Means for the Field","text":"<p>For AI Research: - Rethink scaling laws: Parameter efficiency matters - Architectural innovation: Can beat pure scaling - Small data regimes: TRM shows path forward</p> <p>For Applications: - Edge deployment: Powerful reasoning without cloud - Real-time systems: Fast inference on constrained hardware - Cost reduction: 100-1000x cheaper than LLM APIs</p> <p>For the Future: - Democratization: Powerful AI without massive compute - Sustainability: Lower energy consumption - Accessibility: More researchers can contribute</p>"},{"location":"AIBreakDown/BACKUP/#final-thoughts","title":"Final Thoughts","text":"<p>When I first encountered the TRM paper, I was skeptical. How could a tiny 7M parameter model beat GPT-4 at anything?</p> <p>But the results are undeniable. On systematic reasoning tasks requiring iterative refinement and error correction, TRM's approach of small networks with recursive processing fundamentally outperforms the single-pass autoregressive approach of large language models.</p> <p>This doesn't mean LLMs are obsolete. They excel at language understanding, general knowledge, and open-ended generation. But for reasoning? TRM shows us a better path.</p> <p>The future of AI isn't just about building bigger models. It's about building smarter architectures that use computation more efficiently. TRM is a proof of concept that less can truly be more.</p>"},{"location":"AIBreakDown/BACKUP/#further-resources","title":"Further Resources","text":""},{"location":"AIBreakDown/BACKUP/#paper-and-code","title":"Paper and Code","text":"<ul> <li>Original TRM Paper: Less is More: Recursive Reasoning with Tiny Networks</li> <li>TinyRecursiveModels GitHub: Official Implementation</li> <li>HRM Paper (Predecessor): Hierarchical Reasoning Models</li> </ul>"},{"location":"AIBreakDown/BACKUP/#datasets","title":"Datasets","text":"<ul> <li>ARC-AGI: Abstraction and Reasoning Corpus</li> <li>ARC-AGI Competition: Official Leaderboard</li> <li>Concept ARC: Extended Task Set</li> </ul>"},{"location":"AIBreakDown/BACKUP/#related-research","title":"Related Research","text":"<ul> <li>Deep Equilibrium Models: Bai et al., 2019</li> <li>Adaptive Computation Time: Graves, 2016</li> <li>Deep Supervision: Lee et al., 2015</li> </ul>"},{"location":"AIBreakDown/BACKUP/#community","title":"Community","text":"<ul> <li>AI Engineering Academy: More tutorials on LLM fine-tuning, RAG systems, and prompt engineering</li> <li>ARC-AGI Discord: Active community working on the challenge</li> <li>Twitter/X: Follow @alexjmartin for updates</li> </ul>"},{"location":"AIBreakDown/BACKUP/#video-explanations","title":"Video Explanations","text":"<ul> <li>Detailed Walkthrough: YouTube breakdown of TRM by a community expert</li> <li>Visual Guide: The Remotion animations on this page (all 11 scenes)</li> </ul>"},{"location":"AIBreakDown/BACKUP/#try-it-yourself","title":"Try It Yourself","text":"<p>Want to experiment with transformers and reasoning? Check out: - Our LLM Fine-tuning Tutorials - Hugging Face Transformers Library - Fast.ai Practical Deep Learning</p> <p>This comprehensive guide was written to demystify TRM and make it accessible to anyone interested in efficient AI for reasoning. All visualizations were created using Remotion and React. Full source code available in the AI Engineering Academy repository.</p> <p>For questions, corrections, or discussions, please open an issue on GitHub or reach out on Twitter.</p> <p>Last Updated: November 2024 Author: Adithya S Kolavi License: CC BY 4.0</p>"},{"location":"AIBreakDown/TRM/","title":"TRM Visually Explained","text":""},{"location":"AIBreakDown/TRM/#tiny-recursive-models-trm-when-small-models-think-better","title":"Tiny Recursive Models (TRM): When Small Models Think Better","text":"<p>So here's something that caught my attention recently. A 7 million parameter model is absolutely crushing models that are 100,000 times its size on reasoning tasks. I'm talking about 87.4% accuracy on hard Sudoku puzzles while GPT-4, Claude, and even the massive 671B parameter DeepSeek R1 score 0%. Zero. Nada.</p> <p>And honestly? This isn't about making models bigger. We've been doing that for years. This is about making them think differently.</p> <p>I spent some time diving deep into this paper and created visualizations to help explain what's going on. By the end of this, you'll see why sometimes the smartest solution isn't more parameters - it's a smarter architecture.</p>"},{"location":"AIBreakDown/TRM/#what-is-trm","title":"What is TRM?","text":"<p>TRM (Tiny Recursive Models) is a research breakthrough from Samsung SAIL Montreal. It's designed for structured reasoning tasks - Sudoku puzzles, mazes, abstract reasoning problems (ARC-AGI). The core idea is simple but powerful: instead of processing a problem once with a massive network, TRM processes it many times with a tiny network.</p> <p>Think about how you actually solve a Sudoku puzzle. You don't stare at it for a minute and then write down all 81 numbers in one go. That'd be insane. You fill in some numbers, check your work, realize you made a mistake, backtrack, try again, and slowly iterate toward the solution. That's exactly what TRM does - it iterates and refines.</p> <p></p> <p>The results speak for themselves: - 87.4% on Sudoku-Extreme (vs 55% for its predecessor HRM, 0% for standard LLMs) - 44.6% on ARC-AGI-1 (a hard abstract reasoning benchmark) - Only 7M parameters - you could run this on a laptop</p>"},{"location":"AIBreakDown/TRM/#the-problem-domain-why-sudoku","title":"The Problem Domain: Why Sudoku?","text":"<p>Let's talk about Sudoku for a minute. It's actually a perfect test case for TRM because it needs both local reasoning (can I put a 5 in this cell?) and global reasoning (does this mess up the entire grid?). But there's something deeper going on here.</p> <p>Sudoku represents a class of problems where LLMs just... fail. Completely.</p> <p>Here's why: traditional LLMs process input once and spit out an answer autoregressively - token by token, no going back. For a Sudoku puzzle, this means the model needs to predict all 81 cells in sequence. One wrong prediction early on? The whole thing falls apart. It's like building a house of cards - one mistake and it all collapses.</p> <p>This is why GPT-4, Claude, and even that monster 671B parameter DeepSeek R1 all score 0% on hard Sudoku. They literally cannot iterate and refine their thinking. They get one shot, and if they mess up cell 12, well, cells 13-81 are probably going to be wrong too.</p> <p></p> <p>The visualization shows the transformation from problem to solution:</p> <p>First, you see an input puzzle on the left (with dashes for empty cells) and the solved version on the right. Notice how the puzzle is partially filled - about 25-30 cells are given, and the model must deduce the remaining 50+ cells while maintaining consistency across all rows, columns, and 3\u00d73 boxes.</p> <p>Second, watch how the 9\u00d79 grid gets flattened into a sequence of tokens. This is crucial: machine learning models process sequences, not 2D grids. Each cell becomes a single token, reading left-to-right, top-to-bottom. The 9\u00d79 grid becomes a sequence of 81 tokens.</p> <p>Now here's a technical detail that's actually pretty clever. The vocabulary TRM uses isn't just 0-9 for the digits. It's: - Token 0: Padding (for smaller grids) - Token 1: End-of-sequence marker (marks grid boundaries) - Tokens 2-11: The actual cell values (offset by 2, so token 2 = digit 0, token 11 = digit 9)</p> <p>Why the offset? Because padding and special tokens need their own IDs. It's a bit awkward but it works. This scheme lets TRM handle any grid size up to 30\u00d730, which is crucial for ARC-AGI tasks where grids vary wildly in size.</p> <p>And here's the cool part - this same approach works for way more than Sudoku. Mazes? Grid of walls and paths. ARC-AGI puzzles? Grid of colored cells. Game boards? You get it. If you can represent it as a grid, TRM can learn to reason about it.</p>"},{"location":"AIBreakDown/TRM/#how-data-flows-through-trm","title":"How Data Flows Through TRM","text":"<p>Alright, before we get to the really clever stuff, let's trace how data actually moves through the model. This is where TRM starts to look different from your standard transformer.</p> <p></p> <p>Follow the pipeline from left to right:</p> <p>1. Input tokenization: The 81 cells of a Sudoku grid become token IDs (0-11 as I mentioned earlier). Each token gets embedded into a 512-dimensional vector. Why 512? It's a sweet spot - big enough to capture complex patterns, small enough to not explode the parameter count.</p> <p>2. Puzzle ID embedding: This is clever. When you're training on ARC-AGI, you might see 3 different examples that all follow the same rule (like \"rotate 90 degrees then flip colors\"). The puzzle ID is a learnable embedding that basically tells the model \"hey, these examples are related.\" Without this, the model would treat each example as completely independent. With it, the model can learn \"oh, this is another example of THAT pattern.\"</p> <p>3. Sequence length calculation: For a 9\u00d79 Sudoku: - 81 grid tokens - 1 puzzle ID token - 15 padding tokens (to reach a round number) - Total: 97 tokens</p> <p>For 30\u00d730 grids (ARC-AGI max), the sequence can be up to 900+ tokens. The model uses positional embeddings so it knows which token represents which spatial location.</p> <p>4. Dual latent states: Here's where TRM diverges from standard architectures. The model doesn't just process the input once. It maintains two separate \"thought streams\": - z_H (High-level state): The current hypothesis for the answer. Think of this as the model's draft solution. - z_L (Low-level state): Reasoning workspace. This is where the model explores possibilities before committing to z_H.</p> <p>Both states are initialized as learned embeddings at the start. z_H gets updated rarely (3 times), while z_L gets updated frequently (18 times) during the 21 recursive passes.</p> <p>5. Recursive processing: This is the heart of TRM. The same 2-layer transformer processes the input 21 times in a carefully orchestrated pattern. Each pass refines z_L or z_H based on the current state of both latent variables and the input. We'll break down this structure in the next sections.</p> <p>6. Output prediction: After 21 passes, z_H contains the refined solution. A final linear layer (called the \"reverse embedding\") projects each position in z_H back to token probabilities - 11 classes per cell. The model predicts the most likely token for each position.</p> <p>The key insight: TRM trades space for time. Instead of having billions of parameters (space), it uses the same small network 21 times (time). It's like the difference between having 21 different consultants give you advice once, versus having one really good consultant iterate on their advice 21 times. Sometimes the second approach works better.</p>"},{"location":"AIBreakDown/TRM/#the-big-idea-weight-reuse","title":"The Big Idea: Weight Reuse","text":"<p>Okay, this is where things get interesting. This one design choice explains pretty much everything about why TRM works so well.</p> <p></p> <p>The visualization makes this crystal clear:</p> <p>Traditional approach (left side): Stack 32 unique transformer blocks, each with its own parameters. This is how HRM (TRM's predecessor) worked - different weights for each \"layer\" of reasoning. Total: 109M parameters (27M for HRM after optimizations), 55% accuracy on Sudoku-Extreme.</p> <p>TRM approach (right side): Use 2 transformer blocks repeatedly, 21 times each. Same weights, multiple passes. Total: 7M parameters, 87.4% accuracy.</p> <p>The numbers are kind of ridiculous: 15.6\u00d7 fewer parameters, 1.6\u00d7 better accuracy. But it's not just about being smaller and cheaper - it's about how the model actually learns.</p> <p>Why does weight reuse work so well?</p> <p>When you force a network to reuse the same weights across multiple passes, you're basically saying \"you can't cheat by memorizing specific patterns for each step.\" The network can't learn \"do X on the first pass, do Y on the second pass, do Z on the third.\" Nope. It has to learn ONE general operation that works for all passes.</p> <p>It's like learning to edit your own writing. If you had a different editor for your first draft, second draft, and third draft, each one could develop their own weird specialized tricks. But if the SAME editor has to handle all three drafts? They better learn general principles of good editing that work iteratively. That's what TRM is doing.</p> <p>This has a fancy name: regularization through compression. By cramming knowledge into fewer parameters, you prevent overfitting. The model literally cannot memorize \"if I see this exact pattern, output this.\" There's not enough room. It HAS to learn the actual underlying structure.</p> <p>And here's where it connects to scaling laws. The Chinchilla paper showed there's an optimal model size for any compute budget. But TRM adds a twist - when you only have 1,000 training examples (even with augmentations), the optimal model is way smaller than you'd think. Because smaller models just... can't overfit as easily. They're forced to generalize.</p> <p></p> <p>This alternative view reinforces the concept: traditional stacked layers on the left, recursive loop on the right. Notice how the recursive approach creates effective depth (21 passes \u00d7 2 layers = 42 effective layers) without the parameter cost. You get the benefits of a deep network - gradual refinement, hierarchical features, iterative improvement - without the memory requirements or overfitting risks.</p>"},{"location":"AIBreakDown/TRM/#the-heart-of-trm-two-thinking-spaces","title":"The Heart of TRM: Two Thinking Spaces","text":"<p>This is the part that really made me go \"oh, that's smart.\" TRM maintains two separate \"streams\" of thought that update at different rates. And honestly? This mirrors how we actually think through complex problems.</p> <p>When you're solving something hard, you don't just refine your answer. You also build up internal reasoning - scratch work, if you will - that supports your answer. TRM does the same thing.</p> <p></p> <p>Watch the visualization carefully - it breaks down into three phases:</p> <p>Phase 1 introduces the two states: - z_H (shown in red): Your hypothesis or current answer. This is the model's best guess at the solution - the actual Sudoku grid values it's predicting. - z_L (shown in blue): Your reasoning or working memory. This is scratch space where the model explores constraints, checks consistency, and works through logical implications.</p> <p>Phase 2 shows an L-cycle: the model updates z_L six times while z_H stays frozen. This is where the magic happens - the model is thinking through possibilities WITHOUT committing to an answer. It's doing that \"if this cell is 5, then that row can't have another 5, which means...\" type reasoning. Pure exploration.</p> <p>Phase 3 shows an H-cycle: the reasoning (z_L) informs an update to the answer (z_H). After all that exploration, the model makes one update to its draft solution based on what it figured out.</p> <p>The key numbers: - z_H gets updated only 3 times in the full 21 passes (via H-cycles) - z_L gets updated 18 times (via L-cycles) - Each L-cycle = 6 passes of updating z_L while z_H stays fixed - Then 1 H-update where z_L informs z_H - Total structure: 3 H-cycles \u00d7 (6 L-cycles + 1 H-update) = 3 \u00d7 7 = 21 passes</p> <p>Why two states? Why not one, or three, or ten?</p> <p>At first I thought this seemed kind of arbitrary. But it makes sense when you think about what each state actually does.</p> <p>z_H is your commitment. It's the model's current answer draft. Stop the model at any point and ask \"what's your solution?\" - that's z_H. It only updates 3 times because you don't want to thrash around with constant commitment changes. That'd be unstable.</p> <p>z_L is your scratch paper. This is where the model tests ideas, checks constraints, explores possibilities - without worrying about maintaining a complete solution. It updates 18 times because exploration should be cheap and frequent. Try lots of stuff, commit to little.</p> <p>And here's the thing - if you only had one state, every exploration would overwrite your answer. With two states, you can freely explore in z_L while keeping your stable solution in z_H. It's actually kind of elegant.</p> <p>This design solves a fundamental problem in iterative reasoning: how do you explore possibilities without losing your current best answer? If you only had one state, each exploration would overwrite your previous solution. With two states, you can explore freely in z_L while keeping your stable solution in z_H.</p> <p>Connection to HRM's hierarchical interpretation:</p> <p>The predecessor paper (HRM) had all these biological arguments about the brain operating at different temporal frequencies. Honestly? TRM just... simplifies all that. It's not about hierarchy or biology. It's about having one place for stable answers and another place for messy exploration.</p> <p>The paper actually tested using more states. With 7 states (one per recursion level), accuracy drops to 77.6%. With just one state, it drops to 71.9%. Two states hits 87.4%. Sweet spot confirmed.</p> <p>What's actually in these states?</p> <p>Here's something cool - if you decode z_H back through the reverse embedding, you can literally see the current Sudoku grid. It's interpretable. Real numbers in real cells.</p> <p>z_L though? It's not directly interpretable. It's a latent representation - weird patterns of activation that somehow encode constraints and logical relationships. The model figures out what to put there during training. But the paper shows (Figure 6) that z_L definitely contains different information than z_H. It's not just a copy - it's genuinely doing its own thing.</p> <p>This separation is what makes TRM more than just a recursive model. It's a model with an explicit internal reasoning process.</p>"},{"location":"AIBreakDown/TRM/#the-21-pass-structure","title":"The 21-Pass Structure","text":"<p>Let's zoom out and see how those L-cycles and H-cycles combine into the complete architecture.</p> <p></p> <p>The structure is elegant in its simplicity:</p> <p>Big picture: 3 H-cycles, each containing 7 passes, total 21 passes through the network. The same 2-layer transformer is used for all 21 passes.</p> <p>Inside each H-cycle: - 6 L-cycle passes: Network processes [input, z_H, z_L] \u2192 updates only z_L - 1 H-update pass: Network processes [input, z_H, z_L] \u2192 updates only z_H - Total: 7 passes per H-cycle</p> <p>Training trick - gradient flow: Only H-Cycle 2 (the final one, passes 15-21) receives gradients during training. H-Cycles 0 and 1 (passes 1-14) run forward-only as \"warmup.\"</p> <p>Why this design solves a critical problem:</p> <p>The naive approach would be to backpropagate through all 21 passes. But this creates a massive memory problem. Each pass requires storing activations for backpropagation. With 21 passes, you'd need 21\u00d7 the memory of a single forward pass. For comparison, training GPT-3 sized models already pushes memory limits.</p> <p>TRM's solution is \"deep supervision\" - supervise the output after deep processing, but only backpropagate through the final cycle. The first two H-cycles (14 passes) run in \"warmup\" mode. They update z_L and z_H, but those updates don't receive gradients. Think of them as preprocessing the problem before the model starts learning.</p> <p>Then in the final H-cycle (7 passes), gradients flow backwards through all operations. The model learns how to take a preprocessed state and refine it to the solution.</p> <p>This gives you: - Memory savings: 3\u00d7 reduction (7 passes with gradients vs 21) - Effective depth: The model still gets 21 passes of iterative refinement - Better exploration: Early passes can explore without gradient-driven constraints</p> <p>The effective depth here is 42 layers (2 transformer layers \u00d7 21 passes). Compare this to typical transformers: GPT-2 has 12-48 layers, GPT-3 has 96 layers. TRM achieves similar effective depth with 6.8M parameters instead of billions.</p>"},{"location":"AIBreakDown/TRM/#inside-the-transformer-blocks","title":"Inside the Transformer Blocks","text":"<p>We keep saying \"2 transformer blocks,\" but what's actually inside them? And why only 2 layers instead of the typical 4, 12, or more?</p> <p></p> <p>The visualization breaks down the architecture of each block:</p> <p>Attention mechanism (for TRM-Att variant): - 8 attention heads, each with 64 dimensions (8 \u00d7 64 = 512 total) - Query-Key-Value projections: 786K parameters - RoPE (Rotary Position Embeddings) handles positional information - Attention is non-causal (each token can attend to all tokens, not just previous ones)</p> <p>Why RoPE instead of learned positional embeddings? RoPE encodes position as rotation in the embedding space, which generalizes better to different sequence lengths and requires no additional parameters.</p> <p>MLP (feed-forward network): - SwiGLU activation function (a gated variant more powerful than ReLU) - Gate-up projection: 1.57M parameters (expands 512 dim \u2192 2048 dim) - Down projection: 0.79M parameters (compresses 2048 dim \u2192 512 dim) - RMSNorm for layer normalization (simpler, slightly faster than LayerNorm)</p> <p>Total per block: About 3.4M parameters (1M for attention + 2.4M for MLP)</p> <p>Two blocks stacked: 6.8M parameters total</p> <p>These blocks get reused 21 times, creating 42 effective layers from just 6.8M physical parameters.</p> <p>Why 2 layers is optimal:</p> <p>The ablation study shows something kind of mind-blowing: 4-layer blocks get 79.5% accuracy. 2-layer blocks get 87.4%. Wait, what? Smaller is better?</p> <p>Yeah. With only 1,000 training examples (even with augmentations), 4-layer blocks are just too big. They start memorizing specific puzzles instead of learning how Sudoku actually works. The 2-layer blocks don't have enough capacity to memorize, so they're forced to actually learn the rules.</p> <p>This connects to a broader principle: when data is limited, smaller models trained longer beat larger models trained less. And TRM trains for about 1 million optimizer steps. That's a LOT for a 7M parameter model. But it works.</p> <p>TRM-MLP variant:</p> <p>Interestingly, the paper also presents a variant called TRM-MLP that replaces self-attention with an MLP that operates on the sequence dimension. This works well for Sudoku (87.4% vs 74.7% for attention-based), but poorly on ARC-AGI and mazes. The lesson: for highly structured, fixed-size grids, position-wise MLPs can be more efficient than attention. But attention is needed when the task structure varies.</p>"},{"location":"AIBreakDown/TRM/#deep-supervision-memory-efficient-training","title":"Deep Supervision: Memory-Efficient Training","text":"<p>Here's a clever training strategy that makes TRM practical. This section connects deeply to the 21-pass structure, so let's see how gradient flow works in detail.</p> <p></p> <p>The visualization shows three H-cycles with <code>detach()</code> operations between them - these are the points where gradient flow is broken.</p> <p>H-Cycles 0 &amp; 1 (grayed out): These run forward-only. The model processes the input, updates z_L and z_H, but doesn't compute or store gradients. In PyTorch terms, these operations happen inside a <code>with torch.no_grad():</code> block. No memory cost for storing activations, no computational cost for backpropagation.</p> <p>H-Cycle 2 (highlighted): Full backpropagation happens here. Gradients flow backward through all 7 passes (6 L-cycles + 1 H-update), updating the transformer weights based on the final loss.</p> <p>The benefits: - Memory: 3\u00d7 reduction - store activations for 7 passes instead of 21 - Speed: Backprop is the expensive part of training. By only backpropping through 7 passes, training is roughly 2\u00d7 faster than full backprop through 21 passes. - Quality: Early cycles explore without gradient constraints, then the final cycle learns from that exploration</p> <p>This is called \"deep supervision\" because you're supervising the output after deep recursive processing (21 passes), but only backpropagating through the final cycle.</p> <p>Connection to HRM and the IFT controversy:</p> <p>HRM (TRM's predecessor) used something called the \"Implicit Function Theorem with 1-step gradient approximation.\" The idea was that if the recursive process converges to a fixed point, you can approximate gradients through all passes by only backpropping through the last step.</p> <p>But there's a problem: HRM never actually verified that a fixed point is reached. The paper shows (Figure 3 in Wang et al. 2025) that residuals remain non-zero even after many passes. Using IFT without convergence is theoretically questionable.</p> <p>TRM's approach is simpler and more honest: don't try to approximate gradients through early passes. Just run them forward-only, and fully backprop through the final passes where you actually want to learn. This is more memory-efficient than HRM's approach (which required storing some activations from early passes for the 1-step approximation), and empirically works better (87.4% vs 55% on Sudoku-Extreme).</p> <p>An interesting connection to deep equilibrium models:</p> <p>This approach is reminiscent of Deep Equilibrium Models (DEQ), which solve for fixed points and backprop through them implicitly. But TRM doesn't try to reach equilibrium - it explicitly performs iterative refinement. The warmup cycles effectively give the model a \"head start\" before training kicks in.</p>"},{"location":"AIBreakDown/TRM/#adaptive-computation-matching-effort-to-difficulty","title":"Adaptive Computation: Matching Effort to Difficulty","text":"<p>During training, TRM learns to adapt how much computation it uses based on problem difficulty. This is purely a training optimization - it doesn't affect inference.</p> <p></p> <p>The visualization contrasts: - Easy problems: 2-4 supervision steps might be enough to reach correct solution - Hard problems: Need 12-16 supervision steps to solve</p> <p>The Q-halt mechanism tracks confidence: starting at -4.2 (uncertain, keep going) and increasing to +0.5 (confident, can stop).</p> <p>What is ACT (Adaptive Computation Time)?</p> <p>Without ACT, training would run all N_sup=16 supervision steps for every example. But many examples are easy - after 2-3 steps, the model already has the right answer. Running 13 more steps wastes computation.</p> <p>ACT learns to predict when to stop. The model has an additional head (separate from the main output) that predicts a \"halting probability\" after each supervision step. If the halt probability exceeds a threshold, training moves to the next example.</p> <p>TRM's simpler approach vs HRM:</p> <p>HRM used Q-learning for ACT, which required two forward passes per training step: 1. Current pass: predict answer and halt value 2. Extra pass: predict next-step halt value (for the Q-learning \"continue\" loss)</p> <p>TRM simplifies this to binary classification: after each supervision step, predict whether the current answer is correct. Train with binary cross-entropy loss against the ground truth. Only one forward pass needed.</p> <p>In practice on Sudoku-Extreme, this reduces average supervision steps from 16 to under 2 during training. The model spends more iterations on hard examples (which is good - they need more training), while breezing through easy examples (which is efficient - they don't need more training).</p> <p>Important note: ACT is only used during training to be more efficient. At inference time, TRM always runs the full 16 supervision steps to ensure consistent, maximum performance. There's no early stopping at test time.</p>"},{"location":"AIBreakDown/TRM/#watching-trm-solve-a-puzzle","title":"Watching TRM Solve a Puzzle","text":"<p>Let's see everything come together in one end-to-end demonstration. This visualization connects all the concepts we've covered.</p> <p></p> <p>The visualization shows a complete forward pass through TRM:</p> <p>Initial state: A 70% filled Sudoku grid (about 25 cells given, 56 cells to be solved) with randomly initialized z_H and z_L tensors. The input grid is tokenized and embedded. z_H is initialized with a learned embedding that roughly represents \"no answer yet.\" z_L is initialized similarly.</p> <p>Warmup phase - H-Cycles 0 and 1: Watch the blue (z_L) and red (z_H) tensors pulse as they're updated. During these 14 passes: - z_L updates 12 times (6 per H-cycle), building up reasoning about constraints and possibilities - z_H updates 2 times (once per H-cycle), gradually forming a draft solution - No gradients flow - this is pure forward inference</p> <p>After the warmup, z_H already contains a rough solution (though possibly with errors), and z_L contains accumulated reasoning.</p> <p>Final phase - H-Cycle 2: The supervised cycle, where learning happens. Another 7 passes refine both states: - z_L gets 6 more updates, refining reasoning - z_H gets 1 final update, producing the final answer - Gradients flow backward through all 7 operations</p> <p>Final state: The tensors turn green, indicating successful completion. z_H now contains the complete, correct solution. If you were to decode it through the reverse embedding, you'd get all 81 cells filled correctly.</p> <p>This is the 87.4% accuracy in action - taking a partially filled puzzle and reasoning through to the complete solution through 21 iterative passes.</p> <p>What's happening under the hood:</p> <p>During the L-cycles, the model is effectively checking constraints: \"If cell (3,4) is 7, then cell (3,7) can't be 7, which means...\" This constraint propagation happens in the latent space of z_L.</p> <p>During the H-updates, the model commits to answers: \"Based on all the reasoning in z_L, cell (3,4) should be 7, and cell (3,7) should be 2.\"</p> <p>The iterative structure lets the model fix mistakes. If an early H-update sets cell (3,4) to the wrong value, subsequent L-cycles can detect the inconsistency, and the next H-update can correct it.</p> <p>This iterative refinement - think, answer, think more, refine answer - is fundamentally different from how LLMs work (generate answer autoregressively, no refinement). That's why TRM succeeds where LLMs fail on Sudoku.</p>"},{"location":"AIBreakDown/TRM/#results-how-well-does-it-work","title":"Results: How Well Does It Work?","text":"<p>Let's look at the numbers across different benchmarks, with proper context for each result:</p> Benchmark TRM HRM Others Sudoku-Extreme 87.4% (TRM-MLP) 55% 0% (GPT-4, Claude, DeepSeek R1) ARC-AGI-1 44.6% (TRM-Att) 40.3% 21% (direct prediction) ARC-AGI-2 7.8% (TRM-Att) 5.0% 4.9% (Gemini 2.5 Pro) Maze-Hard 85.3% (TRM-Att) 74.5% - <p>Understanding these benchmarks:</p> <p>Sudoku-Extreme: A dataset of extremely difficult Sudoku puzzles where most given cells are at the minimum (17 givens for 9\u00d79 Sudoku). Only 1,000 training examples are used, but tested on 423,000 examples. The fact that TRM trained on 1K examples generalizes to 423K test cases shows remarkable generalization.</p> <p>ARC-AGI-1 and ARC-AGI-2: The Abstraction and Reasoning Corpus is a benchmark designed to test abstract reasoning - the kind humans excel at but AI struggles with. Each task shows 2-3 input-output examples of a transformation rule (like \"rotate 90 degrees then change red to blue\"), and the model must apply that rule to new inputs. ARC-AGI-2 (released in 2025) is significantly harder than ARC-AGI-1.</p> <p>For context, human performance on ARC-AGI-1 is around 85%. TRM achieves 44.6%, which is impressive for a 7M parameter model. The best LLM results (with heavy test-time compute) reach 37-67% depending on the model and compute budget.</p> <p>Maze-Hard: 30\u00d730 mazes where the shortest path exceeds 110 steps. Both training and test sets have only 1,000 mazes each. This tests whether the model can learn spatial reasoning and pathfinding from limited data.</p> <p>Why TRM-MLP works better for Sudoku:</p> <p>TRM-MLP replaces self-attention with an MLP operating on the sequence dimension. For Sudoku, the grid structure is fixed (always 9\u00d79), and spatial relationships are predetermined (rows, columns, boxes). An MLP can hardcode these spatial relationships through learned weights. Attention is more flexible but less efficient for this specific structure.</p> <p>For ARC-AGI and mazes, grid sizes vary (up to 30\u00d730) and spatial relationships are task-dependent. Here, attention's flexibility is needed, which is why TRM-Att performs better.</p> <p>Comparison to HRM:</p> <p>The most meaningful comparison is to HRM, since both target the same problems with similar approaches. TRM consistently outperforms: - Sudoku: 87.4% vs 55% (59% relative improvement) - ARC-AGI-1: 44.6% vs 40.3% (11% improvement) - ARC-AGI-2: 7.8% vs 5.0% (56% improvement) - Maze: 85.3% vs 74.5% (14% improvement)</p> <p>All while using 3.9\u00d7 fewer parameters (7M vs 27M).</p> <p>The training setup:</p> <p>These results come from training on limited data with heavy augmentation: - Sudoku-Extreme: 1,000 base examples \u00d7 1,000 augmentations (rotations, reflections, number shuffling) = ~1M training examples - ARC-AGI: 800 training tasks + 400 evaluation tasks (used for training) + 160 ConceptARC tasks = 1,360 tasks \u00d7 3 examples per task \u00d7 1,000 augmentations = ~4M training examples - Maze: 1,000 mazes \u00d7 8 dihedral transformations = 8,000 training examples</p> <p>Training runs for about 1 million optimizer steps (roughly 2 days on 4\u00d7 H100 GPUs). The extensive training on small data with heavy augmentation is what enables generalization.</p>"},{"location":"AIBreakDown/TRM/#why-trm-works-key-insights","title":"Why TRM Works: Key Insights","text":"<p>Several architectural choices combine to make TRM effective:</p> <p>Weight reuse forces compression. By using the same 6.8M parameters 21 times, the network must learn general reasoning operations. There's no room for memorizing specific patterns.</p> <p>Dual latent states separate reasoning from refinement. z_L provides 18 updates of scratch space for exploration. z_H provides 3 stable updates toward the final answer. This mirrors how humans think through problems.</p> <p>Deep supervision saves memory while maintaining quality. Training only the final H-cycle reduces memory 3\u00d7, while early cycles still contribute via forward-only warmup.</p> <p>2 layers &gt; 4 layers for small datasets. Experiments show that with limited data (like 1000 Sudoku examples), 2-layer blocks outperform 4-layer blocks. Fewer parameters prevent overfitting.</p>"},{"location":"AIBreakDown/TRM/#compared-to-hrm","title":"Compared to HRM","text":"<p>TRM simplifies and improves upon HRM:</p> Feature HRM TRM Improvement Parameters 27M 7M 3.9\u00d7 reduction Networks 2 separate (f_L, f_H) 1 unified Simpler Layers per block 4 2 Less overfitting Gradient flow 1-step approximation Full backprop Better training ACT mechanism Q-learning (2 passes) Binary classification (1 pass) More efficient Sudoku accuracy 55% 87.4% 59% improvement <p>The takeaway: simplification improved performance. Fewer networks, fewer layers, simpler training - all led to better results.</p>"},{"location":"AIBreakDown/TRM/#want-to-try-it-yourself","title":"Want to Try It Yourself?","text":"<p>The complete implementation is available in the TinyRecursiveModels GitHub repository by Samsung SAIL Montreal.</p> <p>Key files to explore: - <code>models/recursive_reasoning/trm.py</code>: Core TRM architecture implementing the dual latent states (z_H and z_L), L-cycles, H-cycles, and deep supervision - <code>models/recursive_reasoning/hrm.py</code>: HRM implementation for comparison - <code>pretrain.py</code>: Training script with deep supervision, ACT mechanism, and EMA - <code>puzzle_dataset.py</code>: Dataset handling, tokenization, and augmentation strategies - <code>models/layers.py</code>: Transformer building blocks (attention, SwiGLU, RoPE)</p> <p>Paper and additional resources: - TRM Paper: \"Less is More: Recursive Reasoning with Tiny Networks\" (arXiv 2510.04871) - full technical details, ablation studies, and theoretical analysis - HRM Paper: \"Hierarchical Reasoning Model\" (arXiv 2506.21734) - TRM's predecessor - Nature Article: \"'Tiny' AI model beats massive LLMs at logic test\" - external coverage validating the results - ARC-AGI Benchmark: arcprize.org - details on the benchmark and leaderboard - Educational Video: YouTube walkthrough by ARC-AGI researcher (referenced in this post) - explains TRM concepts from a competition perspective</p> <p>Training requirements:</p> <p>If you want to replicate the paper's results: - Compute: 4\u00d7 H100 GPUs for about 2 days (or 8\u00d7 A100s for ~60 hours) - Memory: Each GPU needs ~40GB VRAM for batch size 32 - Dataset: Training data is generated programmatically (code's in the repo)</p> <p>For smaller-scale experiments, you can train on Sudoku with a single A100 or 4090 in about 6-8 hours if you reduce the batch size. Still totally doable.</p>"},{"location":"AIBreakDown/TRM/#what-this-actually-means","title":"What This Actually Means","text":"<p>Look, I think TRM is impressive. But let's be honest about what it is and isn't. It shows that for certain types of problems - structured reasoning with limited data - clever architecture beats raw parameter scaling. But there are caveats.</p> <p>Where TRM excels:</p> <p>The dual latent states (z_H for answers, z_L for reasoning) enable progressive problem-solving similar to human cognition. We work through possibilities before committing to answers. This design works exceptionally well for problems where: - The solution space is well-defined (complete Sudoku grids, valid maze paths) - Iterative refinement is natural (fix mistakes, check constraints) - Solutions can be verified locally (cell-by-cell checking) - Training data is limited but augmentable</p> <p>The \"less is more\" insight:</p> <p>Weight reuse creates deep effective networks (42 layers) without the parameter cost (7M). Forcing the same small network to handle all 21 passes means it learns general reasoning operations rather than task-specific patterns. This is regularization through architectural constraint.</p> <p>But this only works when the data regime is small. With millions of diverse examples, larger models would likely perform better. TRM's strength is specifically in the low-data, structured-reasoning regime where most models overfit.</p> <p>Limitations and stuff that's still unclear:</p> <p>TRM is NOT a general-purpose model. It's not going to replace LLMs. It's designed for specific types of problems. Here's what you need to know:</p> <ul> <li>Grid-based only: The whole tokenization assumes 2D grids. You can't just throw natural language at this. Different problem = different architecture needed.</li> <li>One solution per problem: TRM predicts a single deterministic solution. Creative tasks? Open-ended generation? Not happening without major changes.</li> <li>Compute trade-off: Yeah, it has fewer parameters. But 21 forward passes means you're doing 21\u00d7 more compute at inference than a single-pass model. Training takes 2 days on 4\u00d7 H100s. That's not nothing.</li> <li>Task-specific: A trained TRM is specialized. Unlike GPT-4 which handles everything from code to poetry, TRM does one thing well.</li> </ul> <p>Open questions that remain: - How does this generalize beyond grid-based puzzles? Could similar principles apply to code generation, mathematical proofs, or scientific reasoning? - What's the optimal recursion depth for different problems? The paper uses 21 passes, but is this optimal for all tasks? - Could these ideas integrate with larger language models? A hybrid system with LLM-based reasoning (z_L) and structured answer generation (z_H) might combine strengths of both approaches. - Does the dual-state idea apply to other domains? Medical diagnosis might benefit from separate \"hypothesis\" and \"evidence\" states that update at different rates.</p> <p>The broader significance:</p> <p>TRM offers evidence that we don't always need bigger models. When data is limited and the problem is structured, smaller models with thoughtful architectures can outperform giants. This matters for: - Research labs with limited compute: You don't need hundreds of GPUs to do important AI research - Edge deployment: 7M parameters can run on phones and embedded devices - Environmental impact: Smaller models use less energy for training and inference - Scientific understanding: Simpler models are easier to analyze and understand</p> <p>The lesson here isn't \"small models are always better.\" They're not. The lesson is \"match your architecture to your problem.\" For grid-based reasoning with 1K examples? TRM is near-optimal. For general text understanding with billions of examples? LLMs win.</p> <p>TRM shows us that the future of AI isn't just \"make it bigger.\" It's about understanding what you're trying to solve and designing something that actually fits that problem. That's true whether you've got hundreds of GPUs or just one.</p> <p>And honestly? I think that's a more interesting direction than just throwing more compute at everything.</p> <p>Further Reading &amp; Sources:</p> <p>Primary Sources: - TRM Paper: \"Less is More: Recursive Reasoning with Tiny Networks\" - Jolicoeur-Martineau et al., arXiv 2510.04871 - TinyRecursiveModels GitHub Repository - Official implementation by Samsung SAIL Montreal - HRM Paper: \"Hierarchical Reasoning Model\" - Wang et al., arXiv 2506.21734 (TRM's predecessor)</p> <p>Benchmarks &amp; Competitions: - ARC-AGI Benchmark - Competition homepage and leaderboard - ARC-AGI Prize Analysis - Technical analysis of HRM performance</p> <p>External Coverage: - Nature Article on TRM - \"'Tiny' AI model beats massive LLMs at logic test\" - Educational YouTube Walkthrough - Deep dive by ARC-AGI researcher</p> <p>Related Work: - Chinchilla Scaling Laws Paper - Optimal model sizing principles - Deep Equilibrium Models - Related work on fixed-point recursion - Test-Time Compute Scaling - Alternative approach to improving reasoning</p> <p>This is part of AI Engineering Academy's AI Breakdown series, where I take important research papers and break them down with visualizations and (hopefully) clear explanations. I created all the visualizations for this post using Remotion - check them out throughout the article.</p> <p>If you found this useful, we cover more topics like this at AI Engineering Academy. We're building a place for practical AI engineering knowledge, not just theory.</p>"},{"location":"Agents/","title":"Overview","text":""},{"location":"Agents/#ai-agents-engineering-guide","title":"AI Agents Engineering Guide","text":"<p>Welcome to the AI Agents section of AI Engineering Academy! This module explores the fascinating world of AI agents, from fundamental patterns to practical implementations. Learn how to create, orchestrate, and deploy intelligent agents that can perform complex tasks and reason about their environment.</p>"},{"location":"Agents/#repository-structure","title":"Repository Structure","text":"Category Component Description Patterns Reflection Pattern Self-evaluation and improvement mechanisms Tool Pattern Tool usage and integration frameworks Planning Pattern Strategic decision-making and task planning Multiagent Pattern Implementing collaborative agent systems Projects Multi-document Agents Practical implementation with document processing"},{"location":"Agents/#core-patterns","title":"Core Patterns","text":""},{"location":"Agents/#1-reflection-and-learning","title":"1. Reflection and Learning","text":"<p>Implement self-improvement mechanisms for more capable agents.</p> <ul> <li>Performance self-evaluation</li> <li>Strategy adaptation</li> <li>Learning from experience</li> <li>Error recovery</li> <li>Continuous improvement loops</li> </ul>"},{"location":"Agents/#2-tool-usage","title":"2. Tool Usage","text":"<p>Develop agents that can effectively utilize external tools and APIs.</p> <ul> <li>Tool selection logic</li> <li>API integration patterns</li> <li>Error handling</li> <li>Resource management</li> <li>Tool chain orchestration</li> </ul>"},{"location":"Agents/#3-planning-and-strategy","title":"3. Planning and Strategy","text":"<p>Master strategic decision-making and task planning for autonomous agents.</p> <ul> <li>Goal decomposition</li> <li>Action sequence planning</li> <li>Resource allocation</li> <li>Risk assessment</li> <li>Adaptive planning strategies</li> </ul>"},{"location":"Agents/#4-multi-agent-systems","title":"4. Multi-Agent Systems","text":"<p>Learn to implement collaborative AI systems where multiple agents work together to achieve complex goals.</p> <ul> <li>Agent communication protocols</li> <li>Task distribution and coordination</li> <li>Conflict resolution mechanisms</li> <li>Collaborative problem-solving</li> <li>Emergent behavior management</li> </ul>"},{"location":"Agents/#practical-projects","title":"Practical Projects","text":""},{"location":"Agents/#multi-document-agents","title":"Multi-Document Agents","text":"<p>An implementation showcase for handling multiple documents:</p> <ul> <li>Concurrent document processing</li> <li>Information extraction</li> <li>Cross-reference analysis</li> <li>Content summarization</li> <li>Knowledge synthesis</li> </ul>"},{"location":"Agents/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"Agents/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Agent Design</p> <pre><code> - Clear responsibility definition\n - Robust error handling\n - Efficient resource usage\n - Scalable architecture\n</code></pre> </li> <li> <p>System Integration</p> <pre><code> - API standardization\n - Communication protocols\n - Security considerations\n - Performance optimization\n</code></pre> </li> <li> <p>Testing and Validation</p> <pre><code> - Unit testing strategies\n - Integration testing\n - Performance benchmarking\n - Behavior validation\n</code></pre> </li> </ol>"},{"location":"Agents/#learning-path","title":"\ud83d\udcda Learning Path","text":"<ol> <li>Start with individual pattern notebooks</li> <li>Combine patterns in simple scenarios</li> <li>Implement the multi-document project</li> <li>Develop custom agent systems</li> </ol>"},{"location":"Agents/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please follow these steps:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Implement your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"Agents/#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p> Build smarter agents, create better AI systems!    Made with \u2764\ufe0f by the AI Engineering Academy Team"},{"location":"Agents/MCP/BegineersGuideToMCP/","title":"Beginners Guide to MCP","text":""},{"location":"Agents/MCP/BegineersGuideToMCP/#what-is-mcp-and-its-background","title":"What is MCP and Its Background?","text":"<p>MCP, or Model Context Protocol, is an open standard that helps AI applications, especially large language models (LLMs), connect with external data sources and tools. Think of it like a universal adapter for AI, making it easier for systems like chatbots or coding assistants to access files, APIs, or databases without custom setups for each. It was introduced by Anthropic, a company focused on AI, around November 2024, to solve the problem of AI being isolated from data, which often limits its usefulness.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#how-to-use-it-and-why-it-matters","title":"How to Use It and Why It Matters","text":"<p>You can use MCP with tools like Cursor, an AI-powered code editor, and Claude, an AI model by Anthropic, by setting up MCP servers within their applications. For example, in Claude Desktop, you edit a configuration file to add servers, while in Cursor, you go to the MCP settings to add new servers. This setup lets AI perform tasks like reading files or querying databases directly.</p> <p>MCP is important because it breaks down data silos, making AI more connected and efficient. It allows developers to build smarter AI systems that scale better, which is especially helpful in fields like software development or data analysis. However, its adoption is still in early stages, with some controversy around how widely it's supported across different platforms.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#list-of-open-source-servers-and-building-your-own","title":"List of Open-Source Servers and Building Your Own","text":"<p>There are several open-source MCP servers you can use, such as:</p> <ul> <li>Python SDK (Model Context Protocol Python SDK)</li> <li>ChatSum, for summarizing chat messages</li> <li>Chroma, for semantic document search</li> <li>ClaudePost, for Gmail management</li> </ul> <p>To build your own MCP server, start by checking the official documentation at Model Context Protocol Introduction. It guides you through using SDKs in languages like Python or Java, defining what your server does, and testing it with clients like Claude Desktop. This process might require some coding knowledge, but it's designed to be accessible with the right resources.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#survey-note-comprehensive-analysis-of-model-context-protocol","title":"Survey Note: Comprehensive Analysis of Model Context Protocol","text":"<p>This section provides a detailed exploration of the Model Context Protocol (MCP), covering its definition, origin, functionality, importance, available open-source servers, integration with Cursor and Claude, and a step-by-step guide for building your own server. The analysis is based on recent online resources, reflecting the state as of February 25, 2025, and aims to offer a professional, thorough overview for readers interested in AI integration.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#understanding-mcp-definition-and-origin","title":"Understanding MCP: Definition and Origin","text":"<p>MCP, or Model Context Protocol, is an open protocol designed to standardize how applications provide context to large language models (LLMs). It acts as a universal interface, likened to a USB-C port for AI, enabling seamless connections to data sources and tools. This standardization addresses the challenge of AI models being isolated from data, trapped behind information silos and legacy systems, as noted in Anthropic's introduction (Introducing the Model Context Protocol | Anthropic).</p> <p>The protocol was introduced by Anthropic, PBC, on November 24, 2024, as an open-source initiative to simplify AI integrations. Its development was motivated by the need for a universal standard to replace fragmented, custom implementations, allowing developers to focus on building smarter, scalable AI systems. This origin is detailed in community discussions and official documentation, such as Getting Started: Model Context Protocol | Medium, highlighting its early adoption by companies like Block and Apollo.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#functionality-what-mcp-does","title":"Functionality: What MCP Does","text":"<p>MCP operates on a client-server architecture, where MCP hosts (e.g., Claude Desktop, IDEs, or AI tools) connect to MCP servers that expose specific capabilities. These servers can provide:</p> <ul> <li>Prompts: Pre-defined templates guiding LLM interactions.</li> <li>Resources: Structured data or content for additional context.</li> <li>Tools: Executable functions for actions like fetching data or executing code.</li> </ul> <p>This is outlined in the specification (Server Features \u2013 Model Context Protocol Specification), which details how servers enable rich interactions. For instance, MCP allows AI to access local files, query databases, or integrate with APIs, enhancing real-time data access and workflow automation. Its flexibility is evident in supporting multiple transports (e.g., stdio, sse) and a growing list of pre-built integrations, as seen in Introduction - Model Context Protocol.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#importance-why-mcp-matters","title":"Importance: Why MCP Matters","text":"<p>MCP is crucial for breaking down data silos, a significant barrier in AI development. By providing a standardized way to connect AI with data, it enhances scalability and efficiency, reducing the need for custom integrations. This is particularly valuable in enterprise settings, where AI needs to interact with content repositories, business tools, and development environments. Early adopters, including development tools like Zed and Replit, are integrating MCP to improve context-aware coding, as noted in Anthropic's announcement (Introducing the Model Context Protocol | Anthropic).</p> <p>Its importance also lies in security and flexibility. MCP follows best practices for securing data within infrastructure, ensuring controlled access, and allows switching between LLM providers without reconfiguring integrations. However, its adoption is still evolving, with some debate around support for remote hosts, currently in active development, as mentioned in For Server Developers - Model Context Protocol.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#list-of-open-source-mcp-servers","title":"List of Open-Source MCP Servers","text":"<p>Several open-source MCP servers are available, catering to various use cases. Below is a table summarizing key servers, based on community repositories and official listings:</p> Server Name Description Repository/Link Python SDK Official Python implementation for MCP servers/clients Model Context Protocol Python SDK ChatSum Summarizes chat messages using LLMs GitHub - modelcontextprotocol/servers Chroma Vector database for semantic document search GitHub - modelcontextprotocol/servers ClaudePost Enables email management for Gmail GitHub - modelcontextprotocol/servers Cloudinary Uploads media to Cloudinary and retrieves details GitHub - modelcontextprotocol/servers AWS S3 Fetches objects from AWS S3, e.g., PDF documents GitHub - modelcontextprotocol/servers Airtable Read/write access to Airtable databases GitHub - modelcontextprotocol/servers <p>This list is not exhaustive, and for a broader collection, refer to Awesome MCP Servers, which includes community-contributed servers like MCP-Zotero for Zotero Cloud integration and MCP-Geo for geocoding services.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#integration-with-cursor-and-claude","title":"Integration with Cursor and Claude","text":""},{"location":"Agents/MCP/BegineersGuideToMCP/#using-mcp-with-claude","title":"Using MCP with Claude","text":"<p>MCP integration with Claude is primarily through the Claude Desktop application. To use it:</p> <ol> <li>Ensure you have the latest Claude Desktop installed, available at Claude Desktop Downloads.</li> <li>Enable developer mode by opening Settings from the menu and navigating to the Developer option.</li> <li>Edit the claude_desktop_config.json file (located at ~/Library/Application Support/Claude/claude_desktop_config.json on macOS) to add MCP servers. For example, to add a filesystem server:</li> </ol> <pre><code>{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"@modelcontextprotocol/server-filesystem\"]\n    }\n  }\n}\n</code></pre> <ol> <li>Restart Claude Desktop to apply changes. The MCP tools will appear as icons (e.g., a hammer) in the input box, allowing interaction with server capabilities.</li> </ol> <p>This process is detailed in For Claude Desktop Users - Model Context Protocol, which also notes that MCP currently supports only desktop hosts, with remote hosts in development.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#using-mcp-with-cursor","title":"Using MCP with Cursor","text":"<p>Cursor, an AI-powered code editor by Anysphere, also supports MCP, enabling custom tool integration. To use MCP:</p> <ol> <li>Open Cursor and navigate to \"Features\" &gt; \"MCP\" in the settings.</li> <li>Click \"+ Add New MCP Server\" to configure a server, selecting the transport (e.g., stdio) and providing the command or URL.</li> <li>For example, to add a weather server, you might configure it with a command like npx /path/to/weather-server, as shown in Cursor \u2013 Model Context Protocol.</li> </ol> <p>MCP tools in Cursor are available in the Composer Agent, and users can prompt tool usage intentionally. This integration is still emerging, with community discussions on Cursor as an MCP client - Community Forum highlighting its potential for automating software development tasks.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#guide-to-building-your-own-mcp-server","title":"Guide to Building Your Own MCP Server","text":"<p>Building your own MCP server involves several steps, leveraging official SDKs and documentation. Here's a detailed guide:</p> <ol> <li>Understand the Protocol: Review the MCP specification at Specification \u2013 Model Context Protocol Specification, which covers server features like prompts, resources, and tools.</li> <li>Choose a Language and SDK: Use official SDKs, such as:</li> <li>Python: Model Context Protocol Python SDK</li> <li>Java: Model Context Protocol Java SDK</li> <li>Kotlin: Model Context Protocol Kotlin SDK</li> <li>Set Up the Project: Initialize your project with the chosen SDK. For Python, install via pip install modelcontextprotocol, and for Node.js, use npm install modelcontextprotocol/sdk.</li> <li>Define Server Capabilities: Implement server functions, such as:</li> <li>Resources: Expose data, e.g., fetching files.</li> <li>Tools: Define executable actions, e.g., sending emails.</li> <li>Prompts: Create templates for LLM interactions.</li> <li>Test Locally: Connect your server to a client like Claude Desktop. Configure the client as shown in For Server Developers - Model Context Protocol, which includes a tutorial for building a weather server.</li> <li>Deploy and Share: Once tested, deploy your server locally or remotely (note: remote hosts are in development). Consider contributing to the community via GitHub - modelcontextprotocol/servers.</li> </ol> <p>This process requires technical expertise, but the documentation provides examples, such as building a simple word counter tool, as seen in Getting MCP Server Working with Claude Desktop in WSL | Scott Spence.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#conclusion","title":"Conclusion","text":"<p>MCP represents a significant step forward in AI integration, offering a standardized approach to connect LLMs with data and tools. Its open-source nature, supported by a growing ecosystem of servers and community contributions, makes it a promising tool for developers. While integration with Cursor and Claude is feasible, its evolving nature suggests ongoing developments, particularly for remote host support. For those looking to extend MCP, building custom servers is accessible with official resources, ensuring a robust foundation for future AI applications.</p>"},{"location":"Agents/MCP/BegineersGuideToMCP/#key-points","title":"Key Points","text":"<ul> <li>MCP, or Model Context Protocol, is likely an open standard for connecting AI to data, developed by Anthropic, with research suggesting it enhances AI integration.</li> <li>It seems to have originated around November 2024 to address data connectivity challenges for LLMs.</li> <li>The evidence leans toward MCP enabling AI to access and interact with external data and tools securely.</li> <li>It appears important for breaking data silos and improving AI scalability, though its adoption is still evolving.</li> <li>There are open-source servers like Python SDK and Chroma, with ongoing community contributions.</li> <li>Using MCP with Cursor and Claude involves configuring servers in their respective apps, with details varying by platform.</li> <li>Building your own MCP server seems feasible with official documentation, but may require technical expertise.</li> </ul>"},{"location":"Agents/MCP/BegineersGuideToMCP/#key-citations","title":"Key Citations","text":"<ul> <li>Model Context Protocol Introduction</li> <li>Introducing the Model Context Protocol | Anthropic</li> <li>Getting Started: Model Context Protocol | Medium</li> <li>Server Features \u2013 Model Context Protocol Specification</li> <li>Model Context Protocol Python SDK</li> <li>GitHub - modelcontextprotocol/servers</li> <li>Awesome MCP Servers</li> <li>Claude Desktop Downloads</li> <li>For Claude Desktop Users - Model Context Protocol</li> <li>Cursor \u2013 Model Context Protocol</li> <li>Cursor as an MCP client - Community Forum</li> <li>Specification \u2013 Model Context Protocol Specification</li> <li>For Server Developers - Model Context Protocol</li> <li>Getting MCP Server Working with Claude Desktop in WSL | Scott Spence</li> <li>Model Context Protocol Java SDK</li> <li>Model Context Protocol Kotlin SDK</li> </ul>"},{"location":"Agents/MCP/CreateMCPServe/","title":"Build MCP Server","text":""},{"location":"Agents/MCP/CreateMCPServe/#beginners-guide-to-creating-an-mcp-server","title":"Beginners Guide to creating an MCP server","text":""},{"location":"Agents/MCP/CreateMCPServe/#beginners-guide-to-creating-an-mcp-server_1","title":"Beginners Guide to creating an MCP server","text":"<p>The Model Context Protocol (MCP) is an open standard that helps AI applications, especially large language models (LLMs), connect with external data sources and tools. By creating your own MCP server in Python, you can tailor these connections to suit your specific needs, such as accessing custom databases or files. This guide will walk you through the process step by step, including a practical implementation from scratch.</p>"},{"location":"Agents/MCP/CreateMCPServe/#installing-the-mcp-python-sdk","title":"Installing the MCP Python SDK","text":"<p>First, you need to install the MCP Python SDK, which provides the tools to build your server. You can do this using pip:</p> <pre><code>pip install mcp\n</code></pre> <p>Alternatively, if you're using the <code>uv</code> tool (recommended for Claude Desktop), run:</p> <pre><code>uv add \"mcp[cli]\"\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#creating-an-mcp-server-in-python-step-by-step-guide","title":"Creating an MCP Server in Python: Step-by-Step Guide","text":"<p>To create an MCP server in Python, developers use the official Python SDK, available at Model Context Protocol Python SDK. The SDK implements the full MCP specification, making it easy to build servers that expose resources, prompts, and tools. Here's a detailed guide:</p> <ol> <li>Installation:</li> <li>Install the SDK using <code>pip install mcp</code> or, for Claude Desktop, <code>uv add \"mcp[cli]\"</code>. The latter requires <code>uv</code>, which can be installed on macOS via <code>brew install uv</code>.</li> <li> <p>Basic Server Creation:</p> </li> <li> <p>Use the <code>FastMCP</code> class for high-level implementation. Create a Python script, e.g., <code>server.py</code>, and define the server:</p> <pre><code>from mcp import FastMCP\n\nmcp = FastMCP(\"My First Server\")\n</code></pre> </li> <li> <p>Add tools using <code>@mcp.tool()</code>, e.g.,:</p> <pre><code>@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n</code></pre> </li> <li> <p>Add resources using <code>@mcp.resource()</code>, e.g.,:</p> <pre><code>@mcp.resource(\"greeting\")\ndef get_greeting(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n</code></pre> </li> <li> <p>Running and Testing:</p> </li> <li>For development, run <code>mcp dev server.py</code>. This mode supports testing with MCP Inspector, started via <code>mcp inspector</code>, allowing interaction with the server.</li> <li>For Claude Desktop integration, install with <code>mcp install server.py</code>. This sets up the server to run when Claude Desktop launches, accessible in its MCP settings.</li> <li> <p>Advanced Features:</p> </li> <li> <p>Specify dependencies in <code>FastMCP</code>, e.g., <code>mcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])</code>. Install them in development with <code>mcp dev server.py --with pandas --with numpy</code>.</p> </li> <li> <p>Define prompts using <code>@mcp.prompt()</code>, e.g.,:</p> <pre><code>@mcp.prompt()\ndef review_code(code: str) -&gt; str:\n    return f\"Please review this code:\\\\n\\\\n{code}\"\n</code></pre> </li> <li> <p>For low-level control, use <code>Server(\"example-server\")</code> with lifespan management, as shown in examples like Echo Server and SQLite Explorer in the SDK.</p> </li> </ol>"},{"location":"Agents/MCP/CreateMCPServe/#creating-a-basic-mcp-server","title":"Creating a Basic MCP Server","text":"<p>To create a server, you'll use the <code>FastMCP</code> class from the SDK. Here's how:</p> <ol> <li>Define the Server: Create a Python script, say <code>server.py</code>, and import <code>FastMCP</code>. Give your server a name:</li> </ol> <pre><code>from mcp import FastMCP\n\nmcp = FastMCP(\"My First Server\")\n</code></pre> <ol> <li>Add Tools: Tools are functions the LLM can call, defined with <code>@mcp.tool()</code>. For example, a tool to add two numbers:</li> </ol> <pre><code>@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n</code></pre> <ol> <li>Add Resources: Resources provide data, defined with <code>@mcp.resource()</code>. For example, a greeting resource:</li> </ol> <pre><code>@mcp.resource(\"greeting\")\ndef get_greeting(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#running-and-testing-the-server","title":"Running and Testing the Server","text":"<ul> <li>Development Mode: Run the server for testing with:</li> </ul> <pre><code>mcp dev server.py\n</code></pre> <p>You can test it using MCP Inspector, a tool for interacting with your server:</p> <pre><code>mcp inspector\n</code></pre> <ul> <li>Claude Desktop Integration: To use the server in Claude Desktop, install it with:</li> </ul> <pre><code>mcp install server.py\n</code></pre> <p>It will then be available in Claude Desktop for the LLM to use.</p>"},{"location":"Agents/MCP/CreateMCPServe/#practical-implementation-from-scratch","title":"Practical Implementation from Scratch","text":"<p>Let's build a server that gets the current time and lists files in a directory:</p> <ol> <li>Create a file <code>time_and_files_server.py</code> with:</li> </ol> <pre><code>from mcp import FastMCP\nimport datetime\nimport os\n\nmcp = FastMCP(\"Time and Files Server\")\n\n@mcp.tool()\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time in ISO format\"\"\"\n    return datetime.datetime.now().isoformat()\n\n@mcp.resource(\"files_in_directory\")\ndef list_files(directory: str) -&gt; list[str]:\n    \"\"\"List files in the specified directory\"\"\"\n    return [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n</code></pre> <ol> <li>Run in development mode:</li> </ol> <pre><code>mcp dev time_and_files_server.py\n</code></pre> <ol> <li>Test with MCP Inspector, then install for Claude Desktop:</li> </ol> <pre><code>mcp install time_and_files_server.py\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#more-mcp-server-concept-and-examples","title":"More MCP Server Concept and Examples","text":"<p>Before diving into the examples, let\u2019s look at how MCP servers work in general:</p> <pre><code>sequenceDiagram\n    participant Client as AI Client (e.g., Claude)\n    participant Server as MCP Server\n\n    Client-&gt;&gt;Server: Discover available capabilities\n    Server--&gt;&gt;Client: List of resources, tools\n\n    Client-&gt;&gt;Server: Request resource (e.g., current time)\n    Server--&gt;&gt;Client: Provide current time\n\n    Client-&gt;&gt;Server: Call tool (e.g., add numbers)\n    Server--&gt;&gt;Client: Return result of addition\n</code></pre> <p>This diagram shows the basic flow: the AI client discovers what the server offers, then requests resources or calls tools as needed.</p> <p>Below is a beginner-friendly guide to understanding MCP servers by building three hands-on examples. The Model Context Protocol (MCP) is an open standard that allows AI applications, like Claude or Cursor, to connect to external data sources and tools through a standardized interface. An MCP server is a service that provides these capabilities\u2014such as data (resources) or actions (tools)\u2014to an AI client. In this guide, we\u2019ll create three simple MCP servers, each with code, explanations, and Mermaid workflow diagrams, plus an overall diagram for the MCP server concept.</p>"},{"location":"Agents/MCP/CreateMCPServe/#example-1-current-time-server-resource","title":"Example 1: Current Time Server (Resource)","text":"<p>This server provides a resource that returns the current date and time when requested.</p>"},{"location":"Agents/MCP/CreateMCPServe/#code","title":"Code","text":"<pre><code>from mcp import FastMCP\nimport datetime\n\nmcp = FastMCP(\"Current Time Server\")\n\n@mcp.resource(\"current_time\")\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current date and time\"\"\"\n    return datetime.datetime.now().isoformat()\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>sequenceDiagram\n    participant Client as AI Client\n    participant Server as MCP Server (Current Time)\n\n    Client-&gt;&gt;Server: Request \"current_time\" resource\n    Server--&gt;&gt;Client: Respond with current date and time\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#explanation","title":"Explanation","text":"<ul> <li>What it does: The server defines a resource called <code>\"current_time\"</code>.</li> <li>How it works: When the AI client requests this resource, the server runs the <code>get_current_time</code> function and returns the current date and time in ISO format (e.g., <code>\"2023-10-25T14:30:00.123456\"</code>).</li> <li>Why it\u2019s useful: This shows how an MCP server can provide data to an AI, like real-time information.</li> </ul>"},{"location":"Agents/MCP/CreateMCPServe/#example-2-addition-server-tool","title":"Example 2: Addition Server (Tool)","text":"<p>This server provides a tool that adds two numbers based on input from the client.</p>"},{"location":"Agents/MCP/CreateMCPServe/#code_1","title":"Code","text":"<pre><code>from mcp import FastMCP\n\nmcp = FastMCP(\"Addition Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#workflow-diagram_1","title":"Workflow Diagram","text":"<pre><code>sequenceDiagram\n    participant Client as AI Client\n    participant Server as MCP Server (Addition Tool)\n\n    Client-&gt;&gt;Server: Call \"add\" tool with a=3, b=4\n    Server--&gt;&gt;Client: Respond with 7\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#explanation_1","title":"Explanation","text":"<ul> <li>What it does: The server defines a tool called <code>\"add\"</code> that takes two integers (<code>a</code> and <code>b</code>) and returns their sum.</li> <li>How it works: The client sends a request to use the <code>\"add\"</code> tool with parameters (e.g., <code>a=3, b=4</code>), and the server computes and returns the result (<code>7</code>).</li> <li>Why it\u2019s useful: This demonstrates how an MCP server can perform actions for the AI, extending its capabilities beyond just providing data.</li> </ul>"},{"location":"Agents/MCP/CreateMCPServe/#example-3-file-management-server-resource-and-tool","title":"Example 3: File Management Server (Resource and Tool)","text":"<p>This server combines a resource (listing files) and a tool (creating a file) in a temporary directory, showing how multiple capabilities can work together.</p>"},{"location":"Agents/MCP/CreateMCPServe/#code_2","title":"Code","text":"<pre><code>from mcp import FastMCP\nimport os\nimport tempfile\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Using temporary directory: {temp_dir}\")\n\nmcp = FastMCP(\"File Management Server\")\n\n@mcp.resource(\"files_in_directory\")\ndef list_files() -&gt; list[str]:\n    \"\"\"List files in the temporary directory\"\"\"\n    return [file for file in os.listdir(temp_dir) if os.path.isfile(os.path.join(temp_dir, file))]\n\n@mcp.tool()\ndef create_file(file_name: str, content: str) -&gt; str:\n    \"\"\"Create a new file with the given content in the temporary directory\"\"\"\n    file_path = os.path.join(temp_dir, file_name)\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    return f\"File '{file_name}' created in temporary directory\"\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#workflow-diagram_2","title":"Workflow Diagram","text":"<pre><code>sequenceDiagram\n    participant Client as AI Client\n    participant Server as MCP Server (File Management)\n\n    Client-&gt;&gt;Server: Request \"files_in_directory\" resource\n    Server--&gt;&gt;Client: Respond with list of files\n\n    Client-&gt;&gt;Server: Call \"create_file\" tool with file_name=\"test.txt\", content=\"Hello\"\n    Server--&gt;&gt;Client: Confirm file creation\n</code></pre>"},{"location":"Agents/MCP/CreateMCPServe/#explanation_2","title":"Explanation","text":"<ul> <li>What it does:</li> <li>The <code>\"files_in_directory\"</code> resource returns a list of files in a temporary directory.</li> <li>The <code>\"create_file\"</code> tool creates a new file with specified content in that directory.</li> <li>How it works:</li> <li>The client can request the file list (e.g., initially empty <code>[]</code>).</li> <li>The client can then call <code>\"create_file\"</code> with parameters (e.g., <code>file_name=\"test.txt\", content=\"Hello\"</code>), and the server creates the file and confirms it.</li> <li>Why it\u2019s useful: This shows how an MCP server can combine data retrieval and actions, making it more versatile for AI applications.</li> </ul>"},{"location":"Agents/MCP/CreateMCPServe/#testing-your-servers","title":"Testing Your Servers","text":"<p>To try these servers yourself:</p> <ol> <li>Save the Code: Save each script as a <code>.py</code> file (e.g., <code>current_time_server.py</code>, <code>addition_server.py</code>, <code>file_management_server.py</code>).</li> <li> <p>Run the Server: Open a terminal and start the server in development mode:</p> <pre><code>mcp dev current_time_server.py\n</code></pre> </li> <li> <p>Interact with It: In another terminal, use the MCP Inspector:     Follow the prompts to request resources (e.g., <code>\"current_time\"</code>) or call tools (e.g., <code>\"add\"</code> with <code>a=3, b=4</code>).</p> <pre><code>mcp inspector\n\n    ```\n</code></pre> </li> </ol> <p>The inspector lets you see what the server offers and test its capabilities.</p> <p>This example shows how to create a server with both tools and resources, enhancing your AI's capabilities.</p>"},{"location":"Agents/MCP/CreateMCPServe/#survey-note-comprehensive-guide-to-creating-an-mcp-server-in-python-with-practical-implementation","title":"Survey Note: Comprehensive Guide to Creating an MCP Server in Python with Practical Implementation","text":"<p>This section provides a detailed exploration of creating a Model Context Protocol (MCP) server using Python, including practical implementation from scratch. The analysis is based on recent online resources, reflecting the state as of February 25, 2025, and aims to offer a professional, thorough overview for developers interested in AI integration.</p>"},{"location":"Agents/MCP/CreateMCPServe/#understanding-mcp-and-its-relevance","title":"Understanding MCP and Its Relevance","text":"<p>MCP, or Model Context Protocol, is an open protocol designed to standardize how applications provide context to large language models (LLMs). It acts as a universal interface, likened to a USB-C port for AI, enabling seamless connections to data sources and tools. Introduced by Anthropic, PBC, on November 24, 2024, MCP addresses the challenge of AI models being isolated from data, trapped behind information silos and legacy systems, as noted in Introducing the Model Context Protocol | Anthropic. Its development was motivated by the need for a universal standard to replace fragmented, custom implementations, allowing developers to build smarter, scalable AI systems.</p> <p>The protocol operates on a client-server architecture, where MCP hosts (e.g., Claude Desktop, IDEs) connect to MCP servers that expose specific capabilities. These servers can provide prompts, resources, and tools, as outlined in Specification \u2013 Model Context Protocol Specification. This standardization is crucial for breaking down data silos, enhancing scalability, and ensuring security, though its adoption is still evolving, with some debate around support for remote hosts, currently in active development, as mentioned in For Server Developers - Model Context Protocol.</p>"},{"location":"Agents/MCP/CreateMCPServe/#examples-and-community-contributions","title":"Examples and Community Contributions","text":"<p>The SDK includes examples like Echo Server and SQLite Explorer, available at Model Context Protocol Python SDK. Community servers, such as code-executor for Python code execution and mcp-alchemy for database access, provide further inspiration. These servers, listed in Awesome MCP Servers, show real-world applications, though some, like FastMCP by jlowin, are no longer maintained, as noted in GitHub - jlowin/fastmcp.</p>"},{"location":"Agents/MCP/CreateMCPServe/#challenges-and-considerations","title":"Challenges and Considerations","text":"<p>While creating an MCP server is straightforward, challenges include ensuring security, especially for remote hosts, which are still in development. The protocol's adoption is evolving, with some controversy around compatibility and support across platforms. Developers should refer to For Server Developers - Model Context Protocol for best practices and contribute to discussions at GitHub Discussions.</p>"},{"location":"Agents/MCP/CreateMCPServe/#conclusion","title":"Conclusion","text":"<p>Creating an MCP server in Python using the official SDK is accessible, with <code>FastMCP</code> simplifying the process. The practical implementation of a time and file listing server illustrates its potential, while community examples offer avenues for further exploration. As MCP adoption grows, developers can contribute to its ecosystem, enhancing AI integration and breaking data silos.</p>"},{"location":"Agents/MCP/CreateMCPServe/#key-citations","title":"Key Citations","text":"<ul> <li>Introducing the Model Context Protocol | Anthropic</li> <li>Specification \u2013 Model Context Protocol Specification</li> <li>For Server Developers - Model Context Protocol</li> <li>Model Context Protocol Python SDK</li> <li>Awesome MCP Servers</li> <li>GitHub - jlowin/fastmcp</li> <li>code-executor MCP Server</li> <li>mcp-alchemy MCP Server</li> <li>GitHub Discussions for Python SDK</li> </ul>"},{"location":"Agents/patterns/multiagent_pattern/","title":"Agent Multi-Agent Pattern","text":"<p>source : https://github.com/neural-maze/agentic_patterns</p> <p>You may have heard about frameworks like CrewAI or AutoGen, which allow you to create multi-agent applications.</p> <p>These frameworks implement different variations of the multi-agent pattern, in which tasks are divided into smaller subtasks executed by different roles (e.g. one agent can be a software engineer, another a project manager, etc.)</p> <p>For this final lesson, I wanted to build something more elaborate. That's why I've been working on a \ud835\udc26\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc26\ud835\udc1a\ud835\udc25\ud835\udc22\ud835\udc2c\ud835\udc2d \ud835\udc2f\ud835\udc1e\ud835\udc2b\ud835\udc2c\ud835\udc22\ud835\udc28\ud835\udc27 \ud835\udc28\ud835\udc1f \ud835\udc02\ud835\udc2b\ud835\udc1e\ud835\udc30\ud835\udc00\ud835\udc08, drawing inspiration from two of its key concepts: \ud835\udc02\ud835\udc2b\ud835\udc1e\ud835\udc30 and \ud835\udc00\ud835\udc20\ud835\udc1e\ud835\udc27\ud835\udc2d.</p> <p>Additionally, I've also borrowed ideas from \ud835\udc00\ud835\udc22\ud835\udc2b\ud835\udc1f\ud835\udc25\ud835\udc28\ud835\udc30'\ud835\udc2c \ud835\udc1d\ud835\udc1e\ud835\udc2c\ud835\udc22\ud835\udc20\ud835\udc27 \ud835\udc29\ud835\udc21\ud835\udc22\ud835\udc25\ud835\udc28\ud835\udc2c\ud835\udc28\ud835\udc29\ud835\udc21\ud835\udc32, using &gt;&gt; and &lt;&lt; to define dependencies between my agents. In this micro-CrewAI, \ud835\udc1a\ud835\udc20\ud835\udc1e\ud835\udc27\ud835\udc2d\ud835\udc2c are equivalent to \ud835\udc00\ud835\udc22\ud835\udc2b\ud835\udc1f\ud835\udc25\ud835\udc28\ud835\udc30 \ud835\udc13\ud835\udc1a\ud835\udc2c\ud835\udc24\ud835\udc2c and the \ud835\udc02\ud835\udc2b\ud835\udc1e\ud835\udc30 is equivalent to an \ud835\udc00\ud835\udc22\ud835\udc2b\ud835\udc1f\ud835\udc25\ud835\udc28\ud835\udc30 \ud835\udc03\ud835\udc00\ud835\udc06.</p> <p>Take a look at the previous lessons if you haven't!</p> <ul> <li>First Lesson: The Reflection Pattern</li> <li>Second Lesson: The Tool Pattern</li> <li>Third Lesson: The Planning Pattern</li> </ul> <p>Let's begin!! \ud83d\udcaa</p> <p>First of all, we need an Agent Class. This class implements an Agent, and internally it implements the ReAct technique (check Lesson 3 if you want to see this technique in detail!).</p> In\u00a0[\u00a0]: Copied! <pre>from textwrap import dedent\nfrom collections import deque\n\nfrom colorama import Fore\nfrom graphviz import Digraph  # type: ignore\n\nfrom agentic_patterns.utils.logging import fancy_print\n\nclass Agent:\n    \"\"\"\n    Represents an AI agent that can work as part of a team to complete tasks.\n\n    This class implements an agent with dependencies, context handling, and task execution capabilities.\n    It can be used in a multi-agent system where agents collaborate to solve complex problems.\n\n    Attributes:\n        name (str): The name of the agent.\n        backstory (str): The backstory or background of the agent.\n        task_description (str): A description of the task assigned to the agent.\n        task_expected_output (str): The expected format or content of the task output.\n        react_agent (ReactAgent): An instance of ReactAgent used for generating responses.\n        dependencies (list[Agent]): A list of Agent instances that this agent depends on.\n        dependents (list[Agent]): A list of Agent instances that depend on this agent.\n        context (str): Accumulated context information from other agents.\n\n    Args:\n        name (str): The name of the agent.\n        backstory (str): The backstory or background of the agent.\n        task_description (str): A description of the task assigned to the agent.\n        task_expected_output (str, optional): The expected format or content of the task output. Defaults to \"\".\n        tools (list[Tool] | None, optional): A list of Tool instances available to the agent. Defaults to None.\n        llm (str, optional): The name of the language model to use. Defaults to \"llama-3.1-70b-versatile\".\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        backstory: str,\n        task_description: str,\n        task_expected_output: str = \"\",\n        tools: list[Tool] | None = None,\n        llm: str = \"llama-3.1-70b-versatile\",\n    ):\n        self.name = name\n        self.backstory = backstory\n        self.task_description = task_description\n        self.task_expected_output = task_expected_output\n        self.react_agent = ReactAgent(\n            model=llm, system_prompt=self.backstory, tools=tools or []\n        )\n\n        self.dependencies: list[Agent] = []  # Agents that this agent depends on\n        self.dependents: list[Agent] = []  # Agents that depend on this agent\n\n        self.context = \"\"\n\n        # Automatically register this agent to the active Crew context if one exists\n        Crew.register_agent(self)\n\n    def __repr__(self):\n        return f\"{self.name}\"\n\n    def __rshift__(self, other):\n        \"\"\"\n        Defines the '&gt;&gt;' operator. This operator is used to indicate agent dependency.\n\n        Args:\n            other (Agent): The agent that depends on this agent.\n        \"\"\"\n        self.add_dependent(other)\n        return other  # Allow chaining\n\n    def __lshift__(self, other):\n        \"\"\"\n        Defines the '&lt;&lt;' operator to indicate agent dependency in reverse.\n\n        Args:\n            other (Agent): The agent that this agent depends on.\n\n        Returns:\n            Agent: The `other` agent to allow for chaining.\n        \"\"\"\n        self.add_dependency(other)\n        return other  # Allow chaining\n\n    def __rrshift__(self, other):\n        \"\"\"\n        Defines the '&lt;&lt;' operator.This operator is used to indicate agent dependency.\n\n        Args:\n            other (Agent): The agent that this agent depends on.\n        \"\"\"\n        self.add_dependency(other)\n        return self  # Allow chaining\n\n    def __rlshift__(self, other):\n        \"\"\"\n        Defines the '&lt;&lt;' operator when evaluated from right to left.\n        This operator is used to indicate agent dependency in the normal order.\n\n        Args:\n            other (Agent): The agent that depends on this agent.\n\n        Returns:\n            Agent: The current agent (self) to allow for chaining.\n        \"\"\"\n        self.add_dependent(other)\n        return self  # Allow chaining\n\n    def add_dependency(self, other):\n        \"\"\"\n        Adds a dependency to this agent.\n\n        Args:\n            other (Agent | list[Agent]): The agent(s) that this agent depends on.\n\n        Raises:\n            TypeError: If the dependency is not an Agent or a list of Agents.\n        \"\"\"\n        if isinstance(other, Agent):\n            self.dependencies.append(other)\n            other.dependents.append(self)\n        elif isinstance(other, list) and all(isinstance(item, Agent) for item in other):\n            for item in other:\n                self.dependencies.append(item)\n                item.dependents.append(self)\n        else:\n            raise TypeError(\"The dependency must be an instance or list of Agent.\")\n\n    def add_dependent(self, other):\n        \"\"\"\n        Adds a dependent to this agent.\n\n        Args:\n            other (Agent | list[Agent]): The agent(s) that depend on this agent.\n\n        Raises:\n            TypeError: If the dependent is not an Agent or a list of Agents.\n        \"\"\"\n        if isinstance(other, Agent):\n            other.dependencies.append(self)\n            self.dependents.append(other)\n        elif isinstance(other, list) and all(isinstance(item, Agent) for item in other):\n            for item in other:\n                item.dependencies.append(self)\n                self.dependents.append(item)\n        else:\n            raise TypeError(\"The dependent must be an instance or list of Agent.\")\n\n    def receive_context(self, input_data):\n        \"\"\"\n        Receives and stores context information from other agents.\n\n        Args:\n            input_data (str): The context information to be added.\n        \"\"\"\n        self.context += f\"{self.name} received context: \\n{input_data}\"\n\n    def create_prompt(self):\n        \"\"\"\n        Creates a prompt for the agent based on its task description, expected output, and context.\n\n        Returns:\n            str: The formatted prompt string.\n        \"\"\"\n        prompt = dedent(\n            f\"\"\"\n        You are an AI agent. You are part of a team of agents working together to complete a task.\n        I'm going to give you the task description enclosed in &lt;task_description&gt;&lt;/task_description&gt; tags. I'll also give\n        you the available context from the other agents in &lt;context&gt;&lt;/context&gt; tags. If the context\n        is not available, the &lt;context&gt;&lt;/context&gt; tags will be empty. You'll also receive the task\n        expected output enclosed in &lt;task_expected_output&gt;&lt;/task_expected_output&gt; tags. With all this information\n        you need to create the best possible response, always respecting the format as describe in\n        &lt;task_expected_output&gt;&lt;/task_expected_output&gt; tags. If expected output is not available, just create\n        a meaningful response to complete the task.\n\n        &lt;task_description&gt;\n        {self.task_description}\n        &lt;/task_description&gt;\n\n        &lt;task_expected_output&gt;\n        {self.task_expected_output}\n        &lt;/task_expected_output&gt;\n\n        &lt;context&gt;\n        {self.context}\n        &lt;/context&gt;\n\n        Your response:\n        \"\"\"\n        ).strip()\n\n        return prompt\n\n    def run(self):\n        \"\"\"\n        Runs the agent's task and generates the output.\n\n        This method creates a prompt, runs it through the ReactAgent, and passes the output to all dependent agents.\n\n        Returns:\n            str: The output generated by the agent.\n        \"\"\"\n        msg = self.create_prompt()\n        output = self.react_agent.run(user_msg=msg)\n\n        # Pass the output to all dependents\n        for dependent in self.dependents:\n            dependent.receive_context(output)\n        return output\n    \n\n\n\nclass Crew:\n    \"\"\"\n    A class representing a crew of agents working together.\n\n    This class manages a group of agents, their dependencies, and provides methods\n    for running the agents in a topologically sorted order.\n\n    Attributes:\n        current_crew (Crew): Class-level variable to track the active Crew context.\n        agents (list): A list of agents in the crew.\n    \"\"\"\n\n    current_crew = None\n\n    def __init__(self):\n        self.agents = []\n\n    def __enter__(self):\n        \"\"\"\n        Enters the context manager, setting this crew as the current active context.\n\n        Returns:\n            Crew: The current Crew instance.\n        \"\"\"\n        Crew.current_crew = self\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"\n        Exits the context manager, clearing the active context.\n\n        Args:\n            exc_type: The exception type, if an exception was raised.\n            exc_val: The exception value, if an exception was raised.\n            exc_tb: The traceback, if an exception was raised.\n        \"\"\"\n        Crew.current_crew = None\n\n    def add_agent(self, agent):\n        \"\"\"\n        Adds an agent to the crew.\n\n        Args:\n            agent: The agent to be added to the crew.\n        \"\"\"\n        self.agents.append(agent)\n\n    @staticmethod\n    def register_agent(agent):\n        \"\"\"\n        Registers an agent with the current active crew context.\n\n        Args:\n            agent: The agent to be registered.\n        \"\"\"\n        if Crew.current_crew is not None:\n            Crew.current_crew.add_agent(agent)\n\n    def topological_sort(self):\n        \"\"\"\n        Performs a topological sort of the agents based on their dependencies.\n\n        Returns:\n            list: A list of agents sorted in topological order.\n\n        Raises:\n            ValueError: If there's a circular dependency among the agents.\n        \"\"\"\n        in_degree = {agent: len(agent.dependencies) for agent in self.agents}\n        queue = deque([agent for agent in self.agents if in_degree[agent] == 0])\n\n        sorted_agents = []\n\n        while queue:\n            current_agent = queue.popleft()\n            sorted_agents.append(current_agent)\n\n            for dependent in current_agent.dependents:\n                in_degree[dependent] -= 1\n                if in_degree[dependent] == 0:\n                    queue.append(dependent)\n\n        if len(sorted_agents) != len(self.agents):\n            raise ValueError(\n                \"Circular dependencies detected among agents, preventing a valid topological sort\"\n            )\n\n        return sorted_agents\n\n    def plot(self):\n        \"\"\"\n        Plots the Directed Acyclic Graph (DAG) of agents in the crew using Graphviz.\n\n        Returns:\n            Digraph: A Graphviz Digraph object representing the agent dependencies.\n        \"\"\"\n        dot = Digraph(format=\"png\")  # Set format to PNG for inline display\n\n        # Add nodes and edges for each agent in the crew\n        for agent in self.agents:\n            dot.node(agent.name)\n            for dependency in agent.dependencies:\n                dot.edge(dependency.name, agent.name)\n        return dot\n\n    def run(self):\n        \"\"\"\n        Runs all agents in the crew in topologically sorted order.\n\n        This method executes each agent's run method and prints the results.\n        \"\"\"\n        sorted_agents = self.topological_sort()\n        for agent in sorted_agents:\n            fancy_print(f\"RUNNING AGENT: {agent}\")\n            print(Fore.RED + f\"{agent.run()}\")\n</pre> from textwrap import dedent from collections import deque  from colorama import Fore from graphviz import Digraph  # type: ignore  from agentic_patterns.utils.logging import fancy_print  class Agent:     \"\"\"     Represents an AI agent that can work as part of a team to complete tasks.      This class implements an agent with dependencies, context handling, and task execution capabilities.     It can be used in a multi-agent system where agents collaborate to solve complex problems.      Attributes:         name (str): The name of the agent.         backstory (str): The backstory or background of the agent.         task_description (str): A description of the task assigned to the agent.         task_expected_output (str): The expected format or content of the task output.         react_agent (ReactAgent): An instance of ReactAgent used for generating responses.         dependencies (list[Agent]): A list of Agent instances that this agent depends on.         dependents (list[Agent]): A list of Agent instances that depend on this agent.         context (str): Accumulated context information from other agents.      Args:         name (str): The name of the agent.         backstory (str): The backstory or background of the agent.         task_description (str): A description of the task assigned to the agent.         task_expected_output (str, optional): The expected format or content of the task output. Defaults to \"\".         tools (list[Tool] | None, optional): A list of Tool instances available to the agent. Defaults to None.         llm (str, optional): The name of the language model to use. Defaults to \"llama-3.1-70b-versatile\".     \"\"\"      def __init__(         self,         name: str,         backstory: str,         task_description: str,         task_expected_output: str = \"\",         tools: list[Tool] | None = None,         llm: str = \"llama-3.1-70b-versatile\",     ):         self.name = name         self.backstory = backstory         self.task_description = task_description         self.task_expected_output = task_expected_output         self.react_agent = ReactAgent(             model=llm, system_prompt=self.backstory, tools=tools or []         )          self.dependencies: list[Agent] = []  # Agents that this agent depends on         self.dependents: list[Agent] = []  # Agents that depend on this agent          self.context = \"\"          # Automatically register this agent to the active Crew context if one exists         Crew.register_agent(self)      def __repr__(self):         return f\"{self.name}\"      def __rshift__(self, other):         \"\"\"         Defines the '&gt;&gt;' operator. This operator is used to indicate agent dependency.          Args:             other (Agent): The agent that depends on this agent.         \"\"\"         self.add_dependent(other)         return other  # Allow chaining      def __lshift__(self, other):         \"\"\"         Defines the '&lt;&lt;' operator to indicate agent dependency in reverse.          Args:             other (Agent): The agent that this agent depends on.          Returns:             Agent: The `other` agent to allow for chaining.         \"\"\"         self.add_dependency(other)         return other  # Allow chaining      def __rrshift__(self, other):         \"\"\"         Defines the '&lt;&lt;' operator.This operator is used to indicate agent dependency.          Args:             other (Agent): The agent that this agent depends on.         \"\"\"         self.add_dependency(other)         return self  # Allow chaining      def __rlshift__(self, other):         \"\"\"         Defines the '&lt;&lt;' operator when evaluated from right to left.         This operator is used to indicate agent dependency in the normal order.          Args:             other (Agent): The agent that depends on this agent.          Returns:             Agent: The current agent (self) to allow for chaining.         \"\"\"         self.add_dependent(other)         return self  # Allow chaining      def add_dependency(self, other):         \"\"\"         Adds a dependency to this agent.          Args:             other (Agent | list[Agent]): The agent(s) that this agent depends on.          Raises:             TypeError: If the dependency is not an Agent or a list of Agents.         \"\"\"         if isinstance(other, Agent):             self.dependencies.append(other)             other.dependents.append(self)         elif isinstance(other, list) and all(isinstance(item, Agent) for item in other):             for item in other:                 self.dependencies.append(item)                 item.dependents.append(self)         else:             raise TypeError(\"The dependency must be an instance or list of Agent.\")      def add_dependent(self, other):         \"\"\"         Adds a dependent to this agent.          Args:             other (Agent | list[Agent]): The agent(s) that depend on this agent.          Raises:             TypeError: If the dependent is not an Agent or a list of Agents.         \"\"\"         if isinstance(other, Agent):             other.dependencies.append(self)             self.dependents.append(other)         elif isinstance(other, list) and all(isinstance(item, Agent) for item in other):             for item in other:                 item.dependencies.append(self)                 self.dependents.append(item)         else:             raise TypeError(\"The dependent must be an instance or list of Agent.\")      def receive_context(self, input_data):         \"\"\"         Receives and stores context information from other agents.          Args:             input_data (str): The context information to be added.         \"\"\"         self.context += f\"{self.name} received context: \\n{input_data}\"      def create_prompt(self):         \"\"\"         Creates a prompt for the agent based on its task description, expected output, and context.          Returns:             str: The formatted prompt string.         \"\"\"         prompt = dedent(             f\"\"\"         You are an AI agent. You are part of a team of agents working together to complete a task.         I'm going to give you the task description enclosed in  tags. I'll also give         you the available context from the other agents in  tags. If the context         is not available, the  tags will be empty. You'll also receive the task         expected output enclosed in  tags. With all this information         you need to create the best possible response, always respecting the format as describe in          tags. If expected output is not available, just create         a meaningful response to complete the task.                   {self.task_description}                   {self.task_expected_output}                   {self.context}                   Your response:         \"\"\"         ).strip()          return prompt      def run(self):         \"\"\"         Runs the agent's task and generates the output.          This method creates a prompt, runs it through the ReactAgent, and passes the output to all dependent agents.          Returns:             str: The output generated by the agent.         \"\"\"         msg = self.create_prompt()         output = self.react_agent.run(user_msg=msg)          # Pass the output to all dependents         for dependent in self.dependents:             dependent.receive_context(output)         return output         class Crew:     \"\"\"     A class representing a crew of agents working together.      This class manages a group of agents, their dependencies, and provides methods     for running the agents in a topologically sorted order.      Attributes:         current_crew (Crew): Class-level variable to track the active Crew context.         agents (list): A list of agents in the crew.     \"\"\"      current_crew = None      def __init__(self):         self.agents = []      def __enter__(self):         \"\"\"         Enters the context manager, setting this crew as the current active context.          Returns:             Crew: The current Crew instance.         \"\"\"         Crew.current_crew = self         return self      def __exit__(self, exc_type, exc_val, exc_tb):         \"\"\"         Exits the context manager, clearing the active context.          Args:             exc_type: The exception type, if an exception was raised.             exc_val: The exception value, if an exception was raised.             exc_tb: The traceback, if an exception was raised.         \"\"\"         Crew.current_crew = None      def add_agent(self, agent):         \"\"\"         Adds an agent to the crew.          Args:             agent: The agent to be added to the crew.         \"\"\"         self.agents.append(agent)      @staticmethod     def register_agent(agent):         \"\"\"         Registers an agent with the current active crew context.          Args:             agent: The agent to be registered.         \"\"\"         if Crew.current_crew is not None:             Crew.current_crew.add_agent(agent)      def topological_sort(self):         \"\"\"         Performs a topological sort of the agents based on their dependencies.          Returns:             list: A list of agents sorted in topological order.          Raises:             ValueError: If there's a circular dependency among the agents.         \"\"\"         in_degree = {agent: len(agent.dependencies) for agent in self.agents}         queue = deque([agent for agent in self.agents if in_degree[agent] == 0])          sorted_agents = []          while queue:             current_agent = queue.popleft()             sorted_agents.append(current_agent)              for dependent in current_agent.dependents:                 in_degree[dependent] -= 1                 if in_degree[dependent] == 0:                     queue.append(dependent)          if len(sorted_agents) != len(self.agents):             raise ValueError(                 \"Circular dependencies detected among agents, preventing a valid topological sort\"             )          return sorted_agents      def plot(self):         \"\"\"         Plots the Directed Acyclic Graph (DAG) of agents in the crew using Graphviz.          Returns:             Digraph: A Graphviz Digraph object representing the agent dependencies.         \"\"\"         dot = Digraph(format=\"png\")  # Set format to PNG for inline display          # Add nodes and edges for each agent in the crew         for agent in self.agents:             dot.node(agent.name)             for dependency in agent.dependencies:                 dot.edge(dependency.name, agent.name)         return dot      def run(self):         \"\"\"         Runs all agents in the crew in topologically sorted order.          This method executes each agent's run method and prints the results.         \"\"\"         sorted_agents = self.topological_sort()         for agent in sorted_agents:             fancy_print(f\"RUNNING AGENT: {agent}\")             print(Fore.RED + f\"{agent.run()}\")   <p>Let's create some example agent, to see how it works.</p> In\u00a0[\u00a0]: Copied! <pre>agent_example = Agent(\n    name=\"Poet Agent\",\n    backstory=\"You are a well-known poet, who enjoys creating high quality poetry.\",\n    task_description=\"Write a poem about the meaning of life\",\n    task_expected_output=\"Just output the poem, without any title or introductory sentences\",\n)\n</pre> agent_example = Agent(     name=\"Poet Agent\",     backstory=\"You are a well-known poet, who enjoys creating high quality poetry.\",     task_description=\"Write a poem about the meaning of life\",     task_expected_output=\"Just output the poem, without any title or introductory sentences\", ) In\u00a0[\u00a0]: Copied! <pre>print(agent_example.run())\n</pre> print(agent_example.run()) <p>You can also associate tools with the agent. Let's create a tool for writing some string into a CSV.</p> In\u00a0[\u00a0]: Copied! <pre>from agentic_patterns.tool_pattern.tool import tool\n</pre> from agentic_patterns.tool_pattern.tool import tool In\u00a0[\u00a0]: Copied! <pre>@tool\ndef write_str_to_txt(string_data: str, txt_filename: str):\n    \"\"\"\n    Writes a string to a txt file.\n\n    This function takes a string and writes it to a text file. If the file already exists, \n    it will be overwritten with the new data.\n\n    Args:\n        string_data (str): The string containing the data to be written to the file.\n        txt_filename (str): The name of the text file to which the data should be written.\n    \"\"\"\n    # Write the string data to the text file\n    with open(txt_filename, mode='w', encoding='utf-8') as file:\n        file.write(string_data)\n\n    print(f\"Data successfully written to {txt_filename}\")\n</pre> @tool def write_str_to_txt(string_data: str, txt_filename: str):     \"\"\"     Writes a string to a txt file.      This function takes a string and writes it to a text file. If the file already exists,      it will be overwritten with the new data.      Args:         string_data (str): The string containing the data to be written to the file.         txt_filename (str): The name of the text file to which the data should be written.     \"\"\"     # Write the string data to the text file     with open(txt_filename, mode='w', encoding='utf-8') as file:         file.write(string_data)      print(f\"Data successfully written to {txt_filename}\") In\u00a0[\u00a0]: Copied! <pre>agent_tool_example = Agent(\n    name=\"Writer Agent\",\n    backstory=\"You are a language model specialised in writing text into .txt files\",\n    task_description=\"Write the string 'This is a Tool Agent' into './tool_agent_example.txt'\",\n    task_expected_output=\"A .txt file containing the given string\",\n    tools=write_str_to_txt,\n)\n</pre> agent_tool_example = Agent(     name=\"Writer Agent\",     backstory=\"You are a language model specialised in writing text into .txt files\",     task_description=\"Write the string 'This is a Tool Agent' into './tool_agent_example.txt'\",     task_expected_output=\"A .txt file containing the given string\",     tools=write_str_to_txt, ) In\u00a0[\u00a0]: Copied! <pre>agent_tool_example.run()\n</pre> agent_tool_example.run() <p>Let's define two agents now.</p> In\u00a0[\u00a0]: Copied! <pre>agent_1 = Agent(\n    name=\"Poet Agent\",\n    backstory=\"You are a well-known poet, who enjoys creating high quality poetry.\",\n    task_description=\"Write a poem about the meaning of life\",\n    task_expected_output=\"Just output the poem, without any title or introductory sentences\",\n)\n\nagent_2 = Agent(\n    name=\"Poem Translator Agent\",\n    backstory=\"You are an expert translator especially skilled in Ancient Greek\",\n    task_description=\"Translate a poem into Ancient Greek\", \n    task_expected_output=\"Just output the translated poem and nothing else\"\n)\n</pre> agent_1 = Agent(     name=\"Poet Agent\",     backstory=\"You are a well-known poet, who enjoys creating high quality poetry.\",     task_description=\"Write a poem about the meaning of life\",     task_expected_output=\"Just output the poem, without any title or introductory sentences\", )  agent_2 = Agent(     name=\"Poem Translator Agent\",     backstory=\"You are an expert translator especially skilled in Ancient Greek\",     task_description=\"Translate a poem into Ancient Greek\",      task_expected_output=\"Just output the translated poem and nothing else\" ) <p>We can define the agent dependencies using the <code>&gt;&gt;</code> operator.</p> In\u00a0[\u00a0]: Copied! <pre>agent_1 &gt;&gt; agent_2\n</pre> agent_1 &gt;&gt; agent_2 <p>This means <code>agent_2</code> depends on <code>agent_1</code>. We can check the dependencies and dependents of both agents.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Agent 1 dependencies: \", agent_1.dependencies)\nprint(\"Agent 1 dependents: \", agent_1.dependents)\nprint(\"Agent 2 dependencies: \", agent_2.dependencies)\nprint(\"Agent 2 dependents: \", agent_2.dependents)\n</pre> print(\"Agent 1 dependencies: \", agent_1.dependencies) print(\"Agent 1 dependents: \", agent_1.dependents) print(\"Agent 2 dependencies: \", agent_2.dependencies) print(\"Agent 2 dependents: \", agent_2.dependents) <p>Now, if we run <code>agent_1</code>, the results will be added to <code>agent_2</code>'s context.</p> In\u00a0[\u00a0]: Copied! <pre>print(agent_1.run())\n</pre> print(agent_1.run()) In\u00a0[\u00a0]: Copied! <pre>print(agent_2.context)\n</pre> print(agent_2.context) <p>Now, if we run the second agent, it will use the context received from the previous agent to generate its output.</p> In\u00a0[\u00a0]: Copied! <pre>print(agent_2.run())\n</pre> print(agent_2.run()) In\u00a0[\u00a0]: Copied! <pre>from agentic_patterns.multiagent_pattern.crew import Crew\n</pre> from agentic_patterns.multiagent_pattern.crew import Crew In\u00a0[\u00a0]: Copied! <pre>with Crew() as crew:\n    agent_1 = Agent(\n        name=\"Poet Agent\",\n        backstory=\"You are a well-known poet, who enjoys creating high quality poetry.\",\n        task_description=\"Write a poem about the meaning of life\",\n        task_expected_output=\"Just output the poem, without any title or introductory sentences\",\n    )\n\n    agent_2 = Agent(\n        name=\"Poem Translator Agent\",\n        backstory=\"You are an expert translator especially skilled in Spanish\",\n        task_description=\"Translate a poem into Spanish\", \n        task_expected_output=\"Just output the translated poem and nothing else\"\n    )\n\n    agent_3 = Agent(\n        name=\"Writer Agent\",\n        backstory=\"You are an expert transcriber, that loves writing poems into txt files\",\n        task_description=\"You'll receive a Spanish poem in your context. You need to write the poem into './poem.txt' file\",\n        task_expected_output=\"A txt file containing the greek poem received from the context\",\n        tools=write_str_to_txt,\n    )\n\n    agent_1 &gt;&gt; agent_2 &gt;&gt; agent_3\n</pre> with Crew() as crew:     agent_1 = Agent(         name=\"Poet Agent\",         backstory=\"You are a well-known poet, who enjoys creating high quality poetry.\",         task_description=\"Write a poem about the meaning of life\",         task_expected_output=\"Just output the poem, without any title or introductory sentences\",     )      agent_2 = Agent(         name=\"Poem Translator Agent\",         backstory=\"You are an expert translator especially skilled in Spanish\",         task_description=\"Translate a poem into Spanish\",          task_expected_output=\"Just output the translated poem and nothing else\"     )      agent_3 = Agent(         name=\"Writer Agent\",         backstory=\"You are an expert transcriber, that loves writing poems into txt files\",         task_description=\"You'll receive a Spanish poem in your context. You need to write the poem into './poem.txt' file\",         task_expected_output=\"A txt file containing the greek poem received from the context\",         tools=write_str_to_txt,     )      agent_1 &gt;&gt; agent_2 &gt;&gt; agent_3 In\u00a0[\u00a0]: Copied! <pre>crew.plot()\n</pre> crew.plot() In\u00a0[\u00a0]: Copied! <pre>crew.run()\n</pre> crew.run()"},{"location":"Agents/patterns/multiagent_pattern/#multiagent-pattern-multiagent-collaboration","title":"Multiagent Pattern - Multiagent Collaboration\u00b6","text":""},{"location":"Agents/patterns/multiagent_pattern/#the-agent-class","title":"The Agent Class\u00b6","text":""},{"location":"Agents/patterns/multiagent_pattern/#defining-agent-dependencies","title":"Defining Agent Dependencies\u00b6","text":""},{"location":"Agents/patterns/multiagent_pattern/#the-crew","title":"The Crew\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/","title":"Agent Planning Pattern","text":"<p>source : https://github.com/neural-maze/agentic_patterns</p> <p>So, we've seen agents capable of reflecting and using tools to access the outside world. But ... what about planning, i.e. deciding what sequence of steps to follow to accomplish a large task?</p> <p>That is exactly what the Planning Pattern provides; ways for the LLM to break a task into smaller, more easily accomplished subgoals without losing track of the end goal.</p> <p>The most paradigmatic example of the planning pattern is the ReAct technique, displayed in the diagram above.</p> <p>In this notebook, you'll learn how this technique actually works. This is the third lesson of the \"Agentic Patterns from Scratch\" series. Take a look at the previous lessons if you haven't!</p> <ul> <li>First Lesson: The Reflection Pattern</li> <li>Second Lesson: The Tool Pattern</li> </ul> <p>We start by importing all the libraries we'll be using in this tutorial as well as the Groq client.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport re\nimport math\nimport json\nfrom dotenv import load_dotenv\n\nfrom groq import Groq\n\n# Remember to load the environment variables. You should have the Groq API Key in there :)\nload_dotenv()\n\nMODEL = \"llama-3.1-70b-versatile\"\nGROQ_CLIENT = Groq()\n</pre> import os import re import math import json from dotenv import load_dotenv  from groq import Groq  # Remember to load the environment variables. You should have the Groq API Key in there :) load_dotenv()  MODEL = \"llama-3.1-70b-versatile\" GROQ_CLIENT = Groq() <p>If you are not familiar with the <code>tool</code> decorator, changes are you are missed the previous tutorial about the Tool Pattern. Check the video here.</p> In\u00a0[\u00a0]: Copied! <pre>import re\nfrom dataclasses import dataclass\nimport time\n\nfrom colorama import Fore\nfrom colorama import Style\n\ndef completions_create(client, messages: list, model: str) -&gt; str:\n    \"\"\"\n    Sends a request to the client's `completions.create` method to interact with the language model.\n\n    Args:\n        client (Groq): The Groq client object\n        messages (list[dict]): A list of message objects containing chat history for the model.\n        model (str): The model to use for generating tool calls and responses.\n\n    Returns:\n        str: The content of the model's response.\n    \"\"\"\n    response = client.chat.completions.create(messages=messages, model=model)\n    return str(response.choices[0].message.content)\n\n\ndef build_prompt_structure(prompt: str, role: str, tag: str = \"\") -&gt; dict:\n    \"\"\"\n    Builds a structured prompt that includes the role and content.\n\n    Args:\n        prompt (str): The actual content of the prompt.\n        role (str): The role of the speaker (e.g., user, assistant).\n\n    Returns:\n        dict: A dictionary representing the structured prompt.\n    \"\"\"\n    if tag:\n        prompt = f\"&lt;{tag}&gt;{prompt}&lt;/{tag}&gt;\"\n    return {\"role\": role, \"content\": prompt}\n\n\ndef update_chat_history(history: list, msg: str, role: str):\n    \"\"\"\n    Updates the chat history by appending the latest response.\n\n    Args:\n        history (list): The list representing the current chat history.\n        msg (str): The message to append.\n        role (str): The role type (e.g. 'user', 'assistant', 'system')\n    \"\"\"\n    history.append(build_prompt_structure(prompt=msg, role=role))\n\n\nclass ChatHistory(list):\n    def __init__(self, messages: list | None = None, total_length: int = -1):\n        \"\"\"Initialise the queue with a fixed total length.\n\n        Args:\n            messages (list | None): A list of initial messages\n            total_length (int): The maximum number of messages the chat history can hold.\n        \"\"\"\n        if messages is None:\n            messages = []\n\n        super().__init__(messages)\n        self.total_length = total_length\n\n    def append(self, msg: str):\n        \"\"\"Add a message to the queue.\n\n        Args:\n            msg (str): The message to be added to the queue\n        \"\"\"\n        if len(self) == self.total_length:\n            self.pop(0)\n        super().append(msg)\n\n\nclass FixedFirstChatHistory(ChatHistory):\n    def __init__(self, messages: list | None = None, total_length: int = -1):\n        \"\"\"Initialise the queue with a fixed total length.\n\n        Args:\n            messages (list | None): A list of initial messages\n            total_length (int): The maximum number of messages the chat history can hold.\n        \"\"\"\n        super().__init__(messages, total_length)\n\n    def append(self, msg: str):\n        \"\"\"Add a message to the queue. The first messaage will always stay fixed.\n\n        Args:\n            msg (str): The message to be added to the queue\n        \"\"\"\n        if len(self) == self.total_length:\n            self.pop(1)\n        super().append(msg)\n\n\n\n\n@dataclass\nclass TagContentResult:\n    \"\"\"\n    A data class to represent the result of extracting tag content.\n\n    Attributes:\n        content (List[str]): A list of strings containing the content found between the specified tags.\n        found (bool): A flag indicating whether any content was found for the given tag.\n    \"\"\"\n\n    content: list[str]\n    found: bool\n\n\ndef extract_tag_content(text: str, tag: str) -&gt; TagContentResult:\n    \"\"\"\n    Extracts all content enclosed by specified tags (e.g., &lt;thought&gt;, &lt;response&gt;, etc.).\n\n    Parameters:\n        text (str): The input string containing multiple potential tags.\n        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n\n    Returns:\n        dict: A dictionary with the following keys:\n            - 'content' (list): A list of strings containing the content found between the specified tags.\n            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n    \"\"\"\n    # Build the regex pattern dynamically to find multiple occurrences of the tag\n    tag_pattern = rf\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"\n\n    # Use findall to capture all content between the specified tag\n    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n\n    # Return the dataclass instance with the result\n    return TagContentResult(\n        content=[content.strip() for content in matched_contents],\n        found=bool(matched_contents),\n    )\n\n\ndef fancy_print(message: str) -&gt; None:\n    \"\"\"\n    Displays a fancy print message.\n\n    Args:\n        message (str): The message to display.\n    \"\"\"\n    print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")\n    print(Fore.MAGENTA + f\"{message}\")\n    print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")\n    time.sleep(0.5)\n\n\ndef fancy_step_tracker(step: int, total_steps: int) -&gt; None:\n    \"\"\"\n    Displays a fancy step tracker for each iteration of the generation-reflection loop.\n\n    Args:\n        step (int): The current step in the loop.\n        total_steps (int): The total number of steps in the loop.\n    \"\"\"\n    fancy_print(f\"STEP {step + 1}/{total_steps}\")\n    \nimport json\nfrom typing import Callable\n\n\ndef get_fn_signature(fn: Callable) -&gt; dict:\n    \"\"\"\n    Generates the signature for a given function.\n\n    Args:\n        fn (Callable): The function whose signature needs to be extracted.\n\n    Returns:\n        dict: A dictionary containing the function's name, description,\n              and parameter types.\n    \"\"\"\n    fn_signature: dict = {\n        \"name\": fn.__name__,\n        \"description\": fn.__doc__,\n        \"parameters\": {\"properties\": {}},\n    }\n    schema = {\n        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n    }\n    fn_signature[\"parameters\"][\"properties\"] = schema\n    return fn_signature\n\n\ndef validate_arguments(tool_call: dict, tool_signature: dict) -&gt; dict:\n    \"\"\"\n    Validates and converts arguments in the input dictionary to match the expected types.\n\n    Args:\n        tool_call (dict): A dictionary containing the arguments passed to the tool.\n        tool_signature (dict): The expected function signature and parameter types.\n\n    Returns:\n        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n    \"\"\"\n    properties = tool_signature[\"parameters\"][\"properties\"]\n\n    # TODO: This is overly simplified but enough for simple Tools.\n    type_mapping = {\n        \"int\": int,\n        \"str\": str,\n        \"bool\": bool,\n        \"float\": float,\n    }\n\n    for arg_name, arg_value in tool_call[\"arguments\"].items():\n        expected_type = properties[arg_name].get(\"type\")\n\n        if not isinstance(arg_value, type_mapping[expected_type]):\n            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n\n    return tool_call\n\n\nclass Tool:\n    \"\"\"\n    A class representing a tool that wraps a callable and its signature.\n\n    Attributes:\n        name (str): The name of the tool (function).\n        fn (Callable): The function that the tool represents.\n        fn_signature (str): JSON string representation of the function's signature.\n    \"\"\"\n\n    def __init__(self, name: str, fn: Callable, fn_signature: str):\n        self.name = name\n        self.fn = fn\n        self.fn_signature = fn_signature\n\n    def __str__(self):\n        return self.fn_signature\n\n    def run(self, **kwargs):\n        \"\"\"\n        Executes the tool (function) with provided arguments.\n\n        Args:\n            **kwargs: Keyword arguments passed to the function.\n\n        Returns:\n            The result of the function call.\n        \"\"\"\n        return self.fn(**kwargs)\n\n\ndef tool(fn: Callable):\n    \"\"\"\n    A decorator that wraps a function into a Tool object.\n\n    Args:\n        fn (Callable): The function to be wrapped.\n\n    Returns:\n        Tool: A Tool object containing the function, its name, and its signature.\n    \"\"\"\n\n    def wrapper():\n        fn_signature = get_fn_signature(fn)\n        return Tool(\n            name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)\n        )\n\n    return wrapper()\n</pre> import re from dataclasses import dataclass import time  from colorama import Fore from colorama import Style  def completions_create(client, messages: list, model: str) -&gt; str:     \"\"\"     Sends a request to the client's `completions.create` method to interact with the language model.      Args:         client (Groq): The Groq client object         messages (list[dict]): A list of message objects containing chat history for the model.         model (str): The model to use for generating tool calls and responses.      Returns:         str: The content of the model's response.     \"\"\"     response = client.chat.completions.create(messages=messages, model=model)     return str(response.choices[0].message.content)   def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -&gt; dict:     \"\"\"     Builds a structured prompt that includes the role and content.      Args:         prompt (str): The actual content of the prompt.         role (str): The role of the speaker (e.g., user, assistant).      Returns:         dict: A dictionary representing the structured prompt.     \"\"\"     if tag:         prompt = f\"&lt;{tag}&gt;{prompt}\"     return {\"role\": role, \"content\": prompt}   def update_chat_history(history: list, msg: str, role: str):     \"\"\"     Updates the chat history by appending the latest response.      Args:         history (list): The list representing the current chat history.         msg (str): The message to append.         role (str): The role type (e.g. 'user', 'assistant', 'system')     \"\"\"     history.append(build_prompt_structure(prompt=msg, role=role))   class ChatHistory(list):     def __init__(self, messages: list | None = None, total_length: int = -1):         \"\"\"Initialise the queue with a fixed total length.          Args:             messages (list | None): A list of initial messages             total_length (int): The maximum number of messages the chat history can hold.         \"\"\"         if messages is None:             messages = []          super().__init__(messages)         self.total_length = total_length      def append(self, msg: str):         \"\"\"Add a message to the queue.          Args:             msg (str): The message to be added to the queue         \"\"\"         if len(self) == self.total_length:             self.pop(0)         super().append(msg)   class FixedFirstChatHistory(ChatHistory):     def __init__(self, messages: list | None = None, total_length: int = -1):         \"\"\"Initialise the queue with a fixed total length.          Args:             messages (list | None): A list of initial messages             total_length (int): The maximum number of messages the chat history can hold.         \"\"\"         super().__init__(messages, total_length)      def append(self, msg: str):         \"\"\"Add a message to the queue. The first messaage will always stay fixed.          Args:             msg (str): The message to be added to the queue         \"\"\"         if len(self) == self.total_length:             self.pop(1)         super().append(msg)     @dataclass class TagContentResult:     \"\"\"     A data class to represent the result of extracting tag content.      Attributes:         content (List[str]): A list of strings containing the content found between the specified tags.         found (bool): A flag indicating whether any content was found for the given tag.     \"\"\"      content: list[str]     found: bool   def extract_tag_content(text: str, tag: str) -&gt; TagContentResult:     \"\"\"     Extracts all content enclosed by specified tags (e.g., , , etc.).      Parameters:         text (str): The input string containing multiple potential tags.         tag (str): The name of the tag to search for (e.g., 'thought', 'response').      Returns:         dict: A dictionary with the following keys:             - 'content' (list): A list of strings containing the content found between the specified tags.             - 'found' (bool): A flag indicating whether any content was found for the given tag.     \"\"\"     # Build the regex pattern dynamically to find multiple occurrences of the tag     tag_pattern = rf\"&lt;{tag}&gt;(.*?)\"      # Use findall to capture all content between the specified tag     matched_contents = re.findall(tag_pattern, text, re.DOTALL)      # Return the dataclass instance with the result     return TagContentResult(         content=[content.strip() for content in matched_contents],         found=bool(matched_contents),     )   def fancy_print(message: str) -&gt; None:     \"\"\"     Displays a fancy print message.      Args:         message (str): The message to display.     \"\"\"     print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")     print(Fore.MAGENTA + f\"{message}\")     print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")     time.sleep(0.5)   def fancy_step_tracker(step: int, total_steps: int) -&gt; None:     \"\"\"     Displays a fancy step tracker for each iteration of the generation-reflection loop.      Args:         step (int): The current step in the loop.         total_steps (int): The total number of steps in the loop.     \"\"\"     fancy_print(f\"STEP {step + 1}/{total_steps}\")      import json from typing import Callable   def get_fn_signature(fn: Callable) -&gt; dict:     \"\"\"     Generates the signature for a given function.      Args:         fn (Callable): The function whose signature needs to be extracted.      Returns:         dict: A dictionary containing the function's name, description,               and parameter types.     \"\"\"     fn_signature: dict = {         \"name\": fn.__name__,         \"description\": fn.__doc__,         \"parameters\": {\"properties\": {}},     }     schema = {         k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"     }     fn_signature[\"parameters\"][\"properties\"] = schema     return fn_signature   def validate_arguments(tool_call: dict, tool_signature: dict) -&gt; dict:     \"\"\"     Validates and converts arguments in the input dictionary to match the expected types.      Args:         tool_call (dict): A dictionary containing the arguments passed to the tool.         tool_signature (dict): The expected function signature and parameter types.      Returns:         dict: The tool call dictionary with the arguments converted to the correct types if necessary.     \"\"\"     properties = tool_signature[\"parameters\"][\"properties\"]      # TODO: This is overly simplified but enough for simple Tools.     type_mapping = {         \"int\": int,         \"str\": str,         \"bool\": bool,         \"float\": float,     }      for arg_name, arg_value in tool_call[\"arguments\"].items():         expected_type = properties[arg_name].get(\"type\")          if not isinstance(arg_value, type_mapping[expected_type]):             tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)      return tool_call   class Tool:     \"\"\"     A class representing a tool that wraps a callable and its signature.      Attributes:         name (str): The name of the tool (function).         fn (Callable): The function that the tool represents.         fn_signature (str): JSON string representation of the function's signature.     \"\"\"      def __init__(self, name: str, fn: Callable, fn_signature: str):         self.name = name         self.fn = fn         self.fn_signature = fn_signature      def __str__(self):         return self.fn_signature      def run(self, **kwargs):         \"\"\"         Executes the tool (function) with provided arguments.          Args:             **kwargs: Keyword arguments passed to the function.          Returns:             The result of the function call.         \"\"\"         return self.fn(**kwargs)   def tool(fn: Callable):     \"\"\"     A decorator that wraps a function into a Tool object.      Args:         fn (Callable): The function to be wrapped.      Returns:         Tool: A Tool object containing the function, its name, and its signature.     \"\"\"      def wrapper():         fn_signature = get_fn_signature(fn)         return Tool(             name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)         )      return wrapper()   <p>As we did with the Tool Pattern, we also need a System Prompt for the ReAct technique. This System Prompt is very similar, the difference is that it describes the ReAct loop, so that the LLM is aware of the three operations it's allowed to use:</p> <ol> <li>Thought: The LLM will think about which action to take</li> <li>Action: The LLM will use a Tool to \"act on the environment\"</li> <li>Observation: The LLM will observe the tool output and reflect on the next thing to do.</li> </ol> <p>Another key difference from the Tool Pattern System Prompt is that we are going to enclose all the messages with tags, like these: , . We could implement the ReAct logic without these tags, but I found it eeasier for the LLM to understand the instructions this way.</p> <p>Ok! So without further ado, there's the prompt!</p> In\u00a0[\u00a0]: Copied! <pre># Define the System Prompt as a constant\nREACT_SYSTEM_PROMPT = \"\"\"\nYou are a function calling AI model. You operate by running a loop with the following steps: Thought, Action, Observation.\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.\nYou may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\ninto functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;, \"id\": &lt;monotonically-increasing-id&gt;}\n&lt;/tool_call&gt;\n\nHere are the available tools / actions:\n\n&lt;tools&gt; \n%s\n&lt;/tools&gt;\n\nExample session:\n\n&lt;question&gt;What's the current temperature in Madrid?&lt;/question&gt;\n&lt;thought&gt;I need to get the current weather in Madrid&lt;/thought&gt;\n&lt;tool_call&gt;{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}&lt;/tool_call&gt;\n\nYou will be called again with this:\n\n&lt;observation&gt;{0: {\"temperature\": 25, \"unit\": \"celsius\"}}&lt;/observation&gt;\n\nYou then output:\n\n&lt;response&gt;The current temperature in Madrid is 25 degrees Celsius&lt;/response&gt;\n\nAdditional constraints:\n\n- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with &lt;response&gt;&lt;/response&gt; tags.\n\"\"\"\n</pre> # Define the System Prompt as a constant REACT_SYSTEM_PROMPT = \"\"\" You are a function calling AI model. You operate by running a loop with the following steps: Thought, Action, Observation. You are provided with function signatures within  XML tags. You may call one or more functions to assist with the user query. Don' make assumptions about what values to plug into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.  For each function call return a json object with function name and arguments within  XML tags as follows:   {\"name\": ,\"arguments\": , \"id\": }   Here are the available tools / actions:    %s   Example session:  What's the current temperature in Madrid? I need to get the current weather in Madrid {\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}  You will be called again with this:  {0: {\"temperature\": 25, \"unit\": \"celsius\"}}  You then output:  The current temperature in Madrid is 25 degrees Celsius  Additional constraints:  - If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with  tags. \"\"\" <p>Let's build an example that involves the use of three tools, like the following ones.</p> In\u00a0[\u00a0]: Copied! <pre>@tool\ndef sum_two_elements(a: int, b: int) -&gt; int:\n    \"\"\"\n    Computes the sum of two integers.\n\n    Args:\n        a (int): The first integer to be summed.\n        b (int): The second integer to be summed.\n\n    Returns:\n        int: The sum of `a` and `b`.\n    \"\"\"\n    return a + b\n\n\n@tool\ndef multiply_two_elements(a: int, b: int) -&gt; int:\n    \"\"\"\n    Multiplies two integers.\n\n    Args:\n        a (int): The first integer to multiply.\n        b (int): The second integer to multiply.\n\n    Returns:\n        int: The product of `a` and `b`.\n    \"\"\"\n    return a * b\n\n@tool\ndef compute_log(x: int) -&gt; float | str:\n    \"\"\"\n    Computes the logarithm of an integer `x` with an optional base.\n\n    Args:\n        x (int): The integer value for which the logarithm is computed. Must be greater than 0.\n\n    Returns:\n        float: The logarithm of `x` to the specified `base`.\n    \"\"\"\n    if x &lt;= 0:\n        return \"Logarithm is undefined for values less than or equal to 0.\"\n    \n    return math.log(x)\n\n\navailable_tools = {\n    \"sum_two_elements\": sum_two_elements,\n    \"multiply_two_elements\": multiply_two_elements,\n    \"compute_log\": compute_log\n}\n</pre> @tool def sum_two_elements(a: int, b: int) -&gt; int:     \"\"\"     Computes the sum of two integers.      Args:         a (int): The first integer to be summed.         b (int): The second integer to be summed.      Returns:         int: The sum of `a` and `b`.     \"\"\"     return a + b   @tool def multiply_two_elements(a: int, b: int) -&gt; int:     \"\"\"     Multiplies two integers.      Args:         a (int): The first integer to multiply.         b (int): The second integer to multiply.      Returns:         int: The product of `a` and `b`.     \"\"\"     return a * b  @tool def compute_log(x: int) -&gt; float | str:     \"\"\"     Computes the logarithm of an integer `x` with an optional base.      Args:         x (int): The integer value for which the logarithm is computed. Must be greater than 0.      Returns:         float: The logarithm of `x` to the specified `base`.     \"\"\"     if x &lt;= 0:         return \"Logarithm is undefined for values less than or equal to 0.\"          return math.log(x)   available_tools = {     \"sum_two_elements\": sum_two_elements,     \"multiply_two_elements\": multiply_two_elements,     \"compute_log\": compute_log } <p>Remember that the <code>@tool</code> operator allows us to convert a Python function into a <code>Tool</code> automatically. We cana check that very easily with some of the functions above.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"Tool name: \", sum_two_elements.name)\nprint(\"Tool signature: \", sum_two_elements.fn_signature)\n</pre> print(\"Tool name: \", sum_two_elements.name) print(\"Tool signature: \", sum_two_elements.fn_signature) <p>Now, we just concatenate the tools signature and add them to the System Prompt.</p> In\u00a0[\u00a0]: Copied! <pre>tools_signature = sum_two_elements.fn_signature + \",\\n\" + multiply_two_elements.fn_signature + \",\\n\" + compute_log.fn_signature\n</pre> tools_signature = sum_two_elements.fn_signature + \",\\n\" + multiply_two_elements.fn_signature + \",\\n\" + compute_log.fn_signature In\u00a0[\u00a0]: Copied! <pre>print(tools_signature)\n</pre> print(tools_signature) In\u00a0[\u00a0]: Copied! <pre>REACT_SYSTEM_PROMPT = REACT_SYSTEM_PROMPT % tools_signature\n</pre> REACT_SYSTEM_PROMPT = REACT_SYSTEM_PROMPT % tools_signature In\u00a0[\u00a0]: Copied! <pre>print(REACT_SYSTEM_PROMPT)\n</pre> print(REACT_SYSTEM_PROMPT) In\u00a0[\u00a0]: Copied! <pre>USER_QUESTION = \"I want to calculate the sum of 1234 and 5678 and multiply the result by 5. Then, I want to take the logarithm of this result\"\nchat_history = [\n    {\n        \"role\": \"system\",\n        \"content\": REACT_SYSTEM_PROMPT\n    },\n    {\n        \"role\": \"user\",\n        \"content\": f\"&lt;question&gt;{USER_QUESTION}&lt;/question&gt;\"\n    }\n]\n</pre> USER_QUESTION = \"I want to calculate the sum of 1234 and 5678 and multiply the result by 5. Then, I want to take the logarithm of this result\" chat_history = [     {         \"role\": \"system\",         \"content\": REACT_SYSTEM_PROMPT     },     {         \"role\": \"user\",         \"content\": f\"{USER_QUESTION}\"     } ]  In\u00a0[\u00a0]: Copied! <pre>output = GROQ_CLIENT.chat.completions.create(\n    messages=chat_history,\n    model=MODEL\n).choices[0].message.content\n\nprint(output)\n</pre> output = GROQ_CLIENT.chat.completions.create(     messages=chat_history,     model=MODEL ).choices[0].message.content  print(output) In\u00a0[\u00a0]: Copied! <pre>chat_history.append(\n    {\n        \"role\": \"assistant\",\n        \"content\": output\n    }\n)\n</pre> chat_history.append(     {         \"role\": \"assistant\",         \"content\": output     } ) In\u00a0[\u00a0]: Copied! <pre>tool_call = extract_tag_content(output, tag=\"tool_call\")\n</pre> tool_call = extract_tag_content(output, tag=\"tool_call\") In\u00a0[\u00a0]: Copied! <pre>tool_call\n</pre> tool_call In\u00a0[\u00a0]: Copied! <pre>tool_call = json.loads(tool_call.content[0])\n</pre> tool_call = json.loads(tool_call.content[0]) In\u00a0[\u00a0]: Copied! <pre>tool_call\n</pre> tool_call In\u00a0[\u00a0]: Copied! <pre>tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"])\n</pre> tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"]) In\u00a0[\u00a0]: Copied! <pre>assert tool_result == 1234 + 5678\n</pre> assert tool_result == 1234 + 5678 In\u00a0[\u00a0]: Copied! <pre>chat_history.append(\n    {\n        \"role\": \"user\",\n        \"content\": f\"&lt;observation&gt;{tool_result}&lt;/observation&gt;\"\n    }\n)\n</pre> chat_history.append(     {         \"role\": \"user\",         \"content\": f\"{tool_result}\"     } ) In\u00a0[\u00a0]: Copied! <pre>output = GROQ_CLIENT.chat.completions.create(\n    messages=chat_history,\n    model=MODEL\n).choices[0].message.content\n\nprint(output)\n</pre> output = GROQ_CLIENT.chat.completions.create(     messages=chat_history,     model=MODEL ).choices[0].message.content  print(output) In\u00a0[\u00a0]: Copied! <pre>chat_history.append(\n    {\n        \"role\": \"assistant\",\n        \"content\": output\n    }\n)\n</pre> chat_history.append(     {         \"role\": \"assistant\",         \"content\": output     } ) In\u00a0[\u00a0]: Copied! <pre>tool_call = extract_tag_content(output, tag=\"tool_call\")\ntool_call = json.loads(tool_call.content[0])\ntool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"])\n</pre> tool_call = extract_tag_content(output, tag=\"tool_call\") tool_call = json.loads(tool_call.content[0]) tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"]) In\u00a0[\u00a0]: Copied! <pre>tool_result\n</pre> tool_result In\u00a0[\u00a0]: Copied! <pre>assert tool_result == (1234 + 5678) * 5\n</pre> assert tool_result == (1234 + 5678) * 5 In\u00a0[\u00a0]: Copied! <pre>chat_history.append(\n    {\n        \"role\": \"user\",\n        \"content\": f\"&lt;observation&gt;{tool_result}&lt;/observation&gt;\"\n    }\n)\n</pre> chat_history.append(     {         \"role\": \"user\",         \"content\": f\"{tool_result}\"     } ) In\u00a0[\u00a0]: Copied! <pre>output = GROQ_CLIENT.chat.completions.create(\n    messages=chat_history,\n    model=MODEL\n).choices[0].message.content\n\nprint(output)\n</pre> output = GROQ_CLIENT.chat.completions.create(     messages=chat_history,     model=MODEL ).choices[0].message.content  print(output) In\u00a0[\u00a0]: Copied! <pre>chat_history.append(\n    {\n        \"role\": \"assistant\",\n        \"content\": output\n    }\n)\n</pre> chat_history.append(     {         \"role\": \"assistant\",         \"content\": output     } ) In\u00a0[\u00a0]: Copied! <pre>tool_call = extract_tag_content(output, tag=\"tool_call\")\ntool_call = json.loads(tool_call.content[0])\ntool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"])\n</pre> tool_call = extract_tag_content(output, tag=\"tool_call\") tool_call = json.loads(tool_call.content[0]) tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"]) In\u00a0[\u00a0]: Copied! <pre>tool_result\n</pre> tool_result In\u00a0[\u00a0]: Copied! <pre>assert tool_result == math.log((1234 + 5678) * 5)\n</pre> assert tool_result == math.log((1234 + 5678) * 5) In\u00a0[\u00a0]: Copied! <pre>chat_history.append(\n    {\n        \"role\": \"user\",\n        \"content\": f\"&lt;observation&gt;{tool_result}&lt;/observation&gt;\"\n    }\n)\n</pre> chat_history.append(     {         \"role\": \"user\",         \"content\": f\"{tool_result}\"     } ) In\u00a0[\u00a0]: Copied! <pre>output = GROQ_CLIENT.chat.completions.create(\n    messages=chat_history,\n    model=MODEL\n).choices[0].message.content\n\nprint(output)\n</pre> output = GROQ_CLIENT.chat.completions.create(     messages=chat_history,     model=MODEL ).choices[0].message.content  print(output) In\u00a0[\u00a0]: Copied! <pre>import json\nimport re\n\nfrom colorama import Fore\nfrom dotenv import load_dotenv\nfrom groq import Groq\n\nload_dotenv()\n\nBASE_SYSTEM_PROMPT = \"\"\n\n\nREACT_SYSTEM_PROMPT = \"\"\"\nYou operate by running a loop with the following steps: Thought, Action, Observation.\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.\nYou may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\ninto functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;, \"id\": &lt;monotonically-increasing-id&gt;}\n&lt;/tool_call&gt;\n\nHere are the available tools / actions:\n\n&lt;tools&gt;\n%s\n&lt;/tools&gt;\n\nExample session:\n\n&lt;question&gt;What's the current temperature in Madrid?&lt;/question&gt;\n&lt;thought&gt;I need to get the current weather in Madrid&lt;/thought&gt;\n&lt;tool_call&gt;{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}&lt;/tool_call&gt;\n\nYou will be called again with this:\n\n&lt;observation&gt;{0: {\"temperature\": 25, \"unit\": \"celsius\"}}&lt;/observation&gt;\n\nYou then output:\n\n&lt;response&gt;The current temperature in Madrid is 25 degrees Celsius&lt;/response&gt;\n\nAdditional constraints:\n\n- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with &lt;response&gt;&lt;/response&gt; tags.\n\"\"\"\n\n\nclass ReactAgent:\n    \"\"\"\n    A class that represents an agent using the ReAct logic that interacts with tools to process\n    user inputs, make decisions, and execute tool calls. The agent can run interactive sessions,\n    collect tool signatures, and process multiple tool calls in a given round of interaction.\n\n    Attributes:\n        client (Groq): The Groq client used to handle model-based completions.\n        model (str): The name of the model used for generating responses. Default is \"llama-3.1-70b-versatile\".\n        tools (list[Tool]): A list of Tool instances available for execution.\n        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool instances.\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: Tool | list[Tool],\n        model: str = \"llama-3.1-70b-versatile\",\n        system_prompt: str = BASE_SYSTEM_PROMPT,\n    ) -&gt; None:\n        self.client = Groq()\n        self.model = model\n        self.system_prompt = system_prompt\n        self.tools = tools if isinstance(tools, list) else [tools]\n        self.tools_dict = {tool.name: tool for tool in self.tools}\n\n    def add_tool_signatures(self) -&gt; str:\n        \"\"\"\n        Collects the function signatures of all available tools.\n\n        Returns:\n            str: A concatenated string of all tool function signatures in JSON format.\n        \"\"\"\n        return \"\".join([tool.fn_signature for tool in self.tools])\n\n    def process_tool_calls(self, tool_calls_content: list) -&gt; dict:\n        \"\"\"\n        Processes each tool call, validates arguments, executes the tools, and collects results.\n\n        Args:\n            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n\n        Returns:\n            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n        \"\"\"\n        observations = {}\n        for tool_call_str in tool_calls_content:\n            tool_call = json.loads(tool_call_str)\n            tool_name = tool_call[\"name\"]\n            tool = self.tools_dict[tool_name]\n\n            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n\n            # Validate and execute the tool call\n            validated_tool_call = validate_arguments(\n                tool_call, json.loads(tool.fn_signature)\n            )\n            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n\n            result = tool.run(**validated_tool_call[\"arguments\"])\n            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n\n            # Store the result using the tool call ID\n            observations[validated_tool_call[\"id\"]] = result\n\n        return observations\n\n    def run(\n        self,\n        user_msg: str,\n        max_rounds: int = 10,\n    ) -&gt; str:\n        \"\"\"\n        Executes a user interaction session, where the agent processes user input, generates responses,\n        handles tool calls, and updates chat history until a final response is ready or the maximum\n        number of rounds is reached.\n\n        Args:\n            user_msg (str): The user's input message to start the interaction.\n            max_rounds (int, optional): Maximum number of interaction rounds the agent should perform. Default is 10.\n\n        Returns:\n            str: The final response generated by the agent after processing user input and any tool calls.\n        \"\"\"\n        user_prompt = build_prompt_structure(\n            prompt=user_msg, role=\"user\", tag=\"question\"\n        )\n        if self.tools:\n            self.system_prompt += (\n                \"\\n\" + REACT_SYSTEM_PROMPT % self.add_tool_signatures()\n            )\n\n        chat_history = ChatHistory(\n            [\n                build_prompt_structure(\n                    prompt=self.system_prompt,\n                    role=\"system\",\n                ),\n                user_prompt,\n            ]\n        )\n\n        if self.tools:\n            # Run the ReAct loop for max_rounds\n            for _ in range(max_rounds):\n\n                completion = completions_create(self.client, chat_history, self.model)\n\n                response = extract_tag_content(str(completion), \"response\")\n                if response.found:\n                    return response.content[0]\n\n                thought = extract_tag_content(str(completion), \"thought\")\n                tool_calls = extract_tag_content(str(completion), \"tool_call\")\n\n                update_chat_history(chat_history, completion, \"assistant\")\n\n                print(Fore.MAGENTA + f\"\\nThought: {thought.content[0]}\")\n\n                if tool_calls.found:\n                    observations = self.process_tool_calls(tool_calls.content)\n                    print(Fore.BLUE + f\"\\nObservations: {observations}\")\n                    update_chat_history(chat_history, f\"{observations}\", \"user\")\n\n        return completions_create(self.client, chat_history, self.model)\n</pre> import json import re  from colorama import Fore from dotenv import load_dotenv from groq import Groq  load_dotenv()  BASE_SYSTEM_PROMPT = \"\"   REACT_SYSTEM_PROMPT = \"\"\" You operate by running a loop with the following steps: Thought, Action, Observation. You are provided with function signatures within  XML tags. You may call one or more functions to assist with the user query. Don' make assumptions about what values to plug into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.  For each function call return a json object with function name and arguments within  XML tags as follows:   {\"name\": ,\"arguments\": , \"id\": }   Here are the available tools / actions:   %s   Example session:  What's the current temperature in Madrid? I need to get the current weather in Madrid {\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}  You will be called again with this:  {0: {\"temperature\": 25, \"unit\": \"celsius\"}}  You then output:  The current temperature in Madrid is 25 degrees Celsius  Additional constraints:  - If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with  tags. \"\"\"   class ReactAgent:     \"\"\"     A class that represents an agent using the ReAct logic that interacts with tools to process     user inputs, make decisions, and execute tool calls. The agent can run interactive sessions,     collect tool signatures, and process multiple tool calls in a given round of interaction.      Attributes:         client (Groq): The Groq client used to handle model-based completions.         model (str): The name of the model used for generating responses. Default is \"llama-3.1-70b-versatile\".         tools (list[Tool]): A list of Tool instances available for execution.         tools_dict (dict): A dictionary mapping tool names to their corresponding Tool instances.     \"\"\"      def __init__(         self,         tools: Tool | list[Tool],         model: str = \"llama-3.1-70b-versatile\",         system_prompt: str = BASE_SYSTEM_PROMPT,     ) -&gt; None:         self.client = Groq()         self.model = model         self.system_prompt = system_prompt         self.tools = tools if isinstance(tools, list) else [tools]         self.tools_dict = {tool.name: tool for tool in self.tools}      def add_tool_signatures(self) -&gt; str:         \"\"\"         Collects the function signatures of all available tools.          Returns:             str: A concatenated string of all tool function signatures in JSON format.         \"\"\"         return \"\".join([tool.fn_signature for tool in self.tools])      def process_tool_calls(self, tool_calls_content: list) -&gt; dict:         \"\"\"         Processes each tool call, validates arguments, executes the tools, and collects results.          Args:             tool_calls_content (list): List of strings, each representing a tool call in JSON format.          Returns:             dict: A dictionary where the keys are tool call IDs and values are the results from the tools.         \"\"\"         observations = {}         for tool_call_str in tool_calls_content:             tool_call = json.loads(tool_call_str)             tool_name = tool_call[\"name\"]             tool = self.tools_dict[tool_name]              print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")              # Validate and execute the tool call             validated_tool_call = validate_arguments(                 tool_call, json.loads(tool.fn_signature)             )             print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")              result = tool.run(**validated_tool_call[\"arguments\"])             print(Fore.GREEN + f\"\\nTool result: \\n{result}\")              # Store the result using the tool call ID             observations[validated_tool_call[\"id\"]] = result          return observations      def run(         self,         user_msg: str,         max_rounds: int = 10,     ) -&gt; str:         \"\"\"         Executes a user interaction session, where the agent processes user input, generates responses,         handles tool calls, and updates chat history until a final response is ready or the maximum         number of rounds is reached.          Args:             user_msg (str): The user's input message to start the interaction.             max_rounds (int, optional): Maximum number of interaction rounds the agent should perform. Default is 10.          Returns:             str: The final response generated by the agent after processing user input and any tool calls.         \"\"\"         user_prompt = build_prompt_structure(             prompt=user_msg, role=\"user\", tag=\"question\"         )         if self.tools:             self.system_prompt += (                 \"\\n\" + REACT_SYSTEM_PROMPT % self.add_tool_signatures()             )          chat_history = ChatHistory(             [                 build_prompt_structure(                     prompt=self.system_prompt,                     role=\"system\",                 ),                 user_prompt,             ]         )          if self.tools:             # Run the ReAct loop for max_rounds             for _ in range(max_rounds):                  completion = completions_create(self.client, chat_history, self.model)                  response = extract_tag_content(str(completion), \"response\")                 if response.found:                     return response.content[0]                  thought = extract_tag_content(str(completion), \"thought\")                 tool_calls = extract_tag_content(str(completion), \"tool_call\")                  update_chat_history(chat_history, completion, \"assistant\")                  print(Fore.MAGENTA + f\"\\nThought: {thought.content[0]}\")                  if tool_calls.found:                     observations = self.process_tool_calls(tool_calls.content)                     print(Fore.BLUE + f\"\\nObservations: {observations}\")                     update_chat_history(chat_history, f\"{observations}\", \"user\")          return completions_create(self.client, chat_history, self.model)  In\u00a0[\u00a0]: Copied! <pre>agent = ReactAgent(tools=[sum_two_elements, multiply_two_elements, compute_log])\n</pre> agent = ReactAgent(tools=[sum_two_elements, multiply_two_elements, compute_log]) In\u00a0[\u00a0]: Copied! <pre>agent.run(user_msg=\"I want to calculate the sum of 1234 and 5678 and multiply the result by 5. Then, I want to take the logarithm of this result\")\n</pre> agent.run(user_msg=\"I want to calculate the sum of 1234 and 5678 and multiply the result by 5. Then, I want to take the logarithm of this result\") <p>We did it!! A ReAct Agent working as expected, completely from Scratch! \ud83d\ude80\ud83d\ude80\ud83d\ude80\ud83d\ude80</p>"},{"location":"Agents/patterns/planning_pattern/#planning-pattern-react-technique","title":"Planning Pattern - ReAct Technique\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#relevant-imports-and-groq-client","title":"Relevant imports and Groq Client\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#utils","title":"Utils\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#a-system-prompt-for-the-react-loop","title":"A System Prompt for the ReAct Loop\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#example-step-by-step","title":"Example step by step\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#defining-the-tools","title":"Defining the Tools\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#adding-the-tools-signature-to-the-system-prompt","title":"Adding the Tools signature to the System Prompt\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-1","title":"ReAct Loop Step 1\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-2","title":"ReAct Loop Step 2\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-3","title":"ReAct Loop Step 3\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-4","title":"ReAct Loop Step 4\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-5","title":"ReAct Loop Step 5\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-6","title":"ReAct Loop Step 6\u00b6","text":""},{"location":"Agents/patterns/planning_pattern/#react-loop-step-7","title":"ReAct Loop Step 7\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/","title":"Agent Reflection Pattern","text":"<p>The first thing we need to consider is:</p> <p>What do we want to generate? A poem? An essay? Python code?</p> <p>For this example, I've decided to test the Python coding skills of Llama3 70B (that's the LLM we are going to use for all the tutorials). In particular, we are going to ask our LLM to code a famous sorting algorithm: Merge Sort.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom pprint import pprint\nfrom groq import Groq\nfrom dotenv import load_dotenv\nfrom IPython.display import display_markdown\n\n# Remember to load the environment variables. You should have the Groq API Key in there :)\nload_dotenv()\n\nclient = Groq()\n</pre> import os from pprint import pprint from groq import Groq from dotenv import load_dotenv from IPython.display import display_markdown  # Remember to load the environment variables. You should have the Groq API Key in there :) load_dotenv()  client = Groq() <p>We will start the \"generation\" chat history with the system prompt, as we said before. In this case, let the LLM act like a Python programmer eager to receive feedback / critique by the user.</p> In\u00a0[\u00a0]: Copied! <pre>generation_chat_history = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n        \"respond with a revised version of your previous attempt.\"\n    }\n]\n</pre> generation_chat_history = [     {         \"role\": \"system\",         \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"         \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\"          \"respond with a revised version of your previous attempt.\"     } ] <p>Now, as the user, we are going to ask the LLM to generate an implementation of the Merge Sort algorithm. Just add a new message with the user role to the chat history.</p> In\u00a0[\u00a0]: Copied! <pre>generation_chat_history.append(\n    {\n        \"role\": \"user\",\n        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n    }\n)\n</pre> generation_chat_history.append(     {         \"role\": \"user\",         \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"     } ) <p>Let's generate the first version of the essay.</p> In\u00a0[\u00a0]: Copied! <pre>mergesort_code = client.chat.completions.create(\n    messages=generation_chat_history,\n    model=\"llama3-70b-8192\"\n).choices[0].message.content\n\ngeneration_chat_history.append(\n    {\n        \"role\": \"assistant\",\n        \"content\": mergesort_code\n    }\n)\n</pre> mergesort_code = client.chat.completions.create(     messages=generation_chat_history,     model=\"llama3-70b-8192\" ).choices[0].message.content  generation_chat_history.append(     {         \"role\": \"assistant\",         \"content\": mergesort_code     } ) In\u00a0[\u00a0]: Copied! <pre>display_markdown(mergesort_code, raw=True)\n</pre> display_markdown(mergesort_code, raw=True) <p>Now, let's allow the LLM to reflect on its outputs by defining another system prompt. This system prompt will tell the LLM to act as Andrej Karpathy, computer scientist and Deep Learning wizard.</p> <p>To be honest, I don't think the fact of acting like Andrej Karpathy will influence the LLM outputs, but it was fun :)</p> In\u00a0[\u00a0]: Copied! <pre>reflection_chat_history = [\n    {\n    \"role\": \"system\",\n    \"content\": \"You are Andrej Karpathy, an experienced computer scientist. You are tasked with generating critique and recommendations for the user's code\",\n    }\n]\n</pre> reflection_chat_history = [     {     \"role\": \"system\",     \"content\": \"You are Andrej Karpathy, an experienced computer scientist. You are tasked with generating critique and recommendations for the user's code\",     } ] <p>The user message, in this case,  is the essay generated in the previous step. We simply add the <code>mergesort_code</code> to the <code>reflection_chat_history</code>.</p> In\u00a0[\u00a0]: Copied! <pre>reflection_chat_history.append(\n    {\n        \"role\": \"user\",\n        \"content\": mergesort_code\n    }\n)\n</pre> reflection_chat_history.append(     {         \"role\": \"user\",         \"content\": mergesort_code     } ) <p>Now, let's generate a critique to the Python code.</p> In\u00a0[\u00a0]: Copied! <pre>critique = client.chat.completions.create(\n    messages=reflection_chat_history,\n    model=\"llama3-70b-8192\"\n).choices[0].message.content\n</pre> critique = client.chat.completions.create(     messages=reflection_chat_history,     model=\"llama3-70b-8192\" ).choices[0].message.content In\u00a0[\u00a0]: Copied! <pre>display_markdown(critique, raw=True)\n</pre> display_markdown(critique, raw=True) <p>Finally, we just need to add this critique to the <code>generation_chat_history</code>, in this case, as the <code>user</code> role.</p> In\u00a0[\u00a0]: Copied! <pre>generation_chat_history.append(\n    {\n        \"role\": \"user\",\n        \"content\": critique\n    }\n)\n</pre> generation_chat_history.append(     {         \"role\": \"user\",         \"content\": critique     } ) In\u00a0[\u00a0]: Copied! <pre>essay = client.chat.completions.create(\n    messages=generation_chat_history,\n    model=\"llama3-70b-8192\"\n).choices[0].message.content\n</pre> essay = client.chat.completions.create(     messages=generation_chat_history,     model=\"llama3-70b-8192\" ).choices[0].message.content In\u00a0[\u00a0]: Copied! <pre>display_markdown(essay, raw=True)\n</pre> display_markdown(essay, raw=True) <p>After Generation Step (II) the corrected Python code will be received, once again, by Karpathy. Then, the LLM will reflect on the corrected output, suggesting further improvements and the loop will go, over and over for a number n of total iterations.</p> <p>There's another possibility. Suppose the Reflection step can't find any further improvement. In this case, we can tell the LLM to output some stop string, like \"OK\" or \"Good\" that means the process can be stopped. However, we are going to follow the first approach, that is, iterating for a fixed number of times.</p> <p>Now that you understand the underlying loop of the Reflection Agent, let's implement this agent as a class.</p> In\u00a0[1]: Copied! <pre>import re\nfrom dataclasses import dataclass\nimport time\n\nfrom colorama import Fore\nfrom colorama import Style\n\ndef completions_create(client, messages: list, model: str) -&gt; str:\n    \"\"\"\n    Sends a request to the client's `completions.create` method to interact with the language model.\n\n    Args:\n        client (Groq): The Groq client object\n        messages (list[dict]): A list of message objects containing chat history for the model.\n        model (str): The model to use for generating tool calls and responses.\n\n    Returns:\n        str: The content of the model's response.\n    \"\"\"\n    response = client.chat.completions.create(messages=messages, model=model)\n    return str(response.choices[0].message.content)\n\n\ndef build_prompt_structure(prompt: str, role: str, tag: str = \"\") -&gt; dict:\n    \"\"\"\n    Builds a structured prompt that includes the role and content.\n\n    Args:\n        prompt (str): The actual content of the prompt.\n        role (str): The role of the speaker (e.g., user, assistant).\n\n    Returns:\n        dict: A dictionary representing the structured prompt.\n    \"\"\"\n    if tag:\n        prompt = f\"&lt;{tag}&gt;{prompt}&lt;/{tag}&gt;\"\n    return {\"role\": role, \"content\": prompt}\n\n\ndef update_chat_history(history: list, msg: str, role: str):\n    \"\"\"\n    Updates the chat history by appending the latest response.\n\n    Args:\n        history (list): The list representing the current chat history.\n        msg (str): The message to append.\n        role (str): The role type (e.g. 'user', 'assistant', 'system')\n    \"\"\"\n    history.append(build_prompt_structure(prompt=msg, role=role))\n\n\nclass ChatHistory(list):\n    def __init__(self, messages: list | None = None, total_length: int = -1):\n        \"\"\"Initialise the queue with a fixed total length.\n\n        Args:\n            messages (list | None): A list of initial messages\n            total_length (int): The maximum number of messages the chat history can hold.\n        \"\"\"\n        if messages is None:\n            messages = []\n\n        super().__init__(messages)\n        self.total_length = total_length\n\n    def append(self, msg: str):\n        \"\"\"Add a message to the queue.\n\n        Args:\n            msg (str): The message to be added to the queue\n        \"\"\"\n        if len(self) == self.total_length:\n            self.pop(0)\n        super().append(msg)\n\n\nclass FixedFirstChatHistory(ChatHistory):\n    def __init__(self, messages: list | None = None, total_length: int = -1):\n        \"\"\"Initialise the queue with a fixed total length.\n\n        Args:\n            messages (list | None): A list of initial messages\n            total_length (int): The maximum number of messages the chat history can hold.\n        \"\"\"\n        super().__init__(messages, total_length)\n\n    def append(self, msg: str):\n        \"\"\"Add a message to the queue. The first messaage will always stay fixed.\n\n        Args:\n            msg (str): The message to be added to the queue\n        \"\"\"\n        if len(self) == self.total_length:\n            self.pop(1)\n        super().append(msg)\n\n\n\n\n@dataclass\nclass TagContentResult:\n    \"\"\"\n    A data class to represent the result of extracting tag content.\n\n    Attributes:\n        content (List[str]): A list of strings containing the content found between the specified tags.\n        found (bool): A flag indicating whether any content was found for the given tag.\n    \"\"\"\n\n    content: list[str]\n    found: bool\n\n\ndef extract_tag_content(text: str, tag: str) -&gt; TagContentResult:\n    \"\"\"\n    Extracts all content enclosed by specified tags (e.g., &lt;thought&gt;, &lt;response&gt;, etc.).\n\n    Parameters:\n        text (str): The input string containing multiple potential tags.\n        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n\n    Returns:\n        dict: A dictionary with the following keys:\n            - 'content' (list): A list of strings containing the content found between the specified tags.\n            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n    \"\"\"\n    # Build the regex pattern dynamically to find multiple occurrences of the tag\n    tag_pattern = rf\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"\n\n    # Use findall to capture all content between the specified tag\n    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n\n    # Return the dataclass instance with the result\n    return TagContentResult(\n        content=[content.strip() for content in matched_contents],\n        found=bool(matched_contents),\n    )\n\n\ndef fancy_print(message: str) -&gt; None:\n    \"\"\"\n    Displays a fancy print message.\n\n    Args:\n        message (str): The message to display.\n    \"\"\"\n    print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")\n    print(Fore.MAGENTA + f\"{message}\")\n    print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")\n    time.sleep(0.5)\n\n\ndef fancy_step_tracker(step: int, total_steps: int) -&gt; None:\n    \"\"\"\n    Displays a fancy step tracker for each iteration of the generation-reflection loop.\n\n    Args:\n        step (int): The current step in the loop.\n        total_steps (int): The total number of steps in the loop.\n    \"\"\"\n    fancy_print(f\"STEP {step + 1}/{total_steps}\")\n</pre> import re from dataclasses import dataclass import time  from colorama import Fore from colorama import Style  def completions_create(client, messages: list, model: str) -&gt; str:     \"\"\"     Sends a request to the client's `completions.create` method to interact with the language model.      Args:         client (Groq): The Groq client object         messages (list[dict]): A list of message objects containing chat history for the model.         model (str): The model to use for generating tool calls and responses.      Returns:         str: The content of the model's response.     \"\"\"     response = client.chat.completions.create(messages=messages, model=model)     return str(response.choices[0].message.content)   def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -&gt; dict:     \"\"\"     Builds a structured prompt that includes the role and content.      Args:         prompt (str): The actual content of the prompt.         role (str): The role of the speaker (e.g., user, assistant).      Returns:         dict: A dictionary representing the structured prompt.     \"\"\"     if tag:         prompt = f\"&lt;{tag}&gt;{prompt}\"     return {\"role\": role, \"content\": prompt}   def update_chat_history(history: list, msg: str, role: str):     \"\"\"     Updates the chat history by appending the latest response.      Args:         history (list): The list representing the current chat history.         msg (str): The message to append.         role (str): The role type (e.g. 'user', 'assistant', 'system')     \"\"\"     history.append(build_prompt_structure(prompt=msg, role=role))   class ChatHistory(list):     def __init__(self, messages: list | None = None, total_length: int = -1):         \"\"\"Initialise the queue with a fixed total length.          Args:             messages (list | None): A list of initial messages             total_length (int): The maximum number of messages the chat history can hold.         \"\"\"         if messages is None:             messages = []          super().__init__(messages)         self.total_length = total_length      def append(self, msg: str):         \"\"\"Add a message to the queue.          Args:             msg (str): The message to be added to the queue         \"\"\"         if len(self) == self.total_length:             self.pop(0)         super().append(msg)   class FixedFirstChatHistory(ChatHistory):     def __init__(self, messages: list | None = None, total_length: int = -1):         \"\"\"Initialise the queue with a fixed total length.          Args:             messages (list | None): A list of initial messages             total_length (int): The maximum number of messages the chat history can hold.         \"\"\"         super().__init__(messages, total_length)      def append(self, msg: str):         \"\"\"Add a message to the queue. The first messaage will always stay fixed.          Args:             msg (str): The message to be added to the queue         \"\"\"         if len(self) == self.total_length:             self.pop(1)         super().append(msg)     @dataclass class TagContentResult:     \"\"\"     A data class to represent the result of extracting tag content.      Attributes:         content (List[str]): A list of strings containing the content found between the specified tags.         found (bool): A flag indicating whether any content was found for the given tag.     \"\"\"      content: list[str]     found: bool   def extract_tag_content(text: str, tag: str) -&gt; TagContentResult:     \"\"\"     Extracts all content enclosed by specified tags (e.g., , , etc.).      Parameters:         text (str): The input string containing multiple potential tags.         tag (str): The name of the tag to search for (e.g., 'thought', 'response').      Returns:         dict: A dictionary with the following keys:             - 'content' (list): A list of strings containing the content found between the specified tags.             - 'found' (bool): A flag indicating whether any content was found for the given tag.     \"\"\"     # Build the regex pattern dynamically to find multiple occurrences of the tag     tag_pattern = rf\"&lt;{tag}&gt;(.*?)\"      # Use findall to capture all content between the specified tag     matched_contents = re.findall(tag_pattern, text, re.DOTALL)      # Return the dataclass instance with the result     return TagContentResult(         content=[content.strip() for content in matched_contents],         found=bool(matched_contents),     )   def fancy_print(message: str) -&gt; None:     \"\"\"     Displays a fancy print message.      Args:         message (str): The message to display.     \"\"\"     print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")     print(Fore.MAGENTA + f\"{message}\")     print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")     time.sleep(0.5)   def fancy_step_tracker(step: int, total_steps: int) -&gt; None:     \"\"\"     Displays a fancy step tracker for each iteration of the generation-reflection loop.      Args:         step (int): The current step in the loop.         total_steps (int): The total number of steps in the loop.     \"\"\"     fancy_print(f\"STEP {step + 1}/{total_steps}\")  In\u00a0[\u00a0]: Copied! <pre>from colorama import Fore\nfrom dotenv import load_dotenv\nfrom groq import Groq\nload_dotenv()\n\n\nBASE_GENERATION_SYSTEM_PROMPT = \"\"\"\nYour task is to Generate the best content possible for the user's request.\nIf the user provides critique, respond with a revised version of your previous attempt.\nYou must always output the revised content.\n\"\"\"\n\nBASE_REFLECTION_SYSTEM_PROMPT = \"\"\"\nYou are tasked with generating critique and recommendations to the user's generated content.\nIf the user content has something wrong or something to be improved, output a list of recommendations\nand critiques. If the user content is ok and there's nothing to change, output this: &lt;OK&gt;\n\"\"\"\n\n\nclass ReflectionAgent:\n    \"\"\"\n    A class that implements a Reflection Agent, which generates responses and reflects\n    on them using the LLM to iteratively improve the interaction. The agent first generates\n    responses based on provided prompts and then critiques them in a reflection step.\n\n    Attributes:\n        model (str): The model name used for generating and reflecting on responses.\n        client (Groq): An instance of the Groq client to interact with the language model.\n    \"\"\"\n\n    def __init__(self, model: str = \"llama-3.1-70b-versatile\"):\n        self.client = Groq()\n        self.model = model\n\n    def _request_completion(\n        self,\n        history: list,\n        verbose: int = 0,\n        log_title: str = \"COMPLETION\",\n        log_color: str = \"\",\n    ):\n        \"\"\"\n        A private method to request a completion from the Groq model.\n\n        Args:\n            history (list): A list of messages forming the conversation or reflection history.\n            verbose (int, optional): The verbosity level. Defaults to 0 (no output).\n\n        Returns:\n            str: The model-generated response.\n        \"\"\"\n        output = completions_create(self.client, history, self.model)\n\n        if verbose &gt; 0:\n            print(log_color, f\"\\n\\n{log_title}\\n\\n\", output)\n\n        return output\n\n    def generate(self, generation_history: list, verbose: int = 0) -&gt; str:\n        \"\"\"\n        Generates a response based on the provided generation history using the model.\n\n        Args:\n            generation_history (list): A list of messages forming the conversation or generation history.\n            verbose (int, optional): The verbosity level, controlling printed output. Defaults to 0.\n\n        Returns:\n            str: The generated response.\n        \"\"\"\n        return self._request_completion(\n            generation_history, verbose, log_title=\"GENERATION\", log_color=Fore.BLUE\n        )\n\n    def reflect(self, reflection_history: list, verbose: int = 0) -&gt; str:\n        \"\"\"\n        Reflects on the generation history by generating a critique or feedback.\n\n        Args:\n            reflection_history (list): A list of messages forming the reflection history, typically based on\n                                       the previous generation or interaction.\n            verbose (int, optional): The verbosity level, controlling printed output. Defaults to 0.\n\n        Returns:\n            str: The critique or reflection response from the model.\n        \"\"\"\n        return self._request_completion(\n            reflection_history, verbose, log_title=\"REFLECTION\", log_color=Fore.GREEN\n        )\n\n    def run(\n        self,\n        user_msg: str,\n        generation_system_prompt: str = \"\",\n        reflection_system_prompt: str = \"\",\n        n_steps: int = 10,\n        verbose: int = 0,\n    ) -&gt; str:\n        \"\"\"\n        Runs the ReflectionAgent over multiple steps, alternating between generating a response\n        and reflecting on it for the specified number of steps.\n\n        Args:\n            user_msg (str): The user message or query that initiates the interaction.\n            generation_system_prompt (str, optional): The system prompt for guiding the generation process.\n            reflection_system_prompt (str, optional): The system prompt for guiding the reflection process.\n            n_steps (int, optional): The number of generate-reflect cycles to perform. Defaults to 3.\n            verbose (int, optional): The verbosity level controlling printed output. Defaults to 0.\n\n        Returns:\n            str: The final generated response after all cycles are completed.\n        \"\"\"\n        generation_system_prompt += BASE_GENERATION_SYSTEM_PROMPT\n        reflection_system_prompt += BASE_REFLECTION_SYSTEM_PROMPT\n\n        # Given the iterative nature of the Reflection Pattern, we might exhaust the LLM context (or\n        # make it really slow). That's the reason I'm limitting the chat history to three messages.\n        # The `FixedFirstChatHistory` is a very simple class, that creates a Queue that always keeps\n        # fixeed the first message. I thought this would be useful for maintaining the system prompt\n        # in the chat history.\n        generation_history = FixedFirstChatHistory(\n            [\n                build_prompt_structure(prompt=generation_system_prompt, role=\"system\"),\n                build_prompt_structure(prompt=user_msg, role=\"user\"),\n            ],\n            total_length=3,\n        )\n\n        reflection_history = FixedFirstChatHistory(\n            [build_prompt_structure(prompt=reflection_system_prompt, role=\"system\")],\n            total_length=3,\n        )\n\n        for step in range(n_steps):\n            if verbose &gt; 0:\n                fancy_step_tracker(step, n_steps)\n\n            # Generate the response\n            generation = self.generate(generation_history, verbose=verbose)\n            update_chat_history(generation_history, generation, \"assistant\")\n            update_chat_history(reflection_history, generation, \"user\")\n\n            # Reflect and critique the generation\n            critique = self.reflect(reflection_history, verbose=verbose)\n\n            if \"&lt;OK&gt;\" in critique:\n                # If no additional suggestions are made, stop the loop\n                print(\n                    Fore.RED,\n                    \"\\n\\nStop Sequence found. Stopping the reflection loop ... \\n\\n\",\n                )\n                break\n\n            update_chat_history(generation_history, critique, \"user\")\n            update_chat_history(reflection_history, critique, \"assistant\")\n\n        return generation\n</pre> from colorama import Fore from dotenv import load_dotenv from groq import Groq load_dotenv()   BASE_GENERATION_SYSTEM_PROMPT = \"\"\" Your task is to Generate the best content possible for the user's request. If the user provides critique, respond with a revised version of your previous attempt. You must always output the revised content. \"\"\"  BASE_REFLECTION_SYSTEM_PROMPT = \"\"\" You are tasked with generating critique and recommendations to the user's generated content. If the user content has something wrong or something to be improved, output a list of recommendations and critiques. If the user content is ok and there's nothing to change, output this:  \"\"\"   class ReflectionAgent:     \"\"\"     A class that implements a Reflection Agent, which generates responses and reflects     on them using the LLM to iteratively improve the interaction. The agent first generates     responses based on provided prompts and then critiques them in a reflection step.      Attributes:         model (str): The model name used for generating and reflecting on responses.         client (Groq): An instance of the Groq client to interact with the language model.     \"\"\"      def __init__(self, model: str = \"llama-3.1-70b-versatile\"):         self.client = Groq()         self.model = model      def _request_completion(         self,         history: list,         verbose: int = 0,         log_title: str = \"COMPLETION\",         log_color: str = \"\",     ):         \"\"\"         A private method to request a completion from the Groq model.          Args:             history (list): A list of messages forming the conversation or reflection history.             verbose (int, optional): The verbosity level. Defaults to 0 (no output).          Returns:             str: The model-generated response.         \"\"\"         output = completions_create(self.client, history, self.model)          if verbose &gt; 0:             print(log_color, f\"\\n\\n{log_title}\\n\\n\", output)          return output      def generate(self, generation_history: list, verbose: int = 0) -&gt; str:         \"\"\"         Generates a response based on the provided generation history using the model.          Args:             generation_history (list): A list of messages forming the conversation or generation history.             verbose (int, optional): The verbosity level, controlling printed output. Defaults to 0.          Returns:             str: The generated response.         \"\"\"         return self._request_completion(             generation_history, verbose, log_title=\"GENERATION\", log_color=Fore.BLUE         )      def reflect(self, reflection_history: list, verbose: int = 0) -&gt; str:         \"\"\"         Reflects on the generation history by generating a critique or feedback.          Args:             reflection_history (list): A list of messages forming the reflection history, typically based on                                        the previous generation or interaction.             verbose (int, optional): The verbosity level, controlling printed output. Defaults to 0.          Returns:             str: The critique or reflection response from the model.         \"\"\"         return self._request_completion(             reflection_history, verbose, log_title=\"REFLECTION\", log_color=Fore.GREEN         )      def run(         self,         user_msg: str,         generation_system_prompt: str = \"\",         reflection_system_prompt: str = \"\",         n_steps: int = 10,         verbose: int = 0,     ) -&gt; str:         \"\"\"         Runs the ReflectionAgent over multiple steps, alternating between generating a response         and reflecting on it for the specified number of steps.          Args:             user_msg (str): The user message or query that initiates the interaction.             generation_system_prompt (str, optional): The system prompt for guiding the generation process.             reflection_system_prompt (str, optional): The system prompt for guiding the reflection process.             n_steps (int, optional): The number of generate-reflect cycles to perform. Defaults to 3.             verbose (int, optional): The verbosity level controlling printed output. Defaults to 0.          Returns:             str: The final generated response after all cycles are completed.         \"\"\"         generation_system_prompt += BASE_GENERATION_SYSTEM_PROMPT         reflection_system_prompt += BASE_REFLECTION_SYSTEM_PROMPT          # Given the iterative nature of the Reflection Pattern, we might exhaust the LLM context (or         # make it really slow). That's the reason I'm limitting the chat history to three messages.         # The `FixedFirstChatHistory` is a very simple class, that creates a Queue that always keeps         # fixeed the first message. I thought this would be useful for maintaining the system prompt         # in the chat history.         generation_history = FixedFirstChatHistory(             [                 build_prompt_structure(prompt=generation_system_prompt, role=\"system\"),                 build_prompt_structure(prompt=user_msg, role=\"user\"),             ],             total_length=3,         )          reflection_history = FixedFirstChatHistory(             [build_prompt_structure(prompt=reflection_system_prompt, role=\"system\")],             total_length=3,         )          for step in range(n_steps):             if verbose &gt; 0:                 fancy_step_tracker(step, n_steps)              # Generate the response             generation = self.generate(generation_history, verbose=verbose)             update_chat_history(generation_history, generation, \"assistant\")             update_chat_history(reflection_history, generation, \"user\")              # Reflect and critique the generation             critique = self.reflect(reflection_history, verbose=verbose)              if \"\" in critique:                 # If no additional suggestions are made, stop the loop                 print(                     Fore.RED,                     \"\\n\\nStop Sequence found. Stopping the reflection loop ... \\n\\n\",                 )                 break              update_chat_history(generation_history, critique, \"user\")             update_chat_history(reflection_history, critique, \"assistant\")          return generation  In\u00a0[\u00a0]: Copied! <pre>agent = ReflectionAgent()\n</pre> agent = ReflectionAgent() In\u00a0[\u00a0]: Copied! <pre>generation_system_prompt = \"You are a Python programmer tasked with generating high quality Python code\"\n\nreflection_system_prompt = \"You are Andrej Karpathy, an experienced computer scientist\"\n\nuser_msg = \"Generate a Python implementation of the Merge Sort algorithm\"\n</pre> generation_system_prompt = \"You are a Python programmer tasked with generating high quality Python code\"  reflection_system_prompt = \"You are Andrej Karpathy, an experienced computer scientist\"  user_msg = \"Generate a Python implementation of the Merge Sort algorithm\" In\u00a0[\u00a0]: Copied! <pre>final_response = agent.run(\n    user_msg=user_msg,\n    generation_system_prompt=generation_system_prompt,\n    reflection_system_prompt=reflection_system_prompt,\n    n_steps=10,\n    verbose=1,\n)\n</pre> final_response = agent.run(     user_msg=user_msg,     generation_system_prompt=generation_system_prompt,     reflection_system_prompt=reflection_system_prompt,     n_steps=10,     verbose=1, ) In\u00a0[\u00a0]: Copied! <pre>display_markdown(final_response, raw=True)\n</pre> display_markdown(final_response, raw=True)"},{"location":"Agents/patterns/reflection_pattern/#reflection-pattern","title":"Reflection Pattern\u00b6","text":"<p>source : https://github.com/neural-maze/agentic_patterns</p> <p>The first pattern we are going to implement is the reflection pattern.</p> <p>This pattern allows the LLM to reflect and critique its outputs, following the next steps:</p> <ol> <li>The LLM generates a candidate output. If you look at the diagram above, it happens inside the \"Generate\" box.</li> <li>The LLM reflects on the previous output, suggesting modifications, deletions, improvements to the writing style, etc.</li> <li>The LLM modifies the original output based on the reflections and another iteration begins ...</li> </ol> <p>Now, we are going to build, from scratch, each step, so that you can truly understand how this pattern works.</p>"},{"location":"Agents/patterns/reflection_pattern/#generation-step","title":"Generation Step\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#groq-client-and-relevant-imports","title":"Groq Client and relevant imports\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#reflection-step","title":"Reflection Step\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#generation-step-ii","title":"Generation Step (II)\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#and-the-iteration-starts-again","title":"And the iteration starts again ...\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#implementing-a-class","title":"Implementing a class\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#utils","title":"utils\u00b6","text":""},{"location":"Agents/patterns/reflection_pattern/#final-result","title":"Final result\u00b6","text":""},{"location":"Agents/patterns/tool_pattern/","title":"Agent Tool Pattern","text":"<p>source : https://github.com/neural-maze/agentic_patterns</p> <p>As you may already know, the information stored in LLM weights is (usually) \ud835\udc27\ud835\udc28\ud835\udc2d \ud835\udc1e\ud835\udc27\ud835\udc28\ud835\udc2e\ud835\udc20\ud835\udc21 to give accurate and insightful answers to our questions.</p> <p>That's why we need to provide the LLM with ways to access the outside world. \ud83c\udf0d</p> <p>In practice, you can build tools for whatever you want (at the end of the day they are just functions the LLM can use), from a tool that let's you access Wikipedia, another to analyse the content of YouTube videos or calculate difficult integrals using Wolfram Alpha.</p> <p>The second pattern we are going to implement is the tool pattern.</p> <p>In this notebook, you'll learn how tools actually work. This is the second lesson of the \"Agentic Patterns from Scratch\" series. Take a look at the first lesson if you haven't!</p> <ul> <li>First Lesson: The Reflection Pattern</li> </ul> <p>Take a look at this function \ud83d\udc47</p> In\u00a0[\u00a0]: Copied! <pre>import json\n\ndef get_current_weather(location: str, unit: str):\n\t\"\"\"\n\tGet the current weather in a given location\n\n\tlocation (str): The city and state, e.g. Madrid, Barcelona\n\tunit (str): The unit. It can take two values; \"celsius\", \"fahrenheit\"\n\t\"\"\"\n\tif location == \"Madrid\":\n\t\treturn json.dumps({\"temperature\": 25, \"unit\": unit})\n\n\telse:\n\t\treturn json.dumps({\"temperature\": 58, \"unit\": unit})\n</pre> import json  def get_current_weather(location: str, unit: str): \t\"\"\" \tGet the current weather in a given location  \tlocation (str): The city and state, e.g. Madrid, Barcelona \tunit (str): The unit. It can take two values; \"celsius\", \"fahrenheit\" \t\"\"\" \tif location == \"Madrid\": \t\treturn json.dumps({\"temperature\": 25, \"unit\": unit})  \telse: \t\treturn json.dumps({\"temperature\": 58, \"unit\": unit}) <p>Very simple, right? You provide a <code>location</code> and a <code>unit</code> and it returns the temperature.</p> In\u00a0[\u00a0]: Copied! <pre>get_current_weather(location=\"Madrid\", unit=\"celsius\")\n</pre> get_current_weather(location=\"Madrid\", unit=\"celsius\") <p>But the question is:</p> <p>How can you make this function available to an LLM?</p> <p>An LLM is a type of NLP system, so it expects text as input. But how can we transform this function into text?</p> <p>For the LLM to be aware of this function, we need to provide some relevant information about it in the context. I'm referring to the function name, attributes, description, etc. Take a look at the following System Prompt.</p> <pre>You are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags. \nYou may call one or more functions to assist with the user query. Don't make assumptions about what values to plug \ninto functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;}\n&lt;/tool_call&gt;\n\nHere are the available tools:\n\n&lt;tools&gt; {\n    \"name\": \"get_current_weather\",\n    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n    \"parameters\": {\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\"\n            },\n            \"unit\": {\n                \"type\": \"string\"\n            }\n        }\n    }\n}\n&lt;/tools&gt;\n</pre> <p>As you can see, the LLM enforces the LLM to behave as a <code>function calling AI model</code> who, given a list of function signatures inside the  XML tags will select which one to use. When the model decides a function to use, it will return a json like the following, representing a function call:</p> <pre>&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;}\n&lt;/tool_call&gt;\n</pre> <p>Let's see how it works in practise! \ud83d\udc47</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport re\nfrom groq import Groq\nfrom dotenv import load_dotenv\n\n# Remember to load the environment variables. You should have the Groq API Key in there :)\nload_dotenv()\n\nMODEL = \"llama3-groq-70b-8192-tool-use-preview\"\nGROQ_CLIENT = Groq()\n\n# Define the System Prompt as a constant\nTOOL_SYSTEM_PROMPT = \"\"\"\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags. \nYou may call one or more functions to assist with the user query. Don't make assumptions about what values to plug \ninto functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;}\n&lt;/tool_call&gt;\n\nHere are the available tools:\n\n&lt;tools&gt; {\n    \"name\": \"get_current_weather\",\n    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n    \"parameters\": {\n        \"properties\": {\n            \"location\": {\n                \"type\": \"str\"\n            },\n            \"unit\": {\n                \"type\": \"str\"\n            }\n        }\n    }\n}\n&lt;/tools&gt;\n\"\"\"\n</pre> import os import re from groq import Groq from dotenv import load_dotenv  # Remember to load the environment variables. You should have the Groq API Key in there :) load_dotenv()  MODEL = \"llama3-groq-70b-8192-tool-use-preview\" GROQ_CLIENT = Groq()  # Define the System Prompt as a constant TOOL_SYSTEM_PROMPT = \"\"\" You are a function calling AI model. You are provided with function signatures within  XML tags.  You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug  into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict. For each function call return a json object with function name and arguments within  XML tags as follows:   {\"name\": ,\"arguments\": }   Here are the available tools:   {     \"name\": \"get_current_weather\",     \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",     \"parameters\": {         \"properties\": {             \"location\": {                 \"type\": \"str\"             },             \"unit\": {                 \"type\": \"str\"             }         }     } }  \"\"\" <p>Let's ask a very simple question: <code>\"What's the current temperature in Madrid, in Celsius?\"</code></p> In\u00a0[\u00a0]: Copied! <pre>tool_chat_history = [\n    {\n        \"role\": \"system\",\n        \"content\": TOOL_SYSTEM_PROMPT\n    }\n]\nagent_chat_history = []\n\nuser_msg = {\n    \"role\": \"user\",\n    \"content\": \"What's the current temperature in Madrid, in Celsius?\"\n}\n\ntool_chat_history.append(user_msg)\nagent_chat_history.append(user_msg)\n\noutput = GROQ_CLIENT.chat.completions.create(\n    messages=tool_chat_history,\n    model=MODEL\n).choices[0].message.content\n\nprint(output)\n</pre> tool_chat_history = [     {         \"role\": \"system\",         \"content\": TOOL_SYSTEM_PROMPT     } ] agent_chat_history = []  user_msg = {     \"role\": \"user\",     \"content\": \"What's the current temperature in Madrid, in Celsius?\" }  tool_chat_history.append(user_msg) agent_chat_history.append(user_msg)  output = GROQ_CLIENT.chat.completions.create(     messages=tool_chat_history,     model=MODEL ).choices[0].message.content  print(output) <p>That's an improvement! We may not have the proper answer but, with this information, we can obtain it! How? Well, we just need to:</p> <ol> <li>Parse the LLM output. By this I mean deleting the XML tags</li> <li>Load the output as a proper Python dict</li> </ol> <p>The function below does exactly this.</p> In\u00a0[\u00a0]: Copied! <pre>def parse_tool_call_str(tool_call_str: str):\n    pattern = r'&lt;/?tool_call&gt;'\n    clean_tags = re.sub(pattern, '', tool_call_str)\n    \n    try:\n        tool_call_json = json.loads(clean_tags)\n        return tool_call_json\n    except json.JSONDecodeError:\n        return clean_tags\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return \"There was some error parsing the Tool's output\"\n</pre> def parse_tool_call_str(tool_call_str: str):     pattern = r''     clean_tags = re.sub(pattern, '', tool_call_str)          try:         tool_call_json = json.loads(clean_tags)         return tool_call_json     except json.JSONDecodeError:         return clean_tags     except Exception as e:         print(f\"Unexpected error: {e}\")         return \"There was some error parsing the Tool's output\" In\u00a0[\u00a0]: Copied! <pre>parsed_output = parse_tool_call_str(output)\nparsed_output\n</pre> parsed_output = parse_tool_call_str(output) parsed_output <p>We can simply run the function now, by passing the arguments like this \ud83d\udc47</p> In\u00a0[\u00a0]: Copied! <pre>result = get_current_weather(**parsed_output[\"arguments\"])\n</pre> result = get_current_weather(**parsed_output[\"arguments\"]) In\u00a0[\u00a0]: Copied! <pre>result\n</pre> result <p>That's it! A temperature of 25 degrees Celsius.</p> <p>As you can see, we're dealing with a string, so we can simply add the parsed_output to the <code>chat_history</code> so that the LLM knows the information it has to return to the user.</p> In\u00a0[\u00a0]: Copied! <pre>agent_chat_history.append({\n    \"role\": \"user\",\n    \"content\": f\"Observation: {result}\"\n})\n</pre> agent_chat_history.append({     \"role\": \"user\",     \"content\": f\"Observation: {result}\" }) In\u00a0[\u00a0]: Copied! <pre>GROQ_CLIENT.chat.completions.create(\n    messages=agent_chat_history,\n    model=MODEL\n).choices[0].message.content\n</pre> GROQ_CLIENT.chat.completions.create(     messages=agent_chat_history,     model=MODEL ).choices[0].message.content <p>To recap, we have a way for the LLM to generate <code>tool_calls</code> that we can use later to properly run the functions. But, as you may imagine, there are some pieces missing:</p> <ol> <li>We need to automatically transform any function into a description like we saw in the initial system prompt.</li> <li>We need a way to tell the agent that this function is a tool</li> </ol> <p>Let's do it!</p> <p>We are going to use the <code>tool</code> decorator to transform any Python function into a tool. You can see the implementation here. To test it out, let's make a more complex tool than before. For example, a tool that interacts with Hacker News, getting the current top stories.</p> <p>Reminder: To automatically generate the function signature for the tool, we need a way to infer the arguments types. For this reason, we need to create the typing annotations.</p> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom typing import Callable\n\n\ndef get_fn_signature(fn: Callable) -&gt; dict:\n    \"\"\"\n    Generates the signature for a given function.\n\n    Args:\n        fn (Callable): The function whose signature needs to be extracted.\n\n    Returns:\n        dict: A dictionary containing the function's name, description,\n              and parameter types.\n    \"\"\"\n    fn_signature: dict = {\n        \"name\": fn.__name__,\n        \"description\": fn.__doc__,\n        \"parameters\": {\"properties\": {}},\n    }\n    schema = {\n        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n    }\n    fn_signature[\"parameters\"][\"properties\"] = schema\n    return fn_signature\n\n\ndef validate_arguments(tool_call: dict, tool_signature: dict) -&gt; dict:\n    \"\"\"\n    Validates and converts arguments in the input dictionary to match the expected types.\n\n    Args:\n        tool_call (dict): A dictionary containing the arguments passed to the tool.\n        tool_signature (dict): The expected function signature and parameter types.\n\n    Returns:\n        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n    \"\"\"\n    properties = tool_signature[\"parameters\"][\"properties\"]\n\n    # TODO: This is overly simplified but enough for simple Tools.\n    type_mapping = {\n        \"int\": int,\n        \"str\": str,\n        \"bool\": bool,\n        \"float\": float,\n    }\n\n    for arg_name, arg_value in tool_call[\"arguments\"].items():\n        expected_type = properties[arg_name].get(\"type\")\n\n        if not isinstance(arg_value, type_mapping[expected_type]):\n            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n\n    return tool_call\n\n\nclass Tool:\n    \"\"\"\n    A class representing a tool that wraps a callable and its signature.\n\n    Attributes:\n        name (str): The name of the tool (function).\n        fn (Callable): The function that the tool represents.\n        fn_signature (str): JSON string representation of the function's signature.\n    \"\"\"\n\n    def __init__(self, name: str, fn: Callable, fn_signature: str):\n        self.name = name\n        self.fn = fn\n        self.fn_signature = fn_signature\n\n    def __str__(self):\n        return self.fn_signature\n\n    def run(self, **kwargs):\n        \"\"\"\n        Executes the tool (function) with provided arguments.\n\n        Args:\n            **kwargs: Keyword arguments passed to the function.\n\n        Returns:\n            The result of the function call.\n        \"\"\"\n        return self.fn(**kwargs)\n\n\ndef tool(fn: Callable):\n    \"\"\"\n    A decorator that wraps a function into a Tool object.\n\n    Args:\n        fn (Callable): The function to be wrapped.\n\n    Returns:\n        Tool: A Tool object containing the function, its name, and its signature.\n    \"\"\"\n\n    def wrapper():\n        fn_signature = get_fn_signature(fn)\n        return Tool(\n            name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)\n        )\n\n    return wrapper()\n</pre> import json from typing import Callable   def get_fn_signature(fn: Callable) -&gt; dict:     \"\"\"     Generates the signature for a given function.      Args:         fn (Callable): The function whose signature needs to be extracted.      Returns:         dict: A dictionary containing the function's name, description,               and parameter types.     \"\"\"     fn_signature: dict = {         \"name\": fn.__name__,         \"description\": fn.__doc__,         \"parameters\": {\"properties\": {}},     }     schema = {         k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"     }     fn_signature[\"parameters\"][\"properties\"] = schema     return fn_signature   def validate_arguments(tool_call: dict, tool_signature: dict) -&gt; dict:     \"\"\"     Validates and converts arguments in the input dictionary to match the expected types.      Args:         tool_call (dict): A dictionary containing the arguments passed to the tool.         tool_signature (dict): The expected function signature and parameter types.      Returns:         dict: The tool call dictionary with the arguments converted to the correct types if necessary.     \"\"\"     properties = tool_signature[\"parameters\"][\"properties\"]      # TODO: This is overly simplified but enough for simple Tools.     type_mapping = {         \"int\": int,         \"str\": str,         \"bool\": bool,         \"float\": float,     }      for arg_name, arg_value in tool_call[\"arguments\"].items():         expected_type = properties[arg_name].get(\"type\")          if not isinstance(arg_value, type_mapping[expected_type]):             tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)      return tool_call   class Tool:     \"\"\"     A class representing a tool that wraps a callable and its signature.      Attributes:         name (str): The name of the tool (function).         fn (Callable): The function that the tool represents.         fn_signature (str): JSON string representation of the function's signature.     \"\"\"      def __init__(self, name: str, fn: Callable, fn_signature: str):         self.name = name         self.fn = fn         self.fn_signature = fn_signature      def __str__(self):         return self.fn_signature      def run(self, **kwargs):         \"\"\"         Executes the tool (function) with provided arguments.          Args:             **kwargs: Keyword arguments passed to the function.          Returns:             The result of the function call.         \"\"\"         return self.fn(**kwargs)   def tool(fn: Callable):     \"\"\"     A decorator that wraps a function into a Tool object.      Args:         fn (Callable): The function to be wrapped.      Returns:         Tool: A Tool object containing the function, its name, and its signature.     \"\"\"      def wrapper():         fn_signature = get_fn_signature(fn)         return Tool(             name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)         )      return wrapper()  In\u00a0[\u00a0]: Copied! <pre>import re\nfrom dataclasses import dataclass\nimport time\n\nfrom colorama import Fore\nfrom colorama import Style\n\ndef completions_create(client, messages: list, model: str) -&gt; str:\n    \"\"\"\n    Sends a request to the client's `completions.create` method to interact with the language model.\n\n    Args:\n        client (Groq): The Groq client object\n        messages (list[dict]): A list of message objects containing chat history for the model.\n        model (str): The model to use for generating tool calls and responses.\n\n    Returns:\n        str: The content of the model's response.\n    \"\"\"\n    response = client.chat.completions.create(messages=messages, model=model)\n    return str(response.choices[0].message.content)\n\n\ndef build_prompt_structure(prompt: str, role: str, tag: str = \"\") -&gt; dict:\n    \"\"\"\n    Builds a structured prompt that includes the role and content.\n\n    Args:\n        prompt (str): The actual content of the prompt.\n        role (str): The role of the speaker (e.g., user, assistant).\n\n    Returns:\n        dict: A dictionary representing the structured prompt.\n    \"\"\"\n    if tag:\n        prompt = f\"&lt;{tag}&gt;{prompt}&lt;/{tag}&gt;\"\n    return {\"role\": role, \"content\": prompt}\n\n\ndef update_chat_history(history: list, msg: str, role: str):\n    \"\"\"\n    Updates the chat history by appending the latest response.\n\n    Args:\n        history (list): The list representing the current chat history.\n        msg (str): The message to append.\n        role (str): The role type (e.g. 'user', 'assistant', 'system')\n    \"\"\"\n    history.append(build_prompt_structure(prompt=msg, role=role))\n\n\nclass ChatHistory(list):\n    def __init__(self, messages: list | None = None, total_length: int = -1):\n        \"\"\"Initialise the queue with a fixed total length.\n\n        Args:\n            messages (list | None): A list of initial messages\n            total_length (int): The maximum number of messages the chat history can hold.\n        \"\"\"\n        if messages is None:\n            messages = []\n\n        super().__init__(messages)\n        self.total_length = total_length\n\n    def append(self, msg: str):\n        \"\"\"Add a message to the queue.\n\n        Args:\n            msg (str): The message to be added to the queue\n        \"\"\"\n        if len(self) == self.total_length:\n            self.pop(0)\n        super().append(msg)\n\n\nclass FixedFirstChatHistory(ChatHistory):\n    def __init__(self, messages: list | None = None, total_length: int = -1):\n        \"\"\"Initialise the queue with a fixed total length.\n\n        Args:\n            messages (list | None): A list of initial messages\n            total_length (int): The maximum number of messages the chat history can hold.\n        \"\"\"\n        super().__init__(messages, total_length)\n\n    def append(self, msg: str):\n        \"\"\"Add a message to the queue. The first messaage will always stay fixed.\n\n        Args:\n            msg (str): The message to be added to the queue\n        \"\"\"\n        if len(self) == self.total_length:\n            self.pop(1)\n        super().append(msg)\n\n\n\n\n@dataclass\nclass TagContentResult:\n    \"\"\"\n    A data class to represent the result of extracting tag content.\n\n    Attributes:\n        content (List[str]): A list of strings containing the content found between the specified tags.\n        found (bool): A flag indicating whether any content was found for the given tag.\n    \"\"\"\n\n    content: list[str]\n    found: bool\n\n\ndef extract_tag_content(text: str, tag: str) -&gt; TagContentResult:\n    \"\"\"\n    Extracts all content enclosed by specified tags (e.g., &lt;thought&gt;, &lt;response&gt;, etc.).\n\n    Parameters:\n        text (str): The input string containing multiple potential tags.\n        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n\n    Returns:\n        dict: A dictionary with the following keys:\n            - 'content' (list): A list of strings containing the content found between the specified tags.\n            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n    \"\"\"\n    # Build the regex pattern dynamically to find multiple occurrences of the tag\n    tag_pattern = rf\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"\n\n    # Use findall to capture all content between the specified tag\n    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n\n    # Return the dataclass instance with the result\n    return TagContentResult(\n        content=[content.strip() for content in matched_contents],\n        found=bool(matched_contents),\n    )\n\n\ndef fancy_print(message: str) -&gt; None:\n    \"\"\"\n    Displays a fancy print message.\n\n    Args:\n        message (str): The message to display.\n    \"\"\"\n    print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")\n    print(Fore.MAGENTA + f\"{message}\")\n    print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")\n    time.sleep(0.5)\n\n\ndef fancy_step_tracker(step: int, total_steps: int) -&gt; None:\n    \"\"\"\n    Displays a fancy step tracker for each iteration of the generation-reflection loop.\n\n    Args:\n        step (int): The current step in the loop.\n        total_steps (int): The total number of steps in the loop.\n    \"\"\"\n    fancy_print(f\"STEP {step + 1}/{total_steps}\")\n\nimport json\nfrom typing import Callable\n\n\ndef get_fn_signature(fn: Callable) -&gt; dict:\n    \"\"\"\n    Generates the signature for a given function.\n\n    Args:\n        fn (Callable): The function whose signature needs to be extracted.\n\n    Returns:\n        dict: A dictionary containing the function's name, description,\n              and parameter types.\n    \"\"\"\n    fn_signature: dict = {\n        \"name\": fn.__name__,\n        \"description\": fn.__doc__,\n        \"parameters\": {\"properties\": {}},\n    }\n    schema = {\n        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n    }\n    fn_signature[\"parameters\"][\"properties\"] = schema\n    return fn_signature\n\n\ndef validate_arguments(tool_call: dict, tool_signature: dict) -&gt; dict:\n    \"\"\"\n    Validates and converts arguments in the input dictionary to match the expected types.\n\n    Args:\n        tool_call (dict): A dictionary containing the arguments passed to the tool.\n        tool_signature (dict): The expected function signature and parameter types.\n\n    Returns:\n        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n    \"\"\"\n    properties = tool_signature[\"parameters\"][\"properties\"]\n\n    # TODO: This is overly simplified but enough for simple Tools.\n    type_mapping = {\n        \"int\": int,\n        \"str\": str,\n        \"bool\": bool,\n        \"float\": float,\n    }\n\n    for arg_name, arg_value in tool_call[\"arguments\"].items():\n        expected_type = properties[arg_name].get(\"type\")\n\n        if not isinstance(arg_value, type_mapping[expected_type]):\n            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n\n    return tool_call\n\n\nclass Tool:\n    \"\"\"\n    A class representing a tool that wraps a callable and its signature.\n\n    Attributes:\n        name (str): The name of the tool (function).\n        fn (Callable): The function that the tool represents.\n        fn_signature (str): JSON string representation of the function's signature.\n    \"\"\"\n\n    def __init__(self, name: str, fn: Callable, fn_signature: str):\n        self.name = name\n        self.fn = fn\n        self.fn_signature = fn_signature\n\n    def __str__(self):\n        return self.fn_signature\n\n    def run(self, **kwargs):\n        \"\"\"\n        Executes the tool (function) with provided arguments.\n\n        Args:\n            **kwargs: Keyword arguments passed to the function.\n\n        Returns:\n            The result of the function call.\n        \"\"\"\n        return self.fn(**kwargs)\n\n\ndef tool(fn: Callable):\n    \"\"\"\n    A decorator that wraps a function into a Tool object.\n\n    Args:\n        fn (Callable): The function to be wrapped.\n\n    Returns:\n        Tool: A Tool object containing the function, its name, and its signature.\n    \"\"\"\n\n    def wrapper():\n        fn_signature = get_fn_signature(fn)\n        return Tool(\n            name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)\n        )\n\n    return wrapper()\n</pre> import re from dataclasses import dataclass import time  from colorama import Fore from colorama import Style  def completions_create(client, messages: list, model: str) -&gt; str:     \"\"\"     Sends a request to the client's `completions.create` method to interact with the language model.      Args:         client (Groq): The Groq client object         messages (list[dict]): A list of message objects containing chat history for the model.         model (str): The model to use for generating tool calls and responses.      Returns:         str: The content of the model's response.     \"\"\"     response = client.chat.completions.create(messages=messages, model=model)     return str(response.choices[0].message.content)   def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -&gt; dict:     \"\"\"     Builds a structured prompt that includes the role and content.      Args:         prompt (str): The actual content of the prompt.         role (str): The role of the speaker (e.g., user, assistant).      Returns:         dict: A dictionary representing the structured prompt.     \"\"\"     if tag:         prompt = f\"&lt;{tag}&gt;{prompt}\"     return {\"role\": role, \"content\": prompt}   def update_chat_history(history: list, msg: str, role: str):     \"\"\"     Updates the chat history by appending the latest response.      Args:         history (list): The list representing the current chat history.         msg (str): The message to append.         role (str): The role type (e.g. 'user', 'assistant', 'system')     \"\"\"     history.append(build_prompt_structure(prompt=msg, role=role))   class ChatHistory(list):     def __init__(self, messages: list | None = None, total_length: int = -1):         \"\"\"Initialise the queue with a fixed total length.          Args:             messages (list | None): A list of initial messages             total_length (int): The maximum number of messages the chat history can hold.         \"\"\"         if messages is None:             messages = []          super().__init__(messages)         self.total_length = total_length      def append(self, msg: str):         \"\"\"Add a message to the queue.          Args:             msg (str): The message to be added to the queue         \"\"\"         if len(self) == self.total_length:             self.pop(0)         super().append(msg)   class FixedFirstChatHistory(ChatHistory):     def __init__(self, messages: list | None = None, total_length: int = -1):         \"\"\"Initialise the queue with a fixed total length.          Args:             messages (list | None): A list of initial messages             total_length (int): The maximum number of messages the chat history can hold.         \"\"\"         super().__init__(messages, total_length)      def append(self, msg: str):         \"\"\"Add a message to the queue. The first messaage will always stay fixed.          Args:             msg (str): The message to be added to the queue         \"\"\"         if len(self) == self.total_length:             self.pop(1)         super().append(msg)     @dataclass class TagContentResult:     \"\"\"     A data class to represent the result of extracting tag content.      Attributes:         content (List[str]): A list of strings containing the content found between the specified tags.         found (bool): A flag indicating whether any content was found for the given tag.     \"\"\"      content: list[str]     found: bool   def extract_tag_content(text: str, tag: str) -&gt; TagContentResult:     \"\"\"     Extracts all content enclosed by specified tags (e.g., , , etc.).      Parameters:         text (str): The input string containing multiple potential tags.         tag (str): The name of the tag to search for (e.g., 'thought', 'response').      Returns:         dict: A dictionary with the following keys:             - 'content' (list): A list of strings containing the content found between the specified tags.             - 'found' (bool): A flag indicating whether any content was found for the given tag.     \"\"\"     # Build the regex pattern dynamically to find multiple occurrences of the tag     tag_pattern = rf\"&lt;{tag}&gt;(.*?)\"      # Use findall to capture all content between the specified tag     matched_contents = re.findall(tag_pattern, text, re.DOTALL)      # Return the dataclass instance with the result     return TagContentResult(         content=[content.strip() for content in matched_contents],         found=bool(matched_contents),     )   def fancy_print(message: str) -&gt; None:     \"\"\"     Displays a fancy print message.      Args:         message (str): The message to display.     \"\"\"     print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")     print(Fore.MAGENTA + f\"{message}\")     print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")     time.sleep(0.5)   def fancy_step_tracker(step: int, total_steps: int) -&gt; None:     \"\"\"     Displays a fancy step tracker for each iteration of the generation-reflection loop.      Args:         step (int): The current step in the loop.         total_steps (int): The total number of steps in the loop.     \"\"\"     fancy_print(f\"STEP {step + 1}/{total_steps}\")  import json from typing import Callable   def get_fn_signature(fn: Callable) -&gt; dict:     \"\"\"     Generates the signature for a given function.      Args:         fn (Callable): The function whose signature needs to be extracted.      Returns:         dict: A dictionary containing the function's name, description,               and parameter types.     \"\"\"     fn_signature: dict = {         \"name\": fn.__name__,         \"description\": fn.__doc__,         \"parameters\": {\"properties\": {}},     }     schema = {         k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"     }     fn_signature[\"parameters\"][\"properties\"] = schema     return fn_signature   def validate_arguments(tool_call: dict, tool_signature: dict) -&gt; dict:     \"\"\"     Validates and converts arguments in the input dictionary to match the expected types.      Args:         tool_call (dict): A dictionary containing the arguments passed to the tool.         tool_signature (dict): The expected function signature and parameter types.      Returns:         dict: The tool call dictionary with the arguments converted to the correct types if necessary.     \"\"\"     properties = tool_signature[\"parameters\"][\"properties\"]      # TODO: This is overly simplified but enough for simple Tools.     type_mapping = {         \"int\": int,         \"str\": str,         \"bool\": bool,         \"float\": float,     }      for arg_name, arg_value in tool_call[\"arguments\"].items():         expected_type = properties[arg_name].get(\"type\")          if not isinstance(arg_value, type_mapping[expected_type]):             tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)      return tool_call   class Tool:     \"\"\"     A class representing a tool that wraps a callable and its signature.      Attributes:         name (str): The name of the tool (function).         fn (Callable): The function that the tool represents.         fn_signature (str): JSON string representation of the function's signature.     \"\"\"      def __init__(self, name: str, fn: Callable, fn_signature: str):         self.name = name         self.fn = fn         self.fn_signature = fn_signature      def __str__(self):         return self.fn_signature      def run(self, **kwargs):         \"\"\"         Executes the tool (function) with provided arguments.          Args:             **kwargs: Keyword arguments passed to the function.          Returns:             The result of the function call.         \"\"\"         return self.fn(**kwargs)   def tool(fn: Callable):     \"\"\"     A decorator that wraps a function into a Tool object.      Args:         fn (Callable): The function to be wrapped.      Returns:         Tool: A Tool object containing the function, its name, and its signature.     \"\"\"      def wrapper():         fn_signature = get_fn_signature(fn)         return Tool(             name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)         )      return wrapper()  In\u00a0[\u00a0]: Copied! <pre>import json\nimport requests\nimport json\nimport re\n\nfrom colorama import Fore\nfrom dotenv import load_dotenv\nfrom groq import Groq\n\n\n\nload_dotenv()\n\n\nTOOL_SYSTEM_PROMPT = \"\"\"\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.\nYou may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\ninto functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt;\nXML tags as follows:\n\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;,  \"id\": &lt;monotonically-increasing-id&gt;}\n&lt;/tool_call&gt;\n\nHere are the available tools:\n\n&lt;tools&gt;\n%s\n&lt;/tools&gt;\n\"\"\"\n\n\nclass ToolAgent:\n    \"\"\"\n    The ToolAgent class represents an agent that can interact with a language model and use tools\n    to assist with user queries. It generates function calls based on user input, validates arguments,\n    and runs the respective tools.\n\n    Attributes:\n        tools (Tool | list[Tool]): A list of tools available to the agent.\n        model (str): The model to be used for generating tool calls and responses.\n        client (Groq): The Groq client used to interact with the language model.\n        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool objects.\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: Tool | list[Tool],\n        model: str = \"llama3-groq-70b-8192-tool-use-preview\",\n    ) -&gt; None:\n        self.client = Groq()\n        self.model = model\n        self.tools = tools if isinstance(tools, list) else [tools]\n        self.tools_dict = {tool.name: tool for tool in self.tools}\n\n    def add_tool_signatures(self) -&gt; str:\n        \"\"\"\n        Collects the function signatures of all available tools.\n\n        Returns:\n            str: A concatenated string of all tool function signatures in JSON format.\n        \"\"\"\n        return \"\".join([tool.fn_signature for tool in self.tools])\n\n    def process_tool_calls(self, tool_calls_content: list) -&gt; dict:\n        \"\"\"\n        Processes each tool call, validates arguments, executes the tools, and collects results.\n\n        Args:\n            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n\n        Returns:\n            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n        \"\"\"\n        observations = {}\n        for tool_call_str in tool_calls_content:\n            tool_call = json.loads(tool_call_str)\n            tool_name = tool_call[\"name\"]\n            tool = self.tools_dict[tool_name]\n\n            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n\n            # Validate and execute the tool call\n            validated_tool_call = validate_arguments(\n                tool_call, json.loads(tool.fn_signature)\n            )\n            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n\n            result = tool.run(**validated_tool_call[\"arguments\"])\n            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n\n            # Store the result using the tool call ID\n            observations[validated_tool_call[\"id\"]] = result\n\n        return observations\n\n    def run(\n        self,\n        user_msg: str,\n    ) -&gt; str:\n        \"\"\"\n        Handles the full process of interacting with the language model and executing a tool based on user input.\n\n        Args:\n            user_msg (str): The user's message that prompts the tool agent to act.\n\n        Returns:\n            str: The final output after executing the tool and generating a response from the model.\n        \"\"\"\n        user_prompt = build_prompt_structure(prompt=user_msg, role=\"user\")\n\n        tool_chat_history = ChatHistory(\n            [\n                build_prompt_structure(\n                    prompt=TOOL_SYSTEM_PROMPT % self.add_tool_signatures(),\n                    role=\"system\",\n                ),\n                user_prompt,\n            ]\n        )\n        agent_chat_history = ChatHistory([user_prompt])\n\n        tool_call_response = completions_create(\n            self.client, messages=tool_chat_history, model=self.model\n        )\n        tool_calls = extract_tag_content(str(tool_call_response), \"tool_call\")\n\n        if tool_calls.found:\n            observations = self.process_tool_calls(tool_calls.content)\n            update_chat_history(\n                agent_chat_history, f'f\"Observation: {observations}\"', \"user\"\n            )\n\n        return completions_create(self.client, agent_chat_history, self.model)\n</pre> import json import requests import json import re  from colorama import Fore from dotenv import load_dotenv from groq import Groq    load_dotenv()   TOOL_SYSTEM_PROMPT = \"\"\" You are a function calling AI model. You are provided with function signatures within  XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict. For each function call return a json object with function name and arguments within  XML tags as follows:   {\"name\": ,\"arguments\": ,  \"id\": }   Here are the available tools:   %s  \"\"\"   class ToolAgent:     \"\"\"     The ToolAgent class represents an agent that can interact with a language model and use tools     to assist with user queries. It generates function calls based on user input, validates arguments,     and runs the respective tools.      Attributes:         tools (Tool | list[Tool]): A list of tools available to the agent.         model (str): The model to be used for generating tool calls and responses.         client (Groq): The Groq client used to interact with the language model.         tools_dict (dict): A dictionary mapping tool names to their corresponding Tool objects.     \"\"\"      def __init__(         self,         tools: Tool | list[Tool],         model: str = \"llama3-groq-70b-8192-tool-use-preview\",     ) -&gt; None:         self.client = Groq()         self.model = model         self.tools = tools if isinstance(tools, list) else [tools]         self.tools_dict = {tool.name: tool for tool in self.tools}      def add_tool_signatures(self) -&gt; str:         \"\"\"         Collects the function signatures of all available tools.          Returns:             str: A concatenated string of all tool function signatures in JSON format.         \"\"\"         return \"\".join([tool.fn_signature for tool in self.tools])      def process_tool_calls(self, tool_calls_content: list) -&gt; dict:         \"\"\"         Processes each tool call, validates arguments, executes the tools, and collects results.          Args:             tool_calls_content (list): List of strings, each representing a tool call in JSON format.          Returns:             dict: A dictionary where the keys are tool call IDs and values are the results from the tools.         \"\"\"         observations = {}         for tool_call_str in tool_calls_content:             tool_call = json.loads(tool_call_str)             tool_name = tool_call[\"name\"]             tool = self.tools_dict[tool_name]              print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")              # Validate and execute the tool call             validated_tool_call = validate_arguments(                 tool_call, json.loads(tool.fn_signature)             )             print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")              result = tool.run(**validated_tool_call[\"arguments\"])             print(Fore.GREEN + f\"\\nTool result: \\n{result}\")              # Store the result using the tool call ID             observations[validated_tool_call[\"id\"]] = result          return observations      def run(         self,         user_msg: str,     ) -&gt; str:         \"\"\"         Handles the full process of interacting with the language model and executing a tool based on user input.          Args:             user_msg (str): The user's message that prompts the tool agent to act.          Returns:             str: The final output after executing the tool and generating a response from the model.         \"\"\"         user_prompt = build_prompt_structure(prompt=user_msg, role=\"user\")          tool_chat_history = ChatHistory(             [                 build_prompt_structure(                     prompt=TOOL_SYSTEM_PROMPT % self.add_tool_signatures(),                     role=\"system\",                 ),                 user_prompt,             ]         )         agent_chat_history = ChatHistory([user_prompt])          tool_call_response = completions_create(             self.client, messages=tool_chat_history, model=self.model         )         tool_calls = extract_tag_content(str(tool_call_response), \"tool_call\")          if tool_calls.found:             observations = self.process_tool_calls(tool_calls.content)             update_chat_history(                 agent_chat_history, f'f\"Observation: {observations}\"', \"user\"             )          return completions_create(self.client, agent_chat_history, self.model)  In\u00a0[\u00a0]: Copied! <pre>def fetch_top_hacker_news_stories(top_n: int):\n    \"\"\"\n    Fetch the top stories from Hacker News.\n\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API. \n    Each story contains the title, URL, score, author, and time of submission. The data is fetched \n    from the official Firebase Hacker News API, which returns story details in JSON format.\n\n    Args:\n        top_n (int): The number of top stories to retrieve.\n    \"\"\"\n    top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n    \n    try:\n        response = requests.get(top_stories_url)\n        response.raise_for_status()  # Check for HTTP errors\n        \n        # Get the top story IDs\n        top_story_ids = response.json()[:top_n]\n        \n        top_stories = []\n        \n        # For each story ID, fetch the story details\n        for story_id in top_story_ids:\n            story_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'\n            story_response = requests.get(story_url)\n            story_response.raise_for_status()  # Check for HTTP errors\n            story_data = story_response.json()\n            \n            # Append the story title and URL (or other relevant info) to the list\n            top_stories.append({\n                'title': story_data.get('title', 'No title'),\n                'url': story_data.get('url', 'No URL available'),\n            })\n        \n        return json.dumps(top_stories)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"An error occurred: {e}\")\n        return []\n</pre>   def fetch_top_hacker_news_stories(top_n: int):     \"\"\"     Fetch the top stories from Hacker News.      This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.      Each story contains the title, URL, score, author, and time of submission. The data is fetched      from the official Firebase Hacker News API, which returns story details in JSON format.      Args:         top_n (int): The number of top stories to retrieve.     \"\"\"     top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'          try:         response = requests.get(top_stories_url)         response.raise_for_status()  # Check for HTTP errors                  # Get the top story IDs         top_story_ids = response.json()[:top_n]                  top_stories = []                  # For each story ID, fetch the story details         for story_id in top_story_ids:             story_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'             story_response = requests.get(story_url)             story_response.raise_for_status()  # Check for HTTP errors             story_data = story_response.json()                          # Append the story title and URL (or other relevant info) to the list             top_stories.append({                 'title': story_data.get('title', 'No title'),                 'url': story_data.get('url', 'No URL available'),             })                  return json.dumps(top_stories)      except requests.exceptions.RequestException as e:         print(f\"An error occurred: {e}\")         return [] <p>If we run this Python function, we'll obtain the top HN stories, as you can see below (the top 5 in this case).</p> In\u00a0[\u00a0]: Copied! <pre>json.loads(fetch_top_hacker_news_stories(top_n=5))\n</pre> json.loads(fetch_top_hacker_news_stories(top_n=5)) <p>To transform the <code>fetch_top_hacker_news_stories</code> function into a Tool, we can use the <code>tool</code> decorator.</p> In\u00a0[\u00a0]: Copied! <pre>hn_tool = tool(fetch_top_hacker_news_stories)\n</pre> hn_tool = tool(fetch_top_hacker_news_stories) <p>The Tool has the following parameters: a <code>name</code>, a <code>fn_signature</code> and the <code>fn</code> (this is the function we are going to call, this case <code>fetch_top_hacker_news_stories</code>)</p> In\u00a0[\u00a0]: Copied! <pre>hn_tool.name\n</pre> hn_tool.name <p>By default, the tool gets its name from the function name.</p> In\u00a0[\u00a0]: Copied! <pre>json.loads(hn_tool.fn_signature)\n</pre> json.loads(hn_tool.fn_signature) <p>As you can see, the function signature has been automatically generated. It contains the <code>name</code>, a <code>description</code> (taken from the docstrings) and the <code>parameters</code>, whose types come from the tying annotations. Now that we have a tool, let's run the agent.</p> <p>To create the agent, we just need to pass a list of tools (in this case, just one).</p> In\u00a0[\u00a0]: Copied! <pre>tool_agent = ToolAgent(tools=[hn_tool])\n</pre> tool_agent = ToolAgent(tools=[hn_tool]) <p>A quick check to see that everything works fine. If we ask the agent something unrelated to Hacker News, it shouldn't use the tool.</p> In\u00a0[\u00a0]: Copied! <pre>output = tool_agent.run(user_msg=\"Tell me your name\")\n</pre> output = tool_agent.run(user_msg=\"Tell me your name\") In\u00a0[\u00a0]: Copied! <pre>print(output)\n</pre> print(output) <p>Now, let's ask for specific information about Hacker News.</p> In\u00a0[\u00a0]: Copied! <pre>output = tool_agent.run(user_msg=\"Tell me the top 5 Hacker News stories right now\")\n</pre> output = tool_agent.run(user_msg=\"Tell me the top 5 Hacker News stories right now\") In\u00a0[\u00a0]: Copied! <pre>print(output)\n</pre> print(output) <p>There you have it!! A fully functional Tool!! \ud83d\udee0\ufe0f</p>"},{"location":"Agents/patterns/tool_pattern/#tool-pattern","title":"Tool Pattern\u00b6","text":""},{"location":"Agents/patterns/tool_pattern/#a-simple-function","title":"A simple function\u00b6","text":""},{"location":"Agents/patterns/tool_pattern/#a-system-prompt-that-works","title":"A System Prompt that works\u00b6","text":""},{"location":"Agents/patterns/tool_pattern/#implementing-everything-the-good-way","title":"Implementing everything the good way\u00b6","text":""},{"location":"Agents/patterns/tool_pattern/#the-tool-decorator","title":"The <code>tool</code> decorator\u00b6","text":""},{"location":"Agents/patterns/tool_pattern/#the-toolagent","title":"The <code>ToolAgent</code>\u00b6","text":""},{"location":"Agents/projects/multi_document_agents/","title":"Multi-Document Agents","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-agent-openai\n%pip install llama-index-embeddings-openai\n%pip install llama-index-llms-openai\n</pre> %pip install llama-index-agent-openai %pip install llama-index-embeddings-openai %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n</pre> !pip install llama-index In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import (\n    VectorStoreIndex,\n    SimpleKeywordTableIndex,\n    SimpleDirectoryReader,\n)\nfrom llama_index.core import SummaryIndex\nfrom llama_index.core.schema import IndexNode\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.callbacks import CallbackManager\n</pre> from llama_index.core import (     VectorStoreIndex,     SimpleKeywordTableIndex,     SimpleDirectoryReader, ) from llama_index.core import SummaryIndex from llama_index.core.schema import IndexNode from llama_index.core.tools import QueryEngineTool, ToolMetadata from llama_index.llms.openai import OpenAI from llama_index.core.callbacks import CallbackManager In\u00a0[\u00a0]: Copied! <pre>wiki_titles = [\n    \"Toronto\",\n    \"Seattle\",\n    \"Chicago\",\n    \"Boston\",\n    \"Houston\",\n    \"Tokyo\",\n    \"Berlin\",\n    \"Lisbon\",\n    \"Paris\",\n    \"London\",\n    \"Atlanta\",\n    \"Munich\",\n    \"Shanghai\",\n    \"Beijing\",\n    \"Copenhagen\",\n    \"Moscow\",\n    \"Cairo\",\n    \"Karachi\",\n]\n</pre> wiki_titles = [     \"Toronto\",     \"Seattle\",     \"Chicago\",     \"Boston\",     \"Houston\",     \"Tokyo\",     \"Berlin\",     \"Lisbon\",     \"Paris\",     \"London\",     \"Atlanta\",     \"Munich\",     \"Shanghai\",     \"Beijing\",     \"Copenhagen\",     \"Moscow\",     \"Cairo\",     \"Karachi\", ] In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\nimport requests\n\nfor title in wiki_titles:\n    response = requests.get(\n        \"https://en.wikipedia.org/w/api.php\",\n        params={\n            \"action\": \"query\",\n            \"format\": \"json\",\n            \"titles\": title,\n            \"prop\": \"extracts\",\n            # 'exintro': True,\n            \"explaintext\": True,\n        },\n    ).json()\n    page = next(iter(response[\"query\"][\"pages\"].values()))\n    wiki_text = page[\"extract\"]\n\n    data_path = Path(\"data\")\n    if not data_path.exists():\n        Path.mkdir(data_path)\n\n    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n        fp.write(wiki_text)\n</pre> from pathlib import Path  import requests  for title in wiki_titles:     response = requests.get(         \"https://en.wikipedia.org/w/api.php\",         params={             \"action\": \"query\",             \"format\": \"json\",             \"titles\": title,             \"prop\": \"extracts\",             # 'exintro': True,             \"explaintext\": True,         },     ).json()     page = next(iter(response[\"query\"][\"pages\"].values()))     wiki_text = page[\"extract\"]      data_path = Path(\"data\")     if not data_path.exists():         Path.mkdir(data_path)      with open(data_path / f\"{title}.txt\", \"w\") as fp:         fp.write(wiki_text) In\u00a0[\u00a0]: Copied! <pre># Load all wiki documents\ncity_docs = {}\nfor wiki_title in wiki_titles:\n    city_docs[wiki_title] = SimpleDirectoryReader(\n        input_files=[f\"data/{wiki_title}.txt\"]\n    ).load_data()\n</pre> # Load all wiki documents city_docs = {} for wiki_title in wiki_titles:     city_docs[wiki_title] = SimpleDirectoryReader(         input_files=[f\"data/{wiki_title}.txt\"]     ).load_data() <p>Define Global LLM and Embeddings</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\nSettings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n</pre> from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.core import Settings  Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\") Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\") In\u00a0[\u00a0]: Copied! <pre>from llama_index.agent.openai import OpenAIAgent\nfrom llama_index.core import load_index_from_storage, StorageContext\nfrom llama_index.core.node_parser import SentenceSplitter\nimport os\n\nnode_parser = SentenceSplitter()\n\n# Build agents dictionary\nagents = {}\nquery_engines = {}\n\n# this is for the baseline\nall_nodes = []\n\nfor idx, wiki_title in enumerate(wiki_titles):\n    nodes = node_parser.get_nodes_from_documents(city_docs[wiki_title])\n    all_nodes.extend(nodes)\n\n    if not os.path.exists(f\"./data/{wiki_title}\"):\n        # build vector index\n        vector_index = VectorStoreIndex(nodes)\n        vector_index.storage_context.persist(\n            persist_dir=f\"./data/{wiki_title}\"\n        )\n    else:\n        vector_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=f\"./data/{wiki_title}\"),\n        )\n\n    # build summary index\n    summary_index = SummaryIndex(nodes)\n    # define query engines\n    vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)\n    summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)\n\n    # define tools\n    query_engine_tools = [\n        QueryEngineTool(\n            query_engine=vector_query_engine,\n            metadata=ToolMetadata(\n                name=\"vector_tool\",\n                description=(\n                    \"Useful for questions related to specific aspects of\"\n                    f\" {wiki_title} (e.g. the history, arts and culture,\"\n                    \" sports, demographics, or more).\"\n                ),\n            ),\n        ),\n        QueryEngineTool(\n            query_engine=summary_query_engine,\n            metadata=ToolMetadata(\n                name=\"summary_tool\",\n                description=(\n                    \"Useful for any requests that require a holistic summary\"\n                    f\" of EVERYTHING about {wiki_title}. For questions about\"\n                    \" more specific sections, please use the vector_tool.\"\n                ),\n            ),\n        ),\n    ]\n\n    # build agent\n    function_llm = OpenAI(model=\"gpt-4\")\n    agent = OpenAIAgent.from_tools(\n        query_engine_tools,\n        llm=function_llm,\n        verbose=True,\n        system_prompt=f\"\"\"\\\nYou are a specialized agent designed to answer queries about {wiki_title}.\nYou must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n\"\"\",\n    )\n\n    agents[wiki_title] = agent\n    query_engines[wiki_title] = vector_index.as_query_engine(\n        similarity_top_k=2\n    )\n</pre> from llama_index.agent.openai import OpenAIAgent from llama_index.core import load_index_from_storage, StorageContext from llama_index.core.node_parser import SentenceSplitter import os  node_parser = SentenceSplitter()  # Build agents dictionary agents = {} query_engines = {}  # this is for the baseline all_nodes = []  for idx, wiki_title in enumerate(wiki_titles):     nodes = node_parser.get_nodes_from_documents(city_docs[wiki_title])     all_nodes.extend(nodes)      if not os.path.exists(f\"./data/{wiki_title}\"):         # build vector index         vector_index = VectorStoreIndex(nodes)         vector_index.storage_context.persist(             persist_dir=f\"./data/{wiki_title}\"         )     else:         vector_index = load_index_from_storage(             StorageContext.from_defaults(persist_dir=f\"./data/{wiki_title}\"),         )      # build summary index     summary_index = SummaryIndex(nodes)     # define query engines     vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)     summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)      # define tools     query_engine_tools = [         QueryEngineTool(             query_engine=vector_query_engine,             metadata=ToolMetadata(                 name=\"vector_tool\",                 description=(                     \"Useful for questions related to specific aspects of\"                     f\" {wiki_title} (e.g. the history, arts and culture,\"                     \" sports, demographics, or more).\"                 ),             ),         ),         QueryEngineTool(             query_engine=summary_query_engine,             metadata=ToolMetadata(                 name=\"summary_tool\",                 description=(                     \"Useful for any requests that require a holistic summary\"                     f\" of EVERYTHING about {wiki_title}. For questions about\"                     \" more specific sections, please use the vector_tool.\"                 ),             ),         ),     ]      # build agent     function_llm = OpenAI(model=\"gpt-4\")     agent = OpenAIAgent.from_tools(         query_engine_tools,         llm=function_llm,         verbose=True,         system_prompt=f\"\"\"\\ You are a specialized agent designed to answer queries about {wiki_title}. You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\ \"\"\",     )      agents[wiki_title] = agent     query_engines[wiki_title] = vector_index.as_query_engine(         similarity_top_k=2     ) In\u00a0[\u00a0]: Copied! <pre># define tool for each document agent\nall_tools = []\nfor wiki_title in wiki_titles:\n    wiki_summary = (\n        f\"This content contains Wikipedia articles about {wiki_title}. Use\"\n        f\" this tool if you want to answer any questions about {wiki_title}.\\n\"\n    )\n    doc_tool = QueryEngineTool(\n        query_engine=agents[wiki_title],\n        metadata=ToolMetadata(\n            name=f\"tool_{wiki_title}\",\n            description=wiki_summary,\n        ),\n    )\n    all_tools.append(doc_tool)\n</pre> # define tool for each document agent all_tools = [] for wiki_title in wiki_titles:     wiki_summary = (         f\"This content contains Wikipedia articles about {wiki_title}. Use\"         f\" this tool if you want to answer any questions about {wiki_title}.\\n\"     )     doc_tool = QueryEngineTool(         query_engine=agents[wiki_title],         metadata=ToolMetadata(             name=f\"tool_{wiki_title}\",             description=wiki_summary,         ),     )     all_tools.append(doc_tool) In\u00a0[\u00a0]: Copied! <pre># define an \"object\" index and retriever over these tools\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.objects import ObjectIndex\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n)\n</pre> # define an \"object\" index and retriever over these tools from llama_index.core import VectorStoreIndex from llama_index.core.objects import ObjectIndex  obj_index = ObjectIndex.from_objects(     all_tools,     index_cls=VectorStoreIndex, ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.agent.openai import OpenAIAgent\n\ntop_agent = OpenAIAgent.from_tools(\n    tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n    system_prompt=\"\"\" \\\nYou are an agent designed to answer queries about a set of given cities.\nPlease always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n\n\"\"\",\n    verbose=True,\n)\n</pre> from llama_index.agent.openai import OpenAIAgent  top_agent = OpenAIAgent.from_tools(     tool_retriever=obj_index.as_retriever(similarity_top_k=3),     system_prompt=\"\"\" \\ You are an agent designed to answer queries about a set of given cities. Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\  \"\"\",     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>base_index = VectorStoreIndex(all_nodes)\nbase_query_engine = base_index.as_query_engine(similarity_top_k=4)\n</pre> base_index = VectorStoreIndex(all_nodes) base_query_engine = base_index.as_query_engine(similarity_top_k=4) In\u00a0[\u00a0]: Copied! <pre># should use Boston agent -&gt; vector tool\nresponse = top_agent.query(\"Tell me about the arts and culture in Boston\")\n</pre> # should use Boston agent -&gt; vector tool response = top_agent.query(\"Tell me about the arts and culture in Boston\") <pre>=== Calling Function ===\nCalling function: tool_Boston with args: {\n  \"input\": \"arts and culture\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"arts and culture\"\n}\nGot output: Boston is known for its vibrant arts and culture scene. The city is home to a number of performing arts organizations, including the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. There are also several theaters in or near the Theater District, such as the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre. Boston is a center for contemporary classical music, with groups like the Boston Modern Orchestra Project and Boston Musica Viva. The city also hosts major annual events, such as First Night, the Boston Early Music Festival, and the Boston Arts Festival. In addition, Boston has several art museums and galleries, including the Museum of Fine Arts, the Isabella Stewart Gardner Museum, and the Institute of Contemporary Art.\n========================\nGot output: Boston is renowned for its vibrant arts and culture scene. It is home to numerous performing arts organizations, including the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. The city's Theater District houses several theaters, such as the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre.\n\nBoston is also a hub for contemporary classical music, with groups like the Boston Modern Orchestra Project and Boston Musica Viva. The city hosts major annual events, such as First Night, the Boston Early Music Festival, and the Boston Arts Festival, which contribute to its cultural richness.\n\nIn terms of visual arts, Boston boasts several art museums and galleries. The Museum of Fine Arts, the Isabella Stewart Gardner Museum, and the Institute of Contemporary Art are among the most notable. These institutions offer a wide range of art collections, from ancient to contemporary, attracting art enthusiasts from around the world.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) <pre>Boston has a rich arts and culture scene, with a variety of performing arts organizations and venues. The city is home to renowned institutions such as the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. The Theater District in Boston is a hub for theatrical performances, with theaters like the Cutler Majestic Theatre, Citi Performing Arts Center, Colonial Theater, and Orpheum Theatre.\n\nIn addition to performing arts, Boston also has a thriving contemporary classical music scene, with groups like the Boston Modern Orchestra Project and Boston Musica Viva. The city hosts several annual events that celebrate the arts, including First Night, the Boston Early Music Festival, and the Boston Arts Festival.\n\nBoston is also known for its visual arts scene, with a number of art museums and galleries. The Museum of Fine Arts, the Isabella Stewart Gardner Museum, and the Institute of Contemporary Art are among the notable institutions in the city. These museums offer a diverse range of art collections, spanning from ancient to contemporary art, and attract art enthusiasts from around the world.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Tell me about the arts and culture in Boston\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Tell me about the arts and culture in Boston\" ) print(str(response)) <pre>Boston has a rich arts and culture scene. The city is home to a variety of performing arts organizations, such as the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. Additionally, there are numerous contemporary classical music groups associated with the city's conservatories and universities, like the Boston Modern Orchestra Project and Boston Musica Viva. The Theater District in Boston is a hub for theater, with notable venues including the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre. Boston also hosts several significant annual events, including First Night, the Boston Early Music Festival, the Boston Arts Festival, and the Boston gay pride parade and festival. The city is renowned for its historic sites connected to the American Revolution, as well as its art museums and galleries, such as the Museum of Fine Arts, Isabella Stewart Gardner Museum, and the Institute of Contemporary Art.\n</pre> In\u00a0[\u00a0]: Copied! <pre># should use Houston agent -&gt; vector tool\nresponse = top_agent.query(\n    \"Give me a summary of all the positive aspects of Houston\"\n)\n</pre> # should use Houston agent -&gt; vector tool response = top_agent.query(     \"Give me a summary of all the positive aspects of Houston\" ) <pre>=== Calling Function ===\nCalling function: tool_Houston with args: {\n  \"input\": \"positive aspects\"\n}\n=== Calling Function ===\nCalling function: summary_tool with args: {\n  \"input\": \"positive aspects\"\n}\nGot output: Houston has many positive aspects that make it an attractive place to live and visit. The city's diverse population, with people from different ethnic and religious backgrounds, adds to its cultural richness and inclusiveness. Additionally, Houston is home to the Texas Medical Center, which is the largest concentration of healthcare and research institutions in the world. The presence of NASA's Johnson Space Center also highlights Houston's importance in the fields of medicine and space exploration. The city's strong economy, supported by industries such as energy, manufacturing, aeronautics, and transportation, provides numerous economic opportunities for residents and visitors alike. Furthermore, Houston has a thriving visual and performing arts scene, including a theater district and a variety of museums and galleries. Overall, Houston's diverse community, cultural attractions, and economic prospects make it an exceptionally appealing city.\n========================\nGot output: Houston has numerous positive aspects that make it a desirable place to live and visit. Some of these include:\n\n1. **Diversity**: Houston is known for its diverse population, with people from different ethnic and religious backgrounds. This diversity adds to the city's cultural richness and inclusiveness.\n\n2. **Healthcare and Research Institutions**: The city is home to the Texas Medical Center, the largest concentration of healthcare and research institutions in the world. This makes Houston a hub for medical innovation and healthcare services.\n\n3. **Space Exploration**: Houston is also known for NASA's Johnson Space Center, highlighting the city's significant role in space exploration.\n\n4. **Strong Economy**: Houston's economy is robust and diverse, supported by industries such as energy, manufacturing, aeronautics, and transportation. This provides numerous economic opportunities for its residents.\n\n5. **Arts and Culture**: The city has a thriving visual and performing arts scene, with a theater district and a variety of museums and galleries. This makes Houston a vibrant place for art lovers and creatives.\n\nOverall, these aspects contribute to making Houston an appealing and dynamic city.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) <pre>Houston has numerous positive aspects that make it a desirable place to live and visit. Some of these include:\n\n1. Diversity: Houston is known for its diverse population, with people from different ethnic and religious backgrounds. This diversity adds to the city's cultural richness and inclusiveness.\n\n2. Healthcare and Research Institutions: The city is home to the Texas Medical Center, the largest concentration of healthcare and research institutions in the world. This makes Houston a hub for medical innovation and healthcare services.\n\n3. Space Exploration: Houston is also known for NASA's Johnson Space Center, highlighting the city's significant role in space exploration.\n\n4. Strong Economy: Houston's economy is robust and diverse, supported by industries such as energy, manufacturing, aeronautics, and transportation. This provides numerous economic opportunities for its residents.\n\n5. Arts and Culture: The city has a thriving visual and performing arts scene, with a theater district and a variety of museums and galleries. This makes Houston a vibrant place for art lovers and creatives.\n\nOverall, these aspects contribute to making Houston an appealing and dynamic city.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Give me a summary of all the positive aspects of Houston\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Give me a summary of all the positive aspects of Houston\" ) print(str(response)) <pre>Houston has several positive aspects that contribute to its reputation as a thriving city. It is home to a diverse and growing international community, with a large number of foreign banks and consular offices representing 92 countries. The city has received numerous accolades, including being ranked as one of the best cities for employment, college graduates, and homebuyers. Houston has a strong economy, with a broad industrial base in sectors such as energy, manufacturing, aeronautics, and healthcare. It is also a major center for the oil and gas industry and has the second-most Fortune 500 headquarters in the United States. The city's cultural scene is vibrant, with a variety of annual events celebrating different cultures, as well as a reputation for diverse and excellent food. Houston is known for its world-class museums and performing arts scene. Additionally, the city has made significant investments in renewable energy sources like wind and solar. Overall, Houston offers a high quality of life, reasonable living costs, and abundant employment opportunities.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline: the response doesn't quite match the sources...\nresponse.source_nodes[1].get_content()\n</pre> # baseline: the response doesn't quite match the sources... response.source_nodes[1].get_content() In\u00a0[\u00a0]: Copied! <pre>response = top_agent.query(\n    \"Tell the demographics of Houston, and then compare that with the\"\n    \" demographics of Chicago\"\n)\n</pre> response = top_agent.query(     \"Tell the demographics of Houston, and then compare that with the\"     \" demographics of Chicago\" ) <pre>=== Calling Function ===\nCalling function: tool_Houston with args: {\n  \"input\": \"demographics\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"demographics\"\n}\nGot output: Houston is a majority-minority city with a diverse population. According to the U.S. Census Bureau, in 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in the city is Mexican Americans, followed by Puerto Ricans and Cuban Americans. Houston is also home to the largest African American community west of the Mississippi River. Additionally, Houston has a growing Muslim population, with Muslims estimated to make up 1.2% of the city's population. The city is known for its LGBT community and is home to one of the largest pride parades in the United States. The Hindu, Sikh, and Buddhist communities are also growing in Houston. Overall, Houston is considered one of the most ethnically and culturally diverse metropolitan areas in the country.\n========================\nGot output: Houston is a majority-minority city with a diverse population. According to the U.S. Census Bureau, in 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in the city is Mexican Americans, followed by Puerto Ricans and Cuban Americans. \n\nHouston is also home to the largest African American community west of the Mississippi River. Additionally, Houston has a growing Muslim population, with Muslims estimated to make up 1.2% of the city's population. The city is known for its LGBT community and is home to one of the largest pride parades in the United States. The Hindu, Sikh, and Buddhist communities are also growing in Houston. \n\nOverall, Houston is considered one of the most ethnically and culturally diverse metropolitan areas in the country.\n========================\n=== Calling Function ===\nCalling function: tool_Chicago with args: {\n  \"input\": \"demographics\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"demographics\"\n}\nGot output: Chicago has a diverse demographic makeup. It experienced rapid population growth during its early years, becoming one of the fastest-growing cities in the world. Waves of immigrants from various European countries, as well as African Americans from the American South, contributed to the city's population growth. Over time, Chicago's population has fluctuated, with a decline in the latter half of the 20th century followed by a rise in recent years. As of the latest census estimates, the largest racial or ethnic groups in Chicago are non-Hispanic White, Black, and Hispanic. Additionally, Chicago has a significant LGBT population and is known for its cultural diversity.\n========================\nGot output: Chicago is known for its diverse demographic makeup. The city experienced rapid population growth during its early years, with immigrants from various European countries and African Americans from the American South contributing significantly to this growth. Over time, the population has fluctuated, with a decline in the latter half of the 20th century followed by a rise in recent years. \n\nAs per the latest census estimates, the largest racial or ethnic groups in Chicago are non-Hispanic White, Black, and Hispanic. The city also has a significant LGBT population and is celebrated for its cultural diversity.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) <pre>Houston has a diverse population with a demographic makeup that includes non-Hispanic whites (23.3%), Hispanics and Latino Americans (45.8%), Blacks or African Americans (22.4%), and Asian Americans (6.5%). The largest Hispanic or Latino American ethnic group in Houston is Mexican Americans. Houston is also home to the largest African American community west of the Mississippi River and has a growing Muslim population.\n\nOn the other hand, Chicago is also known for its diverse demographics. The city has a significant non-Hispanic White population, along with a substantial Black population and Hispanic population. Chicago is celebrated for its cultural diversity and has a significant LGBT population.\n\nBoth Houston and Chicago have diverse populations, with a mix of different racial and ethnic groups contributing to their vibrant communities.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Tell the demographics of Houston, and then compare that with the\"\n    \" demographics of Chicago\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Tell the demographics of Houston, and then compare that with the\"     \" demographics of Chicago\" ) print(str(response)) <pre>Houston is the most populous city in Texas and the fourth-most populous city in the United States. It has a population of 2,304,580 as of the 2020 U.S. census. The city is known for its diversity, with a significant proportion of minorities. In 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in Houston is Mexican Americans, comprising 31.6% of the population.\n\nIn comparison, Chicago is the third-most populous city in the United States. According to the 2020 U.S. census, Chicago has a population of 2,746,388. The demographics of Chicago are different from Houston, with non-Hispanic whites making up 32.7% of the population, Hispanics and Latino Americans 29.9%, Blacks or African Americans 29.8%, and Asian Americans 7.6%. The largest Hispanic or Latino American ethnic group in Chicago is Mexican Americans, comprising 21.6% of the population.\n\nOverall, both Houston and Chicago have diverse populations, but the specific demographic composition differs between the two cities.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline: the response tells you nothing about Chicago...\nresponse.source_nodes[3].get_content()\n</pre> # baseline: the response tells you nothing about Chicago... response.source_nodes[3].get_content() In\u00a0[\u00a0]: Copied! <pre>response = top_agent.query(\n    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n    \" and current economy\"\n)\n</pre> response = top_agent.query(     \"Tell me the differences between Shanghai and Beijing in terms of history\"     \" and current economy\" ) <pre>=== Calling Function ===\nCalling function: tool_Shanghai with args: {\n  \"input\": \"history\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"history\"\n}\nGot output: Shanghai has a rich history that dates back to ancient times. However, in the context provided, the history of Shanghai is mainly discussed in relation to its modern development. After the war, Shanghai's economy experienced significant growth, with increased agricultural and industrial output. The city's administrative divisions were rearranged, and it became a center for radical leftism during the 1950s and 1960s. The Cultural Revolution had a severe impact on Shanghai's society, but the city maintained economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government. Economic reforms were initiated in Shanghai in 1990, leading to the development of the Pudong district and its classification as an Alpha+ city.\n========================\nGot output: Shanghai's history is rich and complex, dating back to ancient times. However, its modern development is particularly noteworthy. After the war, Shanghai experienced significant economic growth, with a boost in both agricultural and industrial output. The city's administrative divisions were restructured, and it became a hub for radical leftism during the 1950s and 1960s.\n\nThe Cultural Revolution had a profound impact on Shanghai's society, but despite this, the city managed to maintain economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government.\n\nIn 1990, economic reforms were initiated in Shanghai, leading to the development of the Pudong district. This has helped Shanghai to be classified as an Alpha+ city, indicating its influence on the global economic stage.\n========================\n=== Calling Function ===\nCalling function: tool_Beijing with args: {\n  \"input\": \"history\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"history\"\n}\nGot output: Beijing has a rich history that spans several dynasties. It was the capital of the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged. However, the city faced challenges during the Second Opium War and the Boxer Rebellion, resulting in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, which led to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911.\n========================\nGot output: Beijing has a rich and complex history that spans several dynasties. It served as the capital during the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged.\n\nHowever, the city faced significant challenges during the Second Opium War and the Boxer Rebellion, which resulted in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, leading to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911. Despite these tumultuous events, Beijing has managed to preserve its historical heritage while also evolving into a modern metropolis.\n========================\n=== Calling Function ===\nCalling function: tool_Shanghai with args: {\n  \"input\": \"current economy\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"current economy\"\n}\nGot output: The current economy of Shanghai is strong and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. Shanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CN\u00a54.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. It is also the most expensive city in mainland China to live in. Shanghai is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. It is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. The city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n========================\nGot output: The current economy of Shanghai is robust and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. \n\nShanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CN\u00a54.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. \n\nShanghai is also the most expensive city in mainland China to live in. It is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. The city is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. \n\nThe city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n========================\n=== Calling Function ===\nCalling function: tool_Beijing with args: {\n  \"input\": \"current economy\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"current economy\"\n}\nGot output: The current economy of Beijing is dominated by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%. The city has also identified six high-end economic output zones that are driving local economic growth, including Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n========================\nGot output: The current economy of Beijing is primarily driven by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%.\n\nBeijing has also identified six high-end economic output zones that are driving local economic growth. These include Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response)) <pre>In terms of history, both Shanghai and Beijing have rich and complex pasts. Shanghai's history dates back to ancient times, but its modern development is particularly noteworthy. It experienced significant economic growth after the war and played a major role in China's economic reforms. Beijing, on the other hand, has a history that spans several dynasties and served as the capital during the Ming and Qing dynasties. It has preserved its historical heritage while evolving into a modern metropolis.\n\nIn terms of current economy, Shanghai is a global center for finance and innovation. It has a diverse economy and has experienced rapid development, with a high GDP and significant foreign investment. It is a major player in the global financial industry and is home to the Shanghai Stock Exchange. Beijing's economy is primarily driven by the tertiary sector, with a focus on services such as professional services, information technology, and commercial real estate. It has identified high-end economic output zones that are driving local economic growth.\n\nOverall, both cities have thriving economies, but Shanghai has a stronger focus on finance and global influence, while Beijing has a diverse economy with a focus on services and high-end economic zones.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n    \" and current economy\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Tell me the differences between Shanghai and Beijing in terms of history\"     \" and current economy\" ) print(str(response)) <pre>Shanghai and Beijing have distinct differences in terms of history and current economy. Historically, Shanghai was the largest and most prosperous city in East Asia during the 1930s, while Beijing served as the capital of the Republic of China and later the People's Republic of China. Shanghai experienced significant growth and redevelopment in the 1990s, while Beijing expanded its urban area and underwent rapid development in the last two decades.\n\nIn terms of the current economy, Shanghai is considered the \"showpiece\" of China's booming economy. It is a global center for finance and innovation, with a strong focus on industries such as retail, finance, IT, real estate, machine manufacturing, and automotive manufacturing. Shanghai is also home to the world's busiest container port, the Port of Shanghai. The city has a high GDP and is classified as an Alpha+ city by the Globalization and World Cities Research Network.\n\nOn the other hand, Beijing is a global financial center and ranks third globally in the Global Financial Centres Index. It is also a hub for the Chinese and global technology industry, with a large startup ecosystem. Beijing has a strong presence in industries such as finance, technology, and pharmaceuticals. The city is home to the headquarters of large state banks and insurance companies, as well as the country's financial regulatory agencies.\n\nOverall, while both Shanghai and Beijing are important economic centers in China, Shanghai has a stronger focus on industries such as finance, retail, and manufacturing, while Beijing has a strong presence in finance, technology, and pharmaceuticals.\n</pre>"},{"location":"Agents/projects/multi_document_agents/#multi-document-agents","title":"Multi-Document Agents\u00b6","text":"<p>In this guide, you learn towards setting up an agent that can effectively answer different types of questions over a larger set of documents.</p> <p>These questions include the following</p> <ul> <li>QA over a specific doc</li> <li>QA comparing different docs</li> <li>Summaries over a specific doc</li> <li>Comparing summaries between different docs</li> </ul> <p>We do this with the following architecture:</p> <ul> <li>setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc</li> <li>setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question.</li> </ul>"},{"location":"Agents/projects/multi_document_agents/#setup-and-download-data","title":"Setup and Download Data\u00b6","text":"<p>In this section, we'll define imports and then download Wikipedia articles about different cities. Each article is stored separately.</p> <p>We load in 18 cities - this is not quite at the level of \"hundreds\" of documents but its still large enough to warrant some top-level document retrieval!</p>"},{"location":"Agents/projects/multi_document_agents/#building-multi-document-agents","title":"Building Multi-Document Agents\u00b6","text":"<p>In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index.</p>"},{"location":"Agents/projects/multi_document_agents/#build-document-agent-for-each-document","title":"Build Document Agent for each Document\u00b6","text":"<p>In this section we define \"document agents\" for each document.</p> <p>We define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.</p> <p>This document agent can dynamically choose to perform semantic search or summarization within a given document.</p> <p>We create a separate document agent for each city.</p>"},{"location":"Agents/projects/multi_document_agents/#build-retriever-enabled-openai-agent","title":"Build Retriever-Enabled OpenAI Agent\u00b6","text":"<p>We build a top-level agent that can orchestrate across the different document agents to answer any user query.</p> <p>This agent takes in all document agents as tools. This specific agent <code>RetrieverOpenAIAgent</code> performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).</p> <p>Here we use a top-k retriever, but we encourage you to customize the tool retriever method!</p>"},{"location":"Agents/projects/multi_document_agents/#define-baseline-vector-store-index","title":"Define Baseline Vector Store Index\u00b6","text":"<p>As a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.</p> <p>We set the top_k = 4</p>"},{"location":"Agents/projects/multi_document_agents/#running-example-queries","title":"Running Example Queries\u00b6","text":"<p>Let's run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents.</p>"},{"location":"Deployment/","title":"Introduction","text":""},{"location":"Deployment/#model-deployment-guide","title":"Model Deployment Guide","text":"<p>Welcome to the Model Deployment section of AI Engineering Academy! This module will guide you through the practical aspects of deploying AI models in production environments.</p>"},{"location":"Deployment/#llm-to-prod","title":"LLM to Prod","text":"<p>A blog on how to deploy open source LLMs into Produciton covering TGI,Vllm,SGlang</p>"},{"location":"Deployment/#quantization-techniques","title":"Quantization Techniques","text":"Notebook Description AWQ Quantization Activation-aware Weight Quantization implementation GGUF Quantization GGUF format quantization guide"},{"location":"Deployment/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Interested in contributing to this section? We welcome:</p> <ul> <li>Additional deployment strategies</li> <li>Case studies</li> <li>Performance optimization techniques</li> <li>Best practices documentation</li> </ul> <p>See our contributing guidelines for more information.</p>"},{"location":"Deployment/#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>    Coming Soon: Complete deployment guides for production AI systems!      Made with \u2764\ufe0f by the AI Engineering Academy Team"},{"location":"Deployment/DeployLLMtoProd/","title":"LLMs to Prod","text":"<p>Deploying Large Language Models (LLMs) into production involves choosing the right tools and considering costs. Here\u2019s a breakdown for comparing TGI, vLLM, and SGlang, deploying Llama 3.2 70B (likely Llama 3.1 70B), scaling on Kubernetes, and deciding between hosting your own or using third-party APIs.</p> <p>Comparing TGI, vLLM, and SGlang</p> <ul> <li>TGI (Text Generation Inference) from Hugging Face is great for integrating with their ecosystem, supporting models like Llama, with tensor parallelism for performance.</li> <li>vLLM, developed at UC Berkeley, offers high throughput with Paged Attention, ideal for fast inference on GPUs.</li> <li>SGlang, by LMSYS Org, focuses on efficiency with low-latency serving, suitable for real-time applications.</li> </ul> <p>Each can deploy Llama 3.1 70B, requiring multiple GPUs (e.g., 8 with 40 GB each). TGI is user-friendly for Hugging Face users, vLLM excels in speed, and SGlang offers scalability.</p> <p>Deploying Llama 3.2 70B</p> <p>Given Llama 3.2 models are 1B, 3B, 11B, and 90B, \"Llama 3.2 70B\" likely means Llama 3.1 70B. Deployment steps:</p> <ul> <li>TGI: Use Hugging Face\u2019s launcher, containerize, and distribute across GPUs.</li> <li>vLLM: Leverage Paged Attention, deploy via container images, and scale with Kubernetes.</li> <li>SGlang: Use its runtime, optimize for GPUs, and deploy on clusters.</li> </ul> <p>Deploying on Kubernetes at Scale</p> <p>All three can be containerized and deployed on Kubernetes:</p> <ul> <li>Use Kubernetes for scaling, monitoring, and managing resources across nodes.</li> <li>TGI has Kubernetes deployment examples Hugging Face TGI.</li> <li>vLLM and SGlang also support containerization, with scaling via Kubernetes pods.</li> </ul> <p>Price Comparison and Hosting vs. Third-Party APIs</p> <ul> <li>Hosting Your Own: Costs include GPU hardware (e.g., AWS A100 at ~\\(3/hour each, needing 8 for Llama 3.1 70B, ~\\)24/hour) and maintenance. For high volume, it\u2019s cheaper long-term.</li> <li>Third-Party APIs: OpenAI charges per token (e.g., $0.01/1000 tokens input, $0.03/1000 output). For low volume, it\u2019s easier and cost-effective.</li> <li>Research suggests hosting is better for privacy and high usage, while APIs suit quick setups.</li> </ul> <p>This guide helps decide based on your needs, with Kubernetes offering scalability for all options.</p> <p>Survey Note: Comprehensive Analysis of Deploying LLMs into Production</p> <p>Deploying Large Language Models (LLMs) into production is a complex task, requiring careful selection of tools, infrastructure, and cost management. This analysis compares Text Generation Inference (TGI), vLLM, and SGlang for deploying models like Llama 3.2 70B (likely Llama 3.1 70B given current offerings), discusses deployment on Kubernetes at scale, provides a rough price comparison, and evaluates hosting versus using third-party APIs. The analysis is based on current research and documentation as of February 24, 2025.</p> <p>Introduction to LLM Deployment</p> <p>Deploying LLMs in production involves setting up the model to handle real-world requests efficiently, managing resources, ensuring reliability, and maintaining performance. Given the computational demands of LLMs, specialized frameworks are essential for optimizing inference speed and memory usage. This survey explores three prominent tools: TGI, vLLM, and SGlang, focusing on their capabilities for deploying a large model like Llama 3.1 70B, scaling on Kubernetes, and cost implications.</p> <p>Overview of Deployment Tools</p> <ol> <li>Text Generation Inference (TGI):</li> <li>TGI, developed by Hugging Face, is a toolkit designed for deploying and serving Large Language Models (LLMs). It supports various open-source models, including Llama, Falcon, StarCoder, BLOOM, and GPT-NeoX, making it versatile for production environments.</li> <li>Key features include tensor parallelism for faster inference across multiple GPUs, optimized transformers code using Flash Attention and Paged Attention, and a simple API for compatibility with Hugging Face models. It is already in use by organizations like IBM and Grammarly, indicating robust production readiness.</li> <li>TGI offers distributed tracing via Open Telemetry and Prometheus metrics for monitoring, enhancing operational visibility.</li> <li>vLLM:</li> <li>vLLM, originating from the Sky Computing Lab at UC Berkeley, is an open-source library for fast LLM inference and serving, with over 200k monthly downloads and an Apache 2.0 license. It is designed for high-throughput and memory-efficient serving, leveraging PagedAttention and continuous batching.</li> <li>It supports distributed inference across multiple GPUs, with quantizations like GPTQ, AWQ, INT4, INT8, and FP8, and optimized CUDA kernels including FlashAttention and FlashInfer. vLLM is particularly noted for up to 24x higher throughput compared to Hugging Face Transformers without model changes.</li> <li>It is suitable for applications requiring parallel processing and streaming output, with integration capabilities for platforms like SageMaker and LangChain.</li> <li>SGlang:</li> <li>SGlang, developed by LMSYS Org, is a fast serving framework for LLMs and vision-language models, focusing on efficient execution of complex language model programs. It offers a flexible frontend language for programming LLM applications, including chained generation calls, advanced prompting, and multi-modal inputs.</li> <li>It supports a wide range of generative models (Llama, Gemma, Mistral, QWen, DeepSeek, LLaVA, etc.), embedding models, and reward models, with easy extensibility. SGlang introduces optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding, achieving up to 6.4x higher throughput compared to state-of-the-art systems.</li> <li>Backed by an active community and supported by industry players like NVIDIA and xAI, SGlang is designed for scalability and low-latency inference, suitable for real-time applications.</li> </ol> <p>Deploying Llama 3.2 70B: Clarification and Approach</p> <p>The user query mentions \"Llama 3.2 70B,\" but current documentation as of February 2025 indicates Llama 3.2 models are available in sizes 1B, 3B, 11B, and 90B, with multimodal capabilities for 11B and 90B, and no explicit 70B version Meta Llama. Given this, it is likely the user intended Llama 3.1 70B, which is part of the Llama 3.1 collection with sizes ranging from 8B to 405B, released in July 2024 Meta AI Blog. This analysis will proceed with Llama 3.1 70B as the target model.</p> <ul> <li>System Requirements:</li> <li>Llama 3.1 70B, with 70 billion parameters, requires significant computational resources. In float16 precision, each parameter consumes 2 bytes, totaling approximately 140 GB for model weights. During inference, additional memory is needed for the key-value (KV) cache, potentially requiring 5-10x more memory for high-throughput scenarios.</li> <li>Deployment typically requires multiple GPUs, with recommendations including 8 GPUs with at least 40 GB VRAM each (e.g., NVIDIA A100 or H100). For example, deploying on AWS might use inf2.48xlarge instances with 12 Inferentia2 accelerators, or cloud instances like g5.48xlarge for EC2 Deploy Llama 3 70B on AWS Inferentia2.</li> </ul> <p>Comparison of TGI, vLLM, and SGlang for Llama 3.1 70B</p> <p>To compare these frameworks, we evaluate performance, ease of use, and specific support for deploying Llama 3.1 70B:</p> <ul> <li>Performance:</li> <li>TGI leverages tensor parallelism and optimized attention mechanisms, achieving high-performance text generation. Benchmarks suggest it handles Llama 2 70B with 8 GPUs of 40 GB each, suitable for production but may have higher latency for very large models Deploying LLM Powered Applications with HuggingFace TGI.</li> <li>vLLM, with PagedAttention and continuous batching, offers up to 24x higher throughput than Hugging Face Transformers, making it ideal for high-throughput scenarios. For Llama 3.1 70B, it supports CPU offloading on NVIDIA GH200 instances, expanding available memory Serving Llama 3.1 8B and 70B using vLLM on an NVIDIA GH200 instance.</li> <li>SGlang achieves up to 6.4x higher throughput compared to vLLM and TensorRT-LLM on tasks like agent control and JSON decoding, with optimizations like RadixAttention for KV cache reuse SGLang: Efficient Execution of Structured Language Model Programs. It is designed for low-latency, real-time applications.</li> <li>Ease of Use:</li> <li>TGI offers a simple launcher and integration with Hugging Face Hub, making it user-friendly for developers familiar with the ecosystem. However, distributed setup may require additional configuration.</li> <li>vLLM is noted for its ease of deployment, with pre-configured environments on platforms like AWS and support for Hugging Face models, requiring minimal setup for inference Deploy Large Language Model (LLM) with vLLM on K8s.</li> <li>SGlang provides an intuitive interface for programming LLM applications, but being relatively new, it may have fewer community resources compared to TGI and vLLM SGLang GitHub.</li> <li>Specific Support for Llama 3.1 70B:</li> <li>All three frameworks support Llama models, with TGI explicitly mentioned for Llama 2 and 3 deployments Deploy Llama 2 70B on AWS Inferentia2 with Hugging Face Optimum. For Llama 3.1 70B, TGI\u2019s tensor parallelism is effective.</li> <li>vLLM has detailed guides for deploying Llama 3.1 70B, including on NVIDIA GH200 with CPU offloading, making it suitable for memory-constrained environments Run Llama 3 on Dell PowerEdge XE9680 and AMD MI300x with vLLM.</li> <li>SGlang supports Llama 3.1 70B, with benchmarks showing superior throughput compared to vLLM and TensorRT-LLM, particularly for complex tasks Achieving Faster Open-Source Llama3 Serving with SGLang Runtime.</li> </ul> <p>Deploying Llama 3.1 70B with Each Framework</p> <p>Given the user\u2019s mention of Llama 3.2 70B, current documentation (as of February 24, 2025) shows Llama 3.2 models are 1B, 3B, 11B, and 90B, with no 70B Meta Llama. It\u2019s likely they meant Llama 3.1 70B, part of the Llama 3.1 collection released in July 2024 Meta AI Blog. Here\u2019s how to deploy it:</p> <p>TGI Deployment</p> <ol> <li>Prerequisites: Install TGI, ensure 8 GPUs with 40 GB VRAM each.</li> <li>Steps:</li> <li>Download the model: huggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct.</li> <li>Start the server: tgi-server --model-id meta-llama/Meta-Llama-3-70B-Instruct --num-shards 8.</li> </ol> <p>vLLM Deployment</p> <ol> <li>Prerequisites: Install vLLM, ensure GPU compatibility.</li> <li>Steps:</li> <li>Download the model weights.</li> <li>Start the server: vllm --model meta-llama/Meta-Llama-3-70B-Instruct --tensor-parallel-degree 8.</li> </ol> <p>SGlang Deployment</p> <ol> <li>Prerequisites: Install SGlang, ensure GPU support.</li> <li>Steps:</li> <li>Load the model: pip install sglang[srt].</li> <li>Start the server: sglang serve meta-llama/Meta-Llama-3-70B-Instruct.</li> </ol> <p>Deploying on Kubernetes at Scale</p> <p>All three can be containerized and deployed on Kubernetes for scalability:</p> <p>TGI on Kubernetes</p> <ul> <li>Create a cluster with GPU support using gcloud container clusters create.</li> <li>Define a Deployment:</li> </ul> <p>yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tgi-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tgi-server\n  template:\n    metadata:\n      labels:\n        app: tgi-server\n    spec:\n      containers:\n        - name: tgi-server\n          image: huggingface/tgi-server\n          args:\n            - --model-id=meta-llama/Meta-Llama-3-70B-Instruct\n            - --num-shards=8\n          resources:\n            limits:\n              nvidia.com/gpu: 8\n</code></pre> <ul> <li>Apply with kubectl apply -f deployment.yaml.</li> </ul> <p>vLLM on Kubernetes</p> <ul> <li>Create a cluster with GPU support.</li> <li>Define a Deployment:</li> </ul> <p>yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-server\n  template:\n    metadata:\n      labels:\n        app: vllm-server\n    spec:\n      containers:\n        - name: vllm-server\n          image: vllm/vllm-openai\n          args:\n            - --model=meta-llama/Meta-Llama-3-70B-Instruct\n            - --tensor-parallel-degree=8\n          resources:\n            limits:\n              nvidia.com/gpu: 8\n</code></pre> <ul> <li>Apply with kubectl apply -f deployment.yaml.</li> </ul> <p>SGlang on Kubernetes</p> <ul> <li>Create a cluster with GPU support.</li> <li>Define a Deployment:</li> </ul> <p>yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sglang-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sglang-server\n  template:\n    metadata:\n      labels:\n        app: sglang-server\n    spec:\n      containers:\n        - name: sglang-server\n          image: sglang/sglang-runtime\n          args:\n            - serve\n            - meta-llama/Meta-Llama-3-70B-Instruct\n          resources:\n            limits:\n              nvidia.com/gpu: 8\n</code></pre> <ul> <li>Apply with kubectl apply -f deployment.yaml.</li> </ul> <p>Deploying on Kubernetes at Scale</p> <p>The user query mentions \"Kuberntees,\" likely a typo for \"Kubernetes,\" a standard container orchestration system for managing applications at scale. All three frameworks can be deployed on Kubernetes, leveraging containerization for scalability:</p> <ul> <li>TGI on Kubernetes:</li> <li>Hugging Face provides examples for deploying TGI on Kubernetes, using container images and managing with Kubernetes pods. For Llama 3.1 70B, distribute across multiple nodes with tensor parallelism, ensuring sufficient GPU resources Deploy Meta Llama 3 8B with TGI DLC on GKE.</li> <li>Scaling involves adjusting pod replicas based on load, with monitoring via Kubernetes metrics and Open Telemetry for distributed tracing.</li> <li>vLLM on Kubernetes:</li> <li>vLLM supports deployment on Kubernetes, with guides for using GPU operators and scaling on platforms like Azure Kubernetes Service (AKS). For Llama 3.1 70B, use container images and configure for distributed inference, leveraging PagedAttention for efficiency Deploy Large Language Model (LLM) with vLLM on K8s.</li> <li>Scaling can be automated with Horizontal Pod Autoscaler, monitoring GPU utilization and throughput.</li> <li>SGlang on Kubernetes:</li> <li>SGlang can be containerized and deployed on Kubernetes, with its runtime optimized for GPU utilization. For Llama 3.1 70B, configure for high throughput, using Kubernetes for dynamic workload distribution SGLang: Fast Serving Framework for Large Language and Vision-Language Models on AMD Instinct GPUs.</li> <li>Scaling involves managing pod distribution across nodes, ensuring low-latency inference for real-time applications.</li> </ul> <p>Rough Price Comparison and Hosting vs. Third-Party APIs</p> <p>To compare the cost of hosting your own LLM versus using a third-party API, consider the following factors:</p> <ul> <li>Hosting Your Own LLM:</li> <li>Hardware Costs: Deploying Llama 3.1 70B requires significant GPU resources, typically 8 GPUs with 40 GB VRAM each (e.g., NVIDIA A100 or H100). On AWS, an A100 instance might cost approximately \\(3 per hour each, totaling ~\\)24/hour for 8 GPUs, or $17,280/month for continuous operation Self-Hosting LLaMA 3.1 70B (or any ~70B LLM) Affordably. On-premises, initial hardware costs (e.g., $25,000 per H100 GPU) and power consumption add to the expense.</li> <li>Operational Costs: Include maintenance, cooling, and power, which can be significant for on-premises setups. Cloud hosting reduces some operational overhead but increases hourly costs.</li> <li>Break-even Analysis: For high-volume usage (e.g., 1000+ requests/day), hosting can be cost-effective long-term, especially with optimizations like quantization reducing memory needs.</li> <li>Third-Party APIs (e.g., OpenAI):</li> <li>OpenAI\u2019s pricing is token-based, with GPT-4o at $0.01 per 1000 input tokens and $0.03 per 1000 output tokens as of February 2025 OpenAI Pricing. For Llama 3.1 70B, API providers like Replicate or Groq offer competitive rates, with some at $0.88 per 1M tokens blended Llama 3 70B - Intelligence, Performance &amp; Price Analysis.</li> <li>For low to medium usage (e.g., 100-1000 requests/day), APIs are more cost-effective, with costs below $100/month for small volumes, but scaling up (e.g., 2000 requests/day) can reach $2000/month, making hosting more viable Is Hosting Your Own LLM Cheaper than OpenAI?.</li> <li>Comparison Table(these are rough calculations):</li> </ul> Aspect Hosting Your Own (Llama 3.1 70B) Third-Party API (e.g., OpenAI) Initial Cost High (Hardware ~$200,000 for 8 H100s) Low (No hardware needed) Monthly Cost (Low Use) ~$17,280 (Cloud, continuous) ~$100 (1000 requests/day) Monthly Cost (High Use) ~$17,280 (Fixed) ~$2000 (2000 requests/day) Privacy High (On-premises control) Low (Data sent to provider) Scalability High (Kubernetes, custom scaling) Medium (API limits) Ease of Deployment Medium (Setup complexity) High (Quick integration) <ul> <li>Decision Factors:</li> <li>Host your own LLM for high data privacy needs (e.g., finance, healthcare), large-scale usage where API costs escalate, and when customization is critical. It\u2019s also suitable for long-term cost savings at high volumes.</li> <li>Use third-party APIs for quick deployment, low to medium usage, and when infrastructure management is a burden. They offer ease of use but may compromise on privacy and cost at scale.</li> </ul> <p>Conclusion</p> <p>Research suggests that TGI, vLLM, and SGlang are viable for deploying LLMs like Llama 3.1 70B, each with unique strengths: TGI for ecosystem integration, vLLM for speed, and SGlang for efficiency. Deploying on Kubernetes at scale is feasible for all, with containerization and scaling options. The evidence leans toward hosting your own LLM being cost-effective for high-volume use, while third-party APIs suit lower volumes. An unexpected detail is that Llama 3.2 70B may not exist, likely referring to Llama 3.1 70B, highlighting the importance of model version clarity. Choose based on your usage volume, privacy needs, and infrastructure capabilities.</p> <p>Key Citations</p> <ul> <li>Deploying LLM Powered Applications with HuggingFace TGI</li> <li>vLLM GitHub: A high-throughput and memory-efficient inference and serving engine for LLMs</li> <li>SGLang GitHub: SGLang is a fast serving framework for large language models and vision language models</li> <li>Meta Llama: Llama 3.2 Vision and other models</li> <li>Meta AI Blog: Meta Llama 3</li> <li>Deploy Llama 2 70B on AWS Inferentia2 with Hugging Face Optimum</li> <li>Serving Llama 3.1 8B and 70B using vLLM on an NVIDIA GH200 instance</li> <li>SGLang: Efficient Execution of Structured Language Model Programs</li> <li>Deploy Large Language Model (LLM) with vLLM on K8s</li> <li>Deploy Meta Llama 3 8B with TGI DLC on GKE</li> <li>Self-Hosting LLaMA 3.1 70B (or any ~70B LLM) Affordably</li> <li>Is Hosting Your Own LLM Cheaper than OpenAI?</li> <li>Llama 3 70B - Intelligence, Performance &amp; Price Analysis</li> <li>OpenAI Pricing: API pricing details</li> </ul>"},{"location":"Deployment/Quantization/AWQ_Quantization/","title":"AWQ","text":"In\u00a0[\u00a0]: Copied! <pre># install from source\n\n# !git clone https://github.com/casper-hansen/AutoAWQ\n# %cd AutoAWQ\n# !pip install -e .\n# %cd ..\n\n\n# quick install the most stable version\n!pip install autoawq -q\n</pre> # install from source  # !git clone https://github.com/casper-hansen/AutoAWQ # %cd AutoAWQ # !pip install -e . # %cd ..   # quick install the most stable version !pip install autoawq -q  In\u00a0[\u00a0]: Copied! <pre># install transformers from the source - dev version\n!pip install  git+https://github.com/huggingface/transformers.git -q\n!pip install huggingface_hub\n</pre> # install transformers from the source - dev version !pip install  git+https://github.com/huggingface/transformers.git -q !pip install huggingface_hub  In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\n\nnotebook_login()\n</pre> from huggingface_hub import notebook_login  notebook_login() In\u00a0[\u00a0]: Copied! <pre>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nimport torch\n\nmodel_path = \"PY007/TinyLlama-1.1B-Chat-v0.3\" #replace with your model path or model id\n\nquant_name =  model_path.split(\"/\")[-1] + \"-AWQ\"\n\nquant_path = \"AdithyaSK/\" + quant_name\nquant_config = {\"zero_point\" : True, \"q_group_size\":128,\"w_bit\":4}\n\n#Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path , device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True)\n\n# Quantize\n\nmodel.quantize(tokenizer,quant_config=quant_config)\n\nmodel.save_quantized(quant_name, safetensors=True , shard_size=\"10GB\")\ntokenizer.save_pretrained(quant_name)\n</pre> from awq import AutoAWQForCausalLM from transformers import AutoTokenizer  import torch  model_path = \"PY007/TinyLlama-1.1B-Chat-v0.3\" #replace with your model path or model id  quant_name =  model_path.split(\"/\")[-1] + \"-AWQ\"  quant_path = \"AdithyaSK/\" + quant_name quant_config = {\"zero_point\" : True, \"q_group_size\":128,\"w_bit\":4}  #Load model model = AutoAWQForCausalLM.from_pretrained(model_path , device_map=\"auto\") tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True)  # Quantize  model.quantize(tokenizer,quant_config=quant_config)  model.save_quantized(quant_name, safetensors=True , shard_size=\"10GB\") tokenizer.save_pretrained(quant_name) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import create_repo\n\nrepo_id = \"AdithyaSK/\" + quant_name\n\ncreate_repo(quant_path , private=False)\n</pre> from huggingface_hub import create_repo  repo_id = \"AdithyaSK/\" + quant_name  create_repo(quant_path , private=False) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import HfApi\n\napi = HfApi()\n\npath_in_repo = \"model.safetensors\"\n\nlocal_file_path = \"./\"+ quant_name + \"/\" + path_in_repo\n\napi.upload_file(\n    path_or_fileobj = local_file_path,\n    path_in_repo = path_in_repo,\n    repo_id = repo_id,\n    repo_type = \"model\"\n)\n</pre> from huggingface_hub import HfApi  api = HfApi()  path_in_repo = \"model.safetensors\"  local_file_path = \"./\"+ quant_name + \"/\" + path_in_repo  api.upload_file(     path_or_fileobj = local_file_path,     path_in_repo = path_in_repo,     repo_id = repo_id,     repo_type = \"model\" ) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import HfApi\n\napi = HfApi()\n\nrepo_id = \"AdithyaSK/\"+ quant_name\n\nlocal_file_paths = [\n    \"./\"+ quant_name + \"/config.json\",\n    \"./\"+ quant_name + \"/generation_config.json\",\n    \"./\"+ quant_name + \"/quant_config.json\",\n    \"./\"+ quant_name + \"/special_tokens_map.json\",\n    \"./\"+ quant_name + \"/tokenizer_config.json\",\n    \"./\"+ quant_name + \"/tokenizer.json\",\n]\n\n#Loop thorugh each file and upload\nfor local_file_path in local_file_paths:\n    file_name = local_file_path.split(\"/\")[-1]\n\n    path_in_repo = file_name\n\n    api.upload_file(\n        path_or_fileobj=local_file_path,\n        path_in_repo=path_in_repo,\n        repo_id=repo_id,\n        repo_type=\"model\"\n    )\n    print(f\"Uploaded {file_name} to {repo_id}\")\n</pre> from huggingface_hub import HfApi  api = HfApi()  repo_id = \"AdithyaSK/\"+ quant_name  local_file_paths = [     \"./\"+ quant_name + \"/config.json\",     \"./\"+ quant_name + \"/generation_config.json\",     \"./\"+ quant_name + \"/quant_config.json\",     \"./\"+ quant_name + \"/special_tokens_map.json\",     \"./\"+ quant_name + \"/tokenizer_config.json\",     \"./\"+ quant_name + \"/tokenizer.json\", ]  #Loop thorugh each file and upload for local_file_path in local_file_paths:     file_name = local_file_path.split(\"/\")[-1]      path_in_repo = file_name      api.upload_file(         path_or_fileobj=local_file_path,         path_in_repo=path_in_repo,         repo_id=repo_id,         repo_type=\"model\"     )     print(f\"Uploaded {file_name} to {repo_id}\") In\u00a0[\u00a0]: Copied! <pre>## Load AWQ Model\n\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\n\n### Note that the model must be in safetensors formate!\n\n# model_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\"\nmodel_name_or_path = f\"{repo_id}\"\n\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,trust_remote_code = False, safetensors = True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code = False)\n</pre> ## Load AWQ Model  from awq import AutoAWQForCausalLM from transformers import AutoTokenizer   ### Note that the model must be in safetensors formate!  # model_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\" model_name_or_path = f\"{repo_id}\"  model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,trust_remote_code = False, safetensors = True)  tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code = False) In\u00a0[17]: Copied! <pre># ## Load model in bf16\n\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# import torch\n\n# model_name_or_path = \"\" # model name\n\n# ## Load model\n# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16 , trust_remote_code = True , device =\"cuda\")\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code = False)\n</pre> # ## Load model in bf16  # from transformers import AutoTokenizer, AutoModelForCausalLM # import torch  # model_name_or_path = \"\" # model name  # ## Load model # model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16 , trust_remote_code = True , device =\"cuda\")  # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code = False) In\u00a0[\u00a0]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi In\u00a0[\u00a0]: Copied! <pre>print(torch.cuda.get_device_name())\n</pre> print(torch.cuda.get_device_name()) In\u00a0[\u00a0]: Copied! <pre>import torch\n\nprompt = \"Who played the character Iron man?\"\n\nfromatted_prompt = f\"&lt;|im_start|&gt;users\\n{prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n\ntokens = tokenizer(fromatted_prompt,return_tensors=\"pt\").input_ids.cuda()\n# tokens = tokenizer(fromatted_prompt,return_tensors=\"pt\", device =\"cuda\").input_ids.cuda()\n\n# Generate Output\n\ngeneration_output = model.generate(tokens,do_sample=False,max_new_tokens=512)\n\nprint(tokenizer.decode(generation_output[0],skip_special_tokens=True))\n</pre> import torch  prompt = \"Who played the character Iron man?\"  fromatted_prompt = f\"&lt;|im_start|&gt;users\\n{prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"  tokens = tokenizer(fromatted_prompt,return_tensors=\"pt\").input_ids.cuda() # tokens = tokenizer(fromatted_prompt,return_tensors=\"pt\", device =\"cuda\").input_ids.cuda()  # Generate Output  generation_output = model.generate(tokens,do_sample=False,max_new_tokens=512)  print(tokenizer.decode(generation_output[0],skip_special_tokens=True))"},{"location":"Deployment/Quantization/AWQ_Quantization/#qunatize-with-awq","title":"Qunatize with AWQ\u00b6","text":"<p>Activation Aware Quantization</p> <p>This notebook is for you to qunatize huggingface models in AWQ formate and upload them to the Hub</p> <p>Paper</p>"},{"location":"Deployment/Quantization/AWQ_Quantization/#push-models-and-tokenizers-to-hub","title":"Push models and tokenizers to Hub\u00b6","text":""},{"location":"Deployment/Quantization/AWQ_Quantization/#upload-non-model-files","title":"Upload non-Model Files\u00b6","text":""},{"location":"Deployment/Quantization/AWQ_Quantization/#run-awq-inference-with-autoawq","title":"Run AWQ Inference with AutoAWQ\u00b6","text":""},{"location":"Deployment/Quantization/GGUF_Quantization/","title":"GGUF","text":"<p>Converting HuggingFace Models to GGUF/GGML</p> <p>Download the base model from Huggingface</p> <p>Quantisation</p> <p>Inference using LLama.cpp</p> <p>Inference using ctransformers</p> <p>Pushing to HuggingFace Hub</p> In\u00a0[\u00a0]: Copied! <pre>!pip install huggingface_hub\n</pre> !pip install huggingface_hub In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import snapshot_download\nmodel_id = \"meta-llama/Llama-2-7b-hf\" # @param {type:\"string\"}\nlocal_directory = model_id.split(\"/\")[-1]\nsnapshot_download(repo_id=model_id,\n                  local_dir=local_directory,\n                  local_dir_use_symlinks=False,\n                  revision=\"main\")\n</pre> from huggingface_hub import snapshot_download model_id = \"meta-llama/Llama-2-7b-hf\" # @param {type:\"string\"} local_directory = model_id.split(\"/\")[-1] snapshot_download(repo_id=model_id,                   local_dir=local_directory,                   local_dir_use_symlinks=False,                   revision=\"main\") In\u00a0[\u00a0]: Copied! <pre># @title Installing Llama.cpp\n!apt update -y\n!apt install build-essential git cmake libopenblas-dev libeigen3-dev -y\n\n!git clone https://github.com/ggerganov/llama.cpp\n!pip install -r llama.cpp/requirements.txt\n</pre> # @title Installing Llama.cpp !apt update -y !apt install build-essential git cmake libopenblas-dev libeigen3-dev -y  !git clone https://github.com/ggerganov/llama.cpp !pip install -r llama.cpp/requirements.txt In\u00a0[\u00a0]: Copied! <pre># @title Choose Quantisation Type. { display-mode: \"form\" }\n\n# @markdown ### Enter your model and and Huggingface account:\nMODEL_NAME = 'quantizeModelName'  # @param {type: \"string\"}\n\n# @markdown ### Choose Quantisation Formats:\nq2_k = False # @param {type:\"boolean\"}\nq3_k_l = False # @param {type:\"boolean\"}\nq3_k_m = False # @param {type:\"boolean\"}\nq3_k_s = False # @param {type:\"boolean\"}\nq4_0 = False # @param {type:\"boolean\"}\nq4_1 = False # @param {type:\"boolean\"}\nq4_k_m = True # @param {type:\"boolean\"}\nq4_k_s = False # @param {type:\"boolean\"}\nq5_0 = False # @param {type:\"boolean\"}\nq5_1 = False # @param {type:\"boolean\"}\nq5_k_m = True # @param {type:\"boolean\"}\nq5_k_s = False # @param {type:\"boolean\"}\nq6_k = False # @param {type:\"boolean\"}\nq8_0 = False # @param {type:\"boolean\"}\n\nimport os\n\n# Check if the directory exists\nif not os.path.exists(MODEL_NAME):\n    # If it doesn't exist, create it\n    os.mkdir(MODEL_NAME)\nelse:\n    print(f\"The directory {MODEL_NAME} already exists.\")\n</pre> # @title Choose Quantisation Type. { display-mode: \"form\" }  # @markdown ### Enter your model and and Huggingface account: MODEL_NAME = 'quantizeModelName'  # @param {type: \"string\"}  # @markdown ### Choose Quantisation Formats: q2_k = False # @param {type:\"boolean\"} q3_k_l = False # @param {type:\"boolean\"} q3_k_m = False # @param {type:\"boolean\"} q3_k_s = False # @param {type:\"boolean\"} q4_0 = False # @param {type:\"boolean\"} q4_1 = False # @param {type:\"boolean\"} q4_k_m = True # @param {type:\"boolean\"} q4_k_s = False # @param {type:\"boolean\"} q5_0 = False # @param {type:\"boolean\"} q5_1 = False # @param {type:\"boolean\"} q5_k_m = True # @param {type:\"boolean\"} q5_k_s = False # @param {type:\"boolean\"} q6_k = False # @param {type:\"boolean\"} q8_0 = False # @param {type:\"boolean\"}  import os  # Check if the directory exists if not os.path.exists(MODEL_NAME):     # If it doesn't exist, create it     os.mkdir(MODEL_NAME) else:     print(f\"The directory {MODEL_NAME} already exists.\")  In\u00a0[\u00a0]: Copied! <pre># @title Load in 16bit Precision\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n!python llama.cpp/convert.py {local_directory} --outtype f16 --outfile {fp16}\n</pre> # @title Load in 16bit Precision fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\" !python llama.cpp/convert.py {local_directory} --outtype f16 --outfile {fp16} In\u00a0[\u00a0]: Copied! <pre>!cd llama.cpp &amp;&amp; make LLAMA_OPENBLAS=1\n</pre> !cd llama.cpp &amp;&amp; make LLAMA_OPENBLAS=1 In\u00a0[\u00a0]: Copied! <pre># @title Start Quantisation\n\nQUANTIZATION_METHODS = [\n    (\"q2_k\", q2_k),\n    (\"q3_k_l\", q3_k_l),\n    (\"q3_k_m\", q3_k_m),\n    (\"q3_k_s\", q3_k_s),\n    (\"q4_0\", q4_0),\n    (\"q4_1\", q4_1),\n    (\"q4_k_m\", q4_k_m),\n    (\"q4_k_s\", q4_k_s),\n    (\"q5_0\", q5_0),\n    (\"q5_1\", q5_1),\n    (\"q5_k_m\", q5_k_m),\n    (\"q5_k_s\", q5_k_s),\n    (\"q6_k\", q6_k),\n    (\"q8_0\", q8_0),\n]\n\nfor method, flag in QUANTIZATION_METHODS:\n    if flag:\n        qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n        !./llama.cpp/quantize {fp16} {qtype} {method}\n</pre> # @title Start Quantisation  QUANTIZATION_METHODS = [     (\"q2_k\", q2_k),     (\"q3_k_l\", q3_k_l),     (\"q3_k_m\", q3_k_m),     (\"q3_k_s\", q3_k_s),     (\"q4_0\", q4_0),     (\"q4_1\", q4_1),     (\"q4_k_m\", q4_k_m),     (\"q4_k_s\", q4_k_s),     (\"q5_0\", q5_0),     (\"q5_1\", q5_1),     (\"q5_k_m\", q5_k_m),     (\"q5_k_s\", q5_k_s),     (\"q6_k\", q6_k),     (\"q8_0\", q8_0), ]  for method, flag in QUANTIZATION_METHODS:     if flag:         qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"         !./llama.cpp/quantize {fp16} {qtype} {method} In\u00a0[\u00a0]: Copied! <pre>import os\n\nmodel_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n\nprompt = input(\"Enter your prompt: \")\nchosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n\n# Verify the chosen method is in the list\nif chosen_method not in model_list:\n    print(\"Invalid name\")\nelse:\n    qtype = f\"{MODEL_NAME}/{chosen_method}\"\n    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\"\n</pre> import os  model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]  prompt = input(\"Enter your prompt: \") chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")  # Verify the chosen method is in the list if chosen_method not in model_list:     print(\"Invalid name\") else:     qtype = f\"{MODEL_NAME}/{chosen_method}\"     !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\" In\u00a0[\u00a0]: Copied! <pre>!pip install ctransformers&gt;=0.2.24\n</pre> !pip install ctransformers&gt;=0.2.24 In\u00a0[\u00a0]: Copied! <pre>from ctransformers import AutoModelForCausalLM\nimport os\n\nmodel_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n\nprompt = input(\"Enter your prompt: \")\nchosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n\n# Verify the chosen method is in the list\nif chosen_method not in model_list:\n    print(\"Invalid name\")\nelse:\n    qtype = f\"{MODEL_NAME}/{chosen_method}\"\n    llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=qtype, model_type=\"llama\", gpu_layers=0)\n\nfor text in llm(prompt, stream=True):\n    print(text, end=\"\", flush=True)\n</pre> from ctransformers import AutoModelForCausalLM import os  model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]  prompt = input(\"Enter your prompt: \") chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")  # Verify the chosen method is in the list if chosen_method not in model_list:     print(\"Invalid name\") else:     qtype = f\"{MODEL_NAME}/{chosen_method}\"     llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=qtype, model_type=\"llama\", gpu_layers=0)  for text in llm(prompt, stream=True):     print(text, end=\"\", flush=True)    In\u00a0[\u00a0]: Copied! <pre>username = \"username\"# @param {type:\"string\"}\nfrom huggingface_hub import create_repo, HfApi\n\napi = HfApi()\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n)\n</pre> username = \"username\"# @param {type:\"string\"} from huggingface_hub import create_repo, HfApi  api = HfApi()  # Create empty repo create_repo(     repo_id = f\"{username}/{MODEL_NAME}-GGUF\",     repo_type=\"model\",     exist_ok=True, )  # Upload gguf files api.upload_folder(     folder_path=MODEL_NAME,     repo_id=f\"{username}/{MODEL_NAME}-GGUF\",     allow_patterns=f\"*.gguf\", )   In\u00a0[\u00a0]: Copied! <pre># Upload Tokeniser and other files from the base model\napi.upload_folder(\n    folder_path=local_directory,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns = \"*|!*.bin|!*.safetensors\",\n    ignore_patterns=\"*.bin|*.safetensors\"\n)\n</pre> # Upload Tokeniser and other files from the base model api.upload_folder(     folder_path=local_directory,     repo_id=f\"{username}/{MODEL_NAME}-GGUF\",     allow_patterns = \"*|!*.bin|!*.safetensors\",     ignore_patterns=\"*.bin|*.safetensors\" )"},{"location":"Deployment/Quantization/GGUF_Quantization/#converting-huggingface-models-to-ggufggml","title":"Converting HuggingFace Models to GGUF/GGML\u00b6","text":"<p>This notebook is crafted for the purpose of quantizing Hugging Face models into GGUF format and subsequently uploading them to the Hub. Let's</p> <p>We will be usingLlama.cpp to quantize the model and it supports the following models:</p> <ul> <li>LLaMA \ud83e\udd99</li> <li>LLaMA 2 \ud83e\udd99\ud83e\udd99</li> <li>Falcon</li> <li>Alpaca</li> <li>GPT4All</li> <li>Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2</li> <li>Vigogne (French)</li> <li>Vicuna</li> <li>Koala</li> <li>OpenBuddy \ud83d\udc36 (Multilingual)</li> <li>Pygmalion 7B / Metharme 7B</li> <li>WizardLM</li> <li>Baichuan-7B and its derivations (such as baichuan-7b-sft)</li> <li>Aquila-7B / AquilaChat-7B</li> </ul> <p><code>This notebook can be run on a free Google Colab CPU/CPU machine</code>  On a CPU machine it took me 10 to 15 minutes to quantize a 7b model. On a GPU machine it took me 2 to 3 minutes to quantize a 7b model.</p>"},{"location":"Deployment/Quantization/GGUF_Quantization/#download-the-base-model-from-huggingface","title":"Download the base model from Huggingface\u00b6","text":"<p>Load the base model you want to quantise to GGUF Formate</p>"},{"location":"Deployment/Quantization/GGUF_Quantization/#quantisation","title":"Quantisation\u00b6","text":""},{"location":"Deployment/Quantization/GGUF_Quantization/#inference-using-llamacpp","title":"Inference using LLama.cpp\u00b6","text":""},{"location":"Deployment/Quantization/GGUF_Quantization/#inference-using-ctransformers","title":"Inference using ctransformers\u00b6","text":""},{"location":"Deployment/Quantization/GGUF_Quantization/#pushing-to-huggingface-hub","title":"Pushing to HuggingFace Hub\u00b6","text":""},{"location":"LLM/","title":"All about LLMs","text":""},{"location":"LLM/#large-language-models-llms","title":"Large Language Models (LLMs)","text":"<p>Welcome to the Large Language Models section of the AI Engineering Academy. This module provides a comprehensive understanding of LLMs and their practical applications in AI engineering.</p>"},{"location":"LLM/#repository-structure","title":"Repository Structure","text":"Category Topic Resource Introduction Overview Introduction to LLMs Theory Behind Fine-tuning Pre-Training Pre-Training Supervised Fine-Tuning (SFT) SFT Theory Proximal Policy Optimization (PPO) PPO Theory Direct Preference Optimization (DPO) DPO Theory Observation-Regularized Policy Optimization (ORPO) ORPO Theory Gated Regularized Policy Optimization (GRPO) GRPO Theory Hands-On SFT Overview SFT Implementation Guide Implementation SFT Notebook Hands-On GRPO Guide Hacker Guide to GRPO Implementation Qwen 0.5B GRPO Gemma Overview Gemma Guide Implementation Gemma Fine-tuning Llama2 Overview Llama2 Guide Implementation Llama2 Fine-tuning Advanced QLora Fine-tuning Llama3 Implementation Llama3 Fine-tuning Mistral-7B Overview Mistral Guide Implementation Mistral Fine-tuning Evaluation Evaluation Harness DPO DPO Fine-tuning SFT SFT Trainer Inference ChatML Inference Mixtral Implementation Mixtral Fine-tuning Visual Language Models Florence2 Florence2 Fine-tuning PaliGemma PaliGemma Fine-tuning Architecture Parameter Analysis Parameter Count"},{"location":"LLM/#learning-roadmap","title":"Learning Roadmap","text":"Level Steps Resources Beginner 1. Introduction to LLMs Introduction 2. Understanding core theory Pre-Training, SFT Theory 3. First implementation SFT Guide 4. Practical application Llama2 Fine-tuning Intermediate 1. Advanced techniques DPO Theory, PPO Theory 2. Model implementation Mistral Fine-tuning 3. Architecture concepts Parameter Count Advanced 1. Cutting-edge methods ORPO Theory, GRPO Theory 2. Advanced implementation GRPO Implementation 3. Multimodal models Florence2, PaliGemma"},{"location":"LLM/#contributing","title":"Contributing","text":"<p>We welcome contributions to expand this repository. Please follow the standard pull request process and ensure your contributions align with the overall structure.</p>"},{"location":"LLM/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p> <p>AI Engineering Academy - Advancing the frontier of language model understanding and implementation</p>"},{"location":"LLM/Axolotl/","title":"Index","text":""},{"location":"LLM/Axolotl/#introduction","title":"Introduction","text":"<p>Welcome to an immersive journey into the intricate art of fine-tuning AI models, where precision intertwines with customization. In this blog, we will delve into the profound significance of fine-tuning, unravel its complexities, and unveil a remarkable tool that streamlines this process - Axolotl.</p> <p>Get ready to master the art of fine-tuning an LLM (Large Language Model) using Axolotl. Throughout this tutorial, we'll navigate through the intricacies and adjustable elements of this transformative process.</p> <p>Axolotl predominantly operates through YAML files, requiring you to engage with and customize these files as part of the workflow.</p> <p>The Axolotl repository boasts comprehensive documentation, ensuring a swift grasp of its functionalities to kickstart your journey seamlessly.</p> <p>Join me as we unravel the journey from ideation to fine-tuning an LLM, leveraging Axolotl as our medium of choice for this transformative endeavor. Let's dive in!</p> <p>Introducing the Axolotl UI Editor</p> <p>Before we proceed further, I'd like to introduce the Axolotl UI Editor\u2014a powerful tool designed to facilitate the editing of Axolotl YAML files for fine-tuning. This intuitive editor is a handy creation from CognitiveLab, providing a seamless interface for customizing Axolotl configurations.</p> <p>Explore the Axolotl UI Editor here: Axolotl UI Editor</p> <p>!https://prod-files-secure.s3.us-west-2.amazonaws.com/97859362-02bd-4d74-abc1-b66ffcf4d0ad/6ecd3ef8-c16b-4e2f-bb05-58503b49e969/Untitled.png</p> <p>You can also find the source code for this editor on GitHub: Axolotl UI Editor - GitHub</p> <p>This user-friendly interface simplifies the process of tweaking Axolotl YAML files, making it accessible and efficient for fine-tuning tasks. Now, let's dive deeper into leveraging this tool for our fine-tuning journey.</p>"},{"location":"LLM/Axolotl/#what-is-fine-tuning","title":"What is Fine Tuning?","text":"<p>Before we delve into Axolotl, let's understand the significance of fine-tuning in the world of artificial intelligence. Fine-tuning is the process of taking a pre-trained model and adapting it to a specific task or dataset. It's akin to honing a skill - refining an already proficient model for a specialized purpose.</p> <p>Some examples of fine-tuning tasks can include:</p> <ul> <li>Fine-tuning for generating structured output (e.g., function calling).</li> <li>Fine-tuning to emulate someone's style or behavior.</li> </ul>"},{"location":"LLM/Axolotl/#introduction-to-axolotl","title":"Introduction to Axolotl","text":"<p>Axolotl, a versatile tool designed for AI model fine-tuning, emerges as a game-changer in the field. It supports various Hugging Face models, including llama, pythia, falcon, and mpt, empowering users with a multitude of configurations and architectures.</p>"},{"location":"LLM/Axolotl/#key-features","title":"Key Features:","text":"<ol> <li>Model Support: Easily train models such as llama, pythia, falcon, and mpt.</li> <li>Configurability: Effortlessly customize configurations using a simple YAML file or CLI overwrite.</li> <li>Dataset Flexibility: Load datasets in various formats, use custom formats, or bring your own tokenized datasets.</li> <li>Advanced Techniques: Benefit from integrated features like xformer, flash attention, rope scaling, and multipacking.</li> <li>Scalability: Run on a single GPU or multiple GPUs using FSDP or Deepspeed.</li> <li>Containerization: Seamlessly run with Docker, either locally or in the cloud.</li> <li>Logging and Checkpoints: Log results and optionally save checkpoints to wandb for comprehensive tracking.</li> </ol> <p>Axolotl provides a comprehensive suite of tools and capabilities, making AI model fine-tuning accessible, efficient, and adaptable to diverse use cases. Let's explore how to harness these features for optimizing and refining our models effectively.</p>"},{"location":"LLM/Axolotl/#axolotl-supports-the-following-models","title":"Axolotl supports the following models","text":"fp16/fp32 lora qlora gptq gptq w/flash attn flash attn xformers attn llama \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Mistral \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Mixtral-MoE \u2705 \u2705 \u2705 \u2753 \u2753 \u2753 \u2753 Pythia \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2753 cerebras \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2753 btlm \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2753 mpt \u2705 \u274c \u2753 \u274c \u274c \u274c \u2753 falcon \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2753 gpt-j \u2705 \u2705 \u2705 \u274c \u274c \u2753 \u2753 XGen \u2705 \u2753 \u2705 \u2753 \u2753 \u2753 \u2705 phi \u2705 \u2705 \u2705 \u2753 \u2753 \u2753 \u2753 RWKV \u2705 \u2753 \u2753 \u2753 \u2753 \u2753 \u2753 Qwen \u2705 \u2705 \u2705 \u2753 \u2753 \u2753 \u2753"},{"location":"LLM/Axolotl/#idea-1-youtube-cloner","title":"Idea 1: YouTube Cloner","text":"<p>Have you ever wondered if you could clone a YouTuber? I recently had this intriguing idea to explore whether a language model could mimic the style of popular YouTubers. The concept is simple yet fascinating: using fine-tuning techniques with LoRA, I aim to teach a language model how to emulate the tone, pacing, and content style of specific YouTube channels.</p>"},{"location":"LLM/Axolotl/#project-objective","title":"Project Objective","text":"<p>The aim of the YouTube Cloner project is to investigate the abilities of Language Models (LLMs) in mimicking the speaking style of popular YouTubers. Its main goal is to determine the effectiveness of LLMs in replicating the tone, pacing, and content style of specific YouTube channels through fine-tuning on selected datasets.</p> <p>Objectives</p> <ul> <li>Explore Emulation: Assess the ability of LLMs to mirror the unique tone and pacing of popular YouTube channels.</li> <li>Fine-Tuning Experiment: Implement fine-tuning methods with hand-picked datasets to train LLMs for channel-specific styles.</li> </ul> <p>Join me on this exciting journey as we unravel the potential of language models in mimicking the essence of favorite YouTube personalities. Let's dive into the details and see how we can bring this idea to life!</p>"},{"location":"LLM/Axolotl/#dataset-preparation","title":"Dataset Preparation","text":"<p>Before diving into the fine-tuning process, we need to prepare the dataset that will enable us to achieve our goal.</p> <p>If you're interested in recreating this project, I've open-sourced the web scraping and dataset curation code here. Feel free to star the repository if you find it helpful!</p> <p>I've also made the dataset available here on Hugging Face Datasets. You can explore the notebooks containing the code or directly use the dataset to replicate the steps I followed.</p> <p></p> <p>AdithyaSK/Fireship_transcript_summar_prompt \u00b7 Datasets at Hugging Face</p>"},{"location":"LLM/Axolotl/#tips-on-dataset-curation","title":"Tips on Dataset Curation","text":"<p>Dataset curation is a critical step often overlooked in many guides or tutorials, but I'll delve into the details behind building a good fine-tuning dataset.</p> <p>Firstly, you need to decide your input prompt and what you expect from the model. In the case of the YouTube Cloner, I aimed to use the video title and a brief summary as input to generate the video script.</p> <p>During dataset creation, my primary focus was to gather the YouTube video titles and transcripts and generate concise summaries. Here's how I accomplished this:</p> <ol> <li>Extracted links from a YouTube channel through web scraping.</li> <li>Downloaded audio, titles, and other relevant information.</li> <li>Transcribed the audio into text using a deep-seed audio-to-text API and generated summaries.</li> </ol> <p>Now, we have all the necessary components: titles, summaries, and video transcripts. Next, it's time to format the prompt.</p> <p>When fine-tuning base models like llama or mistal that are not instructionally fine-tuned, you can use your own prompt format. However, if the model is instructionally fine-tuned, you must follow the instruction format.</p> <p>Axolotl supports various dataset types like alpaca, llama, etc., but I generally prefer <code>completion</code> as it offers the most control over dataset formatting.</p> <p>Here is the prompt format I chose for this particular project:</p> <pre><code>&lt;s&gt;\n[INST]\nYou are a YouTuber called Fireship, creating engaging, high-intensity coding tutorials and tech news.\nYou cover a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video: {title}\nand a small summary: {video_summary}\n[/INST]\n\nGenerate the video: {video_transcipt}\n&lt;/s&gt;\n</code></pre> <p>Here, \"\" is the start-of-sentence token and \"\" is the end-of-sentence token. These are unique tokens.</p> <p>The \"\" token indicates to the generator when to stop generating.</p> <p>So, essentially, these are important aspects to consider while fine-tuning.</p> <p>Providing these as the input is crucial.</p> <pre><code>&lt;s&gt;[INST]\nYou are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news.\nyou covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video : Rust in 100 seconds\nand a small summary : A 100 second video on Rust not a code report\n[/INST]\n</code></pre> <p>With the title and summary,</p> <p>And you are making the LLM familiar with giving a response like this.</p> <pre><code>Generate the video : Rust. A fast and memory efficient language known for taking everything that's wrong with low level systems programming languages like C plus plus, c, and assembly, then making it worse by eliminating pointers entirely and providing an unpronounceable name that makes developers angry. It was created by Graydon Hoare inspired by ML functional languages and aimed at building safe reliable software while remaining extremely fast. In fact, its motto is secure concurrency without sacrificing speed, which sounds almost too good to be true. The main problem with unsafe multi threaded programming today is data races. Data races happen when multiple threads try to access the same mutable value at the exact same time, but there's no way to know which thread will win, making your programs unreliable. It can also cause hard to debug errors because you have race conditions where each one occurs randomly based on timing issues between different threads. There are many libraries out there designed to mitigate this issue or prevent it altogether. But, rust takes a very different approach by simply disallowing shared state, requiring all values, including variables and functions to either be immutable or only changed via a unique owner called a reference type or pointer. This means data races cannot occur automatically in a typical application, though they can still be introduced accidentally if someone tries to use the borrow checker to perform some fancy hackery. What that usually results in is clear error messages about double usage of references instead of cryptic seg faults and other run time errors, allowing you to solve problems earlier in development, and with less frustration. And it allows you to do so with patterns already proven to work reliably. For example, smart pointers eliminate the need for things like raw pointers and free store allocators found in C plus plus, ensuring proper garbage collection. Instead of using inheritance, interfaces, generics, traits provide object oriented features such as polymorphism in a statically typed way. As awesome as that sounds, learning curves don't come much steeper than rust, mostly because of its ownership system, which I would describe as more of a philosophy than anything else. If you want the full explanation, subscribe to my channel. Otherwise, just enjoy these 2 hello worlds from the same file. You might think the first line here declares a variable named hello with the string hello world assigned to it. However, you'd be wrong. That doesn't actually define a new variable. Rather, It defines a function with an explicit return type of a string literal. When used in conjunction with println, it prints the string literally twice. Or we could define a global variable with mut, which changes the meaning of the assignment operator to mutate existing memory. Now, let me read you something really scary. To get rid of pointers completely. We have references instead. These act exactly like the address of operators in other languages, except they implement safety checks through rust's highly sophisticated borrow checker. On top of that, you can clone objects into new locations, move values around, deep copy and shallow copy across types, weak references, arc, ref cell, interior, pin, once cell, and on and on. At this point, you should start seeing how rust got its name. If you wanna build a complex multi threaded system with performance requirements. Your best bet may well be learning this crazy language that seems so easy on the surface. This has been the rust programming language in 100 seconds. Hit the like button if you wanna see more short videos like this. Thanks for watching and I will see you in the next one.\n&lt;/s&gt;\n</code></pre> <p>so this will be the main dataset prep part</p> <p>After formatting the dataset, store all the formatted prompts in a field or column named <code>text</code>. This step is crucial before pushing it to Hugging Face.</p> <p>Share a dataset to the Hub</p>"},{"location":"LLM/Axolotl/#fine-tuning","title":"Fine-tuning","text":"<p>After you have uploaded the dataset, 90% of the work is complete. All that's left to do now is use Axolotl to fine-tune the model.</p> <p>Axolotl makes it really easy. All you have to do is determine which type of fine-tuning you want to do, be it Lora, FFT, or qLora.</p> <p>Here is a high-level abstraction to help you decide which one you want to choose:</p> <p>Lora - If you want to fine-tune the model to respond in a particular type - like JSON, like the above YouTube cloner example. Domain adaptation using Lora is tough. Either you have to have a very high rank value to change a lot of parameters, or you will have to overfit on the data which might degrade your model's performance.</p> <p>If you want to fine-tune a model to respond to you in a particular style, then choose Lora. Generally, I fine-tune most of my models using Lora.</p> <p>qLora - If you want to perform Lora but don't have enough compute, you can use qLora which lets you fine-tune models on a lower-end GPU like T4 with 16GB of VRAM, but the time for fine-tuning will increase.</p> <p>FFT - Full weight fine-tuning - in Axolotl you can FFT llama on an A100-80GB variant because it is well optimized. I generally go with FFT for domain or language adaptation, but FFT might decrease the original model's performance.</p> <p>Now, while performing Lora, there are some more things you have to note down.</p> <p>The Rank - R and alpha values.</p> <p>Here is a tweet I wrote about the considerations to make to pick an R and alpha value:</p> <p>https://publish.twitter.com/?url=https://twitter.com/adithya_s_k/status/1744065797268656579#</p> <p>Now that that's out of the way, let's get into fine-tuning the model.</p>"},{"location":"LLM/Axolotl/#prerequisites-optional","title":"Prerequisites (Optional)","text":"<p>There are two primary prerequisites: a server with an Ampere GPU, such as A100, and a functioning conda setup.</p> <p>For the A100 GPU, you can utilize any cloud platform such as Google Cloud, AWS, Lambda Labs, or E2E Networks. Your choice may depend on your region and budget.</p> <p>local install</p> <pre><code>git clone &lt;https://github.com/OpenAccess-AI-Collective/axolotl&gt;\ncd axolotl\n\npip3 install packaging\npip3 install -e '.[flash-attn,deepspeed]'\n</code></pre> <p>docker </p> <pre><code>  docker run --gpus '\"all\"' --rm -it winglian/axolotl:main-py3.10-cu118-2.0.1\n</code></pre> <p>assuming you have set up axolotl properly and have everything properly configured </p> <p>Pick a model you want to finetuned </p> <p>in this case i wanted to using the base mistral model </p> <p>you can see a folder called examples and you can see all the models axolotl supports and their finetuning scripts</p> <p>now lets create a copy of the qLora finetunign yml and call it lora.yml and make the necessary changes</p> <p></p> <p>as we will be using Lora and a different dataset and different rank</p> <p>here is the <code>yml</code> script to finetune the model </p> <pre><code>base_model: mistralai/Mistral-7B-v0.1 \nmodel_type: MistralForCausalLM\ntokenizer_type: LlamaTokenizer\nis_mistral_derived_model: true\n\nload_in_8bit: false\n# change load_in_4bit to false as we will be doing lora finetuning\nload_in_4bit: false # change from true to false\nstrict: false\n\ndatasets:\n  - path: AdithyaSK/Fireship_transcript_summar_prompt # change the dataset to the dataset you have pushed on HF\n    type: completion #change it from alpace to completion\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.1\noutput_dir: ./lora-out # change the directory name - it will the folder in which the final model will be stored\n\nadapter: lora\nlora_model_dir:\n\nsequence_len: 8192\nsample_packing: true\npad_to_sequence_len: true\n\nlora_r: 64 # lets change the rank from 32 to 64 to change more parameters in the model\nlora_alpha: 32 # lets change alpha from 16 to 31 general rule of thumb is r = 2*aplha\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules: \n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: # if you want to track the models \nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4 # gradient accumulation can be increase to 8 but lets it keep it at 4. if you are getting cuda_out_of_memoery you can reduce this number\nmicro_batch_size: 2 \nnum_epochs: 5 # increase the number of epochs to 5. 1 epoch = the model has seen all the data once\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nloss_watchdog_threshold: 5.0\nloss_watchdog_patience: 3\n\nwarmup_steps: 10\nevals_per_epoch: 4\neval_table_size:\neval_max_new_tokens: 128\nsaves_per_epoch: 4 # change it from 1 to 4 to to make multiple save during the Epoch\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens: # very important \n  bos_token: \"&lt;s&gt;\" # these tokens will be added at the first of every prompt\n  eos_token: \"&lt;/s&gt;\" # these tokens will be added at the end of every prompt\n  unk_token: \"&lt;unk&gt;\"\n</code></pre> <p>after you have made the necessary changes to the finetuning script</p> <p>to run the finetuning </p> <pre><code>accelerate launch -m axolotl.cli.train examples/mistral/lora.yml\n</code></pre> <p>to run inference on the model and test out the model </p> <pre><code>python -m axolotl.cli.inference examples/mistral/lora.yml --lora_model_dir=\"./lora-out\"\n</code></pre> <p>to merge the model</p> <pre><code>python3 -m axolotl.cli.merge_lora examples/mistral/lora.yml --lora_model_dir=\"./lora-out\"\n</code></pre> <p>push the model to huggingface </p> <pre><code>pip install -U \"huggingface_hub[cli]\"\nhuggingface-cli upload {name of model} ./{output directory} --repo-type model\n</code></pre>"},{"location":"LLM/Axolotl/#conclusion","title":"Conclusion","text":"<p>In this blog post, we've explored the exciting world of fine-tuning AI models using Axolotl, a powerful tool that enhances the customization and precision of language models. We've discussed the importance of dataset preparation, model selection, and configuration settings for successful fine-tuning projects.</p> <p>By leveraging Axolotl's capabilities, we've embarked on intriguing projects like the YouTube Cloner, aiming to emulate the speaking style of popular YouTubers using language models. Throughout this journey, we've emphasized the creative potential and experimental nature of fine-tuning, showcasing how AI can adapt to specific tasks and datasets.</p> <p>Now equipped with the necessary knowledge and tools, you're ready to embark on your own fine-tuning adventures. Whether it's replicating existing projects or exploring new use cases, Axolotl offers a robust framework to streamline and optimize the fine-tuning process.</p>"},{"location":"LLM/Axolotl/#closing-thoughts","title":"Closing Thoughts","text":"<p>As we conclude this exploration into fine-tuning using Axolotl, it's evident that the tool not only simplifies the process but also enhances the capabilities of AI practitioners. The ability to fine-tune models with precision, coupled with the flexibility offered by Axolotl, opens doors to a new era of AI customization.</p> <p>Embark on your journey with Axolotl, where every fine-tuning endeavor transforms into a seamless and efficient experience. Stay tuned for more insights and updates as we navigate the evolving landscape of AI refinement.</p> <p>If you found this post valuable, make sure to follow me for more insightful content. I frequently write about the practical applications of Generative AI, LLMs, Stable Diffusion, and explore the broader impacts of AI on society. </p> <p>Let's stay connected on Twitter. I'd love to engage in discussions with you. </p> <p>If you're not a Medium member yet and wish to support writers like me, consider signing up through my referral link: Medium Membership. Your support is greatly appreciated! </p>"},{"location":"LLM/Axolotl/#resources","title":"Resources","text":"<p>Fine-tuning Llama 2 with axolotl</p> <p>A Beginner\u2019s Guide to LLM Fine-Tuning</p> <p>Finetuning LLMs For Text-to-Task</p> <p>A poor man's guide to fine-tuning Llama 2</p> <p>Fine Tuning Llama Models With Qlora and Axolotl | ANIMAL-MACHINE</p> <p>https://github.com/OpenAccess-AI-Collective/axolotl</p>"},{"location":"LLM/Gemma/","title":"A Beginner\u2019s Guide to Fine-Tuning Gemma - Adithya S K - Medium","text":"<p>URL Source: https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-gemma-0444d46d821c</p> <p>Published Time: 2024-02-21T20:13:11.688Z</p> <p>Markdown Content: </p> <p>A Comprehensive Guide to Fine-Tuning Gemma</p> <p>Fine-tuning a state-of-the-art language model like Gemma can be an exciting journey. This guide will walk you through the process step by step, from setting up your environment to fine-tuning the model for your specific task. Whether you\u2019re a seasoned machine learning practitioner or a newcomer to the field, this beginner-friendly tutorial will help you harness the power of Gemma for your projects.</p>"},{"location":"LLM/Gemma/#meet-gemma","title":"Meet Gemma","text":"<p>a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.</p>"},{"location":"LLM/Gemma/#colab-notebook-to-finetuning","title":"Colab Notebook to Finetuning","text":""},{"location":"LLM/Gemma/#github-repository","title":"Github Repository","text":""},{"location":"LLM/Gemma/#prerequisites","title":"Prerequisites","text":"<p>Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:</p> <p>1. GPU: gemma-2b \u2014 can be finetuned on T4(free google colab) while gemma-7b requires an A100 GPU.</p> <p>2. Python Packages: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:</p> <p>Let\u2019s begin by checking if your GPU is correctly detected:</p> <p>!pip3 install -q -U bitsandbytes==0.42.0 !pip3 install -q -U peft==0.8.2 !pip3 install -q -U trl==0.7.10 !pip3 install -q -U accelerate==0.27.1 !pip3 install -q -U datasets==2.17.0 !pip3 install -q -U transformers==4.38.0</p> <p>Hugging Face Hub Account: You\u2019ll need an account on the Hugging Face Model Hub. You can sign up here.</p>"},{"location":"LLM/Gemma/#getting-started","title":"Getting Started","text":""},{"location":"LLM/Gemma/#checking-gpu","title":"Checking GPU","text":"<p>Let\u2019s start by checking if your GPU is correctly detected:</p> <p>!nvidia-smi</p> <p>If your GPU is not recognized or you encounter CUDA out-of-memory errors during fine-tuning, consider using a more powerful GPU.</p>"},{"location":"LLM/Gemma/#loading-required-libraries","title":"Loading Required Libraries","text":"<p>We\u2019ll load the necessary Python libraries for our fine-tuning process:</p> <p>import json import pandas as pd import torch from datasets import Dataset, load_dataset from huggingface_hub import notebook_login from peft import LoraConfig, PeftModel from transformers import (     AutoModelForCausalLM,     AutoTokenizer,     BitsAndBytesConfig,     TrainingArguments,     pipeline,     logging, ) from trl import SFTTrainer</p>"},{"location":"LLM/Gemma/#logging-into-hugging-face-hub","title":"Logging into Hugging Face Hub","text":"<p>Log in to the Hugging Face Model Hub using your credentials:</p> <p>notebook_login()</p>"},{"location":"LLM/Gemma/#loading-the-model","title":"Loading the Model","text":"<p>model_id = \"google/gemma-7b-it\" # model_id = \"google/gemma-7b\" # model_id = \"google/gemma-2b-it\" # model_id = \"google/gemma-2b\"model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)</p>"},{"location":"LLM/Gemma/#loading-the-dataset","title":"Loading the Dataset","text":"<p>For this tutorial, we will fine-tune Gemma Instruct for code generation.</p> <p>we will be using this dataset which is curated by TokenBender (e/xperiments) which is a awesome data for finetuning model for code generation. It follows the alpaca style of instructions which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <p>{     \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",     \"input\":\"[1, 2, 3, 4, 5]\",     \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\" }</p> <p>now lets load the dataset using huggingfaces datasets library</p> <p># Load your dataset (replace 'your_dataset_name' and 'split_name' with your actual dataset information) # dataset = load_dataset(\"your_dataset_name\", split=\"split_name\") dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")</p>"},{"location":"LLM/Gemma/#formatting-the-dataset","title":"Formatting the Dataset","text":"<p>Now, let\u2019s format the dataset in the required gemma instruction formate.</p> <p>Many tutorials and blogs skip over this part, but I feel this is a really important step.</p> <p>&lt;start_of_turn&gt;user What is your favorite condiment? &lt;end_of_turn&gt; &lt;start_of_turn&gt;model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;end_of_turn&gt;</p> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p> <p>def generate_prompt(data_point):     \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer:param data_point: dict: Data point     :return: dict: tokenzed prompt     \"\"\"     prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\\                'appropriately completes the request.\\\\n\\\\n'     # Samples with additional context into.     if data_point['input']:         text = f\"\"\"&lt;start_of_turn&gt;user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} &lt;end_of_turn&gt;\\\\n&lt;start_of_turn&gt;model{data_point[\"output\"]} &lt;end_of_turn&gt;\"\"\"     # Without     else:         text = f\"\"\"&lt;start_of_turn&gt;user {prefix_text} {data_point[\"instruction\"]} &lt;end_of_turn&gt;\\\\n&lt;start_of_turn&gt;model{data_point[\"output\"]} &lt;end_of_turn&gt;\"\"\"     return text</p> <p># add the \"prompt\" column in the dataset text_column = [generate_prompt(data_point) for data_point in dataset] dataset = dataset.add_column(\"prompt\", text_column)</p> <p>We'll need to tokenize our data so the model can understand.</p> <p>dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)</p> <p>Split dataset into 90% for training and 10% for testing</p> <p>dataset = dataset.train_test_split(test_size=0.2) train_data = dataset[\"train\"] test_data = dataset[\"test\"]</p>"},{"location":"LLM/Gemma/#after-formatting-we-should-get-something-like-this","title":"After Formatting, We should get something like this","text":"<p>{ \"text\":\"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt; &lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;\", \"instruction\":\"Create a function to calculate the sum of a sequence of integers\", \"input\":\"[1, 2, 3, 4, 5]\", \"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,  sequence: sum += num return sum\", \"prompt\":\"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt; &lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;\" }  </p> <p>While using SFT (Supervised Fine-tuning Trainer) for fine-tuning, we will be only passing in the \u201ctext\u201d column of the dataset for fine-tuning.</p>"},{"location":"LLM/Gemma/#setting-model-parameters-and-lora","title":"Setting Model Parameters and Lora","text":"<p>We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:</p>"},{"location":"LLM/Gemma/#apply-lora","title":"Apply Lora","text":"<p>Here comes the magic with peft! Let\u2019s load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and the prepare_model_for_kbit_training method from PEFT.</p> <p>Here is a tweet on how to pick the best Lora config</p> <p>from peft import LoraConfig, get_peft_model lora_config = LoraConfig(     r=64,     lora_alpha=32,     target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'],     lora_dropout=0.05,     bias=\"none\",     task_type=\"CAUSAL_LM\" )  model = get_peft_model(model, lora_config)</p> <p>Calculating the number of trainable parameters</p> <p>trainable, total = model.get_nb_trainable_parameters() print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")</p> <p>expected output \u2192 Trainable: 200015872 | total: 8737696768 | Percentage: 2.2891%</p>"},{"location":"LLM/Gemma/#fine-tuning-with-qlora-and-supervised-finetuning","title":"Fine-Tuning with qLora and Supervised Finetuning","text":"<p>We\u2019re ready to fine-tune our model using qLora. For this tutorial, we\u2019ll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p>"},{"location":"LLM/Gemma/#new-code-using-sfttrainer","title":"new code using SFTTrainer","text":"<p>import transformersfrom trl import SFTTrainer</p> <p>tokenizer.pad_token = tokenizer.eos_token torch.cuda.empty_cache() trainer = SFTTrainer(     model=model,     train_dataset=train_data,     eval_dataset=test_data,     dataset_text_field=\"prompt\",     peft_config=lora_config,     args=transformers.TrainingArguments(         per_device_train_batch_size=1,         gradient_accumulation_steps=4,         warmup_steps=0.03,         max_steps=100,         learning_rate=2e-4,         logging_steps=1,         output_dir=\"outputs\",         optim=\"paged_adamw_8bit\",         save_strategy=\"epoch\",     ),     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), )</p>"},{"location":"LLM/Gemma/#lets-start-the-training-process","title":"Lets start the training process","text":"<p># Start the training process trainer.train()new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub # Save the fine-tuned model trainer.model.save_pretrained(new_model)</p>"},{"location":"LLM/Gemma/#merge-and-share","title":"Merge and Share","text":"<p>After fine-tuning, if you want to merge the model with LoRA weights or share it with the Hugging Face Model Hub, you can do so. This step is optional and depends on your specific use case.</p> <p># Merge the model with LoRA weights base_model = AutoModelForCausalLM.from_pretrained(     model_id,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= merged_model.merge_and_unload()# Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\") tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\"</p> <p># Push the model and tokenizer to the Hugging Face Model Hub merged_model.push_to_hub(new_model, use_temp_dir=False) tokenizer.push_to_hub(new_model, use_temp_dir=False)</p>"},{"location":"LLM/Gemma/#test-the-merged-model","title":"Test the merged model","text":"<p>def get_completion(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"   prompt_template = \"\"\"   &lt;start_of_turn&gt;user   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   &lt;end_of_turn&gt;\\\\n&lt;start_of_turn&gt;model\"\"\"   prompt = prompt_template.format(query=query)   encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)   model_inputs = encodeds.to(device)   generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   # decoded = tokenizer.batch_decode(generated_ids)   decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)   return (decoded)</p> <p>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer) print(result)</p> <p>And that\u2019s it! You\u2019ve successfully fine-tuned Gemma Instruct for code generation. You can adapt this process for various natural language understanding and generation tasks. Keep exploring and experimenting with Gemma to unlock its full potential for your projects.</p> <p>Happy Fine-Tuning!!</p> <p>If you found this post valuable, make sure to follow me for more insightful content. I frequently write about the practical applications of Generative AI, LLMs, Stable Diffusion, and explore the broader impacts of AI on society.</p> <p>Let\u2019s stay connected on Twitter. I\u2019d love to engage in discussions with you.</p> <p>If you\u2019re not a Medium member yet and wish to support writers like me, consider signing up through my referral link: Medium Membership. Your support is greatly appreciated!</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/","title":"Finetune Gemma","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Wed Feb 21 17:19:05 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n| N/A   33C    P0              42W / 300W |      4MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[2]: Copied! <pre>!pip3 install -q -U bitsandbytes==0.42.0\n!pip3 install -q -U peft==0.8.2\n!pip3 install -q -U trl==0.7.10\n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q -U transformers==4.38.0\n</pre> !pip3 install -q -U bitsandbytes==0.42.0 !pip3 install -q -U peft==0.8.2 !pip3 install -q -U trl==0.7.10 !pip3 install -q -U accelerate==0.27.1 !pip3 install -q -U datasets==2.17.0 !pip3 install -q -U transformers==4.38.0 In\u00a0[4]: Copied! <pre># if you are using google colab\n\n# import os\n# from google.colab import userdata\n# os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n</pre> # if you are using google colab  # import os # from google.colab import userdata # os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN') In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() <p>Now we specify the model ID and then we load it with our previously defined quantization configuration.Now we specify the model ID and then we load it with our previously defined quantization configuration.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"google/gemma-7b-it\"\n# model_id = \"google/gemma-7b\"\n# model_id = \"google/gemma-2b-it\"\n# model_id = \"google/gemma-2b\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  model_id = \"google/gemma-7b-it\" # model_id = \"google/gemma-7b\" # model_id = \"google/gemma-2b-it\" # model_id = \"google/gemma-2b\"  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_use_double_quant=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.bfloat16 )  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True) In\u00a0[10]: Copied! <pre>def get_completion(query: str, model, tokenizer) -&gt; str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  &lt;start_of_turn&gt;user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model\n  \n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  # decoded = tokenizer.batch_decode(generated_ids)\n  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n  return (decoded)\n</pre> def get_completion(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"    prompt_template = \"\"\"   user   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   \\nmodel       \"\"\"   prompt = prompt_template.format(query=query)    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)    model_inputs = encodeds.to(device)     generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   # decoded = tokenizer.batch_decode(generated_ids)   decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)   return (decoded) In\u00a0[11]: Copied! <pre>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer) print(result) <pre>A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n</pre> <pre>\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  code the fibonacci series in python using reccursion\n  \nmodel\n  \n\n   a Python function to calculate the nth Fibonacci number using recursion. Here's the code: \n\n```python\n\ndef fibonacci(n):\n  if n == 0:\n    return 0\n  elif n == 1:\n    return 1\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# Print the nth Fibonacci number\nprint(fibonacci(10))\n```\n\n**Explanation:**\n\n1. The function `fibonacci` takes an integer `n` as input.\n2. If `n` is 0 or 1, it returns the respective base case of 0 or 1.\n3. Otherwise, it recursively calculates the Fibonacci number for `n-1` and `n-2` and adds their sum to return the Fibonacci number for `n`.\n4. The function continues to recurse until `n` reaches the desired number, and the final result is returned.\n\n**Output:**\n\n```\n&gt;&gt;&gt; print(fibonacci(10))\n5\n```\n\nIn this example, the code calculates the 10th Fibonacci number, which is 5.\n\n**Note:** Reccursion can be a powerful technique for solving problems that involve repeated calculations. However, it is important to note that recursion can also lead to stack overflow errors for large values of `n` due to its repeated function calls. For more efficient solutions, iterative approaches are often preferred.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\ndataset\n</pre> from datasets import load_dataset  dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\") dataset In\u00a0[15]: Copied! <pre>df = dataset.to_pandas()\ndf.head(10)\n</pre> df = dataset.to_pandas() df.head(10) Out[15]: input output text instruction 0 [1, 2, 3, 4, 5] # Python code\\ndef sum_sequence(sequence):\\n  ... Below is an instruction that describes a task.... Create a function to calculate the sum of a se... 1 str1 = \"Hello \"\\nstr2 = \"world\" def add_strings(str1, str2):\\n    \"\"\"This func... Below is an instruction that describes a task.... Develop a function that will add two strings 2 #include &lt;map&gt;\\n#include &lt;string&gt;\\n\\nclass Gro... Below is an instruction that describes a task.... Design a data structure in C++ to store inform... 3 [3, 1, 4, 5, 9, 0] def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ... Below is an instruction that describes a task.... Implement a sorting algorithm to sort a given ... 4 Not applicable import UIKit\\n\\nclass ExpenseViewController: U... Below is an instruction that describes a task.... Design a Swift application for tracking expens... 5 Not Applicable &lt;?php\\n$timestamp = $_GET['timestamp'];\\n\\nif(... Below is an instruction that describes a task.... Create a REST API to convert a UNIX timestamp ... 6 website: www.example.com \\ndata to crawl: phon... import requests\\nimport re\\n\\ndef crawl_websit... Below is an instruction that describes a task.... Generate a Python code for crawling a website ... 7 [x*x for x in [1, 2, 3, 5, 8, 13]] Below is an instruction that describes a task.... Create a Python list comprehension to get the ... 8 SELECT * FROM products ORDER BY price DESC LIM... Below is an instruction that describes a task.... Create a MySQL query to find the most expensiv... 9 Not applicable public class Library {\\n \\n // map of books in... Below is an instruction that describes a task.... Create a data structure in Java for storing an... <p>Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :</p> <ol> <li>the function generate_prompt : take the instruction and output and generate a prompt</li> <li>shuffle the dataset</li> <li>tokenizer the dataset</li> </ol> In\u00a0[16]: Copied! <pre>def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"&lt;start_of_turn&gt;user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point[\"output\"]} &lt;end_of_turn&gt;\"\"\"\n    # Without\n    else:\n        text = f\"\"\"&lt;start_of_turn&gt;user {prefix_text} {data_point[\"instruction\"]} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point[\"output\"]} &lt;end_of_turn&gt;\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\n</pre> def generate_prompt(data_point):     \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer      :param data_point: dict: Data point     :return: dict: tokenzed prompt     \"\"\"     prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\                'appropriately completes the request.\\n\\n'     # Samples with additional context into.     if data_point['input']:         text = f\"\"\"user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} \\nmodel{data_point[\"output\"]} \"\"\"     # Without     else:         text = f\"\"\"user {prefix_text} {data_point[\"instruction\"]} \\nmodel{data_point[\"output\"]} \"\"\"     return text  # add the \"prompt\" column in the dataset text_column = [generate_prompt(data_point) for data_point in dataset] dataset = dataset.add_column(\"prompt\", text_column) <p>We'll need to tokenize our data so the model can understand.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n</pre> dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True) <p>Split dataset into 90% for training and 10% for testing</p> In\u00a0[18]: Copied! <pre>dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n</pre> dataset = dataset.train_test_split(test_size=0.2) train_data = dataset[\"train\"] test_data = dataset[\"test\"] In\u00a0[19]: Copied! <pre>print(test_data)\n</pre> print(test_data) <pre>Dataset({\n    features: ['input', 'output', 'text', 'instruction', 'prompt', 'input_ids', 'attention_mask'],\n    num_rows: 24392\n})\n</pre> In\u00a0[20]: Copied! <pre>from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n</pre> from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model model.gradient_checkpointing_enable() model = prepare_model_for_kbit_training(model) In\u00a0[21]: Copied! <pre>print(model)\n</pre> print(model) <pre>GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n    (layers): ModuleList(\n      (0-27): 28 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n)\n</pre> In\u00a0[22]: Copied! <pre>import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)\n</pre> import bitsandbytes as bnb def find_all_linear_names(model):   cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)   lora_module_names = set()   for name, module in model.named_modules():     if isinstance(module, cls):       names = name.split('.')       lora_module_names.add(names[0] if len(names) == 1 else names[-1])     if 'lm_head' in lora_module_names: # needed for 16-bit       lora_module_names.remove('lm_head')   return list(lora_module_names) In\u00a0[23]: Copied! <pre>modules = find_all_linear_names(model)\nprint(modules)\n</pre> modules = find_all_linear_names(model) print(modules) <pre>['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj']\n</pre> In\u00a0[26]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n</pre> from peft import LoraConfig, get_peft_model  lora_config = LoraConfig(     r=64,     lora_alpha=32,     target_modules=modules,     lora_dropout=0.05,     bias=\"none\",     task_type=\"CAUSAL_LM\" )  model = get_peft_model(model, lora_config) In\u00a0[27]: Copied! <pre>trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n</pre> trainable, total = model.get_nb_trainable_parameters() print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\") <pre>Trainable: 200015872 | total: 8737696768 | Percentage: 2.2891%\n</pre> <p>Setting the training arguments:</p> <ul> <li>for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># import transformers\n\n# tokenizer.pad_token = tokenizer.eos_token\n\n\n# trainer = transformers.Trainer(\n#     model=model,\n#     train_dataset=train_data,\n#     eval_dataset=test_data,\n#     args=transformers.TrainingArguments(\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_steps=0.03,\n#         max_steps=100,\n#         learning_rate=2e-4,\n#         fp16=True,\n#         logging_steps=1,\n#         output_dir=\"outputs_mistral_b_finance_finetuned_test\",\n#         optim=\"paged_adamw_8bit\",\n#         save_strategy=\"epoch\",\n#     ),\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n# )\n</pre> # import transformers  # tokenizer.pad_token = tokenizer.eos_token   # trainer = transformers.Trainer( #     model=model, #     train_dataset=train_data, #     eval_dataset=test_data, #     args=transformers.TrainingArguments( #         per_device_train_batch_size=1, #         gradient_accumulation_steps=4, #         warmup_steps=0.03, #         max_steps=100, #         learning_rate=2e-4, #         fp16=True, #         logging_steps=1, #         output_dir=\"outputs_mistral_b_finance_finetuned_test\", #         optim=\"paged_adamw_8bit\", #         save_strategy=\"epoch\", #     ), #     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # )  In\u00a0[\u00a0]: Copied! <pre>#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0.03,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n</pre> #new code using SFTTrainer import transformers  from trl import SFTTrainer  tokenizer.pad_token = tokenizer.eos_token torch.cuda.empty_cache()  trainer = SFTTrainer(     model=model,     train_dataset=train_data,     eval_dataset=test_data,     dataset_text_field=\"prompt\",     peft_config=lora_config,     args=transformers.TrainingArguments(         per_device_train_batch_size=1,         gradient_accumulation_steps=4,         warmup_steps=0.03,         max_steps=100,         learning_rate=2e-4,         logging_steps=1,         output_dir=\"outputs\",         optim=\"paged_adamw_8bit\",         save_strategy=\"epoch\",     ),     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), ) In\u00a0[29]: Copied! <pre>model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n</pre> model.config.use_cache = False  # silence the warnings. Please re-enable for inference! trainer.train() <pre>/home/adithya/miniconda3/envs/gemma-venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n</pre>        [100/100 03:06, Epoch 0/1]      Step Training Loss 1 10.299600 2 6.640600 3 7.763200 4 4.142200 5 3.619800 6 4.840600 7 2.886800 8 1.334500 9 1.177300 10 1.343300 11 1.306000 12 1.122000 13 1.337300 14 1.083400 15 1.399100 16 1.172800 17 0.838900 18 1.027200 19 0.847500 20 1.065700 21 0.944100 22 0.782900 23 1.030100 24 0.911900 25 0.868400 26 0.768300 27 0.845200 28 1.036400 29 0.766000 30 0.741500 31 0.738800 32 0.779200 33 0.833000 34 0.851800 35 0.824800 36 0.757800 37 0.804500 38 0.962900 39 0.716000 40 1.027000 41 0.743100 42 0.932800 43 0.623100 44 0.671600 45 0.807100 46 0.957200 47 0.643900 48 0.867800 49 0.810700 50 0.871100 51 0.628500 52 0.946400 53 0.918600 54 0.920300 55 0.746100 56 0.914800 57 0.705700 58 0.883300 59 1.016300 60 0.583200 61 0.872000 62 0.617600 63 0.858700 64 0.955700 65 0.854500 66 0.778000 67 0.733200 68 0.871200 69 0.847700 70 0.567400 71 1.078200 72 0.945800 73 0.762500 74 0.618800 75 0.803600 76 0.848400 77 0.504300 78 0.685300 79 0.700700 80 0.636400 81 0.832700 82 0.614600 83 0.899000 84 0.623800 85 0.637200 86 0.697500 87 0.770200 88 0.800000 89 0.748600 90 0.897100 91 0.718800 92 0.703900 93 0.635600 94 0.649800 95 0.627200 96 0.785200 97 0.808200 98 0.720500 99 0.656600 100 0.650900 <p> </p> Out[29]: <pre>TrainOutput(global_step=100, training_loss=1.1844707012176514, metrics={'train_runtime': 188.6612, 'train_samples_per_second': 2.12, 'train_steps_per_second': 0.53, 'total_flos': 3947993787666432.0, 'train_loss': 1.1844707012176514, 'epoch': 0.0})</pre> <p>Share adapters on the \ud83e\udd17 Hub</p> In\u00a0[30]: Copied! <pre>new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub\n</pre> new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub In\u00a0[31]: Copied! <pre>trainer.model.save_pretrained(new_model)\n</pre> trainer.model.save_pretrained(new_model) In\u00a0[\u00a0]: Copied! <pre>base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</pre> base_model = AutoModelForCausalLM.from_pretrained(     model_id,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= merged_model.merge_and_unload()  # Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\") tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\" In\u00a0[\u00a0]: Copied! <pre># Push the model and tokenizer to the Hugging Face Model Hub\nmerged_model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n</pre> # Push the model and tokenizer to the Hugging Face Model Hub merged_model.push_to_hub(new_model, use_temp_dir=False) tokenizer.push_to_hub(new_model, use_temp_dir=False) In\u00a0[34]: Copied! <pre>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer) print(result) <pre>A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n</pre> <pre>\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  code the fibonacci series in python using reccursion\n  \nmodel\n  \n\n   a Python program to code the Fibonacci series using recursion. Here's a solution:\n\n```python\ndef fibonacci(n):\n  \"\"\"Calculates the nth Fibonacci number using recursion.\n\n  Args:\n    n: The index of the Fibonacci number to calculate.\n\n  Returns:\n    The nth Fibonacci number.\n  \"\"\"\n\n  # Base case: The first two Fibonacci numbers are 0 and 1.\n  if n &lt;= 1:\n    return n\n\n  # Recursive case: Otherwise, calculate the nth Fibonacci number by adding the previous two numbers.\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)\n\n\n# Print the Fibonacci numbers.\nfor i in range(10):\n  print(fibonacci(i))\n```\n\n**Explanation:**\n\n* The `fibonacci` function takes an integer `n` as input.\n* If `n` is less than or equal to 1, it returns `n` itself, as the base case.\n* Otherwise, it calculates `n`-th Fibonacci number recursively by adding the previous two numbers.\n* The function calls itself with smaller and smaller values of `n` until it reaches the base case.\n\n**Example Usage:**\n\n```python\n# Print the Fibonacci numbers from 0 to 10.\nfor i in range(10):\n  print(fibonacci(i))\n```\n\n**Output:**\n\n```\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\n```\n\n**Note:**\n\n* This program calculates Fibonacci numbers recursively, which means that it may not be very efficient for large values of `n` as it repeats calculations.\n* For more efficient Fibonacci number calculation, consider using an iterative approach rather than recursion.\n</pre>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#instruct-fine-tuning-gemma-using-qlora-and-supervise-finetuning","title":"Instruct Fine-tuning Gemma using qLora and Supervise Finetuning\u00b6","text":"<p>This is a comprahensive notebook and tutorial on how to fine tune the <code>gemma-7b-it</code> Model</p> <p>All the code will be available on my Github. Do drop by and give a follow and a star. adithya-s-k Github Code</p> <p>I also post content about LLMs and what I have been working on Twitter. AdithyaSK (@adithya_s_k) / X</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:</p> <ol> <li>GPU: gemma-2b - can be finetuned on T4(free google colab) while gemma-7b requires an A100 GPU.</li> <li>Python Packages: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:</li> </ol> <p>Let's begin by checking if your GPU is correctly detected:</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#step-2-model-loading","title":"Step 2 - Model loading\u00b6","text":"<p>We'll load the model using QLoRA quantization to reduce the usage of memory</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#promptchat-templates","title":"Prompt/Chat templates\u00b6","text":"<p>Gemma chat template</p> <pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user\nWrite a hello world program&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n</code></pre> <p>As you can see, each turn is preceded by a &lt;start_of_turn&gt; delimiter and then the role of the entity (either user, for content supplied by the user, or model for LLM responses). Turns finish with the &lt;end_of_turn&gt; token.</p> <p>Some other prompt templates - Github , Medium</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#step-3-load-dataset-for-finetuning","title":"Step 3 - Load dataset for finetuning\u00b6","text":""},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#lets-load-the-dataset","title":"Lets Load the Dataset\u00b6","text":"<p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>We will be using this dataset which is curated by TokenBender (e/xperiments) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <pre>{\n  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n  \"input\": \"[1, 2, 3, 4, 5]\",\n  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n}\n</pre>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#formatting-the-dataset","title":"Formatting the Dataset\u00b6","text":"<p>Now, let's format the dataset in the required gemma instruction formate.</p> <p>Many tutorials and blogs skip over this part, but I feel this is a really important step.</p> <pre><code>&lt;start_of_turn&gt;user What is your favorite condiment? &lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;end_of_turn&gt;\n</code></pre> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#after-formatting-we-should-get-something-like-this","title":"After Formatting, We should get something like this\u00b6","text":"<pre>{\n\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n\"input\":\"[1, 2, 3, 4, 5]\",\n\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n sequence: sum += num return sum\",\n\"prompt\":\"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;\"\n\n}\n</pre> <p>While using SFT (Supervised Fine-tuning Trainer) for fine-tuning, we will be only passing in the \u201ctext\u201d column of the dataset for fine-tuning.</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#step-4-apply-lora","title":"Step 4 - Apply Lora\u00b6","text":"<p>Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#step-5-run-the-training","title":"Step 5 - Run the training!\u00b6","text":""},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#fine-tuning-with-qlora-and-supervised-fine-tuning","title":"Fine-Tuning with qLora and Supervised Fine-Tuning\u00b6","text":"<p>We're ready to fine-tune our model using qLora. For this tutorial, we'll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p>"},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#lets-start-training","title":"Lets start training\u00b6","text":""},{"location":"LLM/Gemma/Gemma_finetuning_notebook/#test-out-finetuned-model","title":"Test out Finetuned Model\u00b6","text":""},{"location":"LLM/Gemma/finetune-gemma/","title":"Finetune gemma","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Wed Feb 21 17:19:05 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n| N/A   33C    P0              42W / 300W |      4MiB / 81920MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[2]: Copied! <pre>!pip3 install -q -U bitsandbytes==0.42.0\n!pip3 install -q -U peft==0.8.2\n!pip3 install -q -U trl==0.7.10\n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q -U transformers==4.38.0\n</pre> !pip3 install -q -U bitsandbytes==0.42.0 !pip3 install -q -U peft==0.8.2 !pip3 install -q -U trl==0.7.10 !pip3 install -q -U accelerate==0.27.1 !pip3 install -q -U datasets==2.17.0 !pip3 install -q -U transformers==4.38.0 In\u00a0[2]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_use_double_quant=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.bfloat16 ) <p>Now we specify the model ID and then we load it with our previously defined quantization configuration.Now we specify the model ID and then we load it with our previously defined quantization configuration.</p> In\u00a0[4]: Copied! <pre># if you are using google colab\n\n# import os\n# from google.colab import userdata\n# os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n</pre> # if you are using google colab  # import os # from google.colab import userdata # os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN') In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>model_id = \"google/gemma-7b-it\"\n# model_id = \"google/gemma-7b\"\n# model_id = \"google/gemma-2b-it\"\n# model_id = \"google/gemma-2b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n</pre> model_id = \"google/gemma-7b-it\" # model_id = \"google/gemma-7b\" # model_id = \"google/gemma-2b-it\" # model_id = \"google/gemma-2b\"  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True) In\u00a0[10]: Copied! <pre>def get_completion(query: str, model, tokenizer) -&gt; str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  &lt;start_of_turn&gt;user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model\n  \n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  # decoded = tokenizer.batch_decode(generated_ids)\n  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n  return (decoded)\n</pre> def get_completion(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"    prompt_template = \"\"\"   user   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   \\nmodel       \"\"\"   prompt = prompt_template.format(query=query)    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)    model_inputs = encodeds.to(device)     generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   # decoded = tokenizer.batch_decode(generated_ids)   decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)   return (decoded) In\u00a0[11]: Copied! <pre>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer) print(result) <pre>A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n</pre> <pre>\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  code the fibonacci series in python using reccursion\n  \nmodel\n  \n\n   a Python function to calculate the nth Fibonacci number using recursion. Here's the code: \n\n```python\n\ndef fibonacci(n):\n  if n == 0:\n    return 0\n  elif n == 1:\n    return 1\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# Print the nth Fibonacci number\nprint(fibonacci(10))\n```\n\n**Explanation:**\n\n1. The function `fibonacci` takes an integer `n` as input.\n2. If `n` is 0 or 1, it returns the respective base case of 0 or 1.\n3. Otherwise, it recursively calculates the Fibonacci number for `n-1` and `n-2` and adds their sum to return the Fibonacci number for `n`.\n4. The function continues to recurse until `n` reaches the desired number, and the final result is returned.\n\n**Output:**\n\n```\n&gt;&gt;&gt; print(fibonacci(10))\n5\n```\n\nIn this example, the code calculates the 10th Fibonacci number, which is 5.\n\n**Note:** Reccursion can be a powerful technique for solving problems that involve repeated calculations. However, it is important to note that recursion can also lead to stack overflow errors for large values of `n` due to its repeated function calls. For more efficient solutions, iterative approaches are often preferred.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\ndataset\n</pre> from datasets import load_dataset  dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\") dataset In\u00a0[15]: Copied! <pre>df = dataset.to_pandas()\ndf.head(10)\n</pre> df = dataset.to_pandas() df.head(10) Out[15]: input output text instruction 0 [1, 2, 3, 4, 5] # Python code\\ndef sum_sequence(sequence):\\n  ... Below is an instruction that describes a task.... Create a function to calculate the sum of a se... 1 str1 = \"Hello \"\\nstr2 = \"world\" def add_strings(str1, str2):\\n    \"\"\"This func... Below is an instruction that describes a task.... Develop a function that will add two strings 2 #include &lt;map&gt;\\n#include &lt;string&gt;\\n\\nclass Gro... Below is an instruction that describes a task.... Design a data structure in C++ to store inform... 3 [3, 1, 4, 5, 9, 0] def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ... Below is an instruction that describes a task.... Implement a sorting algorithm to sort a given ... 4 Not applicable import UIKit\\n\\nclass ExpenseViewController: U... Below is an instruction that describes a task.... Design a Swift application for tracking expens... 5 Not Applicable &lt;?php\\n$timestamp = $_GET['timestamp'];\\n\\nif(... Below is an instruction that describes a task.... Create a REST API to convert a UNIX timestamp ... 6 website: www.example.com \\ndata to crawl: phon... import requests\\nimport re\\n\\ndef crawl_websit... Below is an instruction that describes a task.... Generate a Python code for crawling a website ... 7 [x*x for x in [1, 2, 3, 5, 8, 13]] Below is an instruction that describes a task.... Create a Python list comprehension to get the ... 8 SELECT * FROM products ORDER BY price DESC LIM... Below is an instruction that describes a task.... Create a MySQL query to find the most expensiv... 9 Not applicable public class Library {\\n \\n // map of books in... Below is an instruction that describes a task.... Create a data structure in Java for storing an... <p>Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :</p> <ol> <li>the function generate_prompt : take the instruction and output and generate a prompt</li> <li>shuffle the dataset</li> <li>tokenizer the dataset</li> </ol> In\u00a0[16]: Copied! <pre>def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"&lt;start_of_turn&gt;user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point[\"output\"]} &lt;end_of_turn&gt;\"\"\"\n    # Without\n    else:\n        text = f\"\"\"&lt;start_of_turn&gt;user {prefix_text} {data_point[\"instruction\"]} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point[\"output\"]} &lt;end_of_turn&gt;\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\n</pre> def generate_prompt(data_point):     \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer      :param data_point: dict: Data point     :return: dict: tokenzed prompt     \"\"\"     prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\                'appropriately completes the request.\\n\\n'     # Samples with additional context into.     if data_point['input']:         text = f\"\"\"user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} \\nmodel{data_point[\"output\"]} \"\"\"     # Without     else:         text = f\"\"\"user {prefix_text} {data_point[\"instruction\"]} \\nmodel{data_point[\"output\"]} \"\"\"     return text  # add the \"prompt\" column in the dataset text_column = [generate_prompt(data_point) for data_point in dataset] dataset = dataset.add_column(\"prompt\", text_column) <p>We'll need to tokenize our data so the model can understand.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n</pre> dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True) <p>Split dataset into 90% for training and 10% for testing</p> In\u00a0[18]: Copied! <pre>dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n</pre> dataset = dataset.train_test_split(test_size=0.2) train_data = dataset[\"train\"] test_data = dataset[\"test\"] In\u00a0[19]: Copied! <pre>print(test_data)\n</pre> print(test_data) <pre>Dataset({\n    features: ['input', 'output', 'text', 'instruction', 'prompt', 'input_ids', 'attention_mask'],\n    num_rows: 24392\n})\n</pre> In\u00a0[20]: Copied! <pre>from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n</pre> from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model model.gradient_checkpointing_enable() model = prepare_model_for_kbit_training(model) In\u00a0[21]: Copied! <pre>print(model)\n</pre> print(model) <pre>GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n    (layers): ModuleList(\n      (0-27): 28 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n)\n</pre> In\u00a0[22]: Copied! <pre>import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)\n</pre> import bitsandbytes as bnb def find_all_linear_names(model):   cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)   lora_module_names = set()   for name, module in model.named_modules():     if isinstance(module, cls):       names = name.split('.')       lora_module_names.add(names[0] if len(names) == 1 else names[-1])     if 'lm_head' in lora_module_names: # needed for 16-bit       lora_module_names.remove('lm_head')   return list(lora_module_names) In\u00a0[23]: Copied! <pre>modules = find_all_linear_names(model)\nprint(modules)\n</pre> modules = find_all_linear_names(model) print(modules) <pre>['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj']\n</pre> In\u00a0[26]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n</pre> from peft import LoraConfig, get_peft_model  lora_config = LoraConfig(     r=64,     lora_alpha=32,     target_modules=modules,     lora_dropout=0.05,     bias=\"none\",     task_type=\"CAUSAL_LM\" )  model = get_peft_model(model, lora_config) In\u00a0[27]: Copied! <pre>trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n</pre> trainable, total = model.get_nb_trainable_parameters() print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\") <pre>Trainable: 200015872 | total: 8737696768 | Percentage: 2.2891%\n</pre> <p>Setting the training arguments:</p> <ul> <li>for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># import transformers\n\n# tokenizer.pad_token = tokenizer.eos_token\n\n\n# trainer = transformers.Trainer(\n#     model=model,\n#     train_dataset=train_data,\n#     eval_dataset=test_data,\n#     args=transformers.TrainingArguments(\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_steps=0.03,\n#         max_steps=100,\n#         learning_rate=2e-4,\n#         fp16=True,\n#         logging_steps=1,\n#         output_dir=\"outputs_mistral_b_finance_finetuned_test\",\n#         optim=\"paged_adamw_8bit\",\n#         save_strategy=\"epoch\",\n#     ),\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n# )\n</pre> # import transformers  # tokenizer.pad_token = tokenizer.eos_token   # trainer = transformers.Trainer( #     model=model, #     train_dataset=train_data, #     eval_dataset=test_data, #     args=transformers.TrainingArguments( #         per_device_train_batch_size=1, #         gradient_accumulation_steps=4, #         warmup_steps=0.03, #         max_steps=100, #         learning_rate=2e-4, #         fp16=True, #         logging_steps=1, #         output_dir=\"outputs_mistral_b_finance_finetuned_test\", #         optim=\"paged_adamw_8bit\", #         save_strategy=\"epoch\", #     ), #     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # )  In\u00a0[\u00a0]: Copied! <pre>#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0.03,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n</pre> #new code using SFTTrainer import transformers  from trl import SFTTrainer  tokenizer.pad_token = tokenizer.eos_token torch.cuda.empty_cache()  trainer = SFTTrainer(     model=model,     train_dataset=train_data,     eval_dataset=test_data,     dataset_text_field=\"prompt\",     peft_config=lora_config,     args=transformers.TrainingArguments(         per_device_train_batch_size=1,         gradient_accumulation_steps=4,         warmup_steps=0.03,         max_steps=100,         learning_rate=2e-4,         logging_steps=1,         output_dir=\"outputs\",         optim=\"paged_adamw_8bit\",         save_strategy=\"epoch\",     ),     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), ) In\u00a0[29]: Copied! <pre>model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n</pre> model.config.use_cache = False  # silence the warnings. Please re-enable for inference! trainer.train() <pre>/home/adithya/miniconda3/envs/gemma-venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n</pre>        [100/100 03:06, Epoch 0/1]      Step Training Loss 1 10.299600 2 6.640600 3 7.763200 4 4.142200 5 3.619800 6 4.840600 7 2.886800 8 1.334500 9 1.177300 10 1.343300 11 1.306000 12 1.122000 13 1.337300 14 1.083400 15 1.399100 16 1.172800 17 0.838900 18 1.027200 19 0.847500 20 1.065700 21 0.944100 22 0.782900 23 1.030100 24 0.911900 25 0.868400 26 0.768300 27 0.845200 28 1.036400 29 0.766000 30 0.741500 31 0.738800 32 0.779200 33 0.833000 34 0.851800 35 0.824800 36 0.757800 37 0.804500 38 0.962900 39 0.716000 40 1.027000 41 0.743100 42 0.932800 43 0.623100 44 0.671600 45 0.807100 46 0.957200 47 0.643900 48 0.867800 49 0.810700 50 0.871100 51 0.628500 52 0.946400 53 0.918600 54 0.920300 55 0.746100 56 0.914800 57 0.705700 58 0.883300 59 1.016300 60 0.583200 61 0.872000 62 0.617600 63 0.858700 64 0.955700 65 0.854500 66 0.778000 67 0.733200 68 0.871200 69 0.847700 70 0.567400 71 1.078200 72 0.945800 73 0.762500 74 0.618800 75 0.803600 76 0.848400 77 0.504300 78 0.685300 79 0.700700 80 0.636400 81 0.832700 82 0.614600 83 0.899000 84 0.623800 85 0.637200 86 0.697500 87 0.770200 88 0.800000 89 0.748600 90 0.897100 91 0.718800 92 0.703900 93 0.635600 94 0.649800 95 0.627200 96 0.785200 97 0.808200 98 0.720500 99 0.656600 100 0.650900 <p> </p> Out[29]: <pre>TrainOutput(global_step=100, training_loss=1.1844707012176514, metrics={'train_runtime': 188.6612, 'train_samples_per_second': 2.12, 'train_steps_per_second': 0.53, 'total_flos': 3947993787666432.0, 'train_loss': 1.1844707012176514, 'epoch': 0.0})</pre> <p>Share adapters on the \ud83e\udd17 Hub</p> In\u00a0[30]: Copied! <pre>new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub\n</pre> new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub In\u00a0[31]: Copied! <pre>trainer.model.save_pretrained(new_model)\n</pre> trainer.model.save_pretrained(new_model) In\u00a0[\u00a0]: Copied! <pre>base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</pre> base_model = AutoModelForCausalLM.from_pretrained(     model_id,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= merged_model.merge_and_unload()  # Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\") tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\" In\u00a0[\u00a0]: Copied! <pre># Push the model and tokenizer to the Hugging Face Model Hub\nmerged_model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n</pre> # Push the model and tokenizer to the Hugging Face Model Hub merged_model.push_to_hub(new_model, use_temp_dir=False) tokenizer.push_to_hub(new_model, use_temp_dir=False) In\u00a0[34]: Copied! <pre>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer) print(result) <pre>A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n</pre> <pre>\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  code the fibonacci series in python using reccursion\n  \nmodel\n  \n\n   a Python program to code the Fibonacci series using recursion. Here's a solution:\n\n```python\ndef fibonacci(n):\n  \"\"\"Calculates the nth Fibonacci number using recursion.\n\n  Args:\n    n: The index of the Fibonacci number to calculate.\n\n  Returns:\n    The nth Fibonacci number.\n  \"\"\"\n\n  # Base case: The first two Fibonacci numbers are 0 and 1.\n  if n &lt;= 1:\n    return n\n\n  # Recursive case: Otherwise, calculate the nth Fibonacci number by adding the previous two numbers.\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)\n\n\n# Print the Fibonacci numbers.\nfor i in range(10):\n  print(fibonacci(i))\n```\n\n**Explanation:**\n\n* The `fibonacci` function takes an integer `n` as input.\n* If `n` is less than or equal to 1, it returns `n` itself, as the base case.\n* Otherwise, it calculates `n`-th Fibonacci number recursively by adding the previous two numbers.\n* The function calls itself with smaller and smaller values of `n` until it reaches the base case.\n\n**Example Usage:**\n\n```python\n# Print the Fibonacci numbers from 0 to 10.\nfor i in range(10):\n  print(fibonacci(i))\n```\n\n**Output:**\n\n```\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\n```\n\n**Note:**\n\n* This program calculates Fibonacci numbers recursively, which means that it may not be very efficient for large values of `n` as it repeats calculations.\n* For more efficient Fibonacci number calculation, consider using an iterative approach rather than recursion.\n</pre>"},{"location":"LLM/Gemma/finetune-gemma/#instruct-fine-tuning-gemma-using-qlora-and-supervise-finetuning","title":"Instruct Fine-tuning Gemma using qLora and Supervise Finetuning\u00b6","text":"<p>This is a comprahensive notebook and tutorial on how to fine tune the <code>gemma-7b-it</code> Model</p> <p>All the code will be available on my Github. Do drop by and give a follow and a star. adithya-s-k Github Code</p> <p>I also post content about LLMs and what I have been working on Twitter. AdithyaSK (@adithya_s_k) / X</p>"},{"location":"LLM/Gemma/finetune-gemma/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:</p> <ol> <li>GPU: gemma-2b - can be finetuned on T4(free google colab) while gemma-7b requires an A100 GPU.</li> <li>Python Packages: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:</li> </ol> <p>Let's begin by checking if your GPU is correctly detected:</p>"},{"location":"LLM/Gemma/finetune-gemma/#step-2-model-loading","title":"Step 2 - Model loading\u00b6","text":"<p>We'll load the model using QLoRA quantization to reduce the usage of memory</p>"},{"location":"LLM/Gemma/finetune-gemma/#step-3-load-dataset-for-finetuning","title":"Step 3 - Load dataset for finetuning\u00b6","text":""},{"location":"LLM/Gemma/finetune-gemma/#lets-load-the-dataset","title":"Lets Load the Dataset\u00b6","text":"<p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>We will be using this dataset which is curated by TokenBender (e/xperiments) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <pre>{\n  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n  \"input\": \"[1, 2, 3, 4, 5]\",\n  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n}\n</pre>"},{"location":"LLM/Gemma/finetune-gemma/#formatting-the-dataset","title":"Formatting the Dataset\u00b6","text":"<p>Now, let's format the dataset in the required gemma instruction formate.</p> <p>Many tutorials and blogs skip over this part, but I feel this is a really important step.</p> <pre><code>&lt;start_of_turn&gt;user What is your favorite condiment? &lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;end_of_turn&gt;\n</code></pre> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p>"},{"location":"LLM/Gemma/finetune-gemma/#after-formatting-we-should-get-something-like-this","title":"After Formatting, We should get something like this\u00b6","text":"<pre>{\n\"text\":\"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;\",\n\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n\"input\":\"[1, 2, 3, 4, 5]\",\n\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n sequence: sum += num return sum\",\n\"prompt\":\"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;\"\n\n}\n</pre> <p>While using SFT (Supervised Fine-tuning Trainer) for fine-tuning, we will be only passing in the \u201ctext\u201d column of the dataset for fine-tuning.</p>"},{"location":"LLM/Gemma/finetune-gemma/#step-4-apply-lora","title":"Step 4 - Apply Lora\u00b6","text":"<p>Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.</p>"},{"location":"LLM/Gemma/finetune-gemma/#step-5-run-the-training","title":"Step 5 - Run the training!\u00b6","text":""},{"location":"LLM/Gemma/finetune-gemma/#fine-tuning-with-qlora-and-supervised-fine-tuning","title":"Fine-Tuning with qLora and Supervised Fine-Tuning\u00b6","text":"<p>We're ready to fine-tune our model using qLora. For this tutorial, we'll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p>"},{"location":"LLM/Gemma/finetune-gemma/#lets-start-training","title":"Lets start training\u00b6","text":""},{"location":"LLM/Gemma/finetune-gemma/#test-out-finetuned-model","title":"Test out Finetuned Model\u00b6","text":""},{"location":"LLM/Gemma/gemma-sft/","title":"Gemma sft","text":"In\u00a0[\u00a0]: Copied! <pre>from dataclasses import dataclass, field\nfrom typing import Optional\n</pre> from dataclasses import dataclass, field from typing import Optional In\u00a0[\u00a0]: Copied! <pre>import torch\n</pre> import torch In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nimport logging\n</pre> from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments from datasets import load_dataset from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model from trl import SFTTrainer import logging In\u00a0[\u00a0]: Copied! <pre># Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n</pre> # Set up logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass ScriptArguments:\n    \"\"\"\n    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n    \"\"\"\n    per_device_train_batch_size: Optional[int] = field(default=2)\n    per_device_eval_batch_size: Optional[int] = field(default=1)\n    gradient_accumulation_steps: Optional[int] = field(default=8)\n    learning_rate: Optional[float] = field(default=0.0002)\n    max_grad_norm: Optional[float] = field(default=0.3)\n    weight_decay: Optional[int] = field(default=0.001)\n    lora_alpha: Optional[int] = field(default=32)\n    lora_dropout: Optional[float] = field(default=0.1)\n    lora_r: Optional[int] = field(default=64)\n    max_seq_length: Optional[int] = field(default=4096)\n    model_name: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n        }\n    )\n    dataset_name: Optional[str] = field(\n        default=\"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate\",\n        metadata={\"help\": \"The preference dataset to use.\"},\n    )\n    fp16: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enables fp16 training.\"},\n    )\n    bf16: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enables bf16 training.\"},\n    )\n    report_to: Optional[str] = field(\n        default=\"wandb\",\n        metadata={\"help\": \"Enables bf16 training.\"},\n    )\n    packing: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Use packing dataset creating.\"},\n    )\n    gradient_checkpointing: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Enables gradient checkpointing.\"},\n    )\n    use_flash_attention_2: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enables Flash Attention 2.\"},\n    )\n    optim: Optional[str] = field(\n        default=\"paged_adamw_32bit\",\n        metadata={\"help\": \"The optimizer to use.\"},\n    )\n    lr_scheduler_type: str = field(\n        default=\"cosine\",\n        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n    )\n    # max_steps: int = field(default=1000, metadata={\"help\": \"How many optimizer update steps to take\"})\n    num_train_epochs: int = field(default=1, metadata={\"help\": \"How many epochs you want to train it for\"})\n    warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n    save_steps: int = field(default=30, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n    logging_steps: int = field(default=1, metadata={\"help\": \"Log every X updates steps.\"})\n    output_dir: str = field(\n        default=\"Gemma-Hindi-Instruct\",\n        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n    )\n</pre> @dataclass class ScriptArguments:     \"\"\"     These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.     \"\"\"     per_device_train_batch_size: Optional[int] = field(default=2)     per_device_eval_batch_size: Optional[int] = field(default=1)     gradient_accumulation_steps: Optional[int] = field(default=8)     learning_rate: Optional[float] = field(default=0.0002)     max_grad_norm: Optional[float] = field(default=0.3)     weight_decay: Optional[int] = field(default=0.001)     lora_alpha: Optional[int] = field(default=32)     lora_dropout: Optional[float] = field(default=0.1)     lora_r: Optional[int] = field(default=64)     max_seq_length: Optional[int] = field(default=4096)     model_name: Optional[str] = field(         default=None,         metadata={             \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"         }     )     dataset_name: Optional[str] = field(         default=\"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate\",         metadata={\"help\": \"The preference dataset to use.\"},     )     fp16: Optional[bool] = field(         default=False,         metadata={\"help\": \"Enables fp16 training.\"},     )     bf16: Optional[bool] = field(         default=False,         metadata={\"help\": \"Enables bf16 training.\"},     )     report_to: Optional[str] = field(         default=\"wandb\",         metadata={\"help\": \"Enables bf16 training.\"},     )     packing: Optional[bool] = field(         default=True,         metadata={\"help\": \"Use packing dataset creating.\"},     )     gradient_checkpointing: Optional[bool] = field(         default=True,         metadata={\"help\": \"Enables gradient checkpointing.\"},     )     use_flash_attention_2: Optional[bool] = field(         default=False,         metadata={\"help\": \"Enables Flash Attention 2.\"},     )     optim: Optional[str] = field(         default=\"paged_adamw_32bit\",         metadata={\"help\": \"The optimizer to use.\"},     )     lr_scheduler_type: str = field(         default=\"cosine\",         metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},     )     # max_steps: int = field(default=1000, metadata={\"help\": \"How many optimizer update steps to take\"})     num_train_epochs: int = field(default=1, metadata={\"help\": \"How many epochs you want to train it for\"})     warmup_ratio: float = field(default=0.03, metadata={\"help\": \"Fraction of steps to do a warmup for\"})     save_steps: int = field(default=30, metadata={\"help\": \"Save checkpoint every X updates steps.\"})     logging_steps: int = field(default=1, metadata={\"help\": \"Log every X updates steps.\"})     output_dir: str = field(         default=\"Gemma-Hindi-Instruct\",         metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},     ) In\u00a0[\u00a0]: Copied! <pre>parser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n</pre> parser = HfArgumentParser(ScriptArguments) script_args = parser.parse_args_into_dataclasses()[0] <p>def formatting_func(example): text = f\"### USER: {example['data'][0]}\\n### ASSISTANT: {example['data'][1]}\" return text</p> In\u00a0[\u00a0]: Copied! <pre># Load the GG model - this is the local one, update it to the one on the Hub\nmodel_id = \"google/gemma-7b-it\"\n</pre> # Load the GG model - this is the local one, update it to the one on the Hub model_id = \"google/gemma-7b-it\" <p>quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type=\"nf4\" )</p> In\u00a0[\u00a0]: Copied! <pre># Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    # quantization_config=quantization_config, \n    torch_dtype=torch.float32,\n    device_map={\"\": 0},\n)\n</pre> # Load model model = AutoModelForCausalLM.from_pretrained(     model_id,      # quantization_config=quantization_config,      torch_dtype=torch.float32,     device_map={\"\": 0}, ) In\u00a0[\u00a0]: Copied! <pre># Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n</pre> # Load tokenizer tokenizer = AutoTokenizer.from_pretrained(model_id) In\u00a0[\u00a0]: Copied! <pre>lora_config = LoraConfig(\n    r=script_args.lora_r,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    lora_alpha=script_args.lora_alpha,\n    lora_dropout=script_args.lora_dropout\n)\n</pre> lora_config = LoraConfig(     r=script_args.lora_r,     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],     bias=\"none\",     task_type=\"CAUSAL_LM\",     lora_alpha=script_args.lora_alpha,     lora_dropout=script_args.lora_dropout ) In\u00a0[\u00a0]: Copied! <pre>train_dataset = load_dataset(script_args.dataset_name, split=\"train[:5%]\")\n</pre> train_dataset = load_dataset(script_args.dataset_name, split=\"train[:5%]\") In\u00a0[\u00a0]: Copied! <pre># TODO: make that configurable\nYOUR_HF_USERNAME = \"CognitiveLab\"\noutput_dir = f\"{YOUR_HF_USERNAME}/gemma-hindi-instruct\"\n</pre> # TODO: make that configurable YOUR_HF_USERNAME = \"CognitiveLab\" output_dir = f\"{YOUR_HF_USERNAME}/gemma-hindi-instruct\" In\u00a0[\u00a0]: Copied! <pre>training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=script_args.per_device_train_batch_size,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    optim=script_args.optim,\n    save_steps=script_args.save_steps,\n    logging_steps=script_args.logging_steps,\n    learning_rate=script_args.learning_rate,\n    max_grad_norm=script_args.max_grad_norm,\n    # max_steps=script_args.max_steps,\n    num_train_epochs=script_args.num_train_epochs,\n    warmup_ratio=script_args.warmup_ratio,\n    lr_scheduler_type=script_args.lr_scheduler_type,\n    gradient_checkpointing=script_args.gradient_checkpointing,\n    fp16=script_args.fp16,\n    bf16=script_args.bf16,\n    report_to=script_args.report_to,\n)\n</pre> training_arguments = TrainingArguments(     output_dir=output_dir,     per_device_train_batch_size=script_args.per_device_train_batch_size,     gradient_accumulation_steps=script_args.gradient_accumulation_steps,     optim=script_args.optim,     save_steps=script_args.save_steps,     logging_steps=script_args.logging_steps,     learning_rate=script_args.learning_rate,     max_grad_norm=script_args.max_grad_norm,     # max_steps=script_args.max_steps,     num_train_epochs=script_args.num_train_epochs,     warmup_ratio=script_args.warmup_ratio,     lr_scheduler_type=script_args.lr_scheduler_type,     gradient_checkpointing=script_args.gradient_checkpointing,     fp16=script_args.fp16,     bf16=script_args.bf16,     report_to=script_args.report_to, ) In\u00a0[\u00a0]: Copied! <pre>trainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_dataset,\n    peft_config=lora_config,\n    packing=script_args.packing,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=script_args.max_seq_length,\n)\n</pre> trainer = SFTTrainer(     model=model,     args=training_arguments,     train_dataset=train_dataset,     peft_config=lora_config,     packing=script_args.packing,     dataset_text_field=\"text\",     tokenizer=tokenizer,     max_seq_length=script_args.max_seq_length, ) In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() In\u00a0[\u00a0]: Copied! <pre>logger.info(\"Training stage completed\")\npeft_model = script_args.output_dir\ntrainer.model.save_pretrained(peft_model)\n</pre> logger.info(\"Training stage completed\") peft_model = script_args.output_dir trainer.model.save_pretrained(peft_model) In\u00a0[\u00a0]: Copied! <pre>base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, peft_model)\nmerged_model= merged_model.merge_and_unload()\nlogger.info(\"Training stage completed\")\n</pre> base_model = AutoModelForCausalLM.from_pretrained(     model_id,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, peft_model) merged_model= merged_model.merge_and_unload() logger.info(\"Training stage completed\") In\u00a0[\u00a0]: Copied! <pre>merged_model_name = str(peft_model)+\"_merged\"\n# Save the merged model\nlogger.info(\"Merging the model with the PEFT adapter\")\nmerged_model.save_pretrained(merged_model_name,safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\n</pre> merged_model_name = str(peft_model)+\"_merged\" # Save the merged model logger.info(\"Merging the model with the PEFT adapter\") merged_model.save_pretrained(merged_model_name,safe_serialization=True) tokenizer.save_pretrained(\"merged_model\") In\u00a0[\u00a0]: Copied! <pre>logger.info(\"Pushing the model to Huggingface hub\")\ntry:\n    merged_model.push_to_hub(script_args.output_dir, use_temp_dir=False)\n    tokenizer.push_to_hub(script_args.output_dir, use_temp_dir=False)\nexcept Exception as e:\n    logger.info(f\"Error while pushing to huggingface Hub: {e}\")\n</pre> logger.info(\"Pushing the model to Huggingface hub\") try:     merged_model.push_to_hub(script_args.output_dir, use_temp_dir=False)     tokenizer.push_to_hub(script_args.output_dir, use_temp_dir=False) except Exception as e:     logger.info(f\"Error while pushing to huggingface Hub: {e}\") In\u00a0[\u00a0]: Copied! <pre>logger.info(\"Training stage completed\")\n</pre> logger.info(\"Training stage completed\")"},{"location":"LLM/HandsOnWithFinetuning/GRPO/GRPO_finetuning_notebook/","title":"GRPO finetuning notebook","text":"GRPO Finetuning Training a small math reasoner with RL <p>This notebook is an alternate version of the GRPO demo by will brown, training llama-1b on the gsm8k math dataset.</p> <p>We've only implemented a series of changes to make the code more workable on Colab:</p> <ul> <li>Replacement of llama-1b with Qwen-0.5b</li> <li>Generation with vllm, which yields a significant speed-up. Qwen small size makes it possible to run vllm on the same gpu as the one being used for GRPO.</li> <li>Dropping flash-attn (recurrent bug with modeling qwen, not clear why)</li> </ul> <p>First we install vllm. Notice that you'll have to restart the session afterwards.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install vllm\n</pre> !pip install vllm <p>Then we install trl and datasets. It has to be in this order for some reason (bug on trl if you do vllm afterwards)</p> In\u00a0[\u00a0]: Copied! <pre>!pip install trl datasets\n</pre> !pip install trl datasets <p>Now we have everything ready to set up our RL training set and reward policy.</p> <p>First we set the general prompt structure (with the reasoning tags).</p> In\u00a0[\u00a0]: Copied! <pre>import re\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import GRPOConfig, GRPOTrainer\n\n# Load and prep dataset\n\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n&lt;reasoning&gt;\n...\n&lt;/reasoning&gt;\n&lt;answer&gt;\n...\n&lt;/answer&gt;\n\"\"\"\n\nXML_COT_FORMAT = \"\"\"\\\n&lt;reasoning&gt;\n{reasoning}\n&lt;/reasoning&gt;\n&lt;answer&gt;\n{answer}\n&lt;/answer&gt;\n\"\"\"\n</pre> import re import torch from datasets import load_dataset, Dataset from transformers import AutoTokenizer, AutoModelForCausalLM from trl import GRPOConfig, GRPOTrainer  # Load and prep dataset  SYSTEM_PROMPT = \"\"\" Respond in the following format:  ...   ...  \"\"\"  XML_COT_FORMAT = \"\"\"\\  {reasoning}   {answer}  \"\"\" <p>Now we import the gsm8k dataset and restructure it to fit into a conversational prompt format:</p> In\u00a0[\u00a0]: Copied! <pre>def extract_xml_answer(text: str) -&gt; str:\n    answer = text.split(\"&lt;answer&gt;\")[-1]\n    answer = answer.split(\"&lt;/answer&gt;\")[0]\n    return answer.strip()\n\ndef extract_hash_answer(text: str) -&gt; str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n# uncomment middle messages for 1-shot prompting\ndef get_gsm8k_questions(split = \"train\") -&gt; Dataset:\n    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n    data = data.map(lambda x: { # type: ignore\n        'prompt': [\n            {'role': 'system', 'content': SYSTEM_PROMPT},\n            {'role': 'user', 'content': x['question']}\n        ],\n        'answer': extract_hash_answer(x['answer'])\n    }) # type: ignore\n    return data # type: ignore\n\ndataset = get_gsm8k_questions()\n</pre> def extract_xml_answer(text: str) -&gt; str:     answer = text.split(\"\")[-1]     answer = answer.split(\"\")[0]     return answer.strip()  def extract_hash_answer(text: str) -&gt; str | None:     if \"####\" not in text:         return None     return text.split(\"####\")[1].strip()  # uncomment middle messages for 1-shot prompting def get_gsm8k_questions(split = \"train\") -&gt; Dataset:     data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore     data = data.map(lambda x: { # type: ignore         'prompt': [             {'role': 'system', 'content': SYSTEM_PROMPT},             {'role': 'user', 'content': x['question']}         ],         'answer': extract_hash_answer(x['answer'])     }) # type: ignore     return data # type: ignore  dataset = get_gsm8k_questions() <p>We move on now to the reward functions. The most important one is the \"correctness\" function which acts as a verifier (comparison of model completions vs. answer). The three others are formatting functions.</p> In\u00a0[\u00a0]: Copied! <pre># Reward functions\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -&gt; list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n\ndef int_reward_func(completions, **kwargs) -&gt; list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n\ndef strict_format_reward_func(completions, **kwargs) -&gt; list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^&lt;reasoning&gt;\\n.*?\\n&lt;/reasoning&gt;\\n&lt;answer&gt;\\n.*?\\n&lt;/answer&gt;\\n$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef soft_format_reward_func(completions, **kwargs) -&gt; list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"&lt;reasoning&gt;.*?&lt;/reasoning&gt;\\s*&lt;answer&gt;.*?&lt;/answer&gt;\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef count_xml(text) -&gt; float:\n    count = 0.0\n    if text.count(\"&lt;reasoning&gt;\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n&lt;/reasoning&gt;\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n&lt;answer&gt;\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n&lt;/answer&gt;\\n\")[-1])*0.001\n    if text.count(\"\\n&lt;/answer&gt;\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n&lt;/answer&gt;\")[-1]) - 1)*0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -&gt; list[float]:\n    contents = [completion[0][\"content\"] for completion in completions]\n    return [count_xml(c) for c in contents]\n</pre> # Reward functions def correctness_reward_func(prompts, completions, answer, **kwargs) -&gt; list[float]:     responses = [completion[0]['content'] for completion in completions]     q = prompts[0][-1]['content']     extracted_responses = [extract_xml_answer(r) for r in responses]     print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")     return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]  def int_reward_func(completions, **kwargs) -&gt; list[float]:     responses = [completion[0]['content'] for completion in completions]     extracted_responses = [extract_xml_answer(r) for r in responses]     return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]  def strict_format_reward_func(completions, **kwargs) -&gt; list[float]:     \"\"\"Reward function that checks if the completion has a specific format.\"\"\"     pattern = r\"^\\n.*?\\n\\n\\n.*?\\n\\n$\"     responses = [completion[0][\"content\"] for completion in completions]     matches = [re.match(pattern, r) for r in responses]     return [0.5 if match else 0.0 for match in matches]  def soft_format_reward_func(completions, **kwargs) -&gt; list[float]:     \"\"\"Reward function that checks if the completion has a specific format.\"\"\"     pattern = r\".*?\\s*.*?\"     responses = [completion[0][\"content\"] for completion in completions]     matches = [re.match(pattern, r) for r in responses]     return [0.5 if match else 0.0 for match in matches]  def count_xml(text) -&gt; float:     count = 0.0     if text.count(\"\\n\") == 1:         count += 0.125     if text.count(\"\\n\\n\") == 1:         count += 0.125     if text.count(\"\\n\\n\") == 1:         count += 0.125         count -= len(text.split(\"\\n\\n\")[-1])*0.001     if text.count(\"\\n\") == 1:         count += 0.125         count -= (len(text.split(\"\\n\")[-1]) - 1)*0.001     return count  def xmlcount_reward_func(completions, **kwargs) -&gt; list[float]:     contents = [completion[0][\"content\"] for completion in completions]     return [count_xml(c) for c in contents] <p>We now set the training arguments:</p> In\u00a0[\u00a0]: Copied! <pre>model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\noutput_dir=\"outputs/Qwen-0.5B-GRPO\"\nrun_name=\"Qwen-0.5B-GRPO-gsm8k\"\n\ntraining_args = GRPOConfig(\n    output_dir=output_dir,\n    run_name=run_name,\n    learning_rate=5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type='cosine',\n    logging_steps=1,\n    bf16=True,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_generations=16,\n    max_prompt_length=256,\n    max_completion_length=200,\n    num_train_epochs=1,\n    save_steps=100,\n    max_grad_norm=0.1,\n    log_on_each_node=False,\n    use_vllm=True,\n    vllm_gpu_memory_utilization=.3,\n    vllm_device=\"cuda:0\",\n    report_to=\"none\" #I'm disabling Wandb.\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=None\n).to(\"cuda\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n</pre> model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  output_dir=\"outputs/Qwen-0.5B-GRPO\" run_name=\"Qwen-0.5B-GRPO-gsm8k\"  training_args = GRPOConfig(     output_dir=output_dir,     run_name=run_name,     learning_rate=5e-6,     adam_beta1 = 0.9,     adam_beta2 = 0.99,     weight_decay = 0.1,     warmup_ratio = 0.1,     lr_scheduler_type='cosine',     logging_steps=1,     bf16=True,     per_device_train_batch_size=1,     gradient_accumulation_steps=4,     num_generations=16,     max_prompt_length=256,     max_completion_length=200,     num_train_epochs=1,     save_steps=100,     max_grad_norm=0.1,     log_on_each_node=False,     use_vllm=True,     vllm_gpu_memory_utilization=.3,     vllm_device=\"cuda:0\",     report_to=\"none\" #I'm disabling Wandb. )  model = AutoModelForCausalLM.from_pretrained(     model_name,     torch_dtype=torch.bfloat16,     device_map=None ).to(\"cuda\")  tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.pad_token = tokenizer.eos_token <p>And launch the actual training:</p> In\u00a0[\u00a0]: Copied! <pre># use peft at your own risk; not working for me with multi-GPU training\ntrainer = GRPOTrainer(\n    model=model,\n    processing_class=tokenizer,\n    reward_funcs=[\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func],\n    args=training_args,\n    train_dataset=dataset,\n    #peft_config=peft_config\n)\ntrainer.train()\n</pre> # use peft at your own risk; not working for me with multi-GPU training trainer = GRPOTrainer(     model=model,     processing_class=tokenizer,     reward_funcs=[         xmlcount_reward_func,         soft_format_reward_func,         strict_format_reward_func,         int_reward_func,         correctness_reward_func],     args=training_args,     train_dataset=dataset,     #peft_config=peft_config ) trainer.train()"},{"location":"LLM/HandsOnWithFinetuning/GRPO/GRPO_finetuning_notebook/#setting-up-the-models","title":"Setting up the models.\u00b6","text":""},{"location":"LLM/HandsOnWithFinetuning/GRPO/GRPO_finetuning_notebook/#defining-the-rl-rewards","title":"Defining the RL rewards\u00b6","text":""},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/","title":"Hands on with GRPO","text":""},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#hacker-guide-to-grpo","title":"Hacker Guide to GRPO","text":""},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#introduction","title":"Introduction","text":"<p>Group Relative Policy Optimization (GRPO) represents a significant advancement in the field of Large Language Model (LLM) fine-tuning. Traditional reinforcement learning approaches like Proximal Policy Optimization (PPO) have been widely used for fine-tuning LLMs, but they often come with substantial computational overhead due to their requirement for value function estimation and complex reward calculation mechanisms. GRPO, introduced by DeepSeek, offers an elegant solution to these challenges. Instead of relying on a value function, GRPO uses a group-based approach to estimate advantages, which significantly reduces both computational complexity and memory requirements. This makes it particularly attractive for researchers and practitioners working with limited computational resources. The key innovation of GRPO lies in its group-relative reward mechanism. Rather than comparing policy updates against an absolute baseline, it evaluates performance relative to a group of samples. This approach not only simplifies the training process but also provides more stable and efficient learning, especially for tasks requiring complex reasoning like mathematical problem-solving.</p> <p>This guide will walk you through implementing GRPO for fine-tuning language models, with a focus on practical applications and efficient deployment.</p>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#initial-setup","title":"Initial Setup","text":"<p>First, install the required dependencies:</p> <pre><code>pip install rich trl peft datasets transformers evaluate\npip install accelerate\npip install pydantic\npip install vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\npip install wandb\npip install --upgrade \"jinja2&gt;=3.1.0\"\n</code></pre> <p>Reference implementation adapted from: Github gist</p>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#try-on-notebook","title":"Try on Notebook","text":""},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#implementation-details","title":"Implementation Details","text":"<p>Create a file named <code>grpo.py</code> with the following implementation:</p> <pre><code># train_grpo.py\nimport re\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig\nfrom trl import GRPOConfig, GRPOTrainer\n\n# Load and prep dataset\n\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n&lt;reasoning&gt;\n...\n&lt;/reasoning&gt;\n&lt;answer&gt;\n...\n&lt;/answer&gt;\n\"\"\"\n\nXML_COT_FORMAT = \"\"\"\\\n&lt;reasoning&gt;\n{reasoning}\n&lt;/reasoning&gt;\n&lt;answer&gt;\n{answer}\n&lt;/answer&gt;\n\"\"\"\n\ndef extract_xml_answer(text: str) -&gt; str:\n    answer = text.split(\"&lt;answer&gt;\")[-1]\n    answer = answer.split(\"&lt;/answer&gt;\")[0]\n    return answer.strip()\n\ndef extract_hash_answer(text: str) -&gt; str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n\n# Uncomment middle messages for 1-shot prompting\ndef get_gsm8k_questions(split = \"train\") -&gt; Dataset:\n    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n    data = data.map(lambda x: { # type: ignore\n        'prompt': [\n            {'role': 'system', 'content': SYSTEM_PROMPT},\n            #{'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n            #{'role': 'assistant', 'content': XML_COT_FORMAT.format(\n            #    reasoning=\"9 is divisble by 3 and 8 is divisible by 2, but 7 is prime.\",\n            #    answer=\"7\"\n            #)},\n            {'role': 'user', 'content': x['question']}\n        ],\n        'answer': extract_hash_answer(x['answer'])\n    }) # type: ignore\n    return data # type: ignore\n\ndataset = get_gsm8k_questions()\n\n# Reward functions\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -&gt; list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n\ndef int_reward_func(completions, **kwargs) -&gt; list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n\ndef strict_format_reward_func(completions, **kwargs) -&gt; list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^&lt;reasoning&gt;\\n.*?\\n&lt;/reasoning&gt;\\n&lt;answer&gt;\\n.*?\\n&lt;/answer&gt;\\n$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef soft_format_reward_func(completions, **kwargs) -&gt; list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"&lt;reasoning&gt;.*?&lt;/reasoning&gt;\\s*&lt;answer&gt;.*?&lt;/answer&gt;\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef count_xml(text) -&gt; float:\n    count = 0.0\n    if text.count(\"&lt;reasoning&gt;\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n&lt;/reasoning&gt;\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n&lt;answer&gt;\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n&lt;/answer&gt;\\n\")[-1])*0.001\n    if text.count(\"\\n&lt;/answer&gt;\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n&lt;/answer&gt;\")[-1]) - 1)*0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -&gt; list[float]:\n    contents = [completion[0][\"content\"] for completion in completions]\n    return [count_xml(c) for c in contents]\n\n#model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nif \"Llama\" in model_name:\n    output_dir = \"outputs/Llama-1B-GRPO\"\n    run_name = \"Llama-1B-GRPO-gsm8k\"\nelse:\n    output_dir=\"outputs/Qwen-1.5B-GRPO\"\n    run_name=\"Qwen-1.5B-GRPO-gsm8k\"\n\ntraining_args = GRPOConfig(\n    output_dir=output_dir,\n    run_name=run_name,\n    beta = 0.01,\n    learning_rate=5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type='cosine',\n    logging_steps=1,\n    bf16=True,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_generations=16,\n    max_prompt_length=256,\n    max_completion_length=786,\n    num_train_epochs=1,\n    save_steps=100,\n    max_grad_norm=0.1,\n    report_to=\"wandb\",\n    log_on_each_node=False,\n    # use_vllm=True,\n    # vllm_device=\"cuda:0\",\n    # vllm_gpu_memory_utilization= 0.4\n)\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n    task_type=\"CAUSAL_LM\",\n    lora_dropout=0.05,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n    device_map=None\n).to(\"cuda\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Use PEFT at your own risk; not working for multi-GPU training\ntrainer = GRPOTrainer(\n    model=model,\n    processing_class=tokenizer,\n    reward_funcs=[\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func],\n    args=training_args,\n    train_dataset=dataset,\n    #peft_config=peft_config\n)\ntrainer.train()\n</code></pre>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#multi-gpu-setup-guide","title":"Multi-GPU Setup Guide","text":"<ol> <li> <p>First, run <code>nvidia-smi</code> to check the number of available GPUs on your system.</p> </li> <li> <p>Ensure <code>accelerate</code> is installed:</p> </li> </ol> <pre><code>pip install accelerate\n</code></pre> <ol> <li> <p>Configuration Notes:</p> </li> <li> <p>The provided configuration works well with Qwen 0.5B, Qwen 1B, and Llama 1B models on 2 A100 gpus</p> </li> <li>Using VLLM provides significant speed improvements while maintaining similar training quality</li> <li>To enable VLLM, uncomment these lines in the GRPOConfig:</li> </ol> <pre><code>use_vllm=True,\nvllm_device=\"cuda:0\",\nvllm_gpu_memory_utilization=0.4\n</code></pre> <ol> <li> <p>VLLM Memory Usage Guidelines:</p> </li> <li> <p>For models under 1 billion parameters on an A100 GPU, you can lower <code>vllm_gpu_memory_utilization</code> to 0.2</p> </li> <li> <p>For larger models (&gt;3B parameters), test and adjust the utilization accordingly</p> </li> <li> <p>For multi-GPU training, use <code>accelerate</code>:</p> </li> </ol> <pre><code>accelerate launch grpo.py\n</code></pre>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#configuration-and-parameters-guide","title":"Configuration and Parameters Guide","text":""},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#initial-setup-and-authentication","title":"Initial Setup and Authentication","text":"<p>Before running the training, you'll need to set up authentication for both Weights &amp; Biases (wandb) and Hugging Face:</p> <pre><code># Login to Weights &amp; Biases\nwandb login\n\n# Login to Hugging Face\nhuggingface-cli login\n</code></pre>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#understanding-grpo-parameters","title":"Understanding GRPO Parameters","text":"<p>Let's examine each parameter in detail:</p> <ol> <li> <p>Basic Configuration</p> </li> <li> <p><code>output_dir</code>: Directory where model checkpoints and logs will be saved</p> </li> <li><code>run_name</code>: Name of your training run, used for logging and organization</li> <li> <p><code>beta</code>: Controls the strength of the policy update. Values between 0.01 and 0.05 typically yield better results. This parameter is crucial for training stability.</p> </li> <li> <p>Optimization Parameters</p> </li> <li> <p><code>learning_rate</code>: Set to 5e-6 by default. This relatively small learning rate helps prevent destructive updates</p> </li> <li><code>adam_beta1</code> (0.9) and <code>adam_beta2</code> (0.99): Momentum parameters for the Adam optimizer</li> <li><code>weight_decay</code> (0.1): Helps prevent overfitting</li> <li><code>warmup_ratio</code> (0.1): Portion of training used for learning rate warmup</li> <li> <p><code>lr_scheduler_type</code>: Uses 'cosine' schedule for smooth learning rate decay</p> </li> <li> <p>Training Configuration</p> </li> <li><code>logging_steps</code>: Set to 1 for maximum visibility into training progress</li> <li><code>bf16</code>: Enable bfloat16 training for better memory efficiency</li> <li><code>per_device_train_batch_size</code>: Start with 1 and adjust based on memory</li> <li><code>gradient_accumulation_steps</code>: Set to 4 by default, can be adjusted for memory constraints</li> <li><code>num_generations</code>: Number of generations per prompt (16 by default)</li> <li><code>max_prompt_length</code>: Maximum token length for input prompts (256 by default)</li> <li><code>max_completion_length</code>: Maximum token length for completions (786 by default)</li> <li><code>num_train_epochs</code>: Number of training epochs</li> <li><code>save_steps</code>: Frequency of saving checkpoints</li> <li><code>max_grad_norm</code>: Gradient clipping threshold</li> </ol>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#memory-management-and-optimization","title":"Memory Management and Optimization","text":"<p>When encountering CUDA out of memory errors, consider these adjustments in order of priority:</p> <ol> <li>Reduce batch size (<code>per_device_train_batch_size</code>)</li> <li>Decrease gradient accumulation steps</li> <li>Lower the number of generations (<code>num_generations</code>)</li> </ol> <p>However, note that higher numbers of generations often lead to better and faster results. Finding the right balance for your hardware is key.</p>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#inputoutput-length-considerations","title":"Input/Output Length Considerations","text":"<p>Adjust <code>max_prompt_length</code> and <code>max_completion_length</code> based on your specific use case:</p> <pre><code># For short Q&amp;A tasks\nmax_prompt_length = 128\nmax_completion_length = 256\n\n# For complex reasoning tasks\nmax_prompt_length = 512\nmax_completion_length = 1024\n\n# For long-form content generation\nmax_prompt_length = 1024\nmax_completion_length = 2048\n</code></pre>"},{"location":"LLM/HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/#reward-function-design","title":"Reward Function Design","text":"<p>Reward modeling is a critical aspect of GRPO training. Poor reward functions can lead to reward hacking, where the model optimizes for the reward in unintended ways. Here are key principles for reward function design:</p> <ol> <li>Magnitude Balancing: Ensure reward magnitudes are appropriately scaled</li> </ol> <pre><code>def balanced_reward_func(completions, **kwargs) -&gt; list[float]:\n    # Primary task reward: scale of 1.0\n    primary_reward = calculate_primary_reward()\n\n    # Secondary objectives: smaller scale\n    format_reward = check_format() * 0.2\n    quality_reward = assess_quality() * 0.3\n\n    return primary_reward + format_reward + quality_reward\n</code></pre> <ol> <li>Reward Composition: Combine multiple objectives carefully</li> </ol> <pre><code>def composite_reward_func(completions, **kwargs) -&gt; list[float]:\n    rewards = []\n    for completion in completions:\n        # Base reward for task completion\n        reward = assess_task_completion(completion)\n\n        # Penalties for unwanted behaviors\n        if contains_repetition(completion):\n            reward *= 0.8\n\n        # Bonus for exceptional quality\n        if meets_quality_threshold(completion):\n            reward *= 1.2\n\n        rewards.append(reward)\n    return rewards\n</code></pre> <p>The optimal reward function configuration often requires intuition and empirical testing. Start with simple reward functions and gradually add complexity while monitoring training dynamics.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT/","title":"Hands on with SFT","text":"<p>Comprehensive Hands On with Supervised Fine-Tuning for LLMs</p> <p>This section provides a detailed exploration of Supervised Fine-Tuning (SFT) for Large Language Models (LLMs), covering its definition, process, applications, challenges, and best practices. It aims to offer a thorough understanding for researchers, developers, and practitioners, building on the key points and expanding with technical details and examples, as of 10:53 PM IST on Monday, February 24, 2025.</p> <p>Definition and Context</p> <p>Large Language Models (LLMs) are neural networks trained on vast text corpora using self-supervision, such as predicting the next word in a sequence. Examples include models like GPT-3, BERT, and RoBERTa, which are initially trained without explicit labels. Supervised Fine-Tuning, however, involves taking these pre-trained models and further training them on a labeled dataset for a specific task or domain. This process, often referred to as SFT, uses labeled data\u2014pairs of inputs and their corresponding outputs\u2014to adapt the model\u2019s weights, enabling it to learn task-specific patterns and nuances.</p> <p>This differs from unsupervised fine-tuning, which uses unlabeled data (e.g., masked language modeling), and reinforcement learning-based fine-tuning, such as Reinforcement Learning from Human Feedback (RLHF), which optimizes based on a reward signal. SFT is particularly effective when labeled data is available, making it a straightforward and powerful method for task-specific adaptation.</p> <p>Importance and Motivation</p> <p>Pre-trained LLMs are generalists, capable of handling a wide range of language tasks but often underperforming on specific applications without further tuning. SFT addresses this by tailoring the model to excel in areas like text classification, named entity recognition, question-answering, summarization, translation, and chatbot development. For instance, a model fine-tuned for medical terminology can interpret and generate domain-specific jargon better than a generic model, enhancing its utility in healthcare applications.</p> <p>The importance lies in its efficiency and adaptability. As noted in resources like SuperAnnotate: Fine-tuning large language models (LLMs) in 2024, SFT can significantly improve performance with relatively small datasets, often requiring 50-100,000 examples for multi-task learning or just a few hundred to thousands for task-specific fine-tuning. This efficiency is crucial for businesses with limited data and computational resources, making LLMs accessible for specialized applications.</p> <p>Process and Techniques</p> <p>The process of SFT can be broken down into several stages, each critical for success:</p> <ol> <li>Data Collection and Preparation:</li> <li>Gather a labeled dataset relevant to the task, such as prompt-response pairs for instruction fine-tuning or input-output pairs for classification. Open-source datasets like GPT-4all Dataset, AlpacaDataCleaned, and databricks-dolly-15k are commonly used.</li> <li>Preprocess the data by cleaning, tokenizing, and formatting it to ensure compatibility with the model. This step is vital, as data quality directly impacts performance, with challenges like inconsistencies, bias, and missing values needing attention (as highlighted in Kili Technology: What is LLM Fine-Tuning?).</li> <li>Model Selection:</li> <li>Choose a pre-trained LLM based on task requirements, model size, and computational resources. Larger models like GPT-3 offer higher accuracy but require more resources, while smaller models may suffice for specific tasks. Factors like data type, desired outcomes, and budget are crucial, as discussed in Sama: Supervised Fine-Tuning: How to choose the right LLM.</li> <li>Fine-Tuning:</li> <li>Train the model on the task-specific dataset using supervised learning techniques. The model\u2019s weights are adjusted based on the gradients derived from a task-specific loss function, measuring the difference between predictions and ground truth labels. Optimization algorithms like gradient descent are used over multiple epochs to adapt the model.</li> <li>Several techniques enhance this process:<ul> <li>Instruction Fine-Tuning: Trains the model with examples that include instructions, such as \u201csummarize this text,\u201d to improve task-specific responses (e.g., SuperAnnotate: Fine-tuning large language models (LLMs) in 2024).</li> <li>Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) reduce the number of trainable parameters, making fine-tuning more efficient. LoRA adds adapters with weights, freezing the rest, while QLoRA quantizes weights to 4-bit precision, reducing memory usage (as detailed in Medium: Supervised Fine-tuning: customizing LLMs).</li> <li>Batch Packing: Combines inputs to increase batch capabilities, optimizing computational resources (mentioned in the same Medium article).</li> <li>Half Fine-Tuning (HFT): Freezes half the parameters per round while updating the other half, balancing knowledge retention and new skill acquisition, as noted in arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs.</li> </ul> </li> <li>Evaluation:</li> <li>Test the fine-tuned model on a validation set to assess performance metrics like accuracy, F1 score, or BLEU for translation tasks. This step ensures the model generalizes well to unseen data.</li> <li>Deployment:</li> <li>Once validated, deploy the model for real-world use, integrating it into applications like chatbots, content generators, or customer support systems.</li> </ol> <p>Applications and Examples</p> <p>SFT is versatile, applicable to a wide range of tasks:</p> <ul> <li>Text Classification: Classifying documents into categories, such as sentiment analysis on movie reviews.</li> <li>Named Entity Recognition: Identifying entities like names, dates, and locations in text.</li> <li>Question-Answering: Providing accurate answers to specific queries, enhancing virtual assistants.</li> <li>Summarization: Generating concise summaries of longer texts, useful for news aggregation.</li> <li>Translation: Translating text between languages, improving multilingual communication.</li> <li>Chatbots: Creating conversational agents for specific domains, like customer support or healthcare.</li> </ul> <p>A practical example is fine-tuning a pre-trained model for a science educational platform. Initially, it might answer \u201cWhy is the sky blue?\u201d with a simple \u201cBecause of the way the atmosphere scatters sunlight.\u201d After SFT with labeled data, it provides a detailed response: \u201cThe sky appears blue because of Rayleigh scattering... blue light has a shorter wavelength and is scattered... causing the sky to take on a blue hue\u201d (SuperAnnotate: Fine-tuning large language models (LLMs) in 2024).</p> <p>Another example is fine-tuning RoBERTa for sentiment analysis, where the model learns from labeled movie reviews to classify sentiments as positive, negative, or neutral, significantly improving accuracy compared to the base model.</p> <p>Challenges and Best Practices</p> <p>Despite its benefits, SFT faces several challenges:</p> <ul> <li>Catastrophic Forgetting: The model may forget general knowledge while focusing on task-specific learning, a concern addressed by methods like Half Fine-Tuning (HFT) (arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs).</li> <li>Data Quality: Ensuring high-quality labeled data is crucial, with challenges like inconsistencies, bias, and data scarcity in specific domains. Automated tools like the Kili app can help streamline data curation (Kili Technology: What is LLM Fine-Tuning?).</li> <li>Computational Resources: Fine-tuning large models is resource-intensive, necessitating efficient methods like PEFT to reduce costs.</li> </ul> <p>Best practices include:</p> <ul> <li>Choosing the Right Model: Consider model size, performance on similar tasks, and computational resources. Larger models offer higher accuracy but require more resources (Sama: Supervised Fine-Tuning: How to choose the right LLM).</li> <li>Data Preparation: Ensure the dataset is clean, representative, and sufficient, with splits for training, validation, and testing. For instance, 50-100,000 examples may be needed for multi-task learning (SuperAnnotate: Fine-tuning large language models (LLMs) in 2024).</li> <li>Hyperparameter Tuning: Optimize learning rate, batch size, and number of epochs to avoid overfitting or underfitting, as discussed in Data Science Dojo: Fine-tuning LLMs 101.</li> <li>Use Parameter-Efficient Methods: Techniques like LoRA and QLoRA can reduce trainable parameters by up to 10,000 times, making fine-tuning feasible on limited hardware (Medium: Supervised Fine-tuning: customizing LLMs).</li> </ul> <p>Comparative Analysis and Recent Advancements</p> <p>SFT contrasts with other methods like unsupervised fine-tuning (using unlabeled data) and RLHF (optimizing based on rewards). Its advantage lies in its direct use of labeled data, making it suitable for tasks with clear input-output mappings. Recent advancements, such as the TRL library and AutoTrain Advanced tool by Hugging Face, facilitate the process, offering resources for developers (Hugging Face: Fine-Tuning LLMs: Supervised Fine-Tuning and Reward Modelling).</p> <p>Techniques like instruction fine-tuning and PEFT are gaining traction, with research showing significant performance improvements. For example, QLoRA can fine-tune a high-quality chatbot in 24 hours on a single GPU, demonstrating efficiency (arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs).</p> <p>Hands-On Code for SFT Using Hugging Face Libraries</p> <p>Given the focus on hands-on code using Hugging Face\u2019s transformers, PEFT, bitsandbytes, and TRL libraries, here\u2019s a detailed example for fine-tuning an LLM, specifically using gpt2 as the base model, with comments for alternative models. This example demonstrates how to format data in prompt format and perform SFT.</p> <p>Environment Setup</p> <p>First, install the necessary libraries:</p> <p>bash</p> <pre><code>pip install transformers peft bitsandbytes trl\n</code></pre> <p>Data Preparation</p> <p>The data for SFT should be in a format where each sample consists of an input (prompt) and the corresponding output (completion). For instruction-following tasks, the prompt might be \u201cInstruction: Summarize this text. Text: [some text]\u201d and the output is the summary. Popular datasets include GPT-4all Dataset, AlpacaDataCleaned, and databricks-dolly-15k.</p> <p>For this example, let\u2019s create a small dataset:</p> <p>python</p> <pre><code>from datasets import Dataset\n\ndata = [\n    {\"instruction\": \"What is the capital of France?\", \"output\": \"Paris\"},\n    {\"instruction\": \"What is the largest planet in our solar system?\", \"output\": \"Jupiter\"},\n]\ndataset = Dataset.from_list(data)\n</code></pre> <p>The dataset should have columns for the prompt (e.g., \u201cinstruction\u201d) and completion (e.g., \u201coutput\u201d), which will be specified in the trainer.</p> <p>Model Selection and Fine-Tuning</p> <p>Choose a pre-trained model from Hugging Face\u2019s model hub. For this example, we\u2019ll use gpt2, but other models like gpt2-large, bloom-560m, or distilbert-base-uncased can be used (commented out for reference):</p> <p>python</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Choose the model (uncomment other options as needed)\nmodel_name = \"gpt2\"\n# model_name = \"gpt2-large\"  # Larger model for better performance\n# model_name = \"bloom-560m\"  # Another option for decoder-only models\n# model_name = \"distilbert-base-uncased\"  # For encoder-based tasks, though less common for SFT\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# For larger models, use bitsandbytes for quantization to save memory\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n)\n# model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)  # Uncomment for 8-bit loading\n\n# Set up PEFT for parameter-efficient fine-tuning using LoRA\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n)\nmodel = get_peft_model(model, lora_config)\n</code></pre> <p>Training with TRL\u2019s SFTTrainer</p> <p>Use TRL\u2019s SFTTrainer for supervised fine-tuning, specifying the prompt and completion columns:</p> <p>python</p> <pre><code>from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"path/to/output/dir\",\n    overwrite_output_dir=True,\n    num_train_steps=1000,\n    per_device_train_batch_size=4,\n    learning_rate=1e-4,\n    gradient_accumulation_steps=4,\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=2,\n    fp16=True,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    args=training_args,\n    tokenizer=tokenizer,\n    prompt_column=\"instruction\",\n    completion_column=\"output\",\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.model.save_pretrained(\"path/to/save/model\")\n</code></pre> <p>This code demonstrates how to use transformers, PEFT, and TRL for SFT, with bitsandbytes for optional quantization. The SFTTrainer handles the formatting of prompts and completions, making the process streamlined.</p> <p>Evaluation and Deployment</p> <p>After training, evaluate the model on a validation set using metrics like accuracy, F1 score, or BLEU, depending on the task. For deployment, integrate the model into applications like chatbots or content generators, ensuring it performs well in real-world scenarios.</p> <p>Summary Table of Key Techniques</p> Technique Description Use Case LoRA (Low-Rank Adaptation) Updates fewer parameters, making fine-tuning efficient Resource-constrained environments QLoRA (Quantized LoRA) Combines LoRA with 4-bit quantization for memory efficiency Large model fine-tuning Instruction Fine-Tuning Trains with examples including instructions for task-specific responses Chatbots, question-answering Batch Packing Combines inputs to optimize computational resources High-throughput training Half Fine-Tuning (HFT) Freezes half parameters per round to balance knowledge retention and learning Preventing catastrophic forgetting <p>This table summarizes the key techniques discussed, aiding in understanding their applications.</p> <p>Formatting the Dataset for Instruction Fine-Tuning</p> <p>Proper dataset formatting is crucial for SFT. Using the TokenBender/code_instructions_122k_alpaca_style dataset, we\u2019ll convert instruction-output pairs into a \"prompt\" format tailored for instruction fine-tuning. Here\u2019s how:</p> <p>Step 1: Load the Dataset</p> <p>python</p> <pre><code>from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n</code></pre> <p>This dataset contains 122,000 entries with columns: instruction, input, output, and text. For example:</p> <ul> <li>Instruction: \"Create a function to calculate the sum of a sequence of integers\"</li> <li>Input: \"[1, 2, 3, 4, 5]\"</li> <li>Output: \"# Python code\\ndef sum_sequence(sequence):\\n sum = 0\\n for num in sequence:\\n sum += num\\n return sum\"</li> </ul> <p>Step 2: Define the Prompt Generation Function</p> <p>We\u2019ll create a generate_prompt function to format each entry into a structured prompt, distinguishing user instructions from model responses:</p> <p>python</p> <pre><code>def generate_prompt(data_point):\n    \"\"\"Generate a prompt from instruction, input, and output.\"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n    if data_point['input'] and data_point['input'] != \"Not applicable\":\n        text = f\"&lt;start_of_turn&gt;user {prefix_text}{data_point['instruction']} here are the inputs {data_point['input']} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point['output']} &lt;end_of_turn&gt;\"\n    else:\n        text = f\"&lt;start_of_turn&gt;user {prefix_text}{data_point['instruction']} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point['output']} &lt;end_of_turn&gt;\"\n    return text\n</code></pre> <ul> <li>With Input: Includes the input in the prompt (e.g., \"[1, 2, 3, 4, 5]\").</li> <li>Without Input: Omits the input section if it\u2019s empty or \"Not applicable\".</li> </ul> <p>Step 3: Add Prompt Column, Shuffle, Tokenize, and Split</p> <p>python</p> <pre><code># Add \"prompt\" column\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\n\n# Load tokenizer (example with GPT-2)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # Set padding token\n\n# Shuffle and tokenize\ndataset = dataset.shuffle(seed=1234)\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n\n# Split into train (80%) and test (20%)\ndataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n</code></pre> <p>Resulting Prompt Example</p> <p>For the first entry:</p> <p>plaintext</p> <pre><code>&lt;start_of_turn&gt;user Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\nCreate a function to calculate the sum of a sequence of integers here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model # Python code\ndef sum_sequence(sequence):\n sum = 0\n for num in sequence:\n sum += num\n return sum &lt;end_of_turn&gt;\n</code></pre> <p>This format helps the model learn to associate instructions (and inputs) with correct outputs.</p> <p>Example 1: Fine-Tuning for Code Generation</p> <p>Objective</p> <p>Fine-tune a model to generate Python code from natural language instructions using the dataset above.</p> <p>Environment Setup</p> <p>Install required libraries:</p> <p>bash</p> <pre><code>pip install transformers peft bitsandbytes trl datasets\n</code></pre> <p>Code Implementation</p> <p>python</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, get_peft_model\n\n# Load model and tokenizer\nmodel_name = \"gpt2\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Apply LoRA for efficiency\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"c_attn\"],  # GPT-2 specific\n    lora_dropout=0.05,\n    bias=\"none\"\n)\nmodel = get_peft_model(model, lora_config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./code_gen_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=100,\n    save_steps=500,\n)\n\n# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    args=training_args,\n    tokenizer=tokenizer,\n    dataset_text_field=\"prompt\",\n    max_seq_length=512\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.model.save_pretrained(\"./code_gen_model_final\")\n</code></pre> <p>Application Deployment</p> <p>Usage: Send a POST request with {\"instruction\": \"Create a function to reverse a list\", \"input\": \"[1, 2, 3]\"} to get Python code as output.</p> <p>Example 2: Fine-Tuning for Text Summarization</p> <p>Objective</p> <p>Fine-tune a model to summarize text using a synthetic dataset (for illustration).</p> <p>Dataset Preparation</p> <p>Create a small dataset:</p> <p>python</p> <pre><code>data = [\n    {\"instruction\": \"Summarize the following text\", \"input\": \"The quick brown fox jumps over the lazy dog repeatedly.\", \"output\": \"The fox repeatedly jumps over the dog.\"},\n    {\"instruction\": \"Summarize the following text\", \"input\": \"A long story about a hero saving the world.\", \"output\": \"A hero saves the world.\"}\n]\nfrom datasets import Dataset\ndataset = Dataset.from_list(data)\ntext_column = [generate_prompt(dp) for dp in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\ndataset = dataset.shuffle(seed=1234).map(lambda x: tokenizer(x[\"prompt\"], truncation=True, padding=\"max_length\", max_length=128), batched=True)\ntrain_data = dataset  # Small dataset, no split\n</code></pre> <p>Fine-Tuning</p> <p>python</p> <pre><code>model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")  # Smaller model\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = get_peft_model(model, lora_config)  # Reuse LoRA config\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    args=TrainingArguments(\n        output_dir=\"./summary_model\",\n        num_train_epochs=3,\n        per_device_train_batch_size=2,\n        learning_rate=2e-4,\n        fp16=True\n    ),\n    tokenizer=tokenizer,\n    dataset_text_field=\"prompt\",\n    max_seq_length=128\n)\n\ntrainer.train()\ntrainer.model.save_pretrained(\"./summary_model_final\")\n</code></pre> <p>Conclusion</p> <p>Supervised Fine-Tuning is a pivotal technique for adapting pre-trained LLMs to specific tasks, enhancing their performance and utility across various domains. By understanding its process, leveraging efficient techniques like PEFT and TRL, and addressing challenges, developers can unlock the full potential of LLMs, making them indispensable tools for specialized applications. This comprehensive approach ensures both theoretical insight and practical applicability, supported by a wealth of resources and examples.</p> <p>Key Citations</p> <ul> <li>SuperAnnotate Fine-tuning large language models LLMs in 2024</li> <li>Medium Supervised Fine-tuning customizing LLMs</li> <li>arXiv The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs</li> <li>Hugging Face Fine-Tuning LLMs Supervised Fine-Tuning and Reward Modelling</li> <li>Sama Supervised Fine-Tuning How to choose the right LLM</li> <li>Nebius What is supervised fine-tuning in LLMs</li> <li>Turing Fine-Tuning LLMs Overview Methods Best Practices</li> <li>Data Science Dojo Fine-tuning LLMs 101</li> <li>Kili Technology What is LLM Fine-Tuning Everything You Need to Know 2023 Guide</li> <li>GPT-4all Dataset for fine-tuning</li> <li>AlpacaDataCleaned for fine-tuning</li> <li>databricks-dolly-15k for fine-tuning</li> </ul>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/","title":"Finetune Any LLM with SFT","text":"In\u00a0[\u00a0]: Copied! <pre># Install Pytorch &amp; other libraries\n%pip install \"torch==2.4.0\" tensorboard\n\n# Install Hugging Face libraries\n%pip install  --upgrade \\\n  \"transformers==4.44.2\" \\\n  \"datasets==2.21.0\" \\\n  \"accelerate==0.33.0\" \\\n  \"evaluate==0.4.2\" \\\n  \"bitsandbytes==0.43.3\" \\\n  \"trl==0.9.6\" \\\n  \"peft==0.12.0\" \n</pre> # Install Pytorch &amp; other libraries %pip install \"torch==2.4.0\" tensorboard  # Install Hugging Face libraries %pip install  --upgrade \\   \"transformers==4.44.2\" \\   \"datasets==2.21.0\" \\   \"accelerate==0.33.0\" \\   \"evaluate==0.4.2\" \\   \"bitsandbytes==0.43.3\" \\   \"trl==0.9.6\" \\   \"peft==0.12.0\"  <p>If you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention.</p> <p>Note: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of <code>MAX_JOBS</code>. On the <code>g6.2xlarge</code> we used <code>4</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\n# install flash-attn\n!pip install ninja packaging\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n</pre> import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention' # install flash-attn !pip install ninja packaging !MAX_JOBS=4 pip install flash-attn --no-build-isolation <p>Installing flash attention can take quite a bit of time (10-45 minutes).</p> <p>We will use the Hugging Face Hub as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the Hugging Face for this. After you have an account, we will use the <code>login</code> util from the <code>huggingface_hub</code> package to log into our account and store our token (access key) on the disk.</p> In\u00a0[16]: Copied! <pre>from huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)\n</pre> from huggingface_hub import login  login(   token=\"\", # ADD YOUR TOKEN HERE   add_to_git_credential=True )  <pre>Token is valid (permission: write).\nCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\nToken has not been saved to git credential helper.\nYour token has been saved to /home/ubuntu/.cache/huggingface/token\nLogin successful\n</pre> <p>In our example we are going to load our open-source dataset using the \ud83e\udd17 Datasets library and then convert it into the the conversational format, where we include the schema definition in the system message for our assistant. We'll then save the dataset as jsonl file, which we can then use to fine-tune our model. We are randomly downsampling the dataset to only 10,000 samples.</p> <p>Note: This step can be different for your use case. For example, if you have already a dataset from, e.g. working with OpenAI, you can skip this step and go directly to the fine-tuning step.</p> In\u00a0[17]: Copied! <pre>from datasets import load_dataset\n\n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\nSCHEMA:\n{schema}\"\"\"\n\ndef create_conversation(sample):\n  return {\n    \"messages\": [\n      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n      {\"role\": \"user\", \"content\": sample[\"question\"]},\n      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n    ]\n  }  \n\n# Load dataset from the hub\ndataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\n\n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=2500/12500)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk \ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n</pre> from datasets import load_dataset  # Convert dataset to OAI messages system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. SCHEMA: {schema}\"\"\"  def create_conversation(sample):   return {     \"messages\": [       {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},       {\"role\": \"user\", \"content\": sample[\"question\"]},       {\"role\": \"assistant\", \"content\": sample[\"answer\"]}     ]   }    # Load dataset from the hub dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\") dataset = dataset.shuffle().select(range(12500))  # Convert dataset to OAI messages dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False) # split dataset into 10,000 training samples and 2,500 test samples dataset = dataset.train_test_split(test_size=2500/12500)  print(dataset[\"train\"][345][\"messages\"])  # save datasets to disk  dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\") dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\") <pre>Downloading readme:   0%|          | 0.00/4.43k [00:00&lt;?, ?B/s]</pre> <pre>Map:   0%|          | 0/12500 [00:00&lt;?, ? examples/s]</pre> <pre>[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_32 (date VARCHAR, attendance VARCHAR)', 'role': 'system'}, {'content': 'On what Date was the Attendance 73,405?', 'role': 'user'}, {'content': 'SELECT date FROM table_name_32 WHERE attendance = \"73,405\"', 'role': 'assistant'}]\n</pre> <pre>Creating json from Arrow format:   0%|          | 0/10 [00:00&lt;?, ?ba/s]</pre> <pre>Creating json from Arrow format:   0%|          | 0/3 [00:00&lt;?, ?ba/s]</pre> Out[17]: <pre>1187015</pre> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n</pre> from datasets import load_dataset  # Load jsonl data from disk dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\") <p>Next, we will load our LLM. For our use case we are going to use Llama 3.1 8B. But we can easily swap out the model for another model, e.g. Mistral or Mixtral models, TII Falcon, or any other LLMs by changing our <code>model_id</code> variable. We will use bitsandbytes to quantize our model to 4-bit.</p> <p>Note: Be aware the bigger the model the more memory it will require. In our example we will use the 8B version, which can be tuned on 24GB GPUs. If you have a smaller GPU.</p> <p>Correctly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In <code>trl</code> we have a convinient method called setup_chat_format, which:</p> <ul> <li>Adds special tokens to the tokenizer, e.g. <code>&lt;|im_start|&gt;</code> and <code>&lt;|im_end|&gt;</code>, to indicate the start and end of a conversation.</li> <li>Resizes the model\u2019s embedding layer to accommodate the new tokens.</li> <li>Sets the <code>chat_template</code> of the tokenizer, which is used to format the input data into a chat-like format. The default is <code>chatml</code> from OpenAI.</li> </ul> In\u00a0[2]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import setup_chat_format\n\n# Hugging Face model id\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B\" # or `mistralai/Mistral-7B-v0.1`\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'right' # to prevent warnings\n\n# # set chat template to OAI chatML, remove if you start from a fine-tuned model\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig from trl import setup_chat_format  # Hugging Face model id model_id = \"meta-llama/Meta-Llama-3.1-8B\" # or `mistralai/Mistral-7B-v0.1`  # BitsAndBytesConfig int-4 config bnb_config = BitsAndBytesConfig(     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16 )  # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(     model_id,     device_map=\"auto\",     attn_implementation=\"flash_attention_2\",     torch_dtype=torch.bfloat16,     quantization_config=bnb_config ) tokenizer = AutoTokenizer.from_pretrained(model_id) tokenizer.padding_side = 'right' # to prevent warnings  # # set chat template to OAI chatML, remove if you start from a fine-tuned model model, tokenizer = setup_chat_format(model, tokenizer) <pre>Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <p>The\u00a0<code>SFTTrainer</code>\u00a0 supports a native integration with\u00a0<code>peft</code>, which makes it super easy to efficiently tune LLMs using, e.g. QLoRA. We only need to create our\u00a0<code>LoraConfig</code>\u00a0and provide it to the trainer. Our <code>LoraConfig</code> parameters are defined based on the qlora paper and sebastian's blog post.</p> In\u00a0[3]: Copied! <pre>from peft import LoraConfig\n\n# LoRA config based on QLoRA paper &amp; Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=128,\n        lora_dropout=0.05,\n        r=256,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\", \n)\n</pre> from peft import LoraConfig  # LoRA config based on QLoRA paper &amp; Sebastian Raschka experiment peft_config = LoraConfig(         lora_alpha=128,         lora_dropout=0.05,         r=256,         bias=\"none\",         target_modules=\"all-linear\",         task_type=\"CAUSAL_LM\",  ) <p>Before we can start our training we need to define the hyperparameters (<code>TrainingArguments</code>) we want to use.</p> In\u00a0[4]: Copied! <pre>from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"code-llama-3-1-8b-text-to-sql\", # directory to save and repository id\n    num_train_epochs=3,                     # number of training epochs\n    per_device_train_batch_size=1,          # batch size per device during training\n    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=10,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to hub\n    report_to=\"tensorboard\",                # report metrics to tensorboard\n)\n</pre> from transformers import TrainingArguments  args = TrainingArguments(     output_dir=\"code-llama-3-1-8b-text-to-sql\", # directory to save and repository id     num_train_epochs=3,                     # number of training epochs     per_device_train_batch_size=1,          # batch size per device during training     gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass     gradient_checkpointing=True,            # use gradient checkpointing to save memory     optim=\"adamw_torch_fused\",              # use fused adamw optimizer     logging_steps=10,                       # log every 10 steps     save_strategy=\"epoch\",                  # save checkpoint every epoch     learning_rate=2e-4,                     # learning rate, based on QLoRA paper     bf16=True,                              # use bfloat16 precision     tf32=True,                              # use tf32 precision     max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper     warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper     lr_scheduler_type=\"constant\",           # use constant learning rate scheduler     push_to_hub=True,                       # push model to hub     report_to=\"tensorboard\",                # report metrics to tensorboard ) <p>We now have every building block we need to create our\u00a0<code>SFTTrainer</code>\u00a0to start then training our model.</p> In\u00a0[5]: Copied! <pre>from trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n</pre> from trl import SFTTrainer  max_seq_length = 2048 # max sequence length for model and packing of the dataset  trainer = SFTTrainer(     model=model,     args=args,     train_dataset=dataset,     peft_config=peft_config,     max_seq_length=max_seq_length,     tokenizer=tokenizer,     packing=True,     dataset_kwargs={         \"add_special_tokens\": False,  # We template with special tokens         \"append_concat_token\": False, # No need to add additional separator token     } ) <pre>/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:366: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n</pre> <p>Start training our model by calling the <code>train()</code> method on our <code>Trainer</code> instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.</p> In\u00a0[6]: Copied! <pre># start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model \ntrainer.save_model()\n</pre> # start training, the model will be automatically saved to the hub and the output directory trainer.train()  # save model  trainer.save_model() <pre>`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n</pre>        [186/186 2:19:28, Epoch 2/3]      Step Training Loss 10 0.912000 20 0.635400 30 0.605700 40 0.583800 50 0.568800 60 0.554400 70 0.508300 80 0.484600 90 0.475800 100 0.484700 110 0.481500 120 0.478700 130 0.436100 140 0.391300 150 0.396100 160 0.396200 170 0.404600 180 0.408300 <p> </p> <pre>/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n</pre> <pre>events.out.tfevents.1724931649.ip-172-31-27-157.2377.0:   0%|          | 0.00/10.1k [00:00&lt;?, ?B/s]</pre> <p>The training with Flash Attention for 3 epochs with a dataset of 10k samples took 02:05:58 on a <code>g6.2xlarge</code>. The instance costs <code>1,212$/h</code> which brings us to a total cost of only <code>1.8$</code>.</p> In\u00a0[7]: Copied! <pre># free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n</pre> # free the memory again del model del trainer torch.cuda.empty_cache() In\u00a0[8]: Copied! <pre>#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\nfrom peft import AutoPeftModelForCausalLM\n\n# Load PEFT model on CPU\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n)  \n# Merge LoRA and base model and save\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")\n</pre> #### COMMENT IN TO MERGE PEFT AND BASE MODEL #### from peft import AutoPeftModelForCausalLM  # Load PEFT model on CPU model = AutoPeftModelForCausalLM.from_pretrained(     args.output_dir,     torch_dtype=torch.float16,     low_cpu_mem_usage=True, )   # Merge LoRA and base model and save merged_model = model.merge_and_unload() merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\") <pre>Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> In\u00a0[1]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n\nmodel_id = \"./code-llama-3-1-8b-text-to-sql\"\n\n# Load Model with PEFT adapter\nmodel = AutoModelForCausalLM.from_pretrained(\n  model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n</pre> import torch from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM  model_id = \"./code-llama-3-1-8b-text-to-sql\"  # Load Model with PEFT adapter model = AutoModelForCausalLM.from_pretrained(   model_id,   device_map=\"auto\",   torch_dtype=torch.float16 ) tokenizer = AutoTokenizer.from_pretrained(model_id) # load into pipeline pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) <pre>/opt/conda/envs/pytorch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/envs/pytorch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n</pre> <pre>Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <p>Let\u2019s load our test dataset try to generate an instruction.</p> In\u00a0[2]: Copied! <pre>from datasets import load_dataset \nfrom random import randint\n\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Test on sample \nprompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n</pre> from datasets import load_dataset  from random import randint   # Load our test dataset eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\") rand_idx = randint(0, len(eval_dataset))  # Test on sample  prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True) outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)  print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\") print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\") print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\") <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n</pre> <pre>Query:\nWhat is the Highest first elected year that has a district of 06.0 6, and a committee of economic matters?\nOriginal Answer:\nSELECT MAX(first_elected) FROM table_name_99 WHERE district = \"06.0 6\" AND committee = \"economic matters\"\nGenerated Answer:\nSELECT MAX(first_elected) FROM table_name_99 WHERE district = \"06.0 6\" AND committee = \"economic matters\"\n</pre> <p>Nice! Our model was able to generate a SQL query based on the natural language instruction. Lets evaluate our model on the full 2,500 samples of our test dataset. Note: As mentioned above, evaluating generative models is not a trivial task. In our example we used the accuracy of the generated SQL based on the ground truth SQL query as our metric. An alternative way could be to automatically execute the generated SQL query and compare the results with the ground truth. This would be a more accurate metric but requires more work to setup.</p> In\u00a0[3]: Copied! <pre>from tqdm import tqdm\n\n\ndef evaluate(sample):\n    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n        return 1 \n    else:\n        return 0\n\nsuccess_rate = []\nnumber_of_eval_samples = 1000\n# iterate over eval dataset and predict\nfor s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n    success_rate.append(evaluate(s))\n\n# compute accuracy\naccuracy = sum(success_rate)/len(success_rate)\n\nprint(f\"Accuracy: {accuracy*100:.2f}%\")  \n        \n</pre> from tqdm import tqdm   def evaluate(sample):     prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)     outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)     predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()     if predicted_answer == sample[\"messages\"][2][\"content\"]:         return 1      else:         return 0  success_rate = [] number_of_eval_samples = 1000 # iterate over eval dataset and predict for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):     success_rate.append(evaluate(s))  # compute accuracy accuracy = sum(success_rate)/len(success_rate)  print(f\"Accuracy: {accuracy*100:.2f}%\")            <pre>  1%|          | 9/1000 [00:12&lt;21:30,  1.30s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [24:27&lt;00:00,  1.47s/it]</pre> <pre>Accuracy: 80.00%\n</pre> <pre>\n</pre> <p>We evaluated our model on 1000 samples from the evaluation dataset and got an accuracy of 80.00%, which took ~25 minutes. This is quite good, but as mentioned you need to take this metric with a grain of salt. It would be better if we could evaluate our model by running the qureies against a real database and compare the results. Since there might be different \"correct\" SQL queries for the same instruction. There are also several ways on how we could improve the performance by using few-shot learning, using RAG, Self-healing to generate the SQL query.</p> In\u00a0[1]: Copied! <pre>%%bash \n# model=$PWD/{args.output_dir} # path to model\nmodel=$(pwd)/code-llama-3-1-8b-text-to-sql # path to model\nnum_shard=1             # number of shards\nmax_input_length=1024   # max input length\nmax_total_tokens=2048   # max total tokens\n\ndocker run -d --name tgi --gpus all -ti -p 8080:80 \\\n  -e MODEL_ID=/workspace \\\n  -e NUM_SHARD=$num_shard \\\n  -e MAX_INPUT_LENGTH=$max_input_length \\\n  -e MAX_TOTAL_TOKENS=$max_total_tokens \\\n  -v $model:/workspace \\\n  ghcr.io/huggingface/text-generation-inference:2.2.0\n</pre> %%bash  # model=$PWD/{args.output_dir} # path to model model=$(pwd)/code-llama-3-1-8b-text-to-sql # path to model num_shard=1             # number of shards max_input_length=1024   # max input length max_total_tokens=2048   # max total tokens  docker run -d --name tgi --gpus all -ti -p 8080:80 \\   -e MODEL_ID=/workspace \\   -e NUM_SHARD=$num_shard \\   -e MAX_INPUT_LENGTH=$max_input_length \\   -e MAX_TOTAL_TOKENS=$max_total_tokens \\   -v $model:/workspace \\   ghcr.io/huggingface/text-generation-inference:2.2.0 <pre>Unable to find image 'ghcr.io/huggingface/text-generation-inference:2.2.0' locally\n2.2.0: Pulling from huggingface/text-generation-inference\naece8493d397: Already exists\n45f7ea5367fe: Already exists\n3d97a47c3c73: Already exists\n12cd4d19752f: Already exists\nda5a484f9d74: Already exists\n4f4fb700ef54: Already exists\n43566b48e5d6: Already exists\nf165933352a8: Already exists\nf166ffc7c7b4: Already exists\n58165ae83a0e: Already exists\n074d930e1b90: Already exists\n1033b2636622: Already exists\ne0aa534acffe: Already exists\n130989d28b48: Already exists\na65ea9ebfaba: Already exists\n7225b2c46f88: Already exists\n43154e73908f: Already exists\n8f400e318724: Already exists\nf694acf6c40f: Already exists\n44fc79164bc4: Already exists\n8bc7c142e917: Already exists\n021f7d48bdcb: Already exists\nc9d01f7d10cc: Already exists\n400740bc31be: Already exists\nbd4b49ea4512: Already exists\n141228b9bdde: Already exists\n4f4fb700ef54: Already exists\n34d4a7457184: Already exists\n66e724dff43a: Already exists\n25c75c242d08: Already exists\n6a4be63c7e70: Already exists\nb2d83f4bca52: Already exists\n373c47aa4b50: Already exists\n4f4fb700ef54: Already exists\nDigest: sha256:d39d513f13727ffa9b6a4d0e949f36413b944aabc9a236c0aa2986c929906769\nStatus: Downloaded newer image for ghcr.io/huggingface/text-generation-inference:2.2.0\n</pre> <pre>42be7f00ddeb0a3214920a09a5ea303d8eb034942d7020155b6a6761fca87193\n</pre> <p>Once your container is running you can send requests using the <code>openai</code> or <code>huggingface_hub</code> sdk. Here we ll use the <code>openai</code> sdk to send a request to our inference server. If you don't have the <code>openai</code> sdk installed you can install it using <code>pip install openai</code>.</p> In\u00a0[7]: Copied! <pre>from openai import OpenAI\nfrom datasets import load_dataset\nfrom random import randint\n\n# create client \nclient = OpenAI(base_url=\"http://localhost:8080/v1\",api_key=\"-\")\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Take a random sample from the dataset and remove the last message and send it to the model\nresponse = client.chat.completions.create(\n\tmodel=\"code-llama-3-1-8b-text-to-sql\",\n\tmessages=eval_dataset[rand_idx][\"messages\"][:2],\n\tstream=False, # no streaming\n\tmax_tokens=1024,\n)\nresponse = response.choices[0].message.content\n\n# Print results\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{response}\")\n</pre> from openai import OpenAI from datasets import load_dataset from random import randint  # create client  client = OpenAI(base_url=\"http://localhost:8080/v1\",api_key=\"-\")  # Load our test dataset eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\") rand_idx = randint(0, len(eval_dataset))  # Take a random sample from the dataset and remove the last message and send it to the model response = client.chat.completions.create( \tmodel=\"code-llama-3-1-8b-text-to-sql\", \tmessages=eval_dataset[rand_idx][\"messages\"][:2], \tstream=False, # no streaming \tmax_tokens=1024, ) response = response.choices[0].message.content  # Print results print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\") print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\") print(f\"Generated Answer:\\n{response}\") <pre>Query:\nName the first elected for kentucky 1\nOriginal Answer:\nSELECT first_elected FROM table_2668378_5 WHERE district = \"Kentucky 1\"\nGenerated Answer:\nSELECT first_elected FROM table_2668378_5 WHERE district = \"Kentucky 1\"\n</pre> <p>Awesome, Don't forget to stop your container once you are done.</p> In\u00a0[8]: Copied! <pre>!docker stop tgi\n</pre> !docker stop tgi <pre>tgi\n</pre>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#supervised-finetuning-with-llms","title":"Supervised Finetuning with LLMs\u00b6","text":"<p>Large Language Models or LLMs have seen a lot of progress in the last year. We went from now ChatGPT competitor to a whole zoo of LLMs, including Meta AI's Llama 3, Mistrals Mistral &amp; Mixtral models, TII Falcon, and many more. Those LLMs can be used for a variety of tasks, including chatbots, question answering, summarization without any additional training. However, if you want to customize a model for your application. You may need to fine-tune the model on your data to achieve higher quality results than prompting or saving cost by training smaller models more efficient model.</p> <p>This blog post walks you thorugh how to fine-tune open LLMs using Hugging Face TRL, Transformers &amp; datasets in 2024. In the blog, we are going to:</p> <ol> <li>Define our use case</li> <li>Setup development environment</li> <li>Create and prepare the dataset</li> <li>Fine-tune LLM using <code>trl</code> and the <code>SFTTrainer</code></li> <li>Test and evaluate the LLM</li> <li>Deploy the LLM for Production</li> </ol> <p>Note: This blog was created to run on consumer size GPUs (24GB), e.g. NVIDIA A10G or RTX 4090/3090, but can be easily adapted to run on bigger GPUs.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#1-define-our-use-case","title":"1. Define our use case\u00b6","text":"<p>When fine-tuning LLMs, it is important you know your use case and the task you want to solve. This will help you to choose the right model or help you to create a dataset to fine-tune your model. If you haven't defined your use case yet. You might want to go back to the drawing board. I want to mention that not all use cases require fine-tuning and it is always recommended to evaluate and try out already fine-tuned models or API-based models before fine-tuning your own model.</p> <p>As an example, we are going to use the following use case:</p> <p>We want to fine-tune a model, which can generate SQL queries based on a natural language instruction, which can then be integrated into our BI tool. The goal is to reduce the time it takes to create a SQL query and make it easier for non-technical users to create SQL queries.</p> <p>Text to SQL can be a good use case for fine-tuning LLMs, as it is a complex task that requires a lot of (internal) knowledge about the data and the SQL language.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#2-setup-development-environment","title":"2. Setup development environment\u00b6","text":"<p>Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#3-create-and-prepare-the-dataset","title":"3. Create and prepare the dataset\u00b6","text":"<p>Once you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. The dataset should be a diverse set of demonstrations of the task you want to solve. There are several ways to create such a dataset, including:</p> <ul> <li>Using existing open-source datasets, e.g., Spider</li> <li>Using LLMs to create synthetically datasets, e.g., Alpaca</li> <li>Using Humans to create datasets, e.g., Dolly.</li> <li>Using a combination of the above methods, e.g., Orca</li> </ul> <p>Each of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.</p> <p>In our example we will use an already existing dataset called sql-create-context, which contains samples of natural language instructions, schema definitions and the corresponding SQL query.</p> <p>With the latest release of <code>trl</code> we now support popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and <code>trl</code> will take care of the rest. Those formats include:</p> <ul> <li>conversational format</li> </ul> <pre>{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n</pre> <ul> <li>instruction format</li> </ul> <pre>{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n</pre>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#4-fine-tune-llm-using-trl-and-the-sfttrainer","title":"4. Fine-tune LLM using <code>trl</code> and the <code>SFTTrainer</code>\u00b6","text":"<p>We are now ready to fine-tune our model. We will use the SFTTrainer from <code>trl</code> to fine-tune our model. The <code>SFTTrainer</code> makes it straightfoward to supervise fine-tune open LLMs. The <code>SFTTrainer</code> is a subclass of the <code>Trainer</code> from the <code>transformers</code> library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including:</p> <ul> <li>Dataset formatting, including conversational and instruction format</li> <li>Training on completions only, ignoring prompts</li> <li>Packing datasets for more efficient training</li> <li>PEFT (parameter-efficient fine-tuning) support including Q-LoRA</li> <li>Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)</li> </ul> <p>We will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out\u00a0Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\u00a0blog post.</p> <p>Now, lets get started! \ud83d\ude80</p> <p>First, we need to load our dataset from disk.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#merge-lora-adapter-in-to-the-original-model","title":"Merge LoRA adapter in to the original model\u00b6","text":"<p>When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the <code>merge_and_unload</code> method and then save the model with the <code>save_pretrained</code> method. This will save a default model, which can be used for inference.</p> <p>Note: This requires &gt; 30GB CPU Memory.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#4-test-model-and-run-inference","title":"4. Test Model and run Inference\u00b6","text":"<p>After the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.</p> <p>Note: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out Evaluate LLMs and RAG a practical example using Langchain and Hugging Face blog post.</p>"},{"location":"LLM/HandsOnWithFinetuning/SFT/SFT_finetuning_notebook/#6-deploy-the-llm-for-production","title":"6. Deploy the LLM for Production\u00b6","text":"<p>You can now deploy your model to production. For deploying open LLMs into production we recommend using Text Generation Inference (TGI). TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching for the most popular open LLMs, including Llama, Mistral, Mixtral, StarCoder, T5 and more. Text Generation Inference is used by companies as IBM, Grammarly, Uber, Deutsche Telekom, and many more. There are several ways to deploy your model, including:</p> <ul> <li>Deploy LLMs with Hugging Face Inference Endpoints</li> <li>Hugging Face LLM Inference Container for Amazon SageMaker</li> </ul> <p>If you have docker installed you can use the following command to start the inference server.</p> <p>Note: Make sure that you have enough GPU memory to run the container. Restart kernel to remove all allocated GPU memory from the notebook.</p>"},{"location":"LLM/LLMArchitecture/ParameterCount/","title":"Parameter Count","text":""},{"location":"LLM/LLMArchitecture/ParameterCount/#from-7b-to-8b-parameters-understanding-weight-matrix-changes-in-llama-transformer-models","title":"From 7B to 8B Parameters: Understanding Weight Matrix Changes in LLama Transformer Models","text":"<p>URL Source: https://medium.com/@adithyask/from-7b-to-8b-parameters-understanding-weight-matrix-changes-in-llama-transformer-models-31ea7ed5fd88</p> <p>Deep Dive into the Underlying Architecture of LLama3</p> <p>Quick links for Llama3:</p> <p>GitHub repo \u2014 https://github.com/meta-llama/llama3</p> <p>Huggingface model link- https://huggingface.co/meta-llama/Meta-Llama-3-8B</p> <p>Official website \u2014 https://ai.meta.com/blog/meta-llama-3/</p> <p>Quick notes on the release:</p> <ul> <li>8 billion and 70 billion (8B and 70B) contexts</li> <li>Context length up to 8K</li> <li>Knowledge cutoff about a year ago</li> <li>English language only</li> <li>Pretrained on 15 trillion tokens, fine-tuned on 10 million human-annotated instructions (not released)</li> <li>Significantly better than Llama2, which is encouraging for all other open-source software finetunes</li> </ul>"},{"location":"LLM/LLMArchitecture/ParameterCount/#before-going-into-the-differences-at-the-weight-matrix-lets-first-understand-the-high-level-differences","title":"Before going into the differences at the weight matrix, let\u2019s first understand the high-level differences.","text":"<p>This tweet by Andrej Karpathy explains the differences in detail:</p>"},{"location":"LLM/LLMArchitecture/ParameterCount/#if-you-wondered-why-llama3-is-8b-parameters-instead-of-7b-674-b-here-are-the-changes-in-matrix-sizes-between-llama3-and-llama2","title":"If you wondered why LLama3 is 8b parameters instead of 7B (~6.74 B), here are the changes in matrix sizes between LLama3 and LLama2:","text":"<p>Embedding Layer:</p> <ul> <li>LLama3: Dimensions <code>[128256, 4096]</code></li> <li>LLama2: Dimensions <code>[32000, 4096]</code></li> <li>Difference in vocabulary size:</li> <li>LLama3 has a larger vocabulary size compared to LLama2, with 128256 tokens versus 32000 tokens. This difference in vocabulary size leads to a larger embedding matrix in LLama3.</li> </ul> <p>Output Layer (lm_head.weight):</p> <ul> <li>Both LLama3 and LLama2 have the same output layer dimensions <code>[Vocabulary Size, 4096]</code>, where the vocabulary size is dependent on the tokenization scheme. In this case:</li> <li>LLama3: Vocabulary Size = 128256</li> <li>LLama2: Vocabulary Size = 32000</li> </ul> <p>The increase in vocabulary size in LLama3 necessitates a larger embedding layer compared to LLama2, contributing significantly to the increase in total parameters observed between the two models. This change allows LLama3 to handle a larger and potentially more diverse range of tokens during processing, which can be advantageous for certain natural language processing tasks.</p> <p>In addition to the vocabulary size difference in the embedding layer, let\u2019s highlight the changes in dimensions for the <code>mlp.down_proj.weight</code>, <code>mlp.gate_proj.weight</code>, and <code>mlp.up_proj.weight</code> matrices between LLama3 and LLama2:</p> <p>MLP Down Projection (<code>**mlp.down_proj.weight**</code>):</p> <ul> <li>LLama3: Dimensions <code>[4096, 14336]</code></li> <li>LLama2: Dimensions <code>[4096, 11008]</code></li> <li>Difference in matrix size:</li> <li>LLama3 has a wider projection matrix compared to LLama2. The number of output features (14336) in LLama3 is higher than that in LLama2 (11008). This change likely enables a more complex transformation within the multi-layer perceptron (MLP) component of each transformer layer in LLama3.</li> </ul> <p>MLP Gate Projection (<code>**mlp.gate_proj.weight**</code>):</p> <ul> <li>LLama3: Dimensions <code>[14336, 4096]</code></li> <li>LLama2: Dimensions <code>[11008, 4096]</code></li> <li>Difference in matrix size:</li> <li>LLama3 has a larger input dimension for the gate projection compared to LLama2. The number of input features (14336) in LLama3 is higher than that in LLama2 (11008). This alteration can affect the capacity and expressiveness of the MLP gating mechanism within each transformer layer.</li> </ul> <p>MLP Up Projection (<code>**mlp.up_proj.weight**</code>):</p> <ul> <li>LLama3: Dimensions <code>[14336, 4096]</code></li> <li>LLama2: Dimensions <code>[11008, 4096]</code></li> <li>Difference in matrix size:</li> <li>Similar to the gate projection, LLama3 employs a larger input dimension for the up projection compared to LLama2. The number of input features (14336) in LLama3 is greater than that in LLama2 (11008), likely contributing to increased model complexity and capacity in LLama3.</li> </ul> <p>These changes in the dimensions of the projection matrices (<code>mlp.down_proj.weight</code>, <code>mlp.gate_proj.weight</code>, <code>mlp.up_proj.weight</code>) reflect adjustments made to the internal architecture of each transformer layer in LLama3 compared to LLama2. The increase in dimensions allows LLama3 to potentially capture more intricate patterns and dependencies during the processing of input sequences, which can be beneficial for handling diverse and complex language tasks.</p>"},{"location":"LLM/LLMArchitecture/ParameterCount/#weight-matrix-breakdown-and-parameter-calculation","title":"Weight Matrix Breakdown and Parameter Calculation","text":"<p>About 6 to 7 months ago, I always wondered what parameters mean and how they are determined. If you also had questions like this, the explanation below will give you an understanding of how the breakdown and calculation happens.</p>"},{"location":"LLM/LLMArchitecture/ParameterCount/#llama-3-weight-matrix-breakdown","title":"LLama 3 Weight Matrix breakdown","text":"<p>Embedding Layer (<code>**model.embed_tokens.weight**</code>):</p> <ul> <li>Dimensions: <code>[128256, 4096]</code></li> <li>Total parameters: (128256 * 4096 = 525336576)</li> </ul> <p>Each Transformer Layer (<code>**model.layers.0**</code> to <code>**model.layers.31**</code>):</p> <p>Each layer consists of several weight matrices:</p> <ul> <li><code>input_layernorm.weight</code>: <code>[4096]</code></li> <li><code>mlp.down_proj.weight</code>: <code>[4096, 14336]</code></li> <li><code>mlp.gate_proj.weight</code>: <code>[14336, 4096]</code></li> <li><code>mlp.up_proj.weight</code>: <code>[14336, 4096]</code></li> <li><code>post_attention_layernorm.weight</code>: <code>[4096]</code></li> <li><code>self_attn.k_proj.weight</code>: <code>[1024, 4096]</code></li> <li><code>self_attn.o_proj.weight</code>: <code>[4096, 4096]</code></li> <li><code>self_attn.q_proj.weight</code>: <code>[4096, 4096]</code></li> <li><code>self_attn.v_proj.weight</code>: <code>[1024, 4096]</code></li> </ul> <p>Total parameters for each layer:</p> <ul> <li><code>input_layernorm.weight</code>: (4096)</li> <li><code>mlp.down_proj.weight</code>: (4096 * 14336 = 58720256)</li> <li><code>mlp.gate_proj.weight</code>: (14336 * 4096 = 58720256)</li> <li><code>mlp.up_proj.weight</code>: (14336 * 4096 = 58720256)</li> <li><code>post_attention_layernorm.weight</code>: (4096)</li> <li><code>self_attn.k_proj.weight</code>: (1024 * 4096 = 4194304)</li> <li><code>self_attn.o_proj.weight</code>: (4096 * 4096 = 16777216)</li> <li><code>self_attn.q_proj.weight</code>: (4096 * 4096 = 16777216)</li> <li><code>self_attn.v_proj.weight</code>: (1024 * 4096 = 4194304)</li> <li>Total parameters per layer: [4096 + 58720256 + 58720256 + 58720256+ 4096 + 4194304 + 16777216 + 16777216 + 4194304 = 218112000]</li> </ul> <p>Output Layer (<code>**lm_head.weight**</code>):</p> <ul> <li>Dimensions: <code>[128256, 4096]</code></li> <li>Total parameters: (128256 * 4096 = 525336576)</li> </ul> <p>Total Parameters for 32 Layers:</p> <ul> <li>Total parameters per layer: (218112000)</li> <li>Total parameters for 32 layers: (32 * 218112000 = 6979584000)</li> </ul> <p>Input embedding Layer + Output layer = 1050673152</p> <p>Overall parameters = Total Parameters for 32 Layers + Input and Output Layers = 8030257152 (8.03 Billion Parameters)</p> <p>Therefore, the total number of parameters in this transformer architecture with 32 layers is 8,030,257,152 (8.03 B) parameters.</p>"},{"location":"LLM/LLMArchitecture/ParameterCount/#llama-2-weight-matrix-breakdown","title":"Llama 2 Weight Matrix breakdown","text":"<p>To calculate the number of parameters for the given transformer architecture, we need to consider the dimensions of each weight matrix and count the total number of elements (parameters) across all layers.</p> <p>Let\u2019s break down the calculation step by step:</p> <p>Embedding Layer (<code>**model.embed_tokens.weight**</code>):</p> <ul> <li>Dimensions: <code>[32000, 4096]</code></li> <li>Total parameters: (32000 * 4096 = 131072000)</li> </ul> <p>Each Transformer Layer (<code>**model.layers.0**</code>):</p> <p>Each layer consists of several weight matrices:</p> <ul> <li><code>input_layernorm.weight</code>: <code>[4096]</code></li> <li><code>mlp.down_proj.weight</code>: <code>[4096, 11008]</code></li> <li><code>mlp.gate_proj.weight</code>: <code>[11008, 4096]</code></li> <li><code>mlp.up_proj.weight</code>: <code>[11008, 4096]</code></li> <li><code>post_attention_layernorm.weight</code>: <code>[4096]</code></li> <li><code>self_attn.k_proj.weight</code>: <code>[4096, 4096]</code></li> <li><code>self_attn.o_proj.weight</code>: <code>[4096, 4096]</code></li> <li><code>self_attn.q_proj.weight</code>: <code>[4096, 4096]</code></li> <li><code>self_attn.v_proj.weight</code>: <code>[4096, 4096]</code></li> <li><code>self_attn.rotary_emb.inv_freq</code>: <code>[64]</code> (not counted as parameters in this context)</li> </ul> <p>Total parameters for each layer:</p> <ul> <li><code>input_layernorm.weight</code>: (4096)</li> <li><code>mlp.down_proj.weight</code>: (4096 * 11008 = 45088768)</li> <li><code>mlp.gate_proj.weight</code>: (11008 * 4096 = 45088768)</li> <li><code>mlp.up_proj.weight</code>: (11008 * 4096 = 45088768)</li> <li><code>post_attention_layernorm.weight</code>: (4096)</li> <li><code>self_attn.k_proj.weight</code>: (4096 * 4096 = 16777216)</li> <li><code>self_attn.o_proj.weight</code>: (4096 * 4096 = 16777216)</li> <li><code>self_attn.q_proj.weight</code>: (4096 * 4096 = 16777216)</li> <li><code>self_attn.v_proj.weight</code>: (4096 * 4096 = 16777216)</li> <li>Total parameters per layer: [4096 + 45088768+ 45088768+ 45088768+ 4096 + 16777216+ 16777216+ 16777216+ 16777216] = 202383360</li> </ul> <p>Output Layer (<code>**lm_head.weight**</code>):</p> <ul> <li>Dimensions: <code>[32000, 4096]</code></li> <li>Total parameters: (32000 * 4096 = 131072000)</li> </ul> <p>Total Parameters for the Layer:</p> <ul> <li>Total parameters per layer: 202383360</li> <li>Total parameters for the 32 layer: 32*202383360 = 6476267520</li> </ul> <p>Input embedding Layer + Output layer = 262144000</p> <p>Overall parameters = Total Parameters for 32 Layers + Input and Output Layers = 6738411520(6.74 Billion Parameters)</p> <p>Therefore, the total number of parameters in this LLama2\u20137b is 6738411520(6.74 B) parameters.</p> <p>This calculation includes all weight matrices within the specified layer. Note that the calculation does not include the <code>self_attn.rotary_emb.inv_freq</code> parameter as it's a single-dimensional vector and is typically not considered as part of the parameter count for the layer.</p>"},{"location":"LLM/LLMArchitecture/ParameterCount/#closing-thoughts","title":"Closing Thoughts","text":"<ul> <li>Firstly, I was a little sad because there hasn\u2019t been much architectural change, and adding \u201cLlama3\u201d before the name of every finetune seems a bit overboard.</li> <li>From my testing, the model is really capable across a lot of tasks, and it\u2019s quite robust. It\u2019s not as guardrailed as LLama2.</li> <li>Regarding the Indic LLM landscape, it will be very challenging to finetune it for Indic languages because the tokenizer isn\u2019t very efficient. From my testing, it\u2019s only better at tokenizing Devanagari-based text than LLama2. However, for other languages like Kannada, Tamil, Telugu, Malayalam, etc., it\u2019s going to be tough as we\u2019ll have to expand the vocabulary, which will require a lot of continual pretraining, considering this was trained on 15T tokens.</li> </ul> <p>here is a tweet that goes over \u201cwhy it will be diffcult to finetune Llama3 for indic langauges\u201d</p> <p>If you found this post valuable, make sure to follow me for more insightful content. I frequently write about the practical applications of Generative AI, LLMs, Stable Diffusion, and explore the broader impacts of AI on society.</p> <p>Let\u2019s stay connected on Twitter. I\u2019d love to engage in discussions with you.</p> <p>If you\u2019re not a Medium member yet and wish to support writers like me, consider signing up through my referral link: Medium Membership. Your support is greatly appreciated!</p>"},{"location":"LLM/LLama2/Llama2_finetuning_notebook/","title":"Finetune LLama 2","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n!pip install -q datasets bitsandbytes einops wandb\n</pre> !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git !pip install -q datasets bitsandbytes einops wandb In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\n#dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant\n\ndataset_name = 'AlexanderDoria/novel17_test' #french novels\ndataset = load_dataset(dataset_name, split=\"train\")\n</pre> from datasets import load_dataset  #dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant  dataset_name = 'AlexanderDoria/novel17_test' #french novels dataset = load_dataset(dataset_name, split=\"train\") In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\nmodel_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\n</pre> import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer  model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.float16, )  model = AutoModelForCausalLM.from_pretrained(     model_name,     quantization_config=bnb_config,     trust_remote_code=True ) model.config.use_cache = False <p>Let's also load the tokenizer below</p> In\u00a0[\u00a0]: Copied! <pre>tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n</pre> tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token In\u00a0[\u00a0]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n</pre> from peft import LoraConfig, get_peft_model  lora_alpha = 16 lora_dropout = 0.1 lora_r = 64  peft_config = LoraConfig(     lora_alpha=lora_alpha,     lora_dropout=lora_dropout,     r=lora_r,     bias=\"none\",     task_type=\"CAUSAL_LM\" ) <p>Here we will use the <code>SFTTrainer</code> from TRL library that gives a wrapper around transformers <code>Trainer</code> to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\noptim = \"paged_adamw_32bit\"\nsave_steps = 100\nlogging_steps = 10\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nmax_steps = 100\nwarmup_ratio = 0.03\nlr_scheduler_type = \"constant\"\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)\n</pre> from transformers import TrainingArguments  output_dir = \"./results\" per_device_train_batch_size = 4 gradient_accumulation_steps = 4 optim = \"paged_adamw_32bit\" save_steps = 100 logging_steps = 10 learning_rate = 2e-4 max_grad_norm = 0.3 max_steps = 100 warmup_ratio = 0.03 lr_scheduler_type = \"constant\"  training_arguments = TrainingArguments(     output_dir=output_dir,     per_device_train_batch_size=per_device_train_batch_size,     gradient_accumulation_steps=gradient_accumulation_steps,     optim=optim,     save_steps=save_steps,     logging_steps=logging_steps,     learning_rate=learning_rate,     fp16=True,     max_grad_norm=max_grad_norm,     max_steps=max_steps,     warmup_ratio=warmup_ratio,     group_by_length=True,     lr_scheduler_type=lr_scheduler_type, ) <p>Then finally pass everthing to the trainer</p> In\u00a0[\u00a0]: Copied! <pre>from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n</pre> from trl import SFTTrainer  max_seq_length = 512  trainer = SFTTrainer(     model=model,     train_dataset=dataset,     peft_config=peft_config,     dataset_text_field=\"text\",     max_seq_length=max_seq_length,     tokenizer=tokenizer,     args=training_arguments, ) <p>We will also pre-process the model by upcasting the layer norms in float 32 for more stable training</p> In\u00a0[\u00a0]: Copied! <pre>for name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)\n</pre> for name, module in trainer.model.named_modules():     if \"norm\" in name:         module = module.to(torch.float32) <p>Now let's train the model! Simply call <code>trainer.train()</code></p> In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() <p>During training, the model should converge nicely as follows:</p> <p></p> <p>The <code>SFTTrainer</code> also takes care of properly saving only the adapters during training instead of saving the entire model.</p> In\u00a0[\u00a0]: Copied! <pre>model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(\"outputs\")\n</pre> model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training model_to_save.save_pretrained(\"outputs\") In\u00a0[\u00a0]: Copied! <pre>lora_config = LoraConfig.from_pretrained('outputs')\nmodel = get_peft_model(model, lora_config)\n</pre> lora_config = LoraConfig.from_pretrained('outputs') model = get_peft_model(model, lora_config) In\u00a0[\u00a0]: Copied! <pre>dataset['text']\n</pre> dataset['text'] In\u00a0[\u00a0]: Copied! <pre>text = \"\u00c9crire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis \u00e9ton\"\ndevice = \"cuda:0\"\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</pre> text = \"\u00c9crire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis \u00e9ton\" device = \"cuda:0\"  inputs = tokenizer(text, return_tensors=\"pt\").to(device) outputs = model.generate(**inputs, max_new_tokens=50) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import login\nlogin()\n</pre> from huggingface_hub import login login() In\u00a0[\u00a0]: Copied! <pre>model.push_to_hub(\"llama2-qlora-finetunined-french\")\n</pre> model.push_to_hub(\"llama2-qlora-finetunined-french\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"LLM/LLama2/Llama2_finetuning_notebook/#finetune-llama-2-7b-on-a-google-colab","title":"Finetune Llama-2-7b on a Google colab\u00b6","text":"<p>Welcome to this Google Colab notebook that shows how to fine-tune the recent Llama-2-7b model on a single Google colab and turn it into a chatbot</p> <p>We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning</p>"},{"location":"LLM/LLama2/Llama2_finetuning_notebook/#setup","title":"Setup\u00b6","text":"<p>Run the cells below to setup and install the required libraries. For our experiment we will need <code>accelerate</code>, <code>peft</code>, <code>transformers</code>, <code>datasets</code> and TRL to leverage the recent <code>SFTTrainer</code>. We will use <code>bitsandbytes</code> to quantize the base model into 4bit. We will also install <code>einops</code> as it is a requirement to load Falcon models.</p>"},{"location":"LLM/LLama2/Llama2_finetuning_notebook/#dataset","title":"Dataset\u00b6","text":""},{"location":"LLM/LLama2/Llama2_finetuning_notebook/#loading-the-model","title":"Loading the model\u00b6","text":""},{"location":"LLM/LLama2/Llama2_finetuning_notebook/#loading-the-trainer","title":"Loading the trainer\u00b6","text":""},{"location":"LLM/LLama2/Llama2_finetuning_notebook/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/","title":"Finetune LLama 2(QLoRA)","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n!pip install -q datasets bitsandbytes einops wandb\n</pre> !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git !pip install -q datasets bitsandbytes einops wandb In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\n#dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant\n\ndataset_name = 'AlexanderDoria/novel17_test' #french novels\ndataset = load_dataset(dataset_name, split=\"train\")\n</pre> from datasets import load_dataset  #dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant  dataset_name = 'AlexanderDoria/novel17_test' #french novels dataset = load_dataset(dataset_name, split=\"train\") In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\nmodel_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\n</pre> import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer  model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.float16, )  model = AutoModelForCausalLM.from_pretrained(     model_name,     quantization_config=bnb_config,     trust_remote_code=True ) model.config.use_cache = False <p>Let's also load the tokenizer below</p> In\u00a0[\u00a0]: Copied! <pre>tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n</pre> tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token In\u00a0[\u00a0]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n</pre> from peft import LoraConfig, get_peft_model  lora_alpha = 16 lora_dropout = 0.1 lora_r = 64  peft_config = LoraConfig(     lora_alpha=lora_alpha,     lora_dropout=lora_dropout,     r=lora_r,     bias=\"none\",     task_type=\"CAUSAL_LM\" ) <p>Here we will use the <code>SFTTrainer</code> from TRL library that gives a wrapper around transformers <code>Trainer</code> to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\noptim = \"paged_adamw_32bit\"\nsave_steps = 100\nlogging_steps = 10\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nmax_steps = 100\nwarmup_ratio = 0.03\nlr_scheduler_type = \"constant\"\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)\n</pre> from transformers import TrainingArguments  output_dir = \"./results\" per_device_train_batch_size = 4 gradient_accumulation_steps = 4 optim = \"paged_adamw_32bit\" save_steps = 100 logging_steps = 10 learning_rate = 2e-4 max_grad_norm = 0.3 max_steps = 100 warmup_ratio = 0.03 lr_scheduler_type = \"constant\"  training_arguments = TrainingArguments(     output_dir=output_dir,     per_device_train_batch_size=per_device_train_batch_size,     gradient_accumulation_steps=gradient_accumulation_steps,     optim=optim,     save_steps=save_steps,     logging_steps=logging_steps,     learning_rate=learning_rate,     fp16=True,     max_grad_norm=max_grad_norm,     max_steps=max_steps,     warmup_ratio=warmup_ratio,     group_by_length=True,     lr_scheduler_type=lr_scheduler_type, ) <p>Then finally pass everthing to the trainer</p> In\u00a0[\u00a0]: Copied! <pre>from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n</pre> from trl import SFTTrainer  max_seq_length = 512  trainer = SFTTrainer(     model=model,     train_dataset=dataset,     peft_config=peft_config,     dataset_text_field=\"text\",     max_seq_length=max_seq_length,     tokenizer=tokenizer,     args=training_arguments, ) <p>We will also pre-process the model by upcasting the layer norms in float 32 for more stable training</p> In\u00a0[\u00a0]: Copied! <pre>for name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)\n</pre> for name, module in trainer.model.named_modules():     if \"norm\" in name:         module = module.to(torch.float32) <p>Now let's train the model! Simply call <code>trainer.train()</code></p> In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() <p>During training, the model should converge nicely as follows:</p> <p></p> <p>The <code>SFTTrainer</code> also takes care of properly saving only the adapters during training instead of saving the entire model.</p> In\u00a0[\u00a0]: Copied! <pre>model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(\"outputs\")\n</pre> model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training model_to_save.save_pretrained(\"outputs\") In\u00a0[\u00a0]: Copied! <pre>lora_config = LoraConfig.from_pretrained('outputs')\nmodel = get_peft_model(model, lora_config)\n</pre> lora_config = LoraConfig.from_pretrained('outputs') model = get_peft_model(model, lora_config) In\u00a0[\u00a0]: Copied! <pre>dataset['text']\n</pre> dataset['text'] In\u00a0[\u00a0]: Copied! <pre>text = \"\u00c9crire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis \u00e9ton\"\ndevice = \"cuda:0\"\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</pre> text = \"\u00c9crire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis \u00e9ton\" device = \"cuda:0\"  inputs = tokenizer(text, return_tensors=\"pt\").to(device) outputs = model.generate(**inputs, max_new_tokens=50) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import login\nlogin()\n</pre> from huggingface_hub import login login() In\u00a0[\u00a0]: Copied! <pre>model.push_to_hub(\"llama2-qlora-finetunined-french\")\n</pre> model.push_to_hub(\"llama2-qlora-finetunined-french\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/#finetune-llama-2-7b-on-a-google-colab","title":"Finetune Llama-2-7b on a Google colab\u00b6","text":"<p>Welcome to this Google Colab notebook that shows how to fine-tune the recent Llama-2-7b model on a single Google colab and turn it into a chatbot</p> <p>We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning</p>"},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/#setup","title":"Setup\u00b6","text":"<p>Run the cells below to setup and install the required libraries. For our experiment we will need <code>accelerate</code>, <code>peft</code>, <code>transformers</code>, <code>datasets</code> and TRL to leverage the recent <code>SFTTrainer</code>. We will use <code>bitsandbytes</code> to quantize the base model into 4bit. We will also install <code>einops</code> as it is a requirement to load Falcon models.</p>"},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/#dataset","title":"Dataset\u00b6","text":""},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/#loading-the-model","title":"Loading the model\u00b6","text":""},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/#loading-the-trainer","title":"Loading the trainer\u00b6","text":""},{"location":"LLM/LLama2/Llama_2_Fine_Tuning_using_QLora/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"LLM/LLama2/generate_response_stream/","title":"Generate response stream","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig, TextStreamer\nimport argparse\n</pre> import torch from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig, TextStreamer import argparse In\u00a0[\u00a0]: Copied! <pre>def generate_output(model, tokenizer, instruction):\n    batch = tokenizer(instruction, return_tensors=\"pt\", add_special_tokens=True)\n\n    print(\"=\" * 40)\n    model.eval()\n    with torch.no_grad():\n        generation_config = GenerationConfig(\n            repetition_penalty=1.1,\n            max_new_tokens=1024,\n            temperature=0.9,\n            top_p=0.95,\n            top_k=40,\n            bos_token_id=tokenizer.bos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            do_sample=True,\n            use_cache=True,\n            return_dict_in_generate=True,\n            output_attentions=False,\n            output_hidden_states=False,\n            output_scores=False,\n            padding_side='left'\n        )\n        streamer = TextStreamer(tokenizer)\n        generated = model.generate(\n            inputs=batch[\"input_ids\"].to(\"cuda\"),\n            generation_config=generation_config,\n            streamer=streamer,\n        )\n    print(\"=\" * 40)\n    print(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0]))\n</pre> def generate_output(model, tokenizer, instruction):     batch = tokenizer(instruction, return_tensors=\"pt\", add_special_tokens=True)      print(\"=\" * 40)     model.eval()     with torch.no_grad():         generation_config = GenerationConfig(             repetition_penalty=1.1,             max_new_tokens=1024,             temperature=0.9,             top_p=0.95,             top_k=40,             bos_token_id=tokenizer.bos_token_id,             eos_token_id=tokenizer.eos_token_id,             pad_token_id=tokenizer.pad_token_id,             do_sample=True,             use_cache=True,             return_dict_in_generate=True,             output_attentions=False,             output_hidden_states=False,             output_scores=False,             padding_side='left'         )         streamer = TextStreamer(tokenizer)         generated = model.generate(             inputs=batch[\"input_ids\"].to(\"cuda\"),             generation_config=generation_config,             streamer=streamer,         )     print(\"=\" * 40)     print(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0])) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=\"Generate text with LlamaForCausalLM\")\n    parser.add_argument(\"--prompt\", type=str, required=True, help=\"The instruction prompt\")\n    parser.add_argument(\"--model\", type=str, required=True, help=\"The model ID\")\n\n    args = parser.parse_args()\n\n    # Load the specified model and tokenizer\n    model = LlamaForCausalLM.from_pretrained(args.model, device_map={\"\": 0})\n    tokenizer = LlamaTokenizer.from_pretrained(args.model, add_eos_token=True, padding_side=\"left\")\n\n    # Generate output using the provided prompt\n    generate_output(model, tokenizer, args.prompt)\n</pre> if __name__ == \"__main__\":     # Parse command-line arguments     parser = argparse.ArgumentParser(description=\"Generate text with LlamaForCausalLM\")     parser.add_argument(\"--prompt\", type=str, required=True, help=\"The instruction prompt\")     parser.add_argument(\"--model\", type=str, required=True, help=\"The model ID\")      args = parser.parse_args()      # Load the specified model and tokenizer     model = LlamaForCausalLM.from_pretrained(args.model, device_map={\"\": 0})     tokenizer = LlamaTokenizer.from_pretrained(args.model, add_eos_token=True, padding_side=\"left\")      # Generate output using the provided prompt     generate_output(model, tokenizer, args.prompt)"},{"location":"LLM/Llama3/Llama3_finetuning_notebook/","title":"Llama3 finetuning notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n!pip install -q datasets bitsandbytes einops wandb\n</pre> !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git !pip install -q datasets bitsandbytes einops wandb In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\n#dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant\n\ndataset_name = 'AlexanderDoria/novel17_test' #french novels\ndataset = load_dataset(dataset_name, split=\"train\")\n</pre> from datasets import load_dataset  #dataset_name = \"timdettmers/openassistant-guanaco\" ###Human ,.,,,,,, ###Assistant  dataset_name = 'AlexanderDoria/novel17_test' #french novels dataset = load_dataset(dataset_name, split=\"train\") In\u00a0[1]: Copied! <pre>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n</pre> import torch from transformers import AutoModelForCausalLM, AutoTokenizer In\u00a0[2]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(\"./\")\nmodel.config.use_cache = False\n</pre> model = AutoModelForCausalLM.from_pretrained(\"./\") model.config.use_cache = False <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 model = AutoModelForCausalLM.from_pretrained()\n      2 model.config.use_cache = False\n\nTypeError: _BaseAutoModelClass.from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'</pre> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\nmodel_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\nmodel.config.use_cache = False\n</pre> import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer  model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.float16, )  model = AutoModelForCausalLM.from_pretrained(     model_name,     quantization_config=bnb_config,     trust_remote_code=True ) model.config.use_cache = False <p>Let's also load the tokenizer below</p> In\u00a0[\u00a0]: Copied! <pre>tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n</pre> tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token In\u00a0[\u00a0]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n</pre> from peft import LoraConfig, get_peft_model  lora_alpha = 16 lora_dropout = 0.1 lora_r = 64  peft_config = LoraConfig(     lora_alpha=lora_alpha,     lora_dropout=lora_dropout,     r=lora_r,     bias=\"none\",     task_type=\"CAUSAL_LM\" ) <p>Here we will use the <code>SFTTrainer</code> from TRL library that gives a wrapper around transformers <code>Trainer</code> to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\noptim = \"paged_adamw_32bit\"\nsave_steps = 100\nlogging_steps = 10\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nmax_steps = 100\nwarmup_ratio = 0.03\nlr_scheduler_type = \"constant\"\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)\n</pre> from transformers import TrainingArguments  output_dir = \"./results\" per_device_train_batch_size = 4 gradient_accumulation_steps = 4 optim = \"paged_adamw_32bit\" save_steps = 100 logging_steps = 10 learning_rate = 2e-4 max_grad_norm = 0.3 max_steps = 100 warmup_ratio = 0.03 lr_scheduler_type = \"constant\"  training_arguments = TrainingArguments(     output_dir=output_dir,     per_device_train_batch_size=per_device_train_batch_size,     gradient_accumulation_steps=gradient_accumulation_steps,     optim=optim,     save_steps=save_steps,     logging_steps=logging_steps,     learning_rate=learning_rate,     fp16=True,     max_grad_norm=max_grad_norm,     max_steps=max_steps,     warmup_ratio=warmup_ratio,     group_by_length=True,     lr_scheduler_type=lr_scheduler_type, ) <p>Then finally pass everthing to the trainer</p> In\u00a0[\u00a0]: Copied! <pre>from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n</pre> from trl import SFTTrainer  max_seq_length = 512  trainer = SFTTrainer(     model=model,     train_dataset=dataset,     peft_config=peft_config,     dataset_text_field=\"text\",     max_seq_length=max_seq_length,     tokenizer=tokenizer,     args=training_arguments, ) <p>We will also pre-process the model by upcasting the layer norms in float 32 for more stable training</p> In\u00a0[\u00a0]: Copied! <pre>for name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)\n</pre> for name, module in trainer.model.named_modules():     if \"norm\" in name:         module = module.to(torch.float32) <p>Now let's train the model! Simply call <code>trainer.train()</code></p> In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() <p>During training, the model should converge nicely as follows:</p> <p></p> <p>The <code>SFTTrainer</code> also takes care of properly saving only the adapters during training instead of saving the entire model.</p> In\u00a0[\u00a0]: Copied! <pre>model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(\"outputs\")\n</pre> model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training model_to_save.save_pretrained(\"outputs\") In\u00a0[\u00a0]: Copied! <pre>lora_config = LoraConfig.from_pretrained('outputs')\nmodel = get_peft_model(model, lora_config)\n</pre> lora_config = LoraConfig.from_pretrained('outputs') model = get_peft_model(model, lora_config) In\u00a0[\u00a0]: Copied! <pre>dataset['text']\n</pre> dataset['text'] In\u00a0[\u00a0]: Copied! <pre>text = \"\u00c9crire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis \u00e9ton\"\ndevice = \"cuda:0\"\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</pre> text = \"\u00c9crire un texte dans un style baroque sur la glace et le feu ### Assistant: Si j'en luis \u00e9ton\" device = \"cuda:0\"  inputs = tokenizer(text, return_tensors=\"pt\").to(device) outputs = model.generate(**inputs, max_new_tokens=50) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import login\nlogin()\n</pre> from huggingface_hub import login login() In\u00a0[\u00a0]: Copied! <pre>model.push_to_hub(\"llama2-qlora-finetunined-french\")\n</pre> model.push_to_hub(\"llama2-qlora-finetunined-french\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"LLM/Llama3/Llama3_finetuning_notebook/#finetune-llama-3-8b-on-a-google-colab","title":"Finetune Llama-3-8b on a Google colab\u00b6","text":"<p>Welcome to this Google Colab notebook that shows how to fine-tune the recent Llama-2-7b model on a single Google colab and turn it into a chatbot</p> <p>We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning</p>"},{"location":"LLM/Llama3/Llama3_finetuning_notebook/#setup","title":"Setup\u00b6","text":"<p>Run the cells below to setup and install the required libraries. For our experiment we will need <code>accelerate</code>, <code>peft</code>, <code>transformers</code>, <code>datasets</code> and TRL to leverage the recent <code>SFTTrainer</code>. We will use <code>bitsandbytes</code> to quantize the base model into 4bit. We will also install <code>einops</code> as it is a requirement to load Falcon models.</p>"},{"location":"LLM/Llama3/Llama3_finetuning_notebook/#dataset","title":"Dataset\u00b6","text":""},{"location":"LLM/Llama3/Llama3_finetuning_notebook/#loading-the-model","title":"Loading the model\u00b6","text":""},{"location":"LLM/Llama3/Llama3_finetuning_notebook/#loading-the-trainer","title":"Loading the trainer\u00b6","text":""},{"location":"LLM/Llama3/Llama3_finetuning_notebook/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"LLM/Mistral-7b/","title":"A Beginner\u2019s Guide to Fine-Tuning Mistral 7B Instruct Model","text":"<p>URL Source: https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe</p> <p>Published Time: 2023-10-06T18:30:13.121Z</p> <p>Markdown Content: Fine-Tuning for Code Generation Using a Single Google Colab Notebook</p> <p></p> <p>Updated : 10<sup>th</sup> December 2023</p> <p>Fine-tuning a state-of-the-art language model like Mistral 7B Instruct can be an exciting journey. This guide will walk you through the process step by step, from setting up your environment to fine-tuning the model for your specific task. Whether you\u2019re a seasoned machine learning practitioner or a newcomer to the field, this beginner-friendly tutorial will help you harness the power of Mistral 7B for your projects.</p>"},{"location":"LLM/Mistral-7b/#meet-mistral-7b-instruct","title":"Meet Mistral 7B Instruct","text":"<p>The team at MistralAI has created an exceptional language model called Mistral 7B Instruct. It has consistently delivered outstanding results in a range of benchmarks, which positions it as an ideal option for natural language generation and understanding. This guide will concentrate on how to fine-tune the model for coding purposes, but the methodology can effectively be applied to other tasks.</p>"},{"location":"LLM/Mistral-7b/#colab-notebook-to-finetuning-mistral-7b-instruct","title":"Colab Notebook to Finetuning Mistral-7b-Instruct","text":"<p>Code has been updated on December 10<sup>th</sup> , 2023</p>"},{"location":"LLM/Mistral-7b/#prerequisites","title":"Prerequisites","text":"<p>Before diving into the fine-tuning process, make sure you have the following prerequisites in place:</p> <ol> <li>GPU: While this tutorial can run on a free Google Colab notebook with a GPU, it\u2019s recommended to use more powerful GPUs like V100 or A100 for better performance.</li> <li>Python Packages: Ensure you have the required Python packages installed. You can run the following commands to install them:</li> </ol> <p>!pip install -q torch !pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights !pip install -q datasets #huggingface datasets to download and manipulate datasets !pip install -q peft #Parameter efficient finetuning - for qLora Finetuning !pip install -q bitsandbytes #For Model weights quantisation !pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning !pip install -q wandb -U #Used to monitor the model score during training</p> <ol> <li>Hugging Face Hub Account: You\u2019ll need an account on the Hugging Face Model Hub. You can sign up here.</li> </ol>"},{"location":"LLM/Mistral-7b/#getting-started","title":"Getting Started","text":"<p>Let\u2019s start by checking if your GPU is correctly detected:</p> <p>!nvidia-smi</p> <p>If your GPU is not recognized or you encounter CUDA out-of-memory errors during fine-tuning, consider using a more powerful GPU.</p>"},{"location":"LLM/Mistral-7b/#loading-required-libraries","title":"Loading Required Libraries","text":"<p>We\u2019ll load the necessary Python libraries for our fine-tuning process:</p> <p>import json import pandas as pd import torch from datasets import Dataset, load_dataset from huggingface_hub import notebook_login from peft import LoraConfig, PeftModel from transformers import (     AutoModelForCausalLM,     AutoTokenizer,     BitsAndBytesConfig,     TrainingArguments,     pipeline,     logging, ) from trl import SFTTrainer</p>"},{"location":"LLM/Mistral-7b/#logging-into-hugging-face-hub","title":"Logging into Hugging Face Hub","text":"<p>Log in to the Hugging Face Model Hub using your credentials:</p> <p>notebook_login()</p>"},{"location":"LLM/Mistral-7b/#loading-the-dataset","title":"Loading the Dataset","text":"<p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>we will be using this dataset which is curated by TokenBender (e/xperiments) which is a awesome data for finetuning model for code generation. It follows the alpaca style of instructions which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <p>{     \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",     \"input\":\"[1, 2, 3, 4, 5]\",     \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\" }</p> <p>now lets load the dataset using huggingfaces datasets library</p> <p># Load your dataset (replace 'your_dataset_name' and 'split_name' with your actual dataset information) # dataset = load_dataset(\"your_dataset_name\", split=\"split_name\") dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")</p>"},{"location":"LLM/Mistral-7b/#formatting-the-dataset","title":"Formatting the Dataset","text":"<p>Now, let\u2019s format the dataset in the required Mistral-7B-Instruct-v0.1 format.</p> <p>Many tutorial and blogs skip over this part but i feel this is a really important step.</p> <p>We\u2019ll put each instruction and input pair between <code>[INST]</code> and <code>[/INST]</code> output after that, like this:</p> <p>&lt;s&gt;[INST] What is your favourite condiment? [/INST] Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!&lt;/s&gt;</p> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p> <p># this function is used to output the right formate for each row in the dataset def create_text_row(instruction, output, input):     text_row = f\"\"\"&lt;s&gt;[INST] {instruction} here are the inputs {input} [/INST] \\\\n {output} &lt;/s&gt;\"\"\"     return text_row# interate over all the rows formate the dataset and store it in a jsonl file def process_jsonl_file(output_file_path):     with open(output_file_path, \"w\") as output_jsonl_file:         for item in dataset:             json_object = {                 \"text\": create_text_row(item[\"instruction\"], item[\"input\"] ,item[\"output\"]),                 \"instruction\": item[\"instruction\"],                 \"input\": item[\"input\"],                 \"output\": item[\"output\"]             }             output_jsonl_file.write(json.dumps(json_object) + \"\\\\n\")</p> <p># Provide the path where you want to save the formatted dataset process_jsonl_file(\"./training_dataset.jsonl\")</p>"},{"location":"LLM/Mistral-7b/#after-formatting","title":"After Formatting","text":"<p>{ \"text\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST] # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\", \"instruction\":\"Create a function to calculate the sum of a sequence of integers\", \"input\":\"[1, 2, 3, 4, 5]\", \"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\" }</p> <p>While using SFT(Supervised Fine-tuning Trainer) to finetune we will be only passing in the \u201ctext\u201d column of the dataset for finetuning.</p>"},{"location":"LLM/Mistral-7b/#loading-the-training-dataset","title":"Loading the Training Dataset","text":"<p>Now, let\u2019s load the training dataset from the JSONL file we created:</p> <p>train_dataset = load_dataset('json', data_files='./training_dataset.jsonl' , split='train')</p>"},{"location":"LLM/Mistral-7b/#setting-model-parameters","title":"Setting Model Parameters","text":"<p>We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:</p> <p>new_model = \"mistralai-Code-Instruct\" #set the name of the new model################################################################################ # QLoRA parameters  </p>"},{"location":"LLM/Mistral-7b/#_1","title":"A Beginner\u2019s Guide to Fine-Tuning Mistral 7B Instruct Model","text":"<p># LoRA attention dimension lora_r = 64</p> <p># Alpha parameter for LoRA scaling lora_alpha = 16</p> <p># Dropout probability for LoRA layers lora_dropout = 0.1</p>"},{"location":"LLM/Mistral-7b/#_2","title":"##########################################################################","text":"<p># bitsandbytes parameters  </p>"},{"location":"LLM/Mistral-7b/#_3","title":"A Beginner\u2019s Guide to Fine-Tuning Mistral 7B Instruct Model","text":"<p># Activate 4-bit precision base model loading use_4bit = True</p> <p># Compute dtype for 4-bit base models bnb_4bit_compute_dtype = \"float16\"</p> <p># Quantization type (fp4 or nf4) bnb_4bit_quant_type = \"nf4\"</p> <p># Activate nested quantization for 4-bit base models (double quantization) use_nested_quant = False</p>"},{"location":"LLM/Mistral-7b/#_4","title":"##########################################################################","text":"<p># TrainingArguments parameters  </p>"},{"location":"LLM/Mistral-7b/#_5","title":"A Beginner\u2019s Guide to Fine-Tuning Mistral 7B Instruct Model","text":"<p># Output directory where the model predictions and checkpoints will be stored output_dir = \"./results\"</p> <p># Number of training epochs num_train_epochs = 1</p> <p># Enable fp16/bf16 training (set bf16 to True with an A100) fp16 = False bf16 = False</p> <p># Batch size per GPU for training per_device_train_batch_size = 4</p> <p># Batch size per GPU for evaluation per_device_eval_batch_size = 4</p> <p># Number of update steps to accumulate the gradients for gradient_accumulation_steps = 1</p> <p># Enable gradient checkpointing gradient_checkpointing = True</p> <p># Maximum gradient normal (gradient clipping) max_grad_norm = 0.3</p> <p># Initial learning rate (AdamW optimizer) learning_rate = 2e-4</p> <p># Weight decay to apply to all layers except bias/LayerNorm weights weight_decay = 0.001</p> <p># Optimizer to use optim = \"paged_adamw_32bit\"</p> <p># Learning rate schedule (constant a bit better than cosine) lr_scheduler_type = \"constant\"</p> <p># Number of training steps (overrides num_train_epochs) max_steps = -1</p> <p># Ratio of steps for a linear warmup (from 0 to learning rate) warmup_ratio = 0.03</p> <p># Group sequences into batches with same length # Saves memory and speeds up training considerably group_by_length = True</p> <p># Save checkpoint every X updates steps save_steps = 25</p> <p># Log every X updates steps logging_steps = 25</p>"},{"location":"LLM/Mistral-7b/#_6","title":"##########################################################################","text":"<p># SFT parameters  </p>"},{"location":"LLM/Mistral-7b/#_7","title":"A Beginner\u2019s Guide to Fine-Tuning Mistral 7B Instruct Model","text":"<p># Maximum sequence length to use max_seq_length = None</p> <p># Pack multiple short examples in the same input sequence to increase efficiency packing = False</p> <p># Load the entire model on the GPU 0 device_map = {\"\": 0}</p>"},{"location":"LLM/Mistral-7b/#loading-the-base-model","title":"Loading the Base Model","text":"<p>Let\u2019s load the Mistral 7B Instruct base model:</p> <p>model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"# Load the base model with QLoRA configuration compute_dtype = getattr(torch, bnb_4bit_compute_dtype)</p> <p>bnb_config = BitsAndBytesConfig(     load_in_4bit=use_4bit,     bnb_4bit_quant_type=bnb_4bit_quant_type,     bnb_4bit_compute_dtype=compute_dtype,     bnb_4bit_use_double_quant=use_nested_quant, )</p> <p>base_model = AutoModelForCausalLM.from_pretrained(     model_name,     quantization_config=bnb_config,     device_map={\"\": 0} )</p> <p>base_model.config.use_cache = False base_model.config.pretraining_tp = 1</p> <p># Load MitsralAi tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\"pyt</p>"},{"location":"LLM/Mistral-7b/#base-model-inference","title":"Base model Inference","text":"<p>eval_prompt = \"\"\"Print hello world in python c and c++\"\"\"# import random model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")</p> <p>model.eval() with torch.no_grad():     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))Fine-Tuning with qLora</p> <p>The results from the base model tend to be of poor quality and doesn\u2019t always generate sytactically correct code</p>"},{"location":"LLM/Mistral-7b/#fine-tuning-with-qlora-and-supervised-finetuning","title":"Fine-Tuning with qLora and Supervised Finetuning","text":"<p>We\u2019re ready to fine-tune our model using qLora. For this tutorial, we\u2019ll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p> <p># Set LoRA configuration peft_config = LoraConfig(     lora_alpha=lora_alpha,     lora_dropout=lora_dropout,     r=lora_r,     target_modules=[         \"q_proj\",         \"k_proj\",         \"v_proj\",         \"o_proj\",         \"gate_proj\",         \"up_proj\",         \"down_proj\",         \"lm_head\",     ],     bias=\"none\",     task_type=\"CAUSAL_LM\", )# Set training parameters training_arguments = TrainingArguments(     output_dir=output_dir,     num_train_epochs=num_train_epochs,     per_device_train_batch_size=per_device_train_batch_size,     gradient_accumulation_steps=gradient_accumulation_steps,     optim=optim,     save_steps=save_steps,     logging_steps=logging_steps,     learning_rate=learning_rate,     weight_decay=weight_decay,     fp16=fp16,     bf16=bf16,     max_grad_norm=max_grad_norm,     max_steps=100, # the total number of training steps to perform     warmup_ratio=warmup_ratio,     group_by_length=group_by_length,     lr_scheduler_type=lr_scheduler_type,     report_to=\"tensorboard\" )</p> <p># Initialize the SFTTrainer for fine-tuning trainer = SFTTrainer(     model=base_model,     train_dataset=train_dataset,     peft_config=peft_config,     dataset_text_field=\"text\",     max_seq_length=max_seq_length,  # You can specify the maximum sequence length here     tokenizer=tokenizer,     args=training_arguments,     packing=packing, )</p>"},{"location":"LLM/Mistral-7b/#lets-start-the-training-process","title":"Lets start the training process","text":"<p># Start the training process trainer.train()# Save the fine-tuned model trainer.model.save_pretrained(new_model)</p>"},{"location":"LLM/Mistral-7b/#inference-with-fine-tuned-model","title":"Inference with Fine-Tuned Model","text":"<p>Now that we have fine-tuned our model, let\u2019s test its performance with some code generation tasks. Replace <code>eval_prompt</code> with your code generation prompt:</p> <p>eval_prompt = \"\"\"Print hello world in python c and c++\"\"\"model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\") model.eval() with torch.no_grad():     generated_code = tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True) print(generated_code)</p>"},{"location":"LLM/Mistral-7b/#merge-and-share","title":"Merge and Share","text":"<p>After fine-tuning, if you want to merge the model with LoRA weights or share it with the Hugging Face Model Hub, you can do so. This step is optional and depends on your specific use case.</p> <p># Merge the model with LoRA weights base_model = AutoModelForCausalLM.from_pretrained(     model_name,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= model.merge_and_unload()# Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\")</p> <p># Merge the model with LoRA weights base_model = AutoModelForCausalLM.from_pretrained(     model_name,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= merged_model.merge_and_unload()</p> <p># Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\")</p>"},{"location":"LLM/Mistral-7b/#test-the-merged-model","title":"Test the merged model","text":"<p>from random import randrange sample = train_dataset [randrange(len(train_dataset ))]prompt = f\"\"\"&lt;s&gt;  {sample['instruction']} {sample['input']} [INST]</p> <p>\"\"\"</p> <p>input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda() # with torch.inference_mode(): outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)</p> <p>print(f\"Prompt:\\n{prompt}\\n\") print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\") print(f\"\\nGround truth:\\n{sample['output']}\")</p> <p>And that\u2019s it! You\u2019ve successfully fine-tuned Mistral 7B Instruct for code generation. You can adapt this process for various natural language understanding and generation tasks. Keep exploring and experimenting with Mistral 7B to unlock its full potential for your projects.</p> <p>All the code will be available on my github. Do drop by and give a follow and a star</p> <p>I also post content about Generative AI | LLMs | Stable Diffusion and what i have been working on twitter \u2014 AdithyaSK (@adithya_s_k) / X</p>"},{"location":"LLM/Mistral-7b/LLM_evaluation_harness_for_Arc_Easy_and_SST/","title":"Evaluation Mistral","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/EleutherAI/lm-evaluation-harness/\n</pre> !git clone https://github.com/EleutherAI/lm-evaluation-harness/ <pre>Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 22462, done.\nremote: Counting objects: 100% (5248/5248), done.\nremote: Compressing objects: 100% (687/687), done.\nremote: Total 22462 (delta 4760), reused 4854 (delta 4559), pack-reused 17214\nReceiving objects: 100% (22462/22462), 20.70 MiB | 14.91 MiB/s, done.\nResolving deltas: 100% (15465/15465), done.\n</pre> In\u00a0[2]: Copied! <pre>%cd lm-evaluation-harness\n</pre> %cd lm-evaluation-harness <pre>/content/lm-evaluation-harness\n</pre> In\u00a0[3]: Copied! <pre>!git checkout 'e47e01beea79cfe87421e2dac49e64d499c240b4'\n</pre> !git checkout 'e47e01beea79cfe87421e2dac49e64d499c240b4' <pre>Note: switching to 'e47e01beea79cfe87421e2dac49e64d499c240b4'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at e47e01be Merge pull request #435 from EleutherAI/haileyschoelkopf-patch-1\n</pre> In\u00a0[4]: Copied! <pre>!pip install -q -e .\n</pre> !pip install -q -e . <pre>  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 519.6/519.6 kB 8.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.0/77.0 kB 6.6 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 79.5/79.5 kB 6.7 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 85.6/85.6 kB 7.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 227.7/227.7 kB 13.9 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.1/10.1 MB 53.2 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 111.1/111.1 kB 14.4 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 65.6/65.6 kB 9.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.7/7.7 MB 106.1 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.7/2.7 MB 104.3 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 115.3/115.3 kB 16.2 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.8/134.8 kB 18.0 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 302.0/302.0 kB 35.1 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 117.0/117.0 kB 17.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 258.1/258.1 kB 31.0 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 89.0 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.8/3.8 MB 119.3 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 295.0/295.0 kB 34.5 MB/s eta 0:00:00\n  Building wheel for antlr4-python3-runtime (setup.py) ... done\n  Building wheel for rouge-score (setup.py) ... done\n  Building wheel for pycountry (pyproject.toml) ... done\n  Building wheel for sqlitedict (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nllmx 0.0.15a0 requires cohere, which is not installed.\nllmx 0.0.15a0 requires tiktoken, which is not installed.\n</pre> In\u00a0[5]: Copied! <pre>import transformers\nimport accelerate\n</pre> import transformers import accelerate In\u00a0[6]: Copied! <pre>#!pip install bleurt@https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt\n</pre> #!pip install bleurt@https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt <p>https://github.com/google/BIG-bench/issues/934</p> In\u00a0[7]: Copied! <pre># 3 minutes to run with Accuracy of 44%\n%%time\n!python main.py \\\n    --model gpt2 \\\n    --num_fewshot 0 \\\n    --tasks arc_easy \\\n    --device 0\n</pre> # 3 minutes to run with Accuracy of 44% %%time !python main.py \\     --model gpt2 \\     --num_fewshot 0 \\     --tasks arc_easy \\     --device 0 <pre>2023-10-21 03:33:47.513338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nSelected Tasks: ['arc_easy']\nUsing device '0'\nDownloading (\u2026)lve/main/config.json: 100% 665/665 [00:00&lt;00:00, 2.78MB/s]\nDownloading model.safetensors: 100% 548M/548M [00:02&lt;00:00, 228MB/s]\nDownloading (\u2026)neration_config.json: 100% 124/124 [00:00&lt;00:00, 754kB/s]\nDownloading (\u2026)olve/main/vocab.json: 100% 1.04M/1.04M [00:00&lt;00:00, 3.17MB/s]\nDownloading (\u2026)olve/main/merges.txt: 100% 456k/456k [00:00&lt;00:00, 1.87MB/s]\nDownloading (\u2026)/main/tokenizer.json: 100% 1.36M/1.36M [00:00&lt;00:00, 4.19MB/s]\nDownloading builder script: 100% 5.37k/5.37k [00:00&lt;00:00, 12.1MB/s]\nDownloading metadata: 100% 4.47k/4.47k [00:00&lt;00:00, 19.1MB/s]\nDownloading readme: 100% 8.66k/8.66k [00:00&lt;00:00, 20.7MB/s]\nDownloading data: 100% 681M/681M [00:30&lt;00:00, 22.1MB/s]\nGenerating train split: 100% 2251/2251 [00:00&lt;00:00, 7998.81 examples/s]\nGenerating test split: 100% 2376/2376 [00:00&lt;00:00, 10862.60 examples/s]\nGenerating validation split: 100% 570/570 [00:00&lt;00:00, 11669.10 examples/s]\nRunning loglikelihood requests\n100% 9496/9496 [02:48&lt;00:00, 56.19it/s]\n{\n  \"results\": {\n    \"arc_easy\": {\n      \"acc\": 0.43813131313131315,\n      \"acc_stderr\": 0.010180937100600062,\n      \"acc_norm\": 0.3947811447811448,\n      \"acc_norm_stderr\": 0.010030038935883556\n    }\n  },\n  \"versions\": {\n    \"arc_easy\": 0\n  },\n  \"config\": {\n    \"model\": \"gpt2\",\n    \"model_args\": \"\",\n    \"num_fewshot\": 0,\n    \"batch_size\": null,\n    \"device\": \"0\",\n    \"no_cache\": false,\n    \"limit\": null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n  }\n}\ngpt2 (), limit: None, provide_description: False, num_fewshot: 0, batch_size: None\n|  Task  |Version| Metric |Value |   |Stderr|\n|--------|------:|--------|-----:|---|-----:|\n|arc_easy|      0|acc     |0.4381|\u00b1  |0.0102|\n|        |       |acc_norm|0.3948|\u00b1  |0.0100|\n\nCPU times: user 3.22 s, sys: 365 ms, total: 3.59 s\nWall time: 4min 6s\n</pre> In\u00a0[8]: Copied! <pre>#10 minutes to run with Accuracy of 56%\n%%time\n!python main.py \\\n    --model gpt2 \\\n    --model_args pretrained=EleutherAI/gpt-neo-1.3B \\\n    --num_fewshot 0 \\\n    --tasks arc_easy \\\n    --device 0\n</pre> #10 minutes to run with Accuracy of 56% %%time !python main.py \\     --model gpt2 \\     --model_args pretrained=EleutherAI/gpt-neo-1.3B \\     --num_fewshot 0 \\     --tasks arc_easy \\     --device 0 <pre>2023-10-21 03:37:52.813056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nSelected Tasks: ['arc_easy']\nUsing device '0'\nDownloading (\u2026)lve/main/config.json: 100% 1.35k/1.35k [00:00&lt;00:00, 7.19MB/s]\nDownloading model.safetensors: 100% 5.31G/5.31G [00:22&lt;00:00, 231MB/s]\nDownloading (\u2026)okenizer_config.json: 100% 200/200 [00:00&lt;00:00, 892kB/s]\nDownloading (\u2026)olve/main/vocab.json: 100% 798k/798k [00:00&lt;00:00, 2.43MB/s]\nDownloading (\u2026)olve/main/merges.txt: 100% 456k/456k [00:00&lt;00:00, 1.86MB/s]\nDownloading (\u2026)cial_tokens_map.json: 100% 90.0/90.0 [00:00&lt;00:00, 477kB/s]\nRunning loglikelihood requests\n100% 9496/9496 [08:25&lt;00:00, 18.80it/s]\n{\n  \"results\": {\n    \"arc_easy\": {\n      \"acc\": 0.5618686868686869,\n      \"acc_stderr\": 0.010180937100600074,\n      \"acc_norm\": 0.502104377104377,\n      \"acc_norm_stderr\": 0.010259692651537032\n    }\n  },\n  \"versions\": {\n    \"arc_easy\": 0\n  },\n  \"config\": {\n    \"model\": \"gpt2\",\n    \"model_args\": \"pretrained=EleutherAI/gpt-neo-1.3B\",\n    \"num_fewshot\": 0,\n    \"batch_size\": null,\n    \"device\": \"0\",\n    \"no_cache\": false,\n    \"limit\": null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n  }\n}\ngpt2 (pretrained=EleutherAI/gpt-neo-1.3B), limit: None, provide_description: False, num_fewshot: 0, batch_size: None\n|  Task  |Version| Metric |Value |   |Stderr|\n|--------|------:|--------|-----:|---|-----:|\n|arc_easy|      0|acc     |0.5619|\u00b1  |0.0102|\n|        |       |acc_norm|0.5021|\u00b1  |0.0103|\n\nCPU times: user 7.27 s, sys: 774 ms, total: 8.04 s\nWall time: 9min 49s\n</pre> In\u00a0[9]: Copied! <pre>#18 #10 minutes to run with Accuracy of 61%\n%%time\n!python main.py \\\n    --model gpt2 \\\n    --model_args pretrained=EleutherAI/gpt-neo-2.7B \\\n    --num_fewshot 0 \\\n    --tasks arc_easy \\\n    --device 0\n</pre> #18 #10 minutes to run with Accuracy of 61% %%time !python main.py \\     --model gpt2 \\     --model_args pretrained=EleutherAI/gpt-neo-2.7B \\     --num_fewshot 0 \\     --tasks arc_easy \\     --device 0 <pre>2023-10-21 03:47:48.449712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nSelected Tasks: ['arc_easy']\nUsing device '0'\nDownloading (\u2026)lve/main/config.json: 100% 1.46k/1.46k [00:00&lt;00:00, 7.78MB/s]\nDownloading model.safetensors: 100% 10.7G/10.7G [01:10&lt;00:00, 152MB/s] \n^C\nCPU times: user 1.09 s, sys: 250 ms, total: 1.34 s\nWall time: 2min 4s\n</pre> <p>Its best practice to use getpass in these types of notebooks. Your API key should be something like <code>sk-...</code></p> In\u00a0[10]: Copied! <pre>import getpass\nimport os\nopen_ai_key = getpass.getpass('Enter your OPENAI API Key')\n\nos.environ['OPENAI_API_SECRET_KEY'] = open_ai_key\n</pre> import getpass import os open_ai_key = getpass.getpass('Enter your OPENAI API Key')  os.environ['OPENAI_API_SECRET_KEY'] = open_ai_key <pre>Enter your OPENAI API Key\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[11]: Copied! <pre>#2 minutes to run with 88 requests and Accuracy of 86%\n%%time\n!python main.py \\\n    --model gpt3 \\\n    --model_args engine=davinci \\\n    --num_fewshot 2 \\\n    --tasks sst\n</pre> #2 minutes to run with 88 requests and Accuracy of 86% %%time !python main.py \\     --model gpt3 \\     --model_args engine=davinci \\     --num_fewshot 2 \\     --tasks sst <pre>2023-10-21 03:51:06.509333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nSelected Tasks: ['sst']\nDownloading builder script: 100% 28.8k/28.8k [00:00&lt;00:00, 36.3MB/s]\nDownloading metadata: 100% 28.7k/28.7k [00:00&lt;00:00, 35.4MB/s]\nDownloading readme: 100% 27.9k/27.9k [00:00&lt;00:00, 49.1MB/s]\nDownloading data: 100% 7.44M/7.44M [00:00&lt;00:00, 81.4MB/s]\nGenerating train split: 100% 67349/67349 [00:02&lt;00:00, 26948.60 examples/s]\nGenerating validation split: 100% 872/872 [00:00&lt;00:00, 27797.11 examples/s]\nGenerating test split: 100% 1821/1821 [00:00&lt;00:00, 24548.83 examples/s]\nRunning loglikelihood requests\n100% 88/88 [01:34&lt;00:00,  1.07s/it]\n{\n  \"results\": {\n    \"sst\": {\n      \"acc\": 0.8600917431192661,\n      \"acc_stderr\": 0.011753981006588683\n    }\n  },\n  \"versions\": {\n    \"sst\": 0\n  },\n  \"config\": {\n    \"model\": \"gpt3\",\n    \"model_args\": \"engine=davinci\",\n    \"num_fewshot\": 2,\n    \"batch_size\": null,\n    \"device\": null,\n    \"no_cache\": false,\n    \"limit\": null,\n    \"bootstrap_iters\": 100000,\n    \"description_dict\": {}\n  }\n}\ngpt3 (engine=davinci), limit: None, provide_description: False, num_fewshot: 2, batch_size: None\n|Task|Version|Metric|Value |   |Stderr|\n|----|------:|------|-----:|---|-----:|\n|sst |      0|acc   |0.8601|\u00b1  |0.0118|\n\nCPU times: user 821 ms, sys: 121 ms, total: 942 ms\nWall time: 2min 3s\n</pre> In\u00a0[12]: Copied! <pre># After forking...\n!cd .. &amp;&amp; git clone https://github.com/esbenkc/lm-evaluation-harness.git lm-evaluation-harness-new-task\n%cd lm-evaluation-harness-new-task\n!git checkout -b \"cool-patrol\"\n!pip install -q -e \".[dev]\"\n</pre> # After forking... !cd .. &amp;&amp; git clone https://github.com/esbenkc/lm-evaluation-harness.git lm-evaluation-harness-new-task %cd lm-evaluation-harness-new-task !git checkout -b \"cool-patrol\" !pip install -q -e \".[dev]\" <pre>Cloning into 'lm-evaluation-harness-new-task'...\nremote: Enumerating objects: 7910, done.\nremote: Counting objects: 100% (766/766), done.\nremote: Compressing objects: 100% (65/65), done.\nremote: Total 7910 (delta 730), reused 701 (delta 701), pack-reused 7144\nReceiving objects: 100% (7910/7910), 9.49 MiB | 21.74 MiB/s, done.\nResolving deltas: 100% (5080/5080), done.\n[Errno 2] No such file or directory: 'lm-evaluation-harness-new-task'\n/content/lm-evaluation-harness\nSwitched to a new branch 'cool-patrol'\n  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.6/1.6 MB 13.2 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.3/58.3 kB 7.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 203.7/203.7 kB 14.4 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.6/62.6 kB 8.5 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.9/98.9 kB 12.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.7/3.7 MB 28.6 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 227.5/227.5 kB 26.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 468.9/468.9 kB 31.3 MB/s eta 0:00:00\n</pre> In\u00a0[13]: Copied! <pre># See https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_guide.md#creating-your-task-file\n\n!cp templates/new_task.py lm_eval/tasks/cool-patrol.py\n</pre> # See https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_guide.md#creating-your-task-file  !cp templates/new_task.py lm_eval/tasks/cool-patrol.py In\u00a0[13]: Copied! <pre>\n</pre>"},{"location":"LLM/Mistral-7b/LLM_evaluation_harness_for_Arc_Easy_and_SST/#language-model-evaluation-harness-starter-resource","title":"Language model evaluation harness starter resource\u00b6","text":"<p>This showcases how to evaluate your models on the LM Eval harness from EleutherAI</p> <p>Source: https://colab.research.google.com/drive/1zmZfdETnQ-AR2BBIK3pFtnP5937J1yaz?usp=sharing</p>"},{"location":"LLM/Mistral-7b/LLM_evaluation_harness_for_Arc_Easy_and_SST/#making-a-new-task-for-the-harness","title":"Making a new task for the harness\u00b6","text":"<p>This part documents how to create a new task for the language model evaluation harness and is based on this document.</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/","title":"Mistral Colab Finetune ipynb Colab Final","text":"In\u00a0[\u00a0]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <p>Let's define a wrapper function which will get completion from the model from a user question</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets scipy\n!pip install -q trl\n</pre> !pip install -q -U bitsandbytes !pip install -q -U git+https://github.com/huggingface/transformers.git !pip install -q -U git+https://github.com/huggingface/peft.git !pip install -q -U git+https://github.com/huggingface/accelerate.git !pip install -q datasets scipy !pip install -q trl In\u00a0[3]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_use_double_quant=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.bfloat16 ) <p>Now we specify the model ID and then we load it with our previously defined quantization configuration.</p> In\u00a0[\u00a0]: Copied! <pre>model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n</pre> model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True) <p>Run a inference on the base model. The model does not seem to understand our instruction and gives us a list of questions related to our query.</p> In\u00a0[1]: Copied! <pre>def get_completion(query: str, model, tokenizer) -&gt; str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  &lt;s&gt;\n  [INST]\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  [/INST]\n  &lt;/s&gt;\n  &lt;s&gt;\n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  decoded = tokenizer.batch_decode(generated_ids)\n  return (decoded[0])\n</pre> def get_completion(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"    prompt_template = \"\"\"      [INST]   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   [/INST]        \"\"\"   prompt = prompt_template.format(query=query)    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)    model_inputs = encodeds.to(device)     generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   decoded = tokenizer.batch_decode(generated_ids)   return (decoded[0]) In\u00a0[\u00a0]: Copied! <pre>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer) print(result) In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\ndataset\n</pre> from datasets import load_dataset  dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\") dataset In\u00a0[\u00a0]: Copied! <pre>df = dataset.to_pandas()\ndf.head(10)\n</pre> df = dataset.to_pandas() df.head(10) <p>Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :</p> <ol> <li>the function generate_prompt : take the instruction and output and generate a prompt</li> <li>shuffle the dataset</li> <li>tokenizer the dataset</li> </ol> In\u00a0[8]: Copied! <pre>def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"&lt;s&gt;[INST]{prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} [/INST]{data_point[\"output\"]}&lt;/s&gt;\"\"\"\n    # Without\n    else:\n        text = f\"\"\"&lt;s&gt;[INST]{prefix_text} {data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} &lt;/s&gt;\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\n</pre> def generate_prompt(data_point):     \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer      :param data_point: dict: Data point     :return: dict: tokenzed prompt     \"\"\"     prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\                'appropriately completes the request.\\n\\n'     # Samples with additional context into.     if data_point['input']:         text = f\"\"\"[INST]{prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} [/INST]{data_point[\"output\"]}\"\"\"     # Without     else:         text = f\"\"\"[INST]{prefix_text} {data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} \"\"\"     return text  # add the \"prompt\" column in the dataset text_column = [generate_prompt(data_point) for data_point in dataset] dataset = dataset.add_column(\"prompt\", text_column) <p>We'll need to tokenize our data so the model can understand.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n</pre> dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True) <p>Split dataset into 90% for training and 10% for testing</p> In\u00a0[10]: Copied! <pre>dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n</pre> dataset = dataset.train_test_split(test_size=0.2) train_data = dataset[\"train\"] test_data = dataset[\"test\"] In\u00a0[\u00a0]: Copied! <pre>print(test_data)\n</pre> print(test_data) In\u00a0[28]: Copied! <pre>from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n</pre> from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model model.gradient_checkpointing_enable() model = prepare_model_for_kbit_training(model) In\u00a0[\u00a0]: Copied! <pre>print(model)\n</pre> print(model) <p>Use the following function to find out the linear layers for fine tuning. QLoRA paper : \"We find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers is required to match full finetuning performance.\"</p> In\u00a0[14]: Copied! <pre>import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)\n</pre> import bitsandbytes as bnb def find_all_linear_names(model):   cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)   lora_module_names = set()   for name, module in model.named_modules():     if isinstance(module, cls):       names = name.split('.')       lora_module_names.add(names[0] if len(names) == 1 else names[-1])     if 'lm_head' in lora_module_names: # needed for 16-bit       lora_module_names.remove('lm_head')   return list(lora_module_names) In\u00a0[\u00a0]: Copied! <pre>modules = find_all_linear_names(model)\nprint(modules)\n</pre> modules = find_all_linear_names(model) print(modules) In\u00a0[16]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n</pre> from peft import LoraConfig, get_peft_model  lora_config = LoraConfig(     r=8,     lora_alpha=32,     target_modules=modules,     lora_dropout=0.05,     bias=\"none\",     task_type=\"CAUSAL_LM\" )  model = get_peft_model(model, lora_config) In\u00a0[\u00a0]: Copied! <pre>trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n</pre> trainable, total = model.get_nb_trainable_parameters() print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")  In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() <p>Setting the training arguments:</p> <ul> <li>for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># from datasets import load_dataset\n# data = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split='train')\n# data = data.train_test_split(test_size=0.1)\n# train_data = data[\"train\"]\n# test_data = data[\"test\"]\n</pre> # from datasets import load_dataset # data = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split='train') # data = data.train_test_split(test_size=0.1) # train_data = data[\"train\"] # test_data = data[\"test\"] In\u00a0[\u00a0]: Copied! <pre># import transformers\n\n# tokenizer.pad_token = tokenizer.eos_token\n\n\n# trainer = transformers.Trainer(\n#     model=model,\n#     train_dataset=train_data,\n#     eval_dataset=test_data,\n#     args=transformers.TrainingArguments(\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_ratio=0.03,\n#         max_steps=100,\n#         learning_rate=2e-4,\n#         fp16=True,\n#         logging_steps=1,\n#         output_dir=\"outputs_mistral_b_finance_finetuned_test\",\n#         optim=\"paged_adamw_8bit\",\n#         save_strategy=\"epoch\",\n#     ),\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n# )\n</pre> # import transformers  # tokenizer.pad_token = tokenizer.eos_token   # trainer = transformers.Trainer( #     model=model, #     train_dataset=train_data, #     eval_dataset=test_data, #     args=transformers.TrainingArguments( #         per_device_train_batch_size=1, #         gradient_accumulation_steps=4, #         warmup_ratio=0.03, #         max_steps=100, #         learning_rate=2e-4, #         fp16=True, #         logging_steps=1, #         output_dir=\"outputs_mistral_b_finance_finetuned_test\", #         optim=\"paged_adamw_8bit\", #         save_strategy=\"epoch\", #     ), #     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # )  In\u00a0[\u00a0]: Copied! <pre>#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0.03,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n</pre> #new code using SFTTrainer import transformers  from trl import SFTTrainer  tokenizer.pad_token = tokenizer.eos_token torch.cuda.empty_cache()  trainer = SFTTrainer(     model=model,     train_dataset=train_data,     eval_dataset=test_data,     dataset_text_field=\"prompt\",     peft_config=lora_config,     args=transformers.TrainingArguments(         per_device_train_batch_size=1,         gradient_accumulation_steps=4,         warmup_steps=0.03,         max_steps=100,         learning_rate=2e-4,         logging_steps=1,         output_dir=\"outputs\",         optim=\"paged_adamw_8bit\",         save_strategy=\"epoch\",     ),     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), ) <p>Start the training</p> In\u00a0[\u00a0]: Copied! <pre>model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n</pre> model.config.use_cache = False  # silence the warnings. Please re-enable for inference! trainer.train()  <p>Share adapters on the \ud83e\udd17 Hub</p> In\u00a0[21]: Copied! <pre>new_model = \"mistralai-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub\n</pre> new_model = \"mistralai-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub In\u00a0[22]: Copied! <pre>trainer.model.save_pretrained(new_model)\n</pre> trainer.model.save_pretrained(new_model) In\u00a0[\u00a0]: Copied! <pre>base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</pre> base_model = AutoModelForCausalLM.from_pretrained(     model_id,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= merged_model.merge_and_unload()  # Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\") tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\" In\u00a0[\u00a0]: Copied! <pre># Push the model and tokenizer to the Hugging Face Model Hub\nmerged_model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n</pre> # Push the model and tokenizer to the Hugging Face Model Hub merged_model.push_to_hub(new_model, use_temp_dir=False) tokenizer.push_to_hub(new_model, use_temp_dir=False) In\u00a0[34]: Copied! <pre>def get_completion_merged(query: str, model, tokenizer) -&gt; str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  &lt;s&gt;\n  [INST]\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  [/INST]\n  &lt;/s&gt;\n\n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n  generated_ids = merged_model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  decoded = tokenizer.batch_decode(generated_ids)\n  return (decoded[0])\n</pre> def get_completion_merged(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"    prompt_template = \"\"\"      [INST]   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   [/INST]        \"\"\"   prompt = prompt_template.format(query=query)    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)    model_inputs = encodeds.to(device)    generated_ids = merged_model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   decoded = tokenizer.batch_decode(generated_ids)   return (decoded[0]) In\u00a0[\u00a0]: Copied! <pre>result = get_completion_merged(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion_merged(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer) print(result)"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#instruct-fine-tuning-mistral-7b-instruct-using-qlora-and-supervise-finetuning","title":"Instruct Fine-tuning Mistral 7B Instruct using qLora and Supervise Finetuning\u00b6","text":"<p>This is a comprahensive notebook and tutorial on how to fine tune the Mistral-7b-Instruct Model</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#meet-mistral-7b-instruct","title":"Meet Mistral 7B Instruct\u00b6","text":"<p>The team at MistralAI has created an exceptional language model called Mistral 7B Instruct. It has consistently delivered outstanding results in a range of benchmarks, which positions it as an ideal option for natural language generation and understanding. This guide will concentrate on how to fine-tune the model for coding purposes, but the methodology can effectively be applied to other tasks.</p> <p>All the code will be available on my Github. Do drop by and give a follow and a star. adithya-s-k Github Code</p> <p>I also post content about LLMs and what I have been working on Twitter. AdithyaSK (@adithya_s_k) / X</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:</p> <ol> <li>GPU: This tutorial cannot run on free Google Colab; it requires more powerful GPUs, such as the A100.</li> <li>Python Packages: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:</li> </ol> <p>Let's begin by checking if your GPU is correctly detected:</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#step-1-install-necessary-packages","title":"Step 1 - Install necessary packages\u00b6","text":"<p>First, install the dependencies below to get started. As these features are available on the main branches only, we need to install the libraries below from source.</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#step-2-model-loading","title":"Step 2 - Model loading\u00b6","text":"<p>We'll load the model using QLoRA quantization to reduce the usage of memory</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#step-3-load-dataset-for-finetuning","title":"Step 3 - Load dataset for finetuning\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#lets-load-the-dataset","title":"Lets Load the Dataset\u00b6","text":"<p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>We will be using this dataset which is curated by TokenBender (e/xperiments) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <pre>{\n  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n  \"input\": \"[1, 2, 3, 4, 5]\",\n  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n}\n</pre>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#formatting-the-dataset","title":"Formatting the Dataset\u00b6","text":"<p>Now, let's format the dataset in the required Mistral-7B-Instruct-v0.1 format.</p> <p>Many tutorials and blogs skip over this part, but I feel this is a really important step.</p> <p>We'll put each instruction and input pair between <code>[INST]</code> and <code>[/INST]</code> output after that, like this:</p> <pre><code>&lt;s&gt;[INST] What is your favorite condiment? [/INST]\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;/s&gt;\n</code></pre> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#after-formatting-we-should-get-something-like-this","title":"After Formatting, We should get something like this\u00b6","text":"<pre>{\n\"text\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\",\n\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n\"input\":\"[1, 2, 3, 4, 5]\",\n\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n sequence: sum += num return sum\"\n\"prompt\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\"\n\n}\n</pre> <p>While using SFT (Supervised Fine-tuning Trainer) for fine-tuning, we will be only passing in the \u201ctext\u201d column of the dataset for fine-tuning.</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#step-4-apply-lora","title":"Step 4 - Apply Lora\u00b6","text":"<p>Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#step-5-run-the-training","title":"Step 5 - Run the training!\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#fine-tuning-with-qlora-and-supervised-fine-tuning","title":"Fine-Tuning with qLora and Supervised Fine-Tuning\u00b6","text":"<p>We're ready to fine-tune our model using qLora. For this tutorial, we'll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p>"},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#lets-start-the-training-process","title":"Let's start the training process\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_Colab_Finetune_ipynb_Colab_Final/#step-6-evaluating-the-model-qualitatively-run-an-inference","title":"Step 6 Evaluating the model qualitatively: run an inference!\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/","title":"Finetune Mistral","text":"In\u00a0[\u00a0]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <p>Let's define a wrapper function which will get completion from the model from a user question</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets scipy\n!pip install -q trl\n</pre> !pip install -q -U bitsandbytes !pip install -q -U git+https://github.com/huggingface/transformers.git !pip install -q -U git+https://github.com/huggingface/peft.git !pip install -q -U git+https://github.com/huggingface/accelerate.git !pip install -q datasets scipy !pip install -q trl In\u00a0[3]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_use_double_quant=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.bfloat16 ) <p>Now we specify the model ID and then we load it with our previously defined quantization configuration.</p> In\u00a0[\u00a0]: Copied! <pre>model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n</pre> model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True) <p>Run a inference on the base model. The model does not seem to understand our instruction and gives us a list of questions related to our query.</p> In\u00a0[1]: Copied! <pre>def get_completion(query: str, model, tokenizer) -&gt; str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  &lt;s&gt;\n  [INST]\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  [/INST]\n  &lt;/s&gt;\n  &lt;s&gt;\n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  decoded = tokenizer.batch_decode(generated_ids)\n  return (decoded[0])\n</pre> def get_completion(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"    prompt_template = \"\"\"      [INST]   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   [/INST]        \"\"\"   prompt = prompt_template.format(query=query)    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)    model_inputs = encodeds.to(device)     generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   decoded = tokenizer.batch_decode(generated_ids)   return (decoded[0]) In\u00a0[\u00a0]: Copied! <pre>result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer) print(result) In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\ndataset\n</pre> from datasets import load_dataset  dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\") dataset In\u00a0[\u00a0]: Copied! <pre>df = dataset.to_pandas()\ndf.head(10)\n</pre> df = dataset.to_pandas() df.head(10) <p>Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :</p> <ol> <li>the function generate_prompt : take the instruction and output and generate a prompt</li> <li>shuffle the dataset</li> <li>tokenizer the dataset</li> </ol> In\u00a0[8]: Copied! <pre>def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"&lt;s&gt;[INST]{prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} [/INST]{data_point[\"output\"]}&lt;/s&gt;\"\"\"\n    # Without\n    else:\n        text = f\"\"\"&lt;s&gt;[INST]{prefix_text} {data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} &lt;/s&gt;\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\n</pre> def generate_prompt(data_point):     \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer      :param data_point: dict: Data point     :return: dict: tokenzed prompt     \"\"\"     prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\                'appropriately completes the request.\\n\\n'     # Samples with additional context into.     if data_point['input']:         text = f\"\"\"[INST]{prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} [/INST]{data_point[\"output\"]}\"\"\"     # Without     else:         text = f\"\"\"[INST]{prefix_text} {data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} \"\"\"     return text  # add the \"prompt\" column in the dataset text_column = [generate_prompt(data_point) for data_point in dataset] dataset = dataset.add_column(\"prompt\", text_column) <p>We'll need to tokenize our data so the model can understand.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n</pre> dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True) <p>Split dataset into 90% for training and 10% for testing</p> In\u00a0[10]: Copied! <pre>dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n</pre> dataset = dataset.train_test_split(test_size=0.2) train_data = dataset[\"train\"] test_data = dataset[\"test\"] In\u00a0[\u00a0]: Copied! <pre>print(test_data)\n</pre> print(test_data) In\u00a0[28]: Copied! <pre>from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n</pre> from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model model.gradient_checkpointing_enable() model = prepare_model_for_kbit_training(model) In\u00a0[\u00a0]: Copied! <pre>print(model)\n</pre> print(model) <p>Use the following function to find out the linear layers for fine tuning. QLoRA paper : \"We find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers is required to match full finetuning performance.\"</p> In\u00a0[14]: Copied! <pre>import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)\n</pre> import bitsandbytes as bnb def find_all_linear_names(model):   cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)   lora_module_names = set()   for name, module in model.named_modules():     if isinstance(module, cls):       names = name.split('.')       lora_module_names.add(names[0] if len(names) == 1 else names[-1])     if 'lm_head' in lora_module_names: # needed for 16-bit       lora_module_names.remove('lm_head')   return list(lora_module_names) In\u00a0[\u00a0]: Copied! <pre>modules = find_all_linear_names(model)\nprint(modules)\n</pre> modules = find_all_linear_names(model) print(modules) In\u00a0[16]: Copied! <pre>from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n</pre> from peft import LoraConfig, get_peft_model  lora_config = LoraConfig(     r=8,     lora_alpha=32,     target_modules=modules,     lora_dropout=0.05,     bias=\"none\",     task_type=\"CAUSAL_LM\" )  model = get_peft_model(model, lora_config) In\u00a0[\u00a0]: Copied! <pre>trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n</pre> trainable, total = model.get_nb_trainable_parameters() print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")  In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() <p>Setting the training arguments:</p> <ul> <li>for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># from datasets import load_dataset\n# data = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split='train')\n# data = data.train_test_split(test_size=0.1)\n# train_data = data[\"train\"]\n# test_data = data[\"test\"]\n</pre> # from datasets import load_dataset # data = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split='train') # data = data.train_test_split(test_size=0.1) # train_data = data[\"train\"] # test_data = data[\"test\"] In\u00a0[\u00a0]: Copied! <pre># import transformers\n\n# tokenizer.pad_token = tokenizer.eos_token\n\n\n# trainer = transformers.Trainer(\n#     model=model,\n#     train_dataset=train_data,\n#     eval_dataset=test_data,\n#     args=transformers.TrainingArguments(\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_steps=0.03,\n#         max_steps=100,\n#         learning_rate=2e-4,\n#         fp16=True,\n#         logging_steps=1,\n#         output_dir=\"outputs_mistral_b_finance_finetuned_test\",\n#         optim=\"paged_adamw_8bit\",\n#         save_strategy=\"epoch\",\n#     ),\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n# )\n</pre> # import transformers  # tokenizer.pad_token = tokenizer.eos_token   # trainer = transformers.Trainer( #     model=model, #     train_dataset=train_data, #     eval_dataset=test_data, #     args=transformers.TrainingArguments( #         per_device_train_batch_size=1, #         gradient_accumulation_steps=4, #         warmup_steps=0.03, #         max_steps=100, #         learning_rate=2e-4, #         fp16=True, #         logging_steps=1, #         output_dir=\"outputs_mistral_b_finance_finetuned_test\", #         optim=\"paged_adamw_8bit\", #         save_strategy=\"epoch\", #     ), #     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # )  In\u00a0[\u00a0]: Copied! <pre>#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0.03,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n</pre> #new code using SFTTrainer import transformers  from trl import SFTTrainer  tokenizer.pad_token = tokenizer.eos_token torch.cuda.empty_cache()  trainer = SFTTrainer(     model=model,     train_dataset=train_data,     eval_dataset=test_data,     dataset_text_field=\"prompt\",     peft_config=lora_config,     args=transformers.TrainingArguments(         per_device_train_batch_size=1,         gradient_accumulation_steps=4,         warmup_steps=0.03,         max_steps=100,         learning_rate=2e-4,         logging_steps=1,         output_dir=\"outputs\",         optim=\"paged_adamw_8bit\",         save_strategy=\"epoch\",     ),     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), ) <p>Start the training</p> In\u00a0[\u00a0]: Copied! <pre>model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n</pre> model.config.use_cache = False  # silence the warnings. Please re-enable for inference! trainer.train()  <p>Share adapters on the \ud83e\udd17 Hub</p> In\u00a0[21]: Copied! <pre>new_model = \"mistralai-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub\n</pre> new_model = \"mistralai-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub In\u00a0[22]: Copied! <pre>trainer.model.save_pretrained(new_model)\n</pre> trainer.model.save_pretrained(new_model) In\u00a0[\u00a0]: Copied! <pre>base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</pre> base_model = AutoModelForCausalLM.from_pretrained(     model_id,     low_cpu_mem_usage=True,     return_dict=True,     torch_dtype=torch.float16,     device_map={\"\": 0}, ) merged_model= PeftModel.from_pretrained(base_model, new_model) merged_model= merged_model.merge_and_unload()  # Save the merged model merged_model.save_pretrained(\"merged_model\",safe_serialization=True) tokenizer.save_pretrained(\"merged_model\") tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\" In\u00a0[\u00a0]: Copied! <pre># Push the model and tokenizer to the Hugging Face Model Hub\nmerged_model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n</pre> # Push the model and tokenizer to the Hugging Face Model Hub merged_model.push_to_hub(new_model, use_temp_dir=False) tokenizer.push_to_hub(new_model, use_temp_dir=False) In\u00a0[34]: Copied! <pre>def get_completion_merged(query: str, model, tokenizer) -&gt; str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  &lt;s&gt;\n  [INST]\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  [/INST]\n  &lt;/s&gt;\n\n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n  generated_ids = merged_model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  decoded = tokenizer.batch_decode(generated_ids)\n  return (decoded[0])\n</pre> def get_completion_merged(query: str, model, tokenizer) -&gt; str:   device = \"cuda:0\"    prompt_template = \"\"\"      [INST]   Below is an instruction that describes a task. Write a response that appropriately completes the request.   {query}   [/INST]        \"\"\"   prompt = prompt_template.format(query=query)    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)    model_inputs = encodeds.to(device)    generated_ids = merged_model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)   decoded = tokenizer.batch_decode(generated_ids)   return (decoded[0]) In\u00a0[\u00a0]: Copied! <pre>result = get_completion_merged(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)\n</pre> result = get_completion_merged(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer) print(result)"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#instruct-fine-tuning-mistral-7b-instruct-using-qlora-and-supervise-finetuning","title":"Instruct Fine-tuning Mistral 7B Instruct using qLora and Supervise Finetuning\u00b6","text":"<p>This is a comprahensive notebook and tutorial on how to fine tune the Mistral-7b-Instruct Model</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#meet-mistral-7b-instruct","title":"Meet Mistral 7B Instruct\u00b6","text":"<p>The team at MistralAI has created an exceptional language model called Mistral 7B Instruct. It has consistently delivered outstanding results in a range of benchmarks, which positions it as an ideal option for natural language generation and understanding. This guide will concentrate on how to fine-tune the model for coding purposes, but the methodology can effectively be applied to other tasks.</p> <p>All the code will be available on my Github. Do drop by and give a follow and a star. adithya-s-k Github Code</p> <p>I also post content about LLMs and what I have been working on Twitter. AdithyaSK (@adithya_s_k) / X</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:</p> <ol> <li>GPU: This tutorial cannot run on free Google Colab; it requires more powerful GPUs, such as the A100.</li> <li>Python Packages: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:</li> </ol> <p>Let's begin by checking if your GPU is correctly detected:</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#step-1-install-necessary-packages","title":"Step 1 - Install necessary packages\u00b6","text":"<p>First, install the dependencies below to get started. As these features are available on the main branches only, we need to install the libraries below from source.</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#step-2-model-loading","title":"Step 2 - Model loading\u00b6","text":"<p>We'll load the model using QLoRA quantization to reduce the usage of memory</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#step-3-load-dataset-for-finetuning","title":"Step 3 - Load dataset for finetuning\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#lets-load-the-dataset","title":"Lets Load the Dataset\u00b6","text":"<p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>We will be using this dataset which is curated by TokenBender (e/xperiments) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <pre>{\n  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n  \"input\": \"[1, 2, 3, 4, 5]\",\n  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n}\n</pre>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#formatting-the-dataset","title":"Formatting the Dataset\u00b6","text":"<p>Now, let's format the dataset in the required Mistral-7B-Instruct-v0.1 format.</p> <p>Many tutorials and blogs skip over this part, but I feel this is a really important step.</p> <p>We'll put each instruction and input pair between <code>[INST]</code> and <code>[/INST]</code> output after that, like this:</p> <pre><code>&lt;s&gt;[INST] What is your favorite condiment? [/INST]\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;/s&gt;\n</code></pre> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#after-formatting-we-should-get-something-like-this","title":"After Formatting, We should get something like this\u00b6","text":"<pre>{\n\"text\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\",\n\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n\"input\":\"[1, 2, 3, 4, 5]\",\n\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n sequence: sum += num return sum\"\n\"prompt\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\"\n\n}\n</pre> <p>While using SFT (Supervised Fine-tuning Trainer) for fine-tuning, we will be only passing in the \u201ctext\u201d column of the dataset for fine-tuning.</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#step-4-apply-lora","title":"Step 4 - Apply Lora\u00b6","text":"<p>Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#step-5-run-the-training","title":"Step 5 - Run the training!\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#fine-tuning-with-qlora-and-supervised-fine-tuning","title":"Fine-Tuning with qLora and Supervised Fine-Tuning\u00b6","text":"<p>We're ready to fine-tune our model using qLora. For this tutorial, we'll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p>"},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#lets-start-the-training-process","title":"Let's start the training process\u00b6","text":""},{"location":"LLM/Mistral-7b/Mistral_finetuning_notebook/#step-6-evaluating-the-model-qualitatively-run-an-inference","title":"Step 6 Evaluating the model qualitatively: run an inference!\u00b6","text":""},{"location":"LLM/Mistral-7b/SFT/","title":"SFT","text":"In\u00a0[\u00a0]: Copied! <pre># This is a modified version of TRL's `SFTTrainer` example (https://github.com/huggingface/trl/blob/main/examples/scripts/sft_trainer.py), \n# adapted to run with DeepSpeed ZeRO-3 and Mistral-7B-V1.0. The settings below were run on 1 node of 8 x A100 (80GB) GPUs.\n#\n# Usage:\n#   - Install the latest transformers &amp; accelerate versions: `pip install -U transformers accelerate`\n#   - Install deepspeed: `pip install deepspeed==0.9.5`\n#   - Install TRL from main: pip install git+https://github.com/huggingface/trl.git\n#   - Clone the repo: git clone github.com/huggingface/trl.git\n#   - Copy this Gist into trl/examples/scripts\n#   - Run from root of trl repo with: accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --gradient_accumulation_steps 8 examples/scripts/sft_trainer.py  \nfrom dataclasses import dataclass, field\nfrom typing import Optional\n</pre> # This is a modified version of TRL's `SFTTrainer` example (https://github.com/huggingface/trl/blob/main/examples/scripts/sft_trainer.py),  # adapted to run with DeepSpeed ZeRO-3 and Mistral-7B-V1.0. The settings below were run on 1 node of 8 x A100 (80GB) GPUs. # # Usage: #   - Install the latest transformers &amp; accelerate versions: `pip install -U transformers accelerate` #   - Install deepspeed: `pip install deepspeed==0.9.5` #   - Install TRL from main: pip install git+https://github.com/huggingface/trl.git #   - Clone the repo: git clone github.com/huggingface/trl.git #   - Copy this Gist into trl/examples/scripts #   - Run from root of trl repo with: accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml --gradient_accumulation_steps 8 examples/scripts/sft_trainer.py   from dataclasses import dataclass, field from typing import Optional In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, AutoTokenizer\n</pre> import torch from accelerate import Accelerator from datasets import load_dataset from peft import LoraConfig from tqdm import tqdm from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, AutoTokenizer In\u00a0[\u00a0]: Copied! <pre>from trl import SFTTrainer\n</pre> from trl import SFTTrainer In\u00a0[\u00a0]: Copied! <pre>tqdm.pandas()\n</pre> tqdm.pandas() In\u00a0[\u00a0]: Copied! <pre># Define and parse arguments.\n@dataclass\nclass ScriptArguments:\n    \"\"\"\n    The name of the Casual LM model we wish to fine with SFTTrainer\n    \"\"\"\n\n    model_name: Optional[str] = field(default=\"mistralai/Mistral-7B-v0.1\", metadata={\"help\": \"the model name\"})\n    dataset_name: Optional[str] = field(\n        default=\"stingning/ultrachat\", metadata={\"help\": \"the dataset name\"}\n    )\n    dataset_text_field: Optional[str] = field(default=\"text\", metadata={\"help\": \"the text field of the dataset\"})\n    log_with: Optional[str] = field(default=\"wandb\", metadata={\"help\": \"use 'wandb' to log with wandb\"})\n    learning_rate: Optional[float] = field(default=2.0e-5, metadata={\"help\": \"the learning rate\"})\n    batch_size: Optional[int] = field(default=8, metadata={\"help\": \"the batch size\"})\n    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"Input sequence length\"})\n    gradient_accumulation_steps: Optional[int] = field(\n        default=8, metadata={\"help\": \"the number of gradient accumulation steps\"}\n    )\n    load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})\n    load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})\n    use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n    trust_remote_code: Optional[bool] = field(default=False, metadata={\"help\": \"Enable `trust_remote_code`\"})\n    output_dir: Optional[str] = field(default=\"output\", metadata={\"help\": \"the output directory\"})\n    peft_lora_r: Optional[int] = field(default=64, metadata={\"help\": \"the r parameter of the LoRA adapters\"})\n    peft_lora_alpha: Optional[int] = field(default=16, metadata={\"help\": \"the alpha parameter of the LoRA adapters\"})\n    logging_steps: Optional[int] = field(default=5, metadata={\"help\": \"the number of logging steps\"})\n    use_auth_token: Optional[bool] = field(default=True, metadata={\"help\": \"Use HF auth token to access the model\"})\n    num_train_epochs: Optional[int] = field(default=3, metadata={\"help\": \"the number of training epochs\"})\n    max_steps: Optional[int] = field(default=-1, metadata={\"help\": \"the number of training steps\"})\n    save_steps: Optional[int] = field(\n        default=1000, metadata={\"help\": \"Number of updates steps before two checkpoint saves\"}\n    )\n    save_total_limit: Optional[int] = field(default=10, metadata={\"help\": \"Limits total number of checkpoints.\"})\n    push_to_hub: Optional[bool] = field(default=True, metadata={\"help\": \"Push the model to HF Hub\"})\n    hub_model_id: Optional[str] = field(default=\"mistral-7b-finetuned-ultrachat\", metadata={\"help\": \"The name of the model on HF Hub\"})\n</pre> # Define and parse arguments. @dataclass class ScriptArguments:     \"\"\"     The name of the Casual LM model we wish to fine with SFTTrainer     \"\"\"      model_name: Optional[str] = field(default=\"mistralai/Mistral-7B-v0.1\", metadata={\"help\": \"the model name\"})     dataset_name: Optional[str] = field(         default=\"stingning/ultrachat\", metadata={\"help\": \"the dataset name\"}     )     dataset_text_field: Optional[str] = field(default=\"text\", metadata={\"help\": \"the text field of the dataset\"})     log_with: Optional[str] = field(default=\"wandb\", metadata={\"help\": \"use 'wandb' to log with wandb\"})     learning_rate: Optional[float] = field(default=2.0e-5, metadata={\"help\": \"the learning rate\"})     batch_size: Optional[int] = field(default=8, metadata={\"help\": \"the batch size\"})     seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"Input sequence length\"})     gradient_accumulation_steps: Optional[int] = field(         default=8, metadata={\"help\": \"the number of gradient accumulation steps\"}     )     load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})     load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})     use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})     trust_remote_code: Optional[bool] = field(default=False, metadata={\"help\": \"Enable `trust_remote_code`\"})     output_dir: Optional[str] = field(default=\"output\", metadata={\"help\": \"the output directory\"})     peft_lora_r: Optional[int] = field(default=64, metadata={\"help\": \"the r parameter of the LoRA adapters\"})     peft_lora_alpha: Optional[int] = field(default=16, metadata={\"help\": \"the alpha parameter of the LoRA adapters\"})     logging_steps: Optional[int] = field(default=5, metadata={\"help\": \"the number of logging steps\"})     use_auth_token: Optional[bool] = field(default=True, metadata={\"help\": \"Use HF auth token to access the model\"})     num_train_epochs: Optional[int] = field(default=3, metadata={\"help\": \"the number of training epochs\"})     max_steps: Optional[int] = field(default=-1, metadata={\"help\": \"the number of training steps\"})     save_steps: Optional[int] = field(         default=1000, metadata={\"help\": \"Number of updates steps before two checkpoint saves\"}     )     save_total_limit: Optional[int] = field(default=10, metadata={\"help\": \"Limits total number of checkpoints.\"})     push_to_hub: Optional[bool] = field(default=True, metadata={\"help\": \"Push the model to HF Hub\"})     hub_model_id: Optional[str] = field(default=\"mistral-7b-finetuned-ultrachat\", metadata={\"help\": \"The name of the model on HF Hub\"}) In\u00a0[\u00a0]: Copied! <pre>parser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses()[0]\n</pre> parser = HfArgumentParser(ScriptArguments) script_args = parser.parse_args_into_dataclasses()[0] In\u00a0[\u00a0]: Copied! <pre># Step 1: Load the dataset\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_name)\ndataset = load_dataset(script_args.dataset_name, split=\"train[:20000]\")\ndataset = dataset.train_test_split(test_size=0.1)\n</pre> # Step 1: Load the dataset tokenizer = AutoTokenizer.from_pretrained(script_args.model_name) dataset = load_dataset(script_args.dataset_name, split=\"train[:20000]\") dataset = dataset.train_test_split(test_size=0.1) In\u00a0[\u00a0]: Copied! <pre>def prepare_dialogue(example):\n    text = \"\"\n    for idx, msg in enumerate(example[\"data\"]):\n        if idx % 2 == 0:\n            text += f\"&lt;|user|&gt;\\n{msg}{tokenizer.eos_token}\\n\"\n        else:\n            text += f\"&lt;|assistant|&gt;\\n{msg}{tokenizer.eos_token}\\n\"\n    example[\"text\"] = text\n    return example\n</pre> def prepare_dialogue(example):     text = \"\"     for idx, msg in enumerate(example[\"data\"]):         if idx % 2 == 0:             text += f\"&lt;|user|&gt;\\n{msg}{tokenizer.eos_token}\\n\"         else:             text += f\"&lt;|assistant|&gt;\\n{msg}{tokenizer.eos_token}\\n\"     example[\"text\"] = text     return example In\u00a0[\u00a0]: Copied! <pre>dataset = dataset.map(prepare_dialogue, num_proc=4, remove_columns=[\"id\", \"data\"])\n</pre> dataset = dataset.map(prepare_dialogue, num_proc=4, remove_columns=[\"id\", \"data\"]) In\u00a0[\u00a0]: Copied! <pre># Step 2: Load the model\nif script_args.load_in_8bit and script_args.load_in_4bit:\n    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\nelif script_args.load_in_8bit or script_args.load_in_4bit:\n    quantization_config = BitsAndBytesConfig(\n        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit\n    )\n    # Copy the model to each device\n    device_map = {\"\": Accelerator().local_process_index}\n    torch_dtype = torch.bfloat16\nelse:\n    device_map = None\n    quantization_config = None\n    torch_dtype = None\n</pre> # Step 2: Load the model if script_args.load_in_8bit and script_args.load_in_4bit:     raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\") elif script_args.load_in_8bit or script_args.load_in_4bit:     quantization_config = BitsAndBytesConfig(         load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit     )     # Copy the model to each device     device_map = {\"\": Accelerator().local_process_index}     torch_dtype = torch.bfloat16 else:     device_map = None     quantization_config = None     torch_dtype = None In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(\n    script_args.model_name,\n    quantization_config=quantization_config,\n    device_map=device_map,\n    trust_remote_code=script_args.trust_remote_code,\n    torch_dtype=torch_dtype,\n    use_auth_token=script_args.use_auth_token,\n)\n</pre> model = AutoModelForCausalLM.from_pretrained(     script_args.model_name,     quantization_config=quantization_config,     device_map=device_map,     trust_remote_code=script_args.trust_remote_code,     torch_dtype=torch_dtype,     use_auth_token=script_args.use_auth_token, ) In\u00a0[\u00a0]: Copied! <pre># Step 3: Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=script_args.output_dir,\n    per_device_train_batch_size=script_args.batch_size,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    gradient_checkpointing=True,\n    learning_rate=script_args.learning_rate,\n    logging_steps=script_args.logging_steps,\n    num_train_epochs=script_args.num_train_epochs,\n    max_steps=script_args.max_steps,\n    report_to=script_args.log_with,\n    save_steps=script_args.save_steps,\n    save_total_limit=script_args.save_total_limit,\n    push_to_hub=script_args.push_to_hub,\n    hub_model_id=script_args.hub_model_id,\n    bf16=True,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    evaluation_strategy=\"epoch\",\n    logging_first_step=True,\n</pre> # Step 3: Define the training arguments training_args = TrainingArguments(     output_dir=script_args.output_dir,     per_device_train_batch_size=script_args.batch_size,     gradient_accumulation_steps=script_args.gradient_accumulation_steps,     gradient_checkpointing=True,     learning_rate=script_args.learning_rate,     logging_steps=script_args.logging_steps,     num_train_epochs=script_args.num_train_epochs,     max_steps=script_args.max_steps,     report_to=script_args.log_with,     save_steps=script_args.save_steps,     save_total_limit=script_args.save_total_limit,     push_to_hub=script_args.push_to_hub,     hub_model_id=script_args.hub_model_id,     bf16=True,     lr_scheduler_type=\"cosine\",     warmup_ratio=0.1,     evaluation_strategy=\"epoch\",     logging_first_step=True, In\u00a0[\u00a0]: Copied! <pre>)\n</pre> ) In\u00a0[\u00a0]: Copied! <pre># Step 4: Define the LoraConfig\nif script_args.use_peft:\n    peft_config = LoraConfig(\n        r=script_args.peft_lora_r,\n        lora_alpha=script_args.peft_lora_alpha,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\nelse:\n    peft_config = None\n</pre> # Step 4: Define the LoraConfig if script_args.use_peft:     peft_config = LoraConfig(         r=script_args.peft_lora_r,         lora_alpha=script_args.peft_lora_alpha,         bias=\"none\",         task_type=\"CAUSAL_LM\",     ) else:     peft_config = None In\u00a0[\u00a0]: Copied! <pre># Step 5: Define the Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    max_seq_length=script_args.seq_length,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    dataset_text_field=script_args.dataset_text_field,\n    peft_config=peft_config,\n    packing=True,\n)\n</pre> # Step 5: Define the Trainer trainer = SFTTrainer(     model=model,     args=training_args,     max_seq_length=script_args.seq_length,     train_dataset=dataset[\"train\"],     eval_dataset=dataset[\"test\"],     dataset_text_field=script_args.dataset_text_field,     peft_config=peft_config,     packing=True, ) In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() In\u00a0[\u00a0]: Copied! <pre># Step 6: Save the model\ntrainer.save_model(script_args.output_dir)\n</pre> # Step 6: Save the model trainer.save_model(script_args.output_dir)"},{"location":"LLM/Mistral-7b/notebooks_DPO_fine_tuning/","title":"DPO Fine-tuning Mistral","text":"In\u00a0[1]: Copied! <pre>pip install -q datasets trl peft bitsandbytes sentencepiece wandb huggingface_hub\n</pre> pip install -q datasets trl peft bitsandbytes sentencepiece wandb huggingface_hub <pre>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[1]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() <pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026</pre> In\u00a0[1]: Copied! <pre>import os\nimport gc\nimport torch\n\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom trl import DPOTrainer\nimport bitsandbytes as bnb\nimport wandb\nwandb.login()\n%env WANDB_PROJECT=hindi_dpo_test\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nnew_model = \"Hindi-SentenceRetrieval-Tinyllama-1.1B\"\n</pre> import os import gc import torch  import transformers from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig from datasets import load_dataset from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training from trl import DPOTrainer import bitsandbytes as bnb import wandb wandb.login() %env WANDB_PROJECT=hindi_dpo_test  model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" new_model = \"Hindi-SentenceRetrieval-Tinyllama-1.1B\" <pre>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: ahm-rimer. Use `wandb login --relogin` to force relogin\n</pre> <pre>env: WANDB_PROJECT=hindi_dpo_test\n</pre> In\u00a0[2]: Copied! <pre>def chatml_format(example):\n    # Format system\n    if len(example['task']) &gt; 0:\n        message = {\"role\": \"system\", \"content\": example['task']}\n        system = tokenizer.apply_chat_template([message], tokenize=False)\n    else:\n        system = \"\"\n\n    # Format instruction\n    message = {\"role\": \"user\", \"content\": example['query']}\n    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n\n    # Format chosen answer\n    chosen = example['pos'] + \"&lt;|im_end|&gt;\\n\"\n\n    # Format rejected answer\n    rejected = example['neg'] + \"&lt;|im_end|&gt;\\n\"\n\n    return {\n        \"prompt\": system + prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected,\n    }\n\n# Load dataset\ndataset = load_dataset(\"TokenBender/e5_FT_sentence_retrieval_task_Hindi_mini\")['train']\n\n# Save columns\noriginal_columns = dataset.column_names\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# Format dataset\ndataset = dataset.map(\n    chatml_format,\n    remove_columns=original_columns\n)\n</pre> def chatml_format(example):     # Format system     if len(example['task']) &gt; 0:         message = {\"role\": \"system\", \"content\": example['task']}         system = tokenizer.apply_chat_template([message], tokenize=False)     else:         system = \"\"      # Format instruction     message = {\"role\": \"user\", \"content\": example['query']}     prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)      # Format chosen answer     chosen = example['pos'] + \"&lt;|im_end|&gt;\\n\"      # Format rejected answer     rejected = example['neg'] + \"&lt;|im_end|&gt;\\n\"      return {         \"prompt\": system + prompt,         \"chosen\": chosen,         \"rejected\": rejected,     }  # Load dataset dataset = load_dataset(\"TokenBender/e5_FT_sentence_retrieval_task_Hindi_mini\")['train']  # Save columns original_columns = dataset.column_names  # Tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"left\"  # Format dataset dataset = dataset.map(     chatml_format,     remove_columns=original_columns ) In\u00a0[10]: Copied! <pre>dataset\n</pre> dataset Out[10]: <pre>Dataset({\n    features: ['prompt', 'chosen', 'rejected'],\n    num_rows: 6633\n})</pre> In\u00a0[13]: Copied! <pre>print(dataset[0])\n</pre> print(dataset[0]) <pre>{'prompt': '&lt;|system|&gt;\\n\u092a\u094d\u0930\u0936\u094d\u0928 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u090f\u0915 \u0938\u0947\u0932\u093f\u092c\u094d\u0930\u093f\u091f\u0940 \u0915\u093e \u0928\u093e\u092e \u0926\u093f\u090f \u091c\u093e\u0928\u0947 \u092a\u0930, \u0938\u093e\u0915\u094d\u0937\u093e\u0924\u094d\u0915\u093e\u0930 \u0914\u0930 \u091c\u0940\u0935\u0928\u0940 \u092a\u0941\u0928\u0930\u094d\u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u0930\u0947\u0902\u0964&lt;/s&gt;\\n&lt;|user|&gt;\\n\u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u090f\u0915 \u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0905\u092d\u093f\u0928\u0947\u0924\u093e, \u092b\u093f\u0932\u094d\u092e \u0928\u093f\u0930\u094d\u092e\u093e\u0924\u093e \u0914\u0930 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0915\u093e\u0930\u094d\u092f\u0915\u0930\u094d\u0924\u093e \u0939\u0948\u0902\u0964 \u0909\u0928\u094d\u0939\u0947\u0902 \u091b\u0939 \u0905\u0915\u093e\u0926\u092e\u0940 \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930\u094b\u0902, \u091a\u093e\u0930 \u092c\u094d\u0930\u093f\u091f\u093f\u0936 \u0905\u0915\u093e\u0926\u092e\u0940 \u092b\u093f\u0932\u094d\u092e \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930\u094b\u0902 \u0914\u0930 \u0928\u094c \u0938\u094d\u0915\u094d\u0930\u0940\u0928 \u090f\u0915\u094d\u091f\u0930\u094d\u0938 \u0917\u093f\u0932\u094d\u0921 \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930\u094b\u0902 \u0915\u0947 \u0932\u093f\u090f \u0928\u093e\u092e\u093e\u0902\u0915\u093f\u0924 \u0915\u093f\u092f\u093e \u0917\u092f\u093e \u0939\u0948, \u091c\u093f\u0928\u092e\u0947\u0902 \u0938\u0947 \u092a\u094d\u0930\u0924\u094d\u092f\u0947\u0915 \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0914\u0930 \u0917\u094d\u092f\u093e\u0930\u0939 \u0928\u093e\u092e\u093e\u0902\u0915\u0928\u094b\u0902 \u092e\u0947\u0902 \u0938\u0947 \u0924\u0940\u0928 \u0917\u094b\u0932\u094d\u0921\u0928 \u0917\u094d\u0932\u094b\u092c \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u091c\u0940\u0924\u0947 \u0939\u0948\u0902\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0928\u0947 1980 \u0915\u0947 \u0926\u0936\u0915 \u0915\u0947 \u0905\u0902\u0924 \u092e\u0947\u0902 \u091f\u0947\u0932\u0940\u0935\u093f\u091c\u0928 \u0935\u093f\u091c\u094d\u091e\u093e\u092a\u0928\u094b\u0902 \u092e\u0947\u0902 \u0926\u093f\u0916\u093e\u0908 \u0926\u0947\u0915\u0930 \u0905\u092a\u0928\u0947 \u0915\u0930\u093f\u092f\u0930 \u0915\u0940 \u0936\u0941\u0930\u0941\u0906\u0924 \u0915\u0940\u0964 \u0907\u0938\u0915\u0947 \u092c\u093e\u0926 \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0935\u093f\u092d\u093f\u0928\u094d\u0928 \u091f\u0947\u0932\u0940\u0935\u093f\u091c\u0928 \u0936\u094d\u0930\u0943\u0902\u0916\u0932\u093e\u0913\u0902 \u092e\u0947\u0902 \u0906\u0935\u0930\u094d\u0924\u0940 \u092d\u0942\u092e\u093f\u0915\u093e\u090f\u0901 \u0928\u093f\u092d\u093e\u0908\u0902, \u091c\u0948\u0938\u0947 \u0915\u093f \u0938\u094b\u092a \u0913\u092a\u0947\u0930\u093e \u0938\u093e\u0902\u0924\u093e \u092c\u093e\u0930\u092c\u0930\u093e \u0914\u0930 \u0938\u093f\u091f\u0915\u0949\u092e \u0917\u094d\u0930\u094b\u0907\u0902\u0917 \u092a\u0947\u0928\u094d\u0938\u0964 \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0930\u0949\u092c\u0930\u094d\u091f \u0921\u0940 \u0928\u0940\u0930\u094b \u0915\u0947 \u0938\u093e\u0925 \u0938\u0902\u0938\u094d\u092e\u0930\u0923 \u0926\u093f\u0938 \u092c\u0949\u092f\u091c\u093c \u0932\u093e\u0907\u092b \u0915\u0947 \u092b\u093f\u0932\u094d\u092e \u0930\u0942\u092a\u093e\u0902\u0924\u0930\u0923 \u092e\u0947\u0902 \u0905\u092d\u093f\u0928\u092f \u0915\u0930\u0928\u0947 \u0938\u0947 \u092a\u0939\u0932\u0947 \u0915\u094d\u0930\u093f\u091f\u0930\u094d\u0938 3 \u092e\u0947\u0902 \u091c\u094b\u0936 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u0905\u092d\u093f\u0928\u092f \u0915\u0930\u0915\u0947 \u0905\u092a\u0928\u0947 \u092b\u093f\u0932\u094d\u092e \u0915\u0930\u093f\u092f\u0930 \u0915\u0940 \u0936\u0941\u0930\u0941\u0906\u0924 \u0915\u0940\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u094b \u0928\u093e\u091f\u0915 \u0935\u0949\u091f\u094d\u0938 \u0908\u091f\u093f\u0902\u0917 \u0917\u093f\u0932\u094d\u092c\u0930\u094d\u091f \u0917\u094d\u0930\u0947\u092a (1993) \u092e\u0947\u0902 \u0909\u0928\u0915\u0940 \u0938\u0939\u093e\u092f\u0915 \u092d\u0942\u092e\u093f\u0915\u093e \u0915\u0947 \u0932\u093f\u090f \u0938\u0930\u093e\u0939\u093e \u0917\u092f\u093e \u0925\u093e, \u0914\u0930 \u091c\u0947\u092e\u094d\u0938 \u0915\u0948\u092e\u0930\u0942\u0928 \u0915\u0947 \u092e\u0939\u093e\u0915\u093e\u0935\u094d\u092f \u0930\u094b\u092e\u093e\u0902\u0938 \u091f\u093e\u0907\u091f\u0948\u0928\u093f\u0915 (1997) \u0915\u0947 \u0938\u093e\u0925 \u0905\u0902\u0924\u0930\u094d\u0930\u093e\u0937\u094d\u091f\u094d\u0930\u0940\u092f \u092a\u094d\u0930\u0938\u093f\u0926\u094d\u0927\u093f \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u0930\u0928\u0947 \u0938\u0947 \u092a\u0939\u0932\u0947, \u0926 \u092c\u093e\u0938\u094d\u0915\u0947\u091f\u092c\u0949\u0932 \u0921\u093e\u092f\u0930\u0940\u091c\u093c (1995) \u0914\u0930 \u0930\u094b\u092e\u093e\u0902\u091f\u093f\u0915 \u0921\u094d\u0930\u093e\u092e\u093e \u0930\u094b\u092e\u093f\u092f\u094b + \u091c\u0942\u0932\u093f\u092f\u091f (1996) \u0928\u093e\u091f\u0915 \u092e\u0947\u0902 \u092a\u094d\u0930\u092e\u0941\u0916 \u092d\u0942\u092e\u093f\u0915\u093e\u0913\u0902 \u0915\u0947 \u0938\u093e\u0925 \u0938\u093e\u0930\u094d\u0935\u091c\u0928\u093f\u0915 \u092a\u0939\u091a\u093e\u0928 \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u0940, \u091c\u094b \u0909\u0938 \u0938\u092e\u092f \u0924\u0915 \u0915\u0940 \u0938\u092c\u0938\u0947 \u0905\u0927\u093f\u0915 \u0915\u092e\u093e\u0908 \u0915\u0930\u0928\u0947 \u0935\u093e\u0932\u0940 \u092b\u093f\u0932\u094d\u092e \u092c\u0928 \u0917\u0908\u0964 2000 \u0915\u0947 \u0926\u0936\u0915 \u0938\u0947, \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u094b \u092b\u093f\u0932\u094d\u092e \u0936\u0948\u0932\u093f\u092f\u094b\u0902 \u0915\u0940 \u090f\u0915 \u0935\u093f\u0938\u094d\u0924\u0943\u0924 \u0936\u094d\u0930\u0943\u0902\u0916\u0932\u093e \u092e\u0947\u0902 \u0909\u0928\u0915\u0947 \u0915\u093e\u092e \u0915\u0947 \u0932\u093f\u090f \u0906\u0932\u094b\u091a\u0928\u093e\u0924\u094d\u092e\u0915 \u092a\u094d\u0930\u0936\u0902\u0938\u093e \u092e\u093f\u0932\u0940 \u0939\u0948\u0964 \u0909\u0928\u0915\u0940 \u092c\u093e\u0926 \u0915\u0940 \u092b\u093f\u0932\u094d\u092e\u094b\u0902 \u092e\u0947\u0902 \u0926 \u092e\u0948\u0928 \u0907\u0928 \u0926 \u0906\u092f\u0930\u0928 \u092e\u093e\u0938\u094d\u0915 (1998), \u091c\u0940\u0935\u0928\u0940 \u0905\u092a\u0930\u093e\u0927 \u0928\u093e\u091f\u0915 \u0915\u0948\u091a \u092e\u0940 \u0907\u092b \u092f\u0942 \u0915\u0948\u0928 (2002) \u0914\u0930 \u092e\u0939\u093e\u0915\u093e\u0935\u094d\u092f \u0910\u0924\u093f\u0939\u093e\u0938\u093f\u0915 \u0928\u093e\u091f\u0915 \u0917\u0948\u0902\u0917\u094d\u0938 \u0911\u092b \u0928\u094d\u092f\u0942\u092f\u0949\u0930\u094d\u0915 (2002) \u0936\u093e\u092e\u093f\u0932 \u0939\u0948\u0902, \u091c\u094b \u0928\u093f\u0930\u094d\u0926\u0947\u0936\u0915 \u092e\u093e\u0930\u094d\u091f\u093f\u0928 \u0938\u094d\u0915\u0949\u0930\u094d\u0938\u0947\u091c\u093c \u0915\u0947 \u0938\u093e\u0925 \u0909\u0928\u0915\u0947 \u0915\u0908 \u0938\u0939\u092f\u094b\u0917\u094b\u0902 \u092e\u0947\u0902 \u0938\u0947 \u092a\u0939\u0932\u0940 \u0925\u0940\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u094b \u0930\u093e\u091c\u0928\u0940\u0924\u093f\u0915 \u092f\u0941\u0926\u094d\u0927 \u0925\u094d\u0930\u093f\u0932\u0930 \u092c\u094d\u0932\u0921 \u0921\u093e\u092f\u092e\u0902\u0921 (2006), \u0928\u093f\u092f\u094b-\u0928\u0949\u092f\u0930 \u0915\u094d\u0930\u093e\u0907\u092e \u0921\u094d\u0930\u093e\u092e\u093e \u0926 \u0921\u093f\u092a\u093e\u0930\u094d\u091f\u0947\u0921 (2006), \u091c\u093e\u0938\u0942\u0938\u0940 \u0925\u094d\u0930\u093f\u0932\u0930 \u092c\u0949\u0921\u0940 \u0911\u092b \u0932\u093e\u0907\u091c\u093c (2008), \u0921\u094d\u0930\u093e\u092e\u093e \u0930\u093f\u0935\u094b\u0932\u094d\u092f\u0942\u0936\u0928\u0930\u0940 \u0930\u094b\u0921 (2008), \u092e\u0928\u094b\u0935\u0948\u091c\u094d\u091e\u093e\u0928\u093f\u0915 \u0925\u094d\u0930\u093f\u0932\u0930 \u0936\u091f\u0930 \u0906\u0907\u0932\u0948\u0902\u0921 (2010), \u0935\u093f\u091c\u094d\u091e\u093e\u0928 \u0915\u0925\u093e \u0925\u094d\u0930\u093f\u0932\u0930 \u0907\u0902\u0938\u0947\u092a\u094d\u0936\u0928 (2010), \u091c\u0940\u0935\u0928\u0940 \u092b\u093f\u0932\u094d\u092e \u091c\u0947. \u090f\u0921\u0917\u0930 (2011), \u092a\u0936\u094d\u091a\u093f\u092e\u0940 \u091c\u093e\u0902\u0917\u094b \u0905\u0928\u091a\u0947\u0928\u094d\u0921 (2012) \u0914\u0930 \u092a\u0940\u0930\u093f\u092f\u0921 \u0921\u094d\u0930\u093e\u092e\u093e \u0926 \u0917\u094d\u0930\u0947\u091f \u0917\u0948\u091f\u094d\u0938\u092c\u0940 (2013) \u092e\u0947\u0902 \u0909\u0928\u0915\u0947 \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u0915\u0947 \u0932\u093f\u090f \u0938\u0930\u093e\u0939\u093e \u0917\u092f\u093e \u0925\u093e\u0964 \u0926 \u090f\u0935\u093f\u090f\u091f\u0930 (2004) \u092e\u0947\u0902 \u0939\u0949\u0935\u0930\u094d\u0921 \u0939\u094d\u092f\u0942\u091c\u0947\u0938 \u0914\u0930 \u0926 \u0930\u0947\u0935\u0928\u0947\u0902\u091f (2015) \u092e\u0947\u0902 \u0939\u094d\u092f\u0942\u0917 \u0917\u094d\u0932\u093e\u0938 \u0915\u0947 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u0947 \u091a\u093f\u0924\u094d\u0930\u0923 \u0928\u0947 \u0909\u0928\u094d\u0939\u0947\u0902 \u0938\u0930\u094d\u0935\u0936\u094d\u0930\u0947\u0937\u094d\u0920 \u0905\u092d\u093f\u0928\u0947\u0924\u093e-\u092e\u094b\u0936\u0928 \u092a\u093f\u0915\u094d\u091a\u0930 \u0921\u094d\u0930\u093e\u092e\u093e \u0915\u0947 \u0932\u093f\u090f \u0917\u094b\u0932\u094d\u0921\u0928 \u0917\u094d\u0932\u094b\u092c \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u0926\u093f\u0932\u093e\u092f\u093e\u0964 \u0926 \u0935\u0941\u0932\u094d\u092b \u0911\u092b \u0935\u0949\u0932 \u0938\u094d\u091f\u094d\u0930\u0940\u091f (2013) \u092e\u0947\u0902 \u091c\u0949\u0930\u094d\u0921\u0928 \u092c\u0947\u0932\u094d\u092b\u094b\u0930\u094d\u091f \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u0909\u0928\u0915\u0947 \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u0928\u0947 \u0909\u0928\u094d\u0939\u0947\u0902 \u0938\u0930\u094d\u0935\u0936\u094d\u0930\u0947\u0937\u094d\u0920 \u0905\u092d\u093f\u0928\u0947\u0924\u093e-\u092e\u094b\u0936\u0928 \u092a\u093f\u0915\u094d\u091a\u0930 \u092e\u094d\u092f\u0942\u091c\u093f\u0915\u0932 \u092f\u093e \u0915\u0949\u092e\u0947\u0921\u0940 \u0915\u0947 \u0932\u093f\u090f \u0917\u094b\u0932\u094d\u0921\u0928 \u0917\u094d\u0932\u094b\u092c \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u0926\u093f\u0932\u093e\u092f\u093e\u0964 \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0926 \u0930\u0947\u0935\u0928\u0947\u0902\u091f \u0915\u0947 \u0932\u093f\u090f \u0938\u0930\u094d\u0935\u0936\u094d\u0930\u0947\u0937\u094d\u0920 \u0905\u092d\u093f\u0928\u0947\u0924\u093e \u0915\u093e \u0905\u092a\u0928\u093e \u092a\u0939\u0932\u093e \u0905\u0915\u093e\u0926\u092e\u0940 \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u092d\u0940 \u091c\u0940\u0924\u093e\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u090f\u092a\u093f\u092f\u0928 \u0935\u0947 \u092a\u094d\u0930\u094b\u0921\u0915\u094d\u0936\u0902\u0938 \u0915\u0947 \u0938\u0902\u0938\u094d\u0925\u093e\u092a\u0915 \u0939\u0948\u0902-\u090f\u0915 \u0928\u093f\u0930\u094d\u092e\u093e\u0923 \u0915\u0902\u092a\u0928\u0940 \u091c\u093f\u0938\u0928\u0947 \u0909\u0928\u0915\u0940 \u0915\u0941\u091b \u092b\u093f\u0932\u094d\u092e\u094b\u0902 \u0914\u0930 \u0935\u0943\u0924\u094d\u0924\u091a\u093f\u0924\u094d\u0930 \u0936\u094d\u0930\u0943\u0902\u0916\u0932\u093e \u0917\u094d\u0930\u0940\u0928\u094d\u0938\u092c\u0930\u094d\u0917 (2008-2010) \u0915\u093e \u0928\u093f\u0930\u094d\u092e\u093e\u0923 \u0915\u093f\u092f\u093e \u0939\u0948-\u0914\u0930 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u091c\u093e\u0917\u0930\u0942\u0915\u0924\u093e \u0915\u094b \u092c\u0922\u093c\u093e\u0935\u093e \u0926\u0947\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0938\u092e\u0930\u094d\u092a\u093f\u0924 \u090f\u0915 \u0917\u0948\u0930-\u0932\u093e\u092d\u0915\u093e\u0930\u0940 \u0938\u0902\u0917\u0920\u0928 \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928\u0964 \u0935\u0939 \u090f\u0915 \u092a\u094d\u0930\u0924\u093f\u092c\u0926\u094d\u0927 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0915\u093e\u0930\u094d\u092f\u0915\u0930\u094d\u0924\u093e \u092d\u0940 \u0939\u0948\u0902\u0964 2016 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u0947\u0902 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u0930\u093f\u0935\u0930\u094d\u0924\u0928 \u0915\u0947 \u0932\u093f\u090f \u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0936\u093e\u0902\u0924\u093f \u0926\u0942\u0924 \u0928\u093e\u092e\u093f\u0924 \u0915\u093f\u092f\u093e \u0917\u092f\u093e \u0925\u093e \u0914\u0930 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u0930\u093f\u0935\u0930\u094d\u0924\u0928 \u0938\u0947 \u0928\u093f\u092a\u091f\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0909\u0928\u0915\u0947 \u0915\u093e\u092e \u0915\u0947 \u0932\u093f\u090f \u0935\u093f\u0936\u094d\u0935 \u0906\u0930\u094d\u0925\u093f\u0915 \u092e\u0902\u091a \u092e\u0947\u0902 \u0915\u094d\u0930\u093f\u0938\u094d\u091f\u0932 \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0939\u0941\u0906 \u0925\u093e\u0964 2018 \u092e\u0947\u0902, \u091c\u0932 \u092d\u0943\u0902\u0917 \u0915\u0940 \u090f\u0915 \u092a\u094d\u0930\u091c\u093e\u0924\u093f \u0915\u093e \u0928\u093e\u092e \u0909\u0928\u0915\u0940 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923\u0940\u092f \u0938\u0915\u094d\u0930\u093f\u092f\u0924\u093e \u0915\u0940 \u092e\u093e\u0928\u094d\u092f\u0924\u093e \u092e\u0947\u0902 \u0909\u0928\u0915\u0947 \u0928\u093e\u092e \u092a\u0930 \u0930\u0916\u093e \u0917\u092f\u093e \u0925\u093e, \u0917\u094d\u0930\u094c\u0935\u0947\u0932\u093f\u0928\u0938 \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b\u0921\u093f\u0915\u093e\u092a\u094d\u0930\u093f\u092f\u094b\u0908\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u0947 \u092a\u093e\u0938 \u0932\u0949\u0938 \u092b\u0947\u0932\u093f\u091c\u093c, \u0932\u0949\u0938 \u090f\u0902\u091c\u093f\u0932\u094d\u0938 \u092e\u0947\u0902 \u090f\u0915 \u0918\u0930 \u0914\u0930 \u092c\u0948\u091f\u0930\u0940 \u092a\u093e\u0930\u094d\u0915 \u0938\u093f\u091f\u0940, \u0932\u094b\u0905\u0930 \u092e\u0948\u0928\u0939\u091f\u094d\u091f\u0928 \u092e\u0947\u0902 \u090f\u0915 \u0905\u092a\u093e\u0930\u094d\u091f\u092e\u0947\u0902\u091f \u0939\u0948\u0964 \u0935\u0939 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u0930\u093f\u0935\u0930\u094d\u0924\u0928 \u092a\u0930 \u0915\u093e\u0930\u094d\u0930\u0935\u093e\u0908 \u0915\u0947 \u0932\u093f\u090f \u090f\u0915 \u092e\u0941\u0916\u0930 \u0905\u0927\u093f\u0935\u0915\u094d\u0924\u093e \u0930\u0939\u0947 \u0939\u0948\u0902 \u0914\u0930 \u0909\u0928\u094d\u0939\u0947\u0902 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0938\u092e\u0942\u0939\u094b\u0902 \u0938\u0947 \u092a\u094d\u0930\u0936\u0902\u0938\u093e \u092e\u093f\u0932\u0940 \u0939\u0948\u0964 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u0930\u093f\u0935\u0930\u094d\u0924\u0928 \u0915\u0947 \u0916\u0924\u0930\u094b\u0902 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u091c\u093e\u0917\u0930\u0942\u0915\u0924\u093e \u092c\u0922\u093c\u093e\u0928\u0947 \u0915\u0947 \u0909\u0928\u0915\u0947 \u092a\u094d\u0930\u092f\u093e\u0938\u094b\u0902 \u0914\u0930 \u0905\u0915\u094d\u0937\u092f \u090a\u0930\u094d\u091c\u093e \u092e\u0947\u0902 \u0909\u0928\u0915\u0947 \u0928\u093f\u0935\u0947\u0936 \u0915\u0947 \u0932\u093f\u090f \u0909\u0928\u0915\u0940 \u0935\u093f\u0936\u0947\u0937 \u0930\u0942\u092a \u0938\u0947 \u092a\u094d\u0930\u0936\u0902\u0938\u093e \u0915\u0940 \u0917\u0908 \u0939\u0948\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b 1992 \u0938\u0947 \u0936\u093e\u0915\u093e\u0939\u093e\u0930\u0940 \u0930\u0939\u0947 \u0939\u0948\u0902 \u0914\u0930 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923\u0935\u093e\u0926 \u0914\u0930 \u092e\u093e\u0928\u0935\u0940\u092f \u0915\u093e\u0930\u0923\u094b\u0902 \u0915\u0947 \u092a\u094d\u0930\u0924\u093f \u0905\u092a\u0928\u0947 \u0938\u092e\u0930\u094d\u092a\u0923 \u0915\u0947 \u0932\u093f\u090f \u091c\u093e\u0928\u0947 \u091c\u093e\u0924\u0947 \u0939\u0948\u0902\u0964 \u0935\u0939 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0915\u0947 \u092e\u0941\u0926\u094d\u0926\u094b\u0902 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u091c\u093e\u0917\u0930\u0942\u0915\u0924\u093e \u092c\u0922\u093c\u093e\u0928\u0947 \u0915\u0947 \u0905\u092d\u093f\u092f\u093e\u0928\u094b\u0902 \u092e\u0947\u0902 \u0936\u093e\u092e\u093f\u0932 \u0930\u0939\u0947 \u0939\u0948\u0902 \u0914\u0930 \u091f\u0940\u090f\u091c\u0940 \u0939\u094d\u092f\u0942\u0905\u0930 \u0915\u0947 \u092c\u094d\u0930\u093e\u0902\u0921 \u090f\u0902\u092c\u0947\u0938\u0921\u0930 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u0915\u093e\u0930\u094d\u092f \u0915\u093f\u092f\u093e \u0939\u0948\u0964 \u0935\u0939 \u0926 11\u0925 \u0906\u0935\u0930 \u091c\u0948\u0938\u0947 \u0935\u0943\u0924\u094d\u0924\u091a\u093f\u0924\u094d\u0930\u094b\u0902 \u0915\u0947 \u0928\u093f\u0930\u094d\u092e\u093e\u0923 \u092e\u0947\u0902 \u092d\u0940 \u0936\u093e\u092e\u093f\u0932 \u0930\u0939\u0947 \u0939\u0948\u0902\u0964 2010 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u092d\u0942\u0915\u0902\u092a \u0915\u0947 \u092c\u093e\u0926 \u0939\u0948\u0924\u0940 \u092e\u0947\u0902 \u0930\u093e\u0939\u0924 \u092a\u094d\u0930\u092f\u093e\u0938\u094b\u0902 \u0915\u0947 \u0932\u093f\u090f $1 \u092e\u093f\u0932\u093f\u092f\u0928 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2017 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0924\u0942\u092b\u093e\u0928 \u0939\u093e\u0930\u094d\u0935\u0947 \u0915\u0947 \u092a\u0940\u0921\u093c\u093f\u0924\u094b\u0902 \u0915\u094b $1 \u092e\u093f\u0932\u093f\u092f\u0928 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2018 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0917\u0902\u092d\u0940\u0930 \u0930\u0942\u092a \u0938\u0947 \u0932\u0941\u092a\u094d\u0924\u092a\u094d\u0930\u093e\u092f \u0938\u0941\u092e\u093e\u0924\u094d\u0930\u093e \u0939\u093e\u0925\u0940 \u0915\u0947 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0914\u0930 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0915\u093e \u0938\u092e\u0930\u094d\u0925\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928 \u0915\u094b $30 \u0932\u093e\u0916 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2020 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0917\u0902\u092d\u0940\u0930 \u0930\u0942\u092a \u0938\u0947 \u0932\u0941\u092a\u094d\u0924\u092a\u094d\u0930\u093e\u092f \u0938\u0941\u092e\u093e\u0924\u094d\u0930\u093e \u0939\u093e\u0925\u0940 \u0915\u0947 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0914\u0930 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0915\u093e \u0938\u092e\u0930\u094d\u0925\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928 \u0915\u094b $30 \u0932\u093e\u0916 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2021 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0917\u0902\u092d\u0940\u0930 \u0930\u0942\u092a \u0938\u0947 \u0932\u0941\u092a\u094d\u0924\u092a\u094d\u0930\u093e\u092f \u0938\u0941\u092e\u093e\u0924\u094d\u0930\u093e \u0939\u093e\u0925\u0940 \u0915\u0947 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0914\u0930 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0915\u093e \u0938\u092e\u0930\u094d\u0925\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928 \u0915\u094b $43 \u092e\u093f\u0932\u093f\u092f\u0928 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964&lt;/s&gt;\\n&lt;|assistant|&gt;\\n', 'chosen': '\u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u090f\u0915 \u092a\u094d\u0930\u0924\u093f\u092d\u093e\u0936\u093e\u0932\u0940 \u0905\u092d\u093f\u0928\u0947\u0924\u093e \u0939\u0948\u0902 \u091c\u093f\u0928\u094d\u0939\u0947\u0902 \u091f\u093e\u0907\u091f\u0948\u0928\u093f\u0915 \u0914\u0930 \u0926 \u0930\u0947\u0935\u0928\u0947\u0902\u091f \u091c\u0948\u0938\u0940 \u092b\u093f\u0932\u094d\u092e\u094b\u0902 \u092e\u0947\u0902 \u0909\u0928\u0915\u0940 \u092d\u0942\u092e\u093f\u0915\u093e\u0913\u0902 \u0915\u0947 \u0932\u093f\u090f \u091c\u093e\u0928\u093e \u091c\u093e\u0924\u093e \u0939\u0948\u0964 \u0913 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0938\u0915\u094d\u0930\u093f\u092f\u0924\u093e\u092e\u0947 \u0938\u0947\u0939\u094b \u0936\u093e\u092e\u093f\u0932 \u0930\u0939\u0932 \u091b\u0925\u093f \u0906 \u0905\u092a\u0928 \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928\u0915 \u0932\u0947\u0932 \u0915\u0924\u0947\u0915\u094b \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u092f\u0928\u0947 \u091b\u0925\u093f\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u0930\u093f\u0935\u0930\u094d\u0924\u0928 \u092a\u0930 \u0915\u093e\u0930\u094d\u0930\u0935\u093e\u0908 \u0915\u0947 \u0932\u093f\u090f \u090f\u0915 \u092e\u0941\u0916\u0930 \u0905\u0927\u093f\u0935\u0915\u094d\u0924\u093e \u0930\u0939\u0947 \u0939\u0948\u0902 \u0914\u0930 \u0909\u0928\u094d\u0939\u0947\u0902 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0938\u092e\u0942\u0939\u094b\u0902 \u0938\u0947 \u092a\u094d\u0930\u0936\u0902\u0938\u093e \u092e\u093f\u0932\u0940 \u0939\u0948\u0964 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u0930\u093f\u0935\u0930\u094d\u0924\u0928 \u0915\u0947 \u0916\u0924\u0930\u094b\u0902 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u091c\u093e\u0917\u0930\u0942\u0915\u0924\u093e \u092c\u0922\u093c\u093e\u0928\u0947 \u0915\u0947 \u0909\u0928\u0915\u0947 \u092a\u094d\u0930\u092f\u093e\u0938\u094b\u0902 \u0914\u0930 \u0905\u0915\u094d\u0937\u092f \u090a\u0930\u094d\u091c\u093e \u092e\u0947\u0902 \u0909\u0928\u0915\u0947 \u0928\u093f\u0935\u0947\u0936 \u0915\u0947 \u0932\u093f\u090f \u0909\u0928\u0915\u0940 \u0935\u093f\u0936\u0947\u0937 \u0930\u0942\u092a \u0938\u0947 \u092a\u094d\u0930\u0936\u0902\u0938\u093e \u0915\u0940 \u0917\u0908 \u0939\u0948\u0964 \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b 1992 \u0938\u0947 \u0936\u093e\u0915\u093e\u0939\u093e\u0930\u0940 \u0930\u0939\u0947 \u0939\u0948\u0902 \u0914\u0930 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923\u0935\u093e\u0926 \u0914\u0930 \u092e\u093e\u0928\u0935\u0940\u092f \u0915\u093e\u0930\u0923\u094b\u0902 \u0915\u0947 \u092a\u094d\u0930\u0924\u093f \u0905\u092a\u0928\u0947 \u0938\u092e\u0930\u094d\u092a\u0923 \u0915\u0947 \u0932\u093f\u090f \u091c\u093e\u0928\u0947 \u091c\u093e\u0924\u0947 \u0939\u0948\u0902\u0964 \u0935\u0939 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923 \u0915\u0947 \u092e\u0941\u0926\u094d\u0926\u094b\u0902 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u091c\u093e\u0917\u0930\u0942\u0915\u0924\u093e \u092c\u0922\u093c\u093e\u0928\u0947 \u0915\u0947 \u0905\u092d\u093f\u092f\u093e\u0928\u094b\u0902 \u092e\u0947\u0902 \u0936\u093e\u092e\u093f\u0932 \u0930\u0939\u0947 \u0939\u0948\u0902 \u0914\u0930 \u091f\u0940\u090f\u091c\u0940 \u0939\u094d\u092f\u0942\u0905\u0930 \u0915\u0947 \u092c\u094d\u0930\u093e\u0902\u0921 \u090f\u0902\u092c\u0947\u0938\u0921\u0930 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u0915\u093e\u0930\u094d\u092f \u0915\u093f\u092f\u093e \u0939\u0948\u0964 \u0935\u0939 \u0926 11\u0925 \u0906\u0935\u0930 \u091c\u0948\u0938\u0947 \u0935\u0943\u0924\u094d\u0924\u091a\u093f\u0924\u094d\u0930\u094b\u0902 \u0915\u0947 \u0928\u093f\u0930\u094d\u092e\u093e\u0923 \u092e\u0947\u0902 \u092d\u0940 \u0936\u093e\u092e\u093f\u0932 \u0930\u0939\u0947 \u0939\u0948\u0902\u0964 2010 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u092d\u0942\u0915\u0902\u092a \u0915\u0947 \u092c\u093e\u0926 \u0939\u0948\u0924\u0940 \u092e\u0947\u0902 \u0930\u093e\u0939\u0924 \u092a\u094d\u0930\u092f\u093e\u0938\u094b\u0902 \u0915\u0947 \u0932\u093f\u090f $1 \u092e\u093f\u0932\u093f\u092f\u0928 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2017 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0924\u0942\u092b\u093e\u0928 \u0939\u093e\u0930\u094d\u0935\u0947 \u0915\u0947 \u092a\u0940\u0921\u093c\u093f\u0924\u094b\u0902 \u0915\u094b $1 \u092e\u093f\u0932\u093f\u092f\u0928 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2018 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0917\u0902\u092d\u0940\u0930 \u0930\u0942\u092a \u0938\u0947 \u0932\u0941\u092a\u094d\u0924\u092a\u094d\u0930\u093e\u092f \u0938\u0941\u092e\u093e\u0924\u094d\u0930\u093e \u0939\u093e\u0925\u0940 \u0915\u0947 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0914\u0930 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0915\u093e \u0938\u092e\u0930\u094d\u0925\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928 \u0915\u094b $30 \u0932\u093e\u0916 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2020 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0917\u0902\u092d\u0940\u0930 \u0930\u0942\u092a \u0938\u0947 \u0932\u0941\u092a\u094d\u0924\u092a\u094d\u0930\u093e\u092f \u0938\u0941\u092e\u093e\u0924\u094d\u0930\u093e \u0939\u093e\u0925\u0940 \u0915\u0947 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0914\u0930 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0915\u093e \u0938\u092e\u0930\u094d\u0925\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928 \u0915\u094b $30 \u0932\u093e\u0916 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964 2021 \u092e\u0947\u0902, \u0909\u0928\u094d\u0939\u094b\u0902\u0928\u0947 \u0917\u0902\u092d\u0940\u0930 \u0930\u0942\u092a \u0938\u0947 \u0932\u0941\u092a\u094d\u0924\u092a\u094d\u0930\u093e\u092f \u0938\u0941\u092e\u093e\u0924\u094d\u0930\u093e \u0939\u093e\u0925\u0940 \u0915\u0947 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0914\u0930 \u0938\u0902\u0930\u0915\u094d\u0937\u0923 \u0915\u093e \u0938\u092e\u0930\u094d\u0925\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u092b\u093e\u0909\u0902\u0921\u0947\u0936\u0928 \u0915\u094b $43 \u092e\u093f\u0932\u093f\u092f\u0928 \u0915\u093e \u0926\u093e\u0928 \u0926\u093f\u092f\u093e\u0964&lt;|im_end|&gt;\\n', 'rejected': '\u092a\u094d\u0930\u0938\u093f\u0926\u094d\u0927 \u0905\u092d\u093f\u0928\u0947\u0924\u093e \u0914\u0930 \u092a\u0930\u094b\u092a\u0915\u093e\u0930\u0940, \u0932\u093f\u092f\u094b\u0928\u093e\u0930\u094d\u0921\u094b \u0921\u093f\u0915\u0948\u092a\u094d\u0930\u093f\u092f\u094b \u0915\u0947 \u0938\u093e\u0915\u094d\u0937\u093e\u0924\u094d\u0915\u093e\u0930 \u0914\u0930 \u091c\u0940\u0935\u0928\u0940 \u0916\u094b\u091c\u0947\u0902, \u091c\u094b \u0905\u092a\u0928\u0940 \u092a\u0930\u094d\u092f\u093e\u0935\u0930\u0923\u0940\u092f \u0938\u0915\u094d\u0930\u093f\u092f\u0924\u093e \u0914\u0930 \u091f\u093e\u0907\u091f\u0948\u0928\u093f\u0915 \u0914\u0930 \u0926 \u0930\u0947\u0935\u0928\u0947\u0902\u091f \u091c\u0948\u0938\u0940 \u092b\u093f\u0932\u094d\u092e\u094b\u0902 \u092e\u0947\u0902 \u092a\u0941\u0930\u0938\u094d\u0915\u093e\u0930 \u0935\u093f\u091c\u0947\u0924\u093e \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u0915\u0947 \u0932\u093f\u090f \u091c\u093e\u0928\u0947 \u091c\u093e\u0924\u0947 \u0939\u0948\u0902\u0964&lt;|im_end|&gt;\\n'}\n</pre> In\u00a0[3]: Copied! <pre># LoRA configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n)\n</pre> # LoRA configuration peft_config = LoraConfig(     r=16,     lora_alpha=16,     lora_dropout=0.05,     bias=\"none\",     task_type=\"CAUSAL_LM\",     target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj'] ) In\u00a0[4]: Copied! <pre># Model to fine-tune\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True\n)\nmodel.config.use_cache = False\n\n# Reference model\nref_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True\n)\n</pre> # Model to fine-tune model = AutoModelForCausalLM.from_pretrained(     model_name,     torch_dtype=torch.float16,     load_in_4bit=True ) model.config.use_cache = False  # Reference model ref_model = AutoModelForCausalLM.from_pretrained(     model_name,     torch_dtype=torch.float16,     load_in_4bit=True ) In\u00a0[\u00a0]: Copied! <pre># Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    num_train_epochs=3,\n    save_strategy=\"no\",\n    logging_steps=1,\n    output_dir=new_model,\n    optim=\"paged_adamw_32bit\",\n    warmup_steps=100,\n    bf16=True,\n    report_to=\"wandb\",\n)\n\n# Create DPO trainer\ndpo_trainer = DPOTrainer(\n    model,\n    ref_model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    beta=0.1,\n    max_prompt_length=1024,\n    max_length=1536,\n)\n\n# Fine-tune model with DPO\ndpo_trainer.train()\n</pre> # Training arguments training_args = TrainingArguments(     per_device_train_batch_size=,     gradient_accumulation_steps=4,     gradient_checkpointing=True,     learning_rate=5e-5,     lr_scheduler_type=\"cosine\",     num_train_epochs=3,     save_strategy=\"no\",     logging_steps=1,     output_dir=new_model,     optim=\"paged_adamw_32bit\",     warmup_steps=100,     bf16=True,     report_to=\"wandb\", )  # Create DPO trainer dpo_trainer = DPOTrainer(     model,     ref_model,     args=training_args,     train_dataset=dataset,     tokenizer=tokenizer,     peft_config=peft_config,     beta=0.1,     max_prompt_length=1024,     max_length=1536, )  # Fine-tune model with DPO dpo_trainer.train() <pre>/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:314: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n  warnings.warn(\n</pre> <pre>Map:   0%|          | 0/6633 [00:00&lt;?, ? examples/s]</pre> <pre>Token indices sequence length is longer than the specified maximum sequence length for this model (4632 &gt; 2048). Running this sequence through the model will result in indexing errors\nDetected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n</pre>  Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&amp;B docs.   Tracking run with wandb version 0.16.2   Run data is saved locally in <code>/workspace/wandb/run-20240128_065712-os8qoadz</code>  Syncing run logical-monkey-4 to Weights &amp; Biases (docs)   View project at https://wandb.ai/ahm-rimer/hindi_dpo_test   View run at https://wandb.ai/ahm-rimer/hindi_dpo_test/runs/os8qoadz <pre>/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n</pre>        [ 92/621 1:09:37 &lt; 6:49:13, 0.02 it/s, Epoch 0.44/3]      Step Training Loss 1 0.703000 2 0.698300 3 0.703000 4 0.675100 5 0.694200 6 0.675100 7 0.675300 8 0.658600 9 0.641100 10 0.634400 11 0.609300 12 0.606500 13 0.607200 14 0.560900 15 0.563000 16 0.527200 17 0.473300 18 0.486900 19 0.454200 20 0.426300 21 0.381000 22 0.361900 23 0.344100 24 0.298400 25 0.298100 26 0.256600 27 0.245600 28 0.214800 29 0.190200 30 0.163100 31 0.148100 32 0.136700 33 0.117100 34 0.097800 35 0.105300 36 0.072300 37 0.077200 38 0.056100 39 0.051000 40 0.041900 41 0.035800 42 0.031700 43 0.013200 44 0.014600 45 0.036300 46 0.012200 47 0.012500 48 0.011200 49 0.013500 50 0.008400 51 0.004900 52 0.006900 53 0.010800 54 0.006800 55 0.003900 56 0.005600 57 0.002100 58 0.001800 59 0.004600 60 0.001600 61 0.002000 62 0.001400 63 0.001000 64 0.002900 65 0.000800 66 0.004300 67 0.000700 68 0.002700 69 0.000500 70 0.002600 71 0.000600 72 0.000400 73 0.000800 74 0.000700 75 0.000400 76 0.000600 77 0.000900 78 0.000300 79 0.001000 80 0.000300 81 0.002800 82 0.000900 83 0.000300 84 0.000200 85 0.000300 86 0.010300 87 0.001800 88 0.000400 89 0.000300 90 0.000200 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"LLM/Mistral-7b/notebooks_SFTTrainer%20TRL/","title":"SFT Trainer Mistral","text":"In\u00a0[1]: Copied! <pre>!pip install -q \"torch==2.1.2\" tensorboard wandb\n</pre> !pip install -q \"torch==2.1.2\" tensorboard wandb <pre>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</pre> In\u00a0[1]: Copied! <pre># Install Pytorch &amp; other libraries\n!pip install -q \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  -q --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  \"trl==0.7.10\"  \\\n  \"peft==0.7.1\"\n</pre> # Install Pytorch &amp; other libraries !pip install -q \"torch==2.1.2\" tensorboard  # Install Hugging Face libraries !pip install  -q --upgrade \\   \"transformers==4.36.2\" \\   \"datasets==2.16.1\" \\   \"accelerate==0.26.1\" \\   \"evaluate==0.4.1\" \\   \"bitsandbytes==0.42.0\" \\   \"trl==0.7.10\"  \\   \"peft==0.7.1\" <pre>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</pre> In\u00a0[2]: Copied! <pre>!pip install flash-attn\n</pre> !pip install flash-attn <pre>Collecting flash-attn\n  Downloading flash_attn-2.5.0.tar.gz (2.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.5/2.5 MB 12.4 MB/s eta 0:00:0000:0100:01\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.1.2)\nCollecting einops\n  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 kB 2.2 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (23.0)\nCollecting ninja\n  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 307.2/307.2 kB 31.4 MB/s eta 0:00:00\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (11.0.2.54)\nRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (2.1.0)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (12.1.105)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (2.18.1)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (8.9.2.26)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (12.1.105)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (3.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (2023.10.0)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (10.3.2.106)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (12.1.0.106)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (4.5.0)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (12.1.105)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (11.4.5.107)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (1.12)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch-&gt;flash-attn) (3.9.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch-&gt;flash-attn) (12.3.101)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2-&gt;torch-&gt;flash-attn) (2.1.1)\nRequirement already satisfied: mpmath&gt;=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy-&gt;torch-&gt;flash-attn) (1.3.0)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... done\n  Created wheel for flash-attn: filename=flash_attn-2.5.0-cp310-cp310-linux_x86_64.whl size=120823033 sha256=3335e74258645eb190597754d42c2fee391fbdeb772847f9e1de12da60450a33\n  Stored in directory: /root/.cache/pip/wheels/9e/c3/22/a576eb5627fb2c30dc4679a33d67d34d922d6dbeb24a9119b2\nSuccessfully built flash-attn\nInstalling collected packages: ninja, einops, flash-attn\nSuccessfully installed einops-0.7.0 flash-attn-2.5.0 ninja-1.11.1.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</pre> In\u00a0[4]: Copied! <pre>!git config --global credential.helper store\n</pre> !git config --global credential.helper store In\u00a0[5]: Copied! <pre>from huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)\n</pre> from huggingface_hub import login  login(   token=\"\", # ADD YOUR TOKEN HERE   add_to_git_credential=True ) <pre>Token is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n</pre> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\n\ndef create_conversation(sample):\n  return {\n    \"messages\": [\n      {\"role\": \"system\", \"content\": sample[\"task\"]},\n      {\"role\": \"user\", \"content\": sample[\"query\"]},\n      {\"role\": \"assistant\", \"content\": sample[\"pos\"]}\n    ]\n  }\n\n# Load dataset from the hub\ndataset = load_dataset(\"TokenBender/sentence_retrieval_hindi_SFT\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\n\n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=2500/12500)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk\ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n</pre> from datasets import load_dataset  def create_conversation(sample):   return {     \"messages\": [       {\"role\": \"system\", \"content\": sample[\"task\"]},       {\"role\": \"user\", \"content\": sample[\"query\"]},       {\"role\": \"assistant\", \"content\": sample[\"pos\"]}     ]   }  # Load dataset from the hub dataset = load_dataset(\"TokenBender/sentence_retrieval_hindi_SFT\", split=\"train\") dataset = dataset.shuffle().select(range(12500))  # Convert dataset to OAI messages dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False) # split dataset into 10,000 training samples and 2,500 test samples dataset = dataset.train_test_split(test_size=2500/12500)  print(dataset[\"train\"][345][\"messages\"])  # save datasets to disk dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\") dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\") <pre>Map:   0%|          | 0/12500 [00:00&lt;?, ? examples/s]</pre> <pre>[{'content': '\u0926\u0915\u094d\u0937\u093f\u0923 \u092a\u0942\u0930\u094d\u0935 \u090f\u0936\u093f\u092f\u093e \u092e\u0947\u0902 \u092f\u093e\u0924\u094d\u0930\u093e \u0915\u0947 \u0905\u0928\u0941\u092d\u0935\u094b\u0902 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u092c\u094d\u0932\u0949\u0917 \u092a\u094b\u0938\u094d\u091f \u0916\u094b\u091c\u0947\u0902\u0964', 'role': 'system'}, {'content': '\u092e\u0948\u0902\u0928\u0947 \u0939\u093e\u0932 \u0939\u0940 \u092e\u0947\u0902 \u0926\u0915\u094d\u0937\u093f\u0923 \u092a\u0942\u0930\u094d\u0935 \u090f\u0936\u093f\u092f\u093e \u0915\u0940 \u092f\u093e\u0924\u094d\u0930\u093e \u0915\u0940 \u0914\u0930 \u092c\u0948\u0902\u0915\u0949\u0915 \u092e\u0947\u0902 \u091c\u0940\u0935\u0902\u0924 \u0938\u094d\u091f\u094d\u0930\u0940\u091f \u092b\u0942\u0921 \u0926\u0943\u0936\u094d\u092f \u0915\u0940 \u0916\u094b\u091c \u0915\u0930\u0928\u0947, \u0939\u0928\u094b\u0908 \u0915\u0947 \u0938\u092e\u0943\u0926\u094d\u0927 \u0907\u0924\u093f\u0939\u093e\u0938 \u092e\u0947\u0902 \u0916\u0941\u0926 \u0915\u094b \u0921\u0941\u092c\u094b\u0928\u0947 \u0914\u0930 \u092c\u093e\u0932\u0940 \u092e\u0947\u0902 \u091b\u093f\u092a\u0947 \u0939\u0941\u090f \u0930\u0924\u094d\u0928\u094b\u0902 \u0915\u0940 \u0916\u094b\u091c \u0915\u0930\u0928\u0947 \u092e\u0947\u0902 \u090f\u0915 \u0905\u0926\u094d\u092d\u0941\u0924 \u0938\u092e\u092f \u092c\u093f\u0924\u093e\u092f\u093e\u0964 \u0905\u0902\u0926\u0930\u0942\u0928\u0940 \u0938\u0941\u091d\u093e\u0935\u094b\u0902 \u0914\u0930 \u0938\u093f\u092b\u093e\u0930\u093f\u0936\u094b\u0902 \u0915\u0947 \u0932\u093f\u090f \u092e\u0947\u0930\u0947 \u092c\u094d\u0932\u0949\u0917 \u092a\u094b\u0938\u094d\u091f \u0915\u094b \u0926\u0947\u0916\u0947\u0902!', 'role': 'user'}, {'content': '\u0905\u0926\u094d\u0935\u093f\u0924\u0940\u092f \u0906\u0930\u093e\u092e \u0914\u0930 \u0932\u0941\u092d\u093e\u0935\u0928\u0947 \u0926\u0943\u0936\u094d\u092f\u094b\u0902 \u0915\u0940 \u092a\u0947\u0936\u0915\u0936 \u0915\u0930\u0924\u0947 \u0939\u0941\u090f \u0926\u0915\u094d\u0937\u093f\u0923 \u092a\u0942\u0930\u094d\u0935 \u090f\u0936\u093f\u092f\u093e \u092e\u0947\u0902 \u0936\u0940\u0930\u094d\u0937 10 \u0932\u0915\u094d\u091c\u0930\u0940 \u0930\u093f\u0938\u0949\u0930\u094d\u091f\u094d\u0938 \u0915\u0940 \u0916\u094b\u091c \u0915\u0930\u0947\u0902\u0964 \u0907\u0928 \u0935\u093f\u0936\u093f\u0937\u094d\u091f \u0917\u0902\u0924\u0935\u094d\u092f\u094b\u0902 \u092e\u0947\u0902 \u0935\u093f\u0936\u094d\u0935 \u0938\u094d\u0924\u0930\u0940\u092f \u0938\u0941\u0935\u093f\u0927\u093e\u0913\u0902 \u0914\u0930 \u0932\u093e\u0921\u093c-\u092a\u094d\u092f\u093e\u0930 \u0915\u0940 \u0938\u0947\u0935\u093e\u0913\u0902 \u092e\u0947\u0902 \u0936\u093e\u092e\u093f\u0932 \u0939\u094b\u0902, \u091c\u094b \u0907\u0938 \u0915\u094d\u0937\u0947\u0924\u094d\u0930 \u092e\u0947\u0902 \u090f\u0915 \u0906\u0930\u093e\u092e\u0926\u093e\u092f\u0915 \u0938\u0948\u0930 \u0915\u0947 \u0932\u093f\u090f \u090f\u0915\u0926\u092e \u0938\u0939\u0940 \u0939\u0948\u0902\u0964', 'role': 'assistant'}]\n</pre> <pre>Creating json from Arrow format:   0%|          | 0/10 [00:00&lt;?, ?ba/s]</pre> <pre>Creating json from Arrow format:   0%|          | 0/3 [00:00&lt;?, ?ba/s]</pre> Out[1]: <pre>12038569</pre> In\u00a0[2]: Copied! <pre>from datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n</pre> from datasets import load_dataset  # Load jsonl data from disk dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\") <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> In\u00a0[3]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import setup_chat_format\nimport wandb\nwandb.login()\n%env WANDB_PROJECT=hindi_sft_test_tinyllama\n\n# Hugging Face model id\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # or `mistralai/Mistral-7B-v0.1`\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'right' # to prevent warnings\n\n# # set chat template to OAI chatML, remove if you start from a fine-tuned model\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig from trl import setup_chat_format import wandb wandb.login() %env WANDB_PROJECT=hindi_sft_test_tinyllama  # Hugging Face model id model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # or `mistralai/Mistral-7B-v0.1`  # BitsAndBytesConfig int-4 config bnb_config = BitsAndBytesConfig(     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16 )  # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(     model_id,     device_map=\"auto\",     attn_implementation=\"flash_attention_2\",     torch_dtype=torch.bfloat16,     quantization_config=bnb_config ) tokenizer = AutoTokenizer.from_pretrained(model_id) tokenizer.padding_side = 'right' # to prevent warnings  # # set chat template to OAI chatML, remove if you start from a fine-tuned model model, tokenizer = setup_chat_format(model, tokenizer) <pre>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: ahm-rimer. Use `wandb login --relogin` to force relogin\n</pre> <pre>env: WANDB_PROJECT=hindi_sft_test_tinyllama\n</pre> In\u00a0[4]: Copied! <pre>from peft import LoraConfig\n\n# LoRA config based on QLoRA paper &amp; Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.05,\n        r=32,\n        bias=\"none\",\n        target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj'],\n        task_type=\"CAUSAL_LM\",\n)\n</pre> from peft import LoraConfig  # LoRA config based on QLoRA paper &amp; Sebastian Raschka experiment peft_config = LoraConfig(         lora_alpha=16,         lora_dropout=0.05,         r=32,         bias=\"none\",         target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj'],         task_type=\"CAUSAL_LM\", ) In\u00a0[7]: Copied! <pre>from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"tinyllama_hindi_sentence_retrieval_sft\", # directory to save and repository id\n    num_train_epochs=1,                     # number of training epochs\n    per_device_train_batch_size=4,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=1,                       # log every 10 steps\n    save_steps=0.3,                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to hub\n    report_to=\"wandb\",                      # report metrics to wandb\n)\n</pre> from transformers import TrainingArguments  args = TrainingArguments(     output_dir=\"tinyllama_hindi_sentence_retrieval_sft\", # directory to save and repository id     num_train_epochs=1,                     # number of training epochs     per_device_train_batch_size=4,          # batch size per device during training     gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass     gradient_checkpointing=True,            # use gradient checkpointing to save memory     optim=\"adamw_torch_fused\",              # use fused adamw optimizer     logging_steps=1,                       # log every 10 steps     save_steps=0.3,                  # save checkpoint every epoch     learning_rate=2e-4,                     # learning rate, based on QLoRA paper     bf16=True,                              # use bfloat16 precision     tf32=True,                              # use tf32 precision     max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper     warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper     lr_scheduler_type=\"constant\",           # use constant learning rate scheduler     push_to_hub=True,                       # push model to hub     report_to=\"wandb\",                      # report metrics to wandb ) In\u00a0[8]: Copied! <pre>from trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n</pre> from trl import SFTTrainer  max_seq_length = 2048 # max sequence length for model and packing of the dataset  trainer = SFTTrainer(     model=model,     args=args,     train_dataset=dataset,     peft_config=peft_config,     max_seq_length=max_seq_length,     tokenizer=tokenizer,     packing=True,     dataset_kwargs={         \"add_special_tokens\": False,  # We template with special tokens         \"append_concat_token\": False, # No need to add additional separator token     } ) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> In\u00a0[9]: Copied! <pre># start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model\ntrainer.save_model()\n</pre> # start training, the model will be automatically saved to the hub and the output directory trainer.train()  # save model trainer.save_model()  Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&amp;B docs.   Tracking run with wandb version 0.16.2   Run data is saved locally in <code>/workspace/wandb/run-20240128_191736-13n2gxgh</code>  Syncing run twilight-plant-1 to Weights &amp; Biases (docs)   View project at https://wandb.ai/ahm-rimer/hindi_sft_test_tinyllama   View run at https://wandb.ai/ahm-rimer/hindi_sft_test_tinyllama/runs/13n2gxgh <pre>You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n</pre>        [622/622 41:28, Epoch 1/1]      Step Training Loss 1 1.197200 2 1.141000 3 1.131400 4 1.086400 5 1.089900 6 1.004200 7 1.032800 8 1.062700 9 1.045000 10 0.994600 11 0.979000 12 0.966600 13 0.980000 14 0.914500 15 0.952300 16 0.915400 17 0.941800 18 0.949200 19 0.864800 20 0.937400 21 0.959400 22 0.929800 23 0.892400 24 0.900700 25 0.891200 26 0.910400 27 0.850800 28 0.912600 29 0.832900 30 0.846400 31 0.840500 32 0.856000 33 0.793800 34 0.901100 35 0.871500 36 0.834300 37 0.832300 38 0.810800 39 0.840100 40 0.886200 41 0.823800 42 0.823300 43 0.868200 44 0.851900 45 0.845500 46 0.829100 47 0.826400 48 0.850900 49 0.808600 50 0.832700 51 0.784200 52 0.810200 53 0.785500 54 0.776400 55 0.784800 56 0.796800 57 0.803300 58 0.776000 59 0.829500 60 0.748200 61 0.778100 62 0.757000 63 0.818700 64 0.846200 65 0.811500 66 0.804400 67 0.752500 68 0.768000 69 0.773200 70 0.763800 71 0.725100 72 0.794800 73 0.734700 74 0.732800 75 0.758000 76 0.710200 77 0.781100 78 0.753400 79 0.701600 80 0.758800 81 0.837000 82 0.789900 83 0.775300 84 0.737000 85 0.776300 86 0.755400 87 0.745100 88 0.743800 89 0.693900 90 0.733400 91 0.786900 92 0.766600 93 0.769400 94 0.720600 95 0.730200 96 0.729800 97 0.740800 98 0.767000 99 0.757500 100 0.737800 101 0.728100 102 0.755200 103 0.698300 104 0.711400 105 0.766700 106 0.749500 107 0.705200 108 0.680300 109 0.674500 110 0.706600 111 0.759000 112 0.699500 113 0.709700 114 0.714800 115 0.708000 116 0.700300 117 0.673500 118 0.760100 119 0.694300 120 0.706500 121 0.721300 122 0.698400 123 0.738900 124 0.729600 125 0.696200 126 0.676000 127 0.695700 128 0.729200 129 0.730000 130 0.719900 131 0.726200 132 0.693100 133 0.706900 134 0.708700 135 0.691700 136 0.682500 137 0.727800 138 0.633700 139 0.710700 140 0.653100 141 0.717000 142 0.732800 143 0.677000 144 0.688600 145 0.673100 146 0.678900 147 0.679900 148 0.667800 149 0.643900 150 0.679000 151 0.666700 152 0.695600 153 0.655300 154 0.710500 155 0.659700 156 0.717600 157 0.657500 158 0.657900 159 0.695600 160 0.673400 161 0.642500 162 0.702800 163 0.713500 164 0.674100 165 0.746000 166 0.676800 167 0.669100 168 0.668800 169 0.655000 170 0.684400 171 0.688200 172 0.705100 173 0.669600 174 0.654800 175 0.691300 176 0.640200 177 0.691600 178 0.701600 179 0.718500 180 0.629500 181 0.706600 182 0.661800 183 0.649300 184 0.687800 185 0.623300 186 0.729500 187 0.645000 188 0.723100 189 0.665900 190 0.628100 191 0.707700 192 0.676500 193 0.644600 194 0.658400 195 0.729700 196 0.668800 197 0.672800 198 0.667000 199 0.679100 200 0.656400 201 0.633200 202 0.651700 203 0.648600 204 0.603300 205 0.655100 206 0.637800 207 0.624800 208 0.635600 209 0.640000 210 0.693500 211 0.677000 212 0.625200 213 0.668800 214 0.633200 215 0.643800 216 0.677900 217 0.602000 218 0.616500 219 0.653500 220 0.641100 221 0.624500 222 0.684600 223 0.670300 224 0.675900 225 0.609500 226 0.600900 227 0.642300 228 0.607700 229 0.666700 230 0.613300 231 0.661400 232 0.661800 233 0.627900 234 0.707200 235 0.611800 236 0.611900 237 0.574400 238 0.623300 239 0.681000 240 0.622300 241 0.651900 242 0.614700 243 0.654900 244 0.663600 245 0.670500 246 0.619700 247 0.586900 248 0.644200 249 0.614600 250 0.641000 251 0.633500 252 0.645700 253 0.672500 254 0.635300 255 0.644100 256 0.641300 257 0.569300 258 0.674100 259 0.622000 260 0.659600 261 0.605200 262 0.628800 263 0.606600 264 0.591900 265 0.623100 266 0.604400 267 0.605600 268 0.655400 269 0.695500 270 0.618400 271 0.669500 272 0.641000 273 0.626000 274 0.617500 275 0.620000 276 0.638700 277 0.592700 278 0.648200 279 0.636100 280 0.581300 281 0.557300 282 0.643300 283 0.646800 284 0.625300 285 0.654400 286 0.607100 287 0.593400 288 0.596900 289 0.539600 290 0.620200 291 0.595400 292 0.589700 293 0.642000 294 0.569100 295 0.595600 296 0.594500 297 0.646400 298 0.630300 299 0.658800 300 0.614100 301 0.663500 302 0.649000 303 0.609400 304 0.615200 305 0.628400 306 0.599600 307 0.611500 308 0.605600 309 0.590200 310 0.607900 311 0.627600 312 0.623900 313 0.643100 314 0.609400 315 0.582000 316 0.574000 317 0.600700 318 0.599200 319 0.596700 320 0.620400 321 0.579700 322 0.666400 323 0.576000 324 0.644500 325 0.593400 326 0.624900 327 0.577800 328 0.618400 329 0.586700 330 0.608200 331 0.598000 332 0.580400 333 0.624300 334 0.567800 335 0.593700 336 0.554100 337 0.719700 338 0.551600 339 0.565500 340 0.590000 341 0.591700 342 0.584800 343 0.605800 344 0.641100 345 0.588000 346 0.615200 347 0.567100 348 0.610200 349 0.626000 350 0.610900 351 0.591800 352 0.585600 353 0.599700 354 0.606800 355 0.571400 356 0.612700 357 0.585900 358 0.625800 359 0.642900 360 0.550300 361 0.566100 362 0.604000 363 0.600600 364 0.627300 365 0.521300 366 0.622500 367 0.562700 368 0.577400 369 0.546600 370 0.576200 371 0.582100 372 0.604100 373 0.632300 374 0.626800 375 0.593400 376 0.614400 377 0.566200 378 0.608800 379 0.562100 380 0.564600 381 0.576500 382 0.572100 383 0.573600 384 0.600700 385 0.500700 386 0.618800 387 0.561100 388 0.605900 389 0.579300 390 0.615000 391 0.540200 392 0.561600 393 0.563700 394 0.573000 395 0.597400 396 0.554300 397 0.565700 398 0.620500 399 0.513900 400 0.539300 401 0.609100 402 0.547700 403 0.557300 404 0.585300 405 0.586300 406 0.598300 407 0.547800 408 0.530200 409 0.620100 410 0.568500 411 0.596900 412 0.610400 413 0.587900 414 0.553600 415 0.608500 416 0.519700 417 0.613200 418 0.579200 419 0.613900 420 0.596300 421 0.546900 422 0.589300 423 0.589900 424 0.580600 425 0.584400 426 0.639800 427 0.584700 428 0.596400 429 0.532800 430 0.629400 431 0.560600 432 0.565700 433 0.570000 434 0.595200 435 0.554300 436 0.626400 437 0.611700 438 0.584300 439 0.574700 440 0.611400 441 0.554900 442 0.586000 443 0.594200 444 0.532100 445 0.580600 446 0.590500 447 0.551300 448 0.556200 449 0.566300 450 0.600100 451 0.597400 452 0.526500 453 0.609900 454 0.572600 455 0.629700 456 0.509900 457 0.585800 458 0.569600 459 0.541300 460 0.525000 461 0.543200 462 0.597100 463 0.539400 464 0.566400 465 0.594900 466 0.595700 467 0.530100 468 0.525500 469 0.540600 470 0.577400 471 0.543700 472 0.534800 473 0.607000 474 0.624600 475 0.571200 476 0.500100 477 0.571600 478 0.548500 479 0.546200 480 0.550800 481 0.553000 482 0.541900 483 0.520500 484 0.566200 485 0.573500 486 0.581800 487 0.622700 488 0.547400 489 0.566500 490 0.542000 491 0.544900 492 0.541100 493 0.515500 494 0.587000 495 0.518900 496 0.514400 497 0.545600 498 0.595700 499 0.551900 500 0.539100 501 0.548600 502 0.556300 503 0.523200 504 0.556300 505 0.558400 506 0.508500 507 0.553200 508 0.557600 509 0.572900 510 0.597800 511 0.524900 512 0.529500 513 0.566900 514 0.562600 515 0.546500 516 0.517900 517 0.531000 518 0.571500 519 0.503300 520 0.578200 521 0.598000 522 0.505400 523 0.533900 524 0.527300 525 0.552600 526 0.554500 527 0.534700 528 0.561500 529 0.553300 530 0.509700 531 0.531900 532 0.525000 533 0.571200 534 0.525800 535 0.593100 536 0.545800 537 0.522400 538 0.588000 539 0.556900 540 0.553500 541 0.561000 542 0.546200 543 0.510300 544 0.552300 545 0.526000 546 0.531100 547 0.509700 548 0.482200 549 0.547000 550 0.532000 551 0.534600 552 0.546000 553 0.542100 554 0.518800 555 0.603500 556 0.514000 557 0.538500 558 0.551000 559 0.548400 560 0.542600 561 0.533900 562 0.572400 563 0.556300 564 0.538900 565 0.586900 566 0.518200 567 0.472500 568 0.554000 569 0.530600 570 0.552300 571 0.523500 572 0.586100 573 0.540100 574 0.561500 575 0.540900 576 0.525000 577 0.542000 578 0.605800 579 0.549400 580 0.508100 581 0.523500 582 0.526300 583 0.521100 584 0.525300 585 0.523600 586 0.506800 587 0.547200 588 0.550000 589 0.571600 590 0.539200 591 0.561000 592 0.529800 593 0.488400 594 0.512300 595 0.503700 596 0.520400 597 0.523200 598 0.527600 599 0.569400 600 0.515700 601 0.540700 602 0.504500 603 0.523900 604 0.527400 605 0.539900 606 0.507100 607 0.484200 608 0.525100 609 0.568100 610 0.565100 611 0.535700 612 0.507300 613 0.529300 614 0.543900 615 0.531400 616 0.520300 617 0.527800 618 0.560800 619 0.522200 620 0.491600 621 0.548300 622 0.560200 <pre>/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n</pre> <pre>adapter_model.safetensors:   0%|          | 0.00/50.5M [00:00&lt;?, ?B/s]</pre> In\u00a0[10]: Copied! <pre># free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n</pre> # free the memory again del model del trainer torch.cuda.empty_cache() In\u00a0[11]: Copied! <pre>#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import AutoPeftModelForCausalLM\n\n# # Load PEFT model on CPU\nconfig = PeftConfig.from_pretrained(args.output_dir)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(args.output_dir)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel = PeftModel.from_pretrained(model, args.output_dir)\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n)\n# # Merge LoRA and base model and save\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")\n</pre>  #### COMMENT IN TO MERGE PEFT AND BASE MODEL #### from peft import PeftModel, PeftConfig from transformers import AutoModelForCausalLM, AutoTokenizer from peft import AutoPeftModelForCausalLM  # # Load PEFT model on CPU config = PeftConfig.from_pretrained(args.output_dir) model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True) tokenizer = AutoTokenizer.from_pretrained(args.output_dir) model.resize_token_embeddings(len(tokenizer)) model = PeftModel.from_pretrained(model, args.output_dir) model = AutoPeftModelForCausalLM.from_pretrained(     args.output_dir,     torch_dtype=torch.float16,     low_cpu_mem_usage=True, ) # # Merge LoRA and base model and save merged_model = model.merge_and_unload() merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\") <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> In\u00a0[12]: Copied! <pre>import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n#peft_model_id = \"./tinyllama_hindi_sft_sentence_retrieval\"\npeft_model_id = args.output_dir\n\n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n</pre> import torch from peft import AutoPeftModelForCausalLM from transformers import AutoTokenizer, pipeline  #peft_model_id = \"./tinyllama_hindi_sft_sentence_retrieval\" peft_model_id = args.output_dir  # Load Model with PEFT adapter model = AutoPeftModelForCausalLM.from_pretrained(   peft_model_id,   device_map=\"auto\",   torch_dtype=torch.float16 ) # load into pipeline pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) <pre>The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n</pre> In\u00a0[13]: Copied! <pre>from datasets import load_dataset\nfrom random import randint\n\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Test on sample\nprompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n</pre> from datasets import load_dataset from random import randint   # Load our test dataset eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\") rand_idx = randint(0, len(eval_dataset))  # Test on sample prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True) outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)  print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\") print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\") print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\") <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [32,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [33,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [34,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [35,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [36,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [37,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [38,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [39,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [40,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [41,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [42,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [43,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [44,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [45,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [46,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [47,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [48,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [49,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [50,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [51,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [52,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [53,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [54,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [55,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [56,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [57,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [58,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [59,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [60,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [61,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [62,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [292,0,0], thread: [63,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.\n</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[13], line 11\n      9 # Test on sample\n     10 prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n---&gt; 11 outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n     13 print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n     14 print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208, in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\n    167 def __call__(self, text_inputs, **kwargs):\n    168     \"\"\"\n    169     Complete the prompt(s) given as inputs.\n    170 \n   (...)\n    206           ids of the generated text.\n    207     \"\"\"\n--&gt; 208     return super().__call__(text_inputs, **kwargs)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1140, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n   1132     return next(\n   1133         iter(\n   1134             self.get_iterator(\n   (...)\n   1137         )\n   1138     )\n   1139 else:\n-&gt; 1140     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1147, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n   1145 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\n   1146     model_inputs = self.preprocess(inputs, **preprocess_params)\n-&gt; 1147     model_outputs = self.forward(model_inputs, **forward_params)\n   1148     outputs = self.postprocess(model_outputs, **postprocess_params)\n   1149     return outputs\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1046, in Pipeline.forward(self, model_inputs, **forward_params)\n   1044     with inference_context():\n   1045         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n-&gt; 1046         model_outputs = self._forward(model_inputs, **forward_params)\n   1047         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(\"cpu\"))\n   1048 else:\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:271, in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)\n    268         generate_kwargs[\"min_length\"] += prefix_length\n    270 # BS x SL\n--&gt; 271 generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n    272 out_b = generated_sequence.shape[0]\n    273 if self.framework == \"pt\":\n\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1130, in PeftModelForCausalLM.generate(self, **kwargs)\n   1128     self.base_model.generation_config = self.generation_config\n   1129 try:\n-&gt; 1130     outputs = self.base_model.generate(**kwargs)\n   1131 except:\n   1132     self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n    112 @functools.wraps(func)\n    113 def decorate_context(*args, **kwargs):\n    114     with ctx_factory():\n--&gt; 115         return func(*args, **kwargs)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1718, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n   1701     return self.assisted_decoding(\n   1702         input_ids,\n   1703         assistant_model=assistant_model,\n   (...)\n   1714         **model_kwargs,\n   1715     )\n   1716 if generation_mode == GenerationMode.GREEDY_SEARCH:\n   1717     # 11. run greedy search\n-&gt; 1718     return self.greedy_search(\n   1719         input_ids,\n   1720         logits_processor=logits_processor,\n   1721         stopping_criteria=stopping_criteria,\n   1722         pad_token_id=generation_config.pad_token_id,\n   1723         eos_token_id=generation_config.eos_token_id,\n   1724         output_scores=generation_config.output_scores,\n   1725         return_dict_in_generate=generation_config.return_dict_in_generate,\n   1726         synced_gpus=synced_gpus,\n   1727         streamer=streamer,\n   1728         **model_kwargs,\n   1729     )\n   1731 elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n   1732     if not model_kwargs[\"use_cache\"]:\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2579, in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n   2576 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n   2578 # forward pass to get next token\n-&gt; 2579 outputs = self(\n   2580     **model_inputs,\n   2581     return_dict=True,\n   2582     output_attentions=output_attentions,\n   2583     output_hidden_states=output_hidden_states,\n   2584 )\n   2586 if synced_gpus and this_peer_finished:\n   2587     continue  # don't waste resources running the code we don't need\n\nFile /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517 else:\n-&gt; 1518     return self._call_impl(*args, **kwargs)\n\nFile /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\n   1523 # this function, and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1527     return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result = None\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1181, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\n   1178 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n   1180 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-&gt; 1181 outputs = self.model(\n   1182     input_ids=input_ids,\n   1183     attention_mask=attention_mask,\n   1184     position_ids=position_ids,\n   1185     past_key_values=past_key_values,\n   1186     inputs_embeds=inputs_embeds,\n   1187     use_cache=use_cache,\n   1188     output_attentions=output_attentions,\n   1189     output_hidden_states=output_hidden_states,\n   1190     return_dict=return_dict,\n   1191 )\n   1193 hidden_states = outputs[0]\n   1194 if self.config.pretraining_tp &gt; 1:\n\nFile /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517 else:\n-&gt; 1518     return self._call_impl(*args, **kwargs)\n\nFile /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have any hooks, we want to skip the rest of the logic in\n   1523 # this function, and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1525         or _global_backward_pre_hooks or _global_backward_hooks\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1527     return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result = None\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1033, in LlamaModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\n   1029     attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n   1030 elif self._use_sdpa and not output_attentions:\n   1031     # output_attentions=True can not be supported when using SDPA, and we fall back on\n   1032     # the manual implementation that requires a 4D causal mask in all cases.\n-&gt; 1033     attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n   1034         attention_mask,\n   1035         (batch_size, seq_length),\n   1036         inputs_embeds,\n   1037         past_key_values_length,\n   1038     )\n   1039 else:\n   1040     # 4d mask is passed through the layers\n   1041     attention_mask = _prepare_4d_causal_attention_mask(\n   1042         attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n   1043     )\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:343, in _prepare_4d_causal_attention_mask_for_sdpa(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\n    340 is_tracing = torch.jit.is_tracing()\n    342 if attention_mask is not None:\n--&gt; 343     if torch.all(attention_mask == 1):\n    344         if is_tracing:\n    345             pass\n\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"LLM/Mistral-7b/notebooks_chatml_inference/","title":"ChatML Inference","text":"In\u00a0[1]: Copied! <pre>!pip install -qU transformers accelerate\n</pre> !pip install -qU transformers accelerate <pre>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</pre> In\u00a0[2]: Copied! <pre>from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"TokenBender/navaran_hindi_dpo_merged\"\nmessages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\n\ntokenizer = AutoTokenizer.from_pretrained(model)\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n</pre> from transformers import AutoTokenizer import transformers import torch  model = \"TokenBender/navaran_hindi_dpo_merged\" messages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]  tokenizer = AutoTokenizer.from_pretrained(model) prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) pipeline = transformers.pipeline(     \"text-generation\",     model=model,     torch_dtype=torch.float16,     device_map=\"auto\", )  outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95) print(outputs[0][\"generated_text\"]) <pre>tokenizer_config.json:   0%|          | 0.00/1.60k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.model:   0%|          | 0.00/493k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/1.80M [00:00&lt;?, ?B/s]</pre> <pre>added_tokens.json:   0%|          | 0.00/51.0 [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/420 [00:00&lt;?, ?B/s]</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>config.json:   0%|          | 0.00/653 [00:00&lt;?, ?B/s]</pre> <pre>model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00&lt;?, ?B/s]</pre> <pre>Downloading shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00&lt;?, ?B/s]</pre> <pre>model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00&lt;?, ?B/s]</pre> <pre>model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00&lt;?, ?B/s]</pre> <pre>Loading checkpoint shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>generation_config.json:   0%|          | 0.00/115 [00:00&lt;?, ?B/s]</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n</pre> <pre>&lt;|im_start|&gt;user\nWhat is a large language model?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nA large language model is a type of artificial intelligence algorithm that is designed to understand and generate human language. These models are trained on vast amounts of text data, allowing them to learn patterns and relationships within language. Large language models are used in various applications, such as natural language processing, machine translation, and chatbots. They can understand and generate text in a way that is similar to how humans do, making them a powerful tool for language understanding and generation.\n</pre> In\u00a0[15]: Copied! <pre>messages = [{\"role\": \"user\", \"content\": \"\u0935\u0930\u094d\u091a\u0941\u0905\u0932 \u0930\u093f\u092f\u0932\u093f\u091f\u0940 \u0914\u0930 \u0911\u0917\u092e\u0947\u0902\u091f\u0947\u0921 \u0930\u093f\u092f\u0932\u093f\u091f\u0940 \u092e\u0947\u0902 \u0915\u094d\u092f\u093e \u0905\u0902\u0924\u0930 \u0939\u0948?\"}]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipeline(prompt, max_new_tokens=1024, do_sample=True, temperature=0.1, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n</pre> messages = [{\"role\": \"user\", \"content\": \"\u0935\u0930\u094d\u091a\u0941\u0905\u0932 \u0930\u093f\u092f\u0932\u093f\u091f\u0940 \u0914\u0930 \u0911\u0917\u092e\u0947\u0902\u091f\u0947\u0921 \u0930\u093f\u092f\u0932\u093f\u091f\u0940 \u092e\u0947\u0902 \u0915\u094d\u092f\u093e \u0905\u0902\u0924\u0930 \u0939\u0948?\"}] prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) outputs = pipeline(prompt, max_new_tokens=1024, do_sample=True, temperature=0.1, top_k=50, top_p=0.95) print(outputs[0][\"generated_text\"]) <pre>/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n</pre> <pre>&lt;|im_start|&gt;user\n\u0935\u0930\u094d\u091a\u0941\u0905\u0932 \u0930\u093f\u092f\u0932\u093f\u091f\u0940 \u0914\u0930 \u0911\u0917\u092e\u0947\u0902\u091f\u0947\u0921 \u0930\u093f\u092f\u0932\u093f\u091f\u0940 \u092e\u0947\u0902 \u0915\u094d\u092f\u093e \u0905\u0902\u0924\u0930 \u0939\u0948?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\u0909\u0924\u094d\u0924\u0930: \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0935\u093f\u0915\u093e\u0938 \u0915\u093e \u0907\u0924\u093f\u0939\u093e\u0938 \u0914\u0930 \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0909\u0926\u094d\u092f\u094b\u0917 \u092a\u0930 \u0907\u0938\u0915\u093e \u092a\u094d\u0930\u092d\u093e\u0935\u0964 \u092f\u0939 \u0932\u0947\u0916 \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0935\u093f\u0915\u093e\u0938 \u0915\u0947 \u0935\u093f\u0915\u093e\u0938 \u0914\u0930 \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0909\u0926\u094d\u092f\u094b\u0917 \u092a\u0930 \u0907\u0938\u0915\u0947 \u092a\u094d\u0930\u092d\u093e\u0935 \u0915\u0940 \u092a\u0921\u093c\u0924\u093e\u0932 \u0915\u0930\u0924\u093e \u0939\u0948\u0964 \u092f\u0939 \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0935\u093f\u0915\u093e\u0938 \u0915\u0947 \u0935\u093f\u092d\u093f\u0928\u094d\u0928 \u091a\u0930\u0923\u094b\u0902 \u0914\u0930 \u0935\u093f\u092d\u093f\u0928\u094d\u0928 \u092a\u094d\u0932\u0947\u091f\u092b\u093e\u0930\u094d\u092e\u094b\u0902 \u092a\u0930 \u0907\u0938\u0915\u0947 \u092a\u094d\u0930\u092d\u093e\u0935 \u092a\u0930 \u091a\u0930\u094d\u091a\u093e \u0915\u0930\u0924\u093e \u0939\u0948\u0964 \u091c\u092c\u0915\u093f \u092f\u0939 \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0935\u093f\u0915\u093e\u0938 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u092e\u0942\u0932\u094d\u092f\u0935\u093e\u0928 \u091c\u093e\u0928\u0915\u093e\u0930\u0940 \u092a\u094d\u0930\u0926\u093e\u0928 \u0915\u0930\u0924\u093e \u0939\u0948, \u092f\u0939 \u0935\u093f\u0936\u0947\u0937 \u0930\u0942\u092a \u0938\u0947 \u0935\u0940\u0921\u093f\u092f\u094b \u0917\u0947\u092e \u0935\u093f\u0915\u093e\u0938 \u0915\u0947 \u0932\u093f\u090f \u0909\u092a\u092f\u094b\u0917 \u0915\u093f\u090f \u091c\u093e\u0928\u0947 \u0935\u093e\u0932\u0947 \u0909\u0928\u094d\u0928\u0924 \u092a\u094d\u0930\u094b\u0917\u094d\u0930\u093e\u092e\u093f\u0902\u0917 \u092d\u093e\u0937\u093e\u0913\u0902 \u092a\u0930 \u0927\u094d\u092f\u093e\u0928 \u0915\u0947\u0902\u0926\u094d\u0930\u093f\u0924 \u0928\u0939\u0940\u0902 \u0915\u0930\u0924\u093e \u0939\u0948\u0964\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/","title":"Mixtral","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets scipy\n!pip install -q trl\n!pip install flash-attn --no-build-isolation\n</pre> !pip install -q -U bitsandbytes !pip install -q -U git+https://github.com/huggingface/transformers.git !pip install -q -U git+https://github.com/huggingface/peft.git !pip install -q -U git+https://github.com/huggingface/accelerate.git !pip install -q datasets scipy !pip install -q trl !pip install flash-attn --no-build-isolation In\u00a0[9]: Copied! <pre>model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n</pre> model_id = \"mistralai/Mixtral-8x7B-v0.1\" In\u00a0[10]: Copied! <pre>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\n</pre> import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig  nf4_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_quant_type=\"nf4\",    bnb_4bit_use_double_quant=True,    bnb_4bit_compute_dtype=torch.bfloat16 ) In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map='auto',\n    quantization_config=nf4_config,\n    use_cache=False,\n    attn_implementation=\"flash_attention_2\"\n\n)\n</pre> model = AutoModelForCausalLM.from_pretrained(     model_id,     device_map='auto',     quantization_config=nf4_config,     use_cache=False,     attn_implementation=\"flash_attention_2\"  ) In\u00a0[13]: Copied! <pre>tokenizer = AutoTokenizer.from_pretrained(model_id)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</pre> tokenizer = AutoTokenizer.from_pretrained(model_id)  tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\" <p>Let's example how well the model does at this task currently:</p> In\u00a0[14]: Copied! <pre>def generate_response(prompt, model):\n  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n  model_inputs = encoded_input.to('cuda')\n\n  generated_ids = model.generate(**model_inputs,\n                                 max_new_tokens=512,\n                                 do_sample=True,\n                                 pad_token_id=tokenizer.eos_token_id)\n\n  decoded_output = tokenizer.batch_decode(generated_ids)\n\n  return decoded_output[0].replace(prompt, \"\")\n</pre> def generate_response(prompt, model):   encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)   model_inputs = encoded_input.to('cuda')    generated_ids = model.generate(**model_inputs,                                  max_new_tokens=512,                                  do_sample=True,                                  pad_token_id=tokenizer.eos_token_id)    decoded_output = tokenizer.batch_decode(generated_ids)    return decoded_output[0].replace(prompt, \"\") In\u00a0[\u00a0]: Copied! <pre>prompt=\"\"\"[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM. \\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[\\INST]\"\"\"\n\ngenerate_response(prompt, model)\n</pre> prompt=\"\"\"[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM. \\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[\\INST]\"\"\"  generate_response(prompt, model) In\u00a0[\u00a0]: Copied! <pre>print(model)\n</pre> print(model) In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\ndataset\n</pre> from datasets import load_dataset  dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\") dataset In\u00a0[\u00a0]: Copied! <pre>df = dataset.to_pandas()\ndf.head(10)\n</pre> df = dataset.to_pandas() df.head(10) <p>Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :</p> <ol> <li>the function generate_prompt : take the instruction and output and generate a prompt</li> <li>shuffle the dataset</li> <li>tokenizer the dataset</li> </ol> In\u00a0[19]: Copied! <pre>def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"&lt;s&gt;[INST]{prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} [/INST]{data_point[\"output\"]}&lt;/s&gt;\"\"\"\n    # Without\n    else:\n        text = f\"\"\"&lt;s&gt;[INST]{prefix_text} {data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} &lt;/s&gt;\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)\n</pre> def generate_prompt(data_point):     \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer      :param data_point: dict: Data point     :return: dict: tokenzed prompt     \"\"\"     prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\                'appropriately completes the request.\\n\\n'     # Samples with additional context into.     if data_point['input']:         text = f\"\"\"[INST]{prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} [/INST]{data_point[\"output\"]}\"\"\"     # Without     else:         text = f\"\"\"[INST]{prefix_text} {data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} \"\"\"     return text  # add the \"prompt\" column in the dataset text_column = [generate_prompt(data_point) for data_point in dataset] dataset = dataset.add_column(\"prompt\", text_column) In\u00a0[\u00a0]: Copied! <pre>dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n</pre> dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True) In\u00a0[21]: Copied! <pre>dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n</pre> dataset = dataset.train_test_split(test_size=0.2) train_data = dataset[\"train\"] test_data = dataset[\"test\"] In\u00a0[\u00a0]: Copied! <pre>train_data\n</pre> train_data In\u00a0[\u00a0]: Copied! <pre>train_data[\"input_ids\"][:10]\n</pre> train_data[\"input_ids\"][:10] In\u00a0[\u00a0]: Copied! <pre>print(test_data)\n</pre> print(test_data) In\u00a0[28]: Copied! <pre>from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n        target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    task_type=\"CAUSAL_LM\"\n)\n</pre> from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training  peft_config = LoraConfig(     lora_alpha=16,     lora_dropout=0.1,     r=64,     bias=\"none\",         target_modules=[         \"q_proj\",         \"k_proj\",         \"v_proj\",         \"o_proj\",         \"gate_proj\",         \"up_proj\",         \"down_proj\",         \"lm_head\",     ],     task_type=\"CAUSAL_LM\" ) <p>we need to prepare the model to be trained in 4bit so we will use the  <code>prepare_model_for_kbit_training</code> function from peft</p> <p>Indented block</p> In\u00a0[29]: Copied! <pre>model = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n</pre> model = prepare_model_for_kbit_training(model) model = get_peft_model(model, peft_config) In\u00a0[30]: Copied! <pre>def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n</pre> def print_trainable_parameters(model):     \"\"\"     Prints the number of trainable parameters in the model.     \"\"\"     trainable_params = 0     all_param = 0     for _, param in model.named_parameters():         all_param += param.numel()         if param.requires_grad:             trainable_params += param.numel()     print(         f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"     ) In\u00a0[\u00a0]: Copied! <pre>print_trainable_parameters(model)\n</pre> print_trainable_parameters(model) In\u00a0[\u00a0]: Copied! <pre>print(model)\n</pre> print(model) In\u00a0[33]: Copied! <pre>if torch.cuda.device_count() &gt; 1: # If more than 1 GPU\n    print(torch.cuda.device_count())\n    model.is_parallelizable = True\n    model.model_parallel = True\n</pre> if torch.cuda.device_count() &gt; 1: # If more than 1 GPU     print(torch.cuda.device_count())     model.is_parallelizable = True     model.model_parallel = True In\u00a0[36]: Copied! <pre>from transformers import TrainingArguments\n\nargs = TrainingArguments(\n  output_dir = \"Mixtral_Alpace_v3\",\n  #num_train_epochs=5,\n  max_steps = 100, # comment out this line if you want to train in epochs\n  per_device_train_batch_size = 32,\n  warmup_steps = 0.03,\n  logging_steps=10,\n  save_strategy=\"epoch\",\n  #evaluation_strategy=\"epoch\",\n  evaluation_strategy=\"steps\",\n  eval_steps=10, # comment out this line if you want to evaluate at the end of each epoch\n  learning_rate=2.5e-5,\n  bf16=True,\n  # lr_scheduler_type='constant',\n)\n</pre> from transformers import TrainingArguments  args = TrainingArguments(   output_dir = \"Mixtral_Alpace_v3\",   #num_train_epochs=5,   max_steps = 100, # comment out this line if you want to train in epochs   per_device_train_batch_size = 32,   warmup_steps = 0.03,   logging_steps=10,   save_strategy=\"epoch\",   #evaluation_strategy=\"epoch\",   evaluation_strategy=\"steps\",   eval_steps=10, # comment out this line if you want to evaluate at the end of each epoch   learning_rate=2.5e-5,   bf16=True,   # lr_scheduler_type='constant', ) <p>Setting up the trainer.</p> <p><code>max_seq_length</code>: Context window size</p> In\u00a0[\u00a0]: Copied! <pre>from trl import SFTTrainer\n\nmax_seq_length = 1024\n\ntrainer = SFTTrainer(\n  model=model,\n  peft_config=peft_config,\n  max_seq_length=max_seq_length,\n  tokenizer=tokenizer,\n  packing=True,\n  args=args,\n  dataset_text_field=\"prompt\",\n  train_dataset=train_data,\n  eval_dataset=test_data,\n)\n</pre> from trl import SFTTrainer  max_seq_length = 1024  trainer = SFTTrainer(   model=model,   peft_config=peft_config,   max_seq_length=max_seq_length,   tokenizer=tokenizer,   packing=True,   args=args,   dataset_text_field=\"prompt\",   train_dataset=train_data,   eval_dataset=test_data, ) In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() In\u00a0[\u00a0]: Copied! <pre>trainer.save_model(\"Mixtral_Alpace_v2\")\n</pre> trainer.save_model(\"Mixtral_Alpace_v2\") In\u00a0[\u00a0]: Copied! <pre># !pip install huggingface-hub -qU\n</pre> # !pip install huggingface-hub -qU In\u00a0[\u00a0]: Copied! <pre># from huggingface_hub import notebook_login\n\n# notebook_login()\n</pre> # from huggingface_hub import notebook_login  # notebook_login() In\u00a0[\u00a0]: Copied! <pre># trainer.push_to_hub(\"Promptengineering/mistral-instruct-generation\")\n</pre> # trainer.push_to_hub(\"Promptengineering/mistral-instruct-generation\") In\u00a0[\u00a0]: Copied! <pre>merged_model = model.merge_and_unload()\n</pre> merged_model = model.merge_and_unload() In\u00a0[\u00a0]: Copied! <pre>def generate_response(prompt, model):\n  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n  model_inputs = encoded_input.to('cuda')\n\n  generated_ids = model.generate(**model_inputs,\n                                 max_new_tokens=150,\n                                 do_sample=True,\n                                 pad_token_id=tokenizer.eos_token_id)\n\n  decoded_output = tokenizer.batch_decode(generated_ids)\n\n  return decoded_output[0]\n</pre> def generate_response(prompt, model):   encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)   model_inputs = encoded_input.to('cuda')    generated_ids = model.generate(**model_inputs,                                  max_new_tokens=150,                                  do_sample=True,                                  pad_token_id=tokenizer.eos_token_id)    decoded_output = tokenizer.batch_decode(generated_ids)    return decoded_output[0] In\u00a0[\u00a0]: Copied! <pre>prompt = \"[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM.\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[/INST]\"\n</pre> prompt = \"[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM.\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[/INST]\"  In\u00a0[\u00a0]: Copied! <pre>generate_response(prompt, merged_model)\n</pre> generate_response(prompt, merged_model) In\u00a0[\u00a0]: Copied! <pre>250*32\n</pre> 250*32"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#mixtral-8x7b-mixture-of-experts","title":"MIXTRAL 8x7B - Mixture of Experts\u00b6","text":"<p>This will not run on the free T4 GPU from Google Colab. You will need A100 to run this.</p>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#install-required-packages","title":"Install Required Packages\u00b6","text":""},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#loading-the-base-model","title":"Loading the Base Model\u00b6","text":"<p>Load the model in <code>4bit</code>, with double quantization, with <code>bfloat16</code> as the compute dtype.</p> <p>In this case we are using the instruct-tuned model - instead of the base model. For fine-tuning a base model will need a lot more data!</p>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#load-dataset-for-finetuning","title":"Load dataset for finetuning\u00b6","text":""},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#lets-load-the-dataset","title":"Lets Load the Dataset\u00b6","text":"<p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>We will be using this dataset which is curated by TokenBender (e/xperiments) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <pre>{\n  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n  \"input\": \"[1, 2, 3, 4, 5]\",\n  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n}\n</pre>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#formatting-the-dataset","title":"Formatting the Dataset\u00b6","text":"<p>Now, let's format the dataset in the required Mistral-7B-Instruct-v0.1 format.</p> <p>Many tutorials and blogs skip over this part, but I feel this is a really important step.</p> <p>We'll put each instruction and input pair between <code>[INST]</code> and <code>[/INST]</code> output after that, like this:</p> <pre><code>&lt;s&gt;[INST] What is your favorite condiment? [/INST]\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;/s&gt;\n</code></pre> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#after-formatting-we-should-get-something-like-this","title":"After Formatting, We should get something like this\u00b6","text":"<pre>{\n\"text\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\",\n\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n\"input\":\"[1, 2, 3, 4, 5]\",\n\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n sequence: sum += num return sum\"\n\"prompt\":\"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\n# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;\"\n\n}\n</pre> <p>While using SFT (Supervised Fine-tuning Trainer) for fine-tuning, we will be only passing in the \u201ctext\u201d column of the dataset for fine-tuning.</p>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#setting-up-the-training","title":"Setting up the Training\u00b6","text":"<p>we will be using the <code>huggingface</code> and the <code>peft</code> library!</p>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#model-after-adding-lora-config","title":"Model after Adding Lora Config\u00b6","text":""},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#hyper-paramters-for-training","title":"Hyper-paramters for training\u00b6","text":"<p>These parameters will depend on how long you want to run training for. Most important to consider:</p> <p><code>num_train_epochs/max_steps</code>: How many iterations over the data you want to do, BE CAREFUL, don't try too many, you will over-fit!!!!!</p> <p><code>learning_rate</code>: Controls the speed of convergence</p>"},{"location":"LLM/Mixtral/Mixtral_fine_tuning/#save-model-and-push-to-hub","title":"Save Model and Push to Hub\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/","title":"Write Deep Learning Code Locally and Run on GPUs Instantly","text":""},{"location":"LLM/ServerLessFinetuning/#write-code-locally-and-run-it-on-gpus-in-seconds","title":"Write code locally and run it on GPUs in Seconds","text":""},{"location":"LLM/ServerLessFinetuning/#stop-paying-for-idle-gpus-serverless-training-with-modal","title":"Stop Paying for Idle GPUs: Serverless Training with Modal","text":"<p>So let's face it, if you are doing anything with deep learning, GPUs are a must.</p> <p>They are expensive, and infrastructure is hard to set up. Most of the time, you're coding when the GPUs are sitting idle, and it's a pain to pay for the uptime when no deep learning scripts work on the first go.</p> <p>This was a problem I faced as \"GPU Poor\". I didn't want to spend money on GPUs when I was coding or doing something that didn't leverage the GPU compute. Even for things like downloading data, models, and data transformation, you don't need a GPU but still have to do it on a GPU.</p> <p>And especially on cloud providers, where you will have to worry about infrastructure. You can set up a VM with a GPU attached, then choose an image which is not even well-documented. If not done properly, you will have to install CUDA and stuff from scratch. If that also doesn't work, most of the time you resort to using a Docker container with the right installations.</p> <p>And if you start doing multi-GPU training, that's one more burden. Some GPU images don't even support NCCL for communication between GPU nodes, so you will have to be careful about that as well.</p> <p>So if you just want to set up a GPU and run, it's a lot of effort. There are providers like Runpod, Vast AI, and others that make it easier.</p> <p>I run a research lab called CognitiveLab [cognitivelab.in], where we do a bunch of model training, synthetic data generation, RL runs, and more. We wanted something that was easy to use, train, and flexible enough so that we don't need to be constrained by it.</p> <p>But when I looked for a solution where I could write my code locally on my machine and run it on a GPU, I stumbled across this beautiful solution called Modal. It's been 1 year since I started using it, and it's been a blessing.</p> <p>I will be covering the following:</p> <ul> <li>How to handle datasets on Modal efficiently, including creating and managing volumes for seamless data access.</li> <li>Writing training scripts using libraries like Unsloth and Axolotl to fine-tune models with minimal effort.</li> <li>Evaluating trained models with automated metrics to ensure performance and reliability.</li> <li>Serving these models in a scalable and high-throughput manner using vLLM for real-world applications.</li> </ul> <p>By the end of this, you'll have a clear understanding of how to write and experiment with training scripts locally and run them on GPUs as quickly as possible using Modal.</p> <p>I will be mainly covering SFT examples, but if you guys are interested, I will write a blog on how to set it up for RL with RL training and reward environments happening on different GPUs.</p>"},{"location":"LLM/ServerLessFinetuning/#inspiration","title":"Inspiration","text":"<p>Thinking Machines, the startup founded by ex-OpenAI CTO Mira Murati, recently launched Tinker, which allows developers to write training loops in Python on their laptops and run them on distributed GPUs.</p> <p>Check out their announcement: Thinking Machines on X.</p> <p>This is every developer's dream! However, I have been using Modal to achieve something similar for quite some time now.</p> <p>PS: From the looks of it, their API is much more sophisticated. They have implemented several optimizations under the hood, such as efficient batching. Here is a tweet that dives deeper into the details.</p> <p>This is really cool, but you can do the same thing using @modal with just a few more lines of code.You write the training loops on your local machine and then run it in on any number of GPUs and its way more flexible:- It works with existing repos- You only pay for the time\u2026 https://t.co/IVOalMvFVC</p>\u2014 Adithya S K (@adithya_s_k) October 3, 2025 <p>Lot of you were interested in how to do this, so here we go.</p>"},{"location":"LLM/ServerLessFinetuning/#ok-what-is-modal","title":"Ok, what is Modal?","text":"<p>You would have come across the term Serverless GPUs.</p> <p>Let's just say Modal is a GPU provider platform that does right by serverless GPUs, and they have one of the best developer experiences ever.</p> <p>If you are doing anything in Python, training models, deploying them, writing servers, building agentic systems, then Modal can be used.</p> <p>As per the official Modal website:</p> <p>AI infrastructure that developers love, and that's 100% factual.</p> <p>Run inference, train, batch process with sub-second cold start, instant auto-scaling, and a developer experience that feels local.</p> <p>Fun fact: Even Lovable uses Modal for running their sandbox.</p>"},{"location":"LLM/ServerLessFinetuning/#getting-started","title":"Getting Started","text":"<p>First, all you have to do is:</p> <pre><code>pip install modal\n</code></pre> <p>and do:</p> <pre><code>modal setup\n</code></pre> <p>You can also authenticate through their API keys:</p> <pre><code>export MODAL_TOKEN_ID=\nexport MODAL_TOKEN_SECRET=\n</code></pre> <p>This is all you need to set up Modal.</p>"},{"location":"LLM/ServerLessFinetuning/#core-concepts","title":"Core Concepts","text":"<p>With Modal, you always start by creating an App, an Image, and Volumes.</p> <p>App - To create an App, it's pretty simple:</p> <pre><code>import modal\n\n# Create the Modal app\napp = modal.App(\"&lt;app_name&gt;\")\n</code></pre> <p>Volumes - Then we can create or use existing volumes.</p> <p>You can think of volumes as a storage file system where you can store anything like model weights, datasets, scores, and more.</p> <p>If you want something to persist, add it in the volume. The best part is, for a function, you can have multiple volumes with different routes; you can have a volume for model weights in the <code>/model</code> path and for the dataset in the <code>/dataset</code> path.</p> <p>Something like this:</p> <pre><code>dataset_volume = modal.Volume.from_name(\"dataset-volume\", create_if_missing=True)\nmodel_volume = modal.Volume.from_name(\"model-volume\", create_if_missing=True)\n</code></pre> <p>Then you write the mapping that will be passed into functions:</p> <pre><code>volume_config = {\n    \"/dataset\": dataset_volume,\n    \"/model\": model_volume\n}\n</code></pre> <p>This is just to illustrate how you can attach volumes to any function. This gives us awesome power.</p> <p>You can download datasets, process them all on CPU instances, and when it comes time to train, just attach the same volume and use it, which makes life much easier.</p> <p>I generally create a volume for a single experiment or training run so that I have everything consolidated that can be used across the project.</p> <p>Images - Next thing will be the images.</p> <p>This is the most important part. Defining an image can be tricky at first, but once it's done, you don't have to worry about it. Initially, it can take up some time.</p> <p>But it's very important. Refer to Modal Image docs to see all the ways to create an image.</p> <p>Here is a sample example image:</p> <pre><code>train_image = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .uv_pip_install(\n        \"accelerate==1.9.0\",\n        \"datasets==3.6.0\",\n        \"hf-transfer==0.1.9\",\n        \"huggingface_hub==0.34.2\",\n        \"peft==0.16.0\",\n        \"transformers==4.54.0\",\n        \"trl==0.19.1\",\n        \"unsloth[cu128-torch270]==2025.7.8\",\n        \"unsloth_zoo==2025.7.10\",\n        \"wandb==0.21.0\",\n    )\n    .env({\"HF_HOME\": \"/model_cache\"})\n)\n</code></pre> <p>So the base image uses Debian with Python 3.11, and then you install all the packages using uv. Then you set the environment <code>HF_HOME</code> so that everything is cached, and you won't have to download again and again. This is a good starting image.</p> <p>Pro tip: I have a set of images that you can use for anything training-related. I have images to serve LLMs using vLLM, SGLang, training using Unsloth, MS Swift, and more. I will share these and go deeper into how to create images in a better way later.</p> <p>Functions - There is one more thing: Functions.</p> <p>You can basically have any Python function to make it run on GPU or CPU on Modal. All you have to do is add a decorator:</p> <pre><code>@app.function(\n    image=image,\n    secrets=[modal.Secret.from_dotenv()],  # local .env variables\n    volumes={\"/data\": volume},\n    timeout=3600,  # 1 hour timeout\n)\ndef any_python_function():\n    # Your code here\n    pass\n</code></pre> <p>This is where you define which image the function will use, what secrets you will be passing, which volumes will be attached, and what is the timeout (there is a general timeout of 24 hrs).</p> <p>To know all the properties of a function, refer to Modal Function docs</p> <p>Now the basics are out of the way. Let's do some training, fine-tuning, evaluation, and serving.</p>"},{"location":"LLM/ServerLessFinetuning/#tutorials","title":"Tutorials","text":"<p>I have created comprehensive tutorials for each training approach:</p>"},{"location":"LLM/ServerLessFinetuning/#1-training-nanogpt-on-modal","title":"1. Training NanoGPT on Modal","text":"<p>\ud83d\udcc4 View Python Script</p> <p>Learn how to take an existing codebase (Andrej Karpathy's nanoGPT) and run it on Modal's serverless GPUs with minimal modifications. Perfect for beginners to understand:</p> <ul> <li>How to copy local repositories into Modal containers</li> <li>Data preparation, training, and sampling pipelines</li> <li>Managing persistent storage with Modal volumes</li> <li>Running existing Python projects on remote GPUs</li> </ul> Level GPU Required Time Beginner 1\u00d7 A100-40GB (or T4/L40S for testing) 30 mins - 2 hours"},{"location":"LLM/ServerLessFinetuning/#2-training-nanochat-on-modal","title":"2. Training Nanochat on Modal","text":"<p>Build Your Own ChatGPT from Scratch - The Complete Pipeline</p> <p>\ud83d\udcc4 View Python Script</p> <p>The ultimate educational LLM training pipeline covering every step from raw text to a functioning ChatGPT. You'll learn:</p> <ul> <li>Training a custom BPE tokenizer (like GPT-4)</li> <li>Base model pretraining from random initialization</li> <li>Midtraining for conversation format and tool use</li> <li>Supervised fine-tuning on multiple tasks</li> <li>Reinforcement learning for improved reasoning</li> <li>Comprehensive evaluation on real benchmarks</li> <li>Deployment with CLI and web UI</li> </ul> Level GPU Required Time Advanced 4-8\u00d7 A100-80GB 4-8 hours (full speedrun) <p>This is the most comprehensive tutorial - you'll understand exactly how ChatGPT works by building one yourself. Perfect for those who want to go deep.</p>"},{"location":"LLM/ServerLessFinetuning/#3-fine-tuning-gemma-3-4b-with-unsloth","title":"3. Fine-tuning Gemma 3-4B with Unsloth","text":"<p>End-to-end vision-language model training and deployment</p> <p>\ud83d\udcc4 View Python Script</p> <p>A production-grade pipeline covering the complete ML workflow from data to deployment. You'll learn:</p> <ul> <li>Fine-tuning vision-language models with LoRA</li> <li>Optimized single-GPU training with Unsloth</li> <li>Model evaluation with automated metrics</li> <li>Serving with vLLM for high-throughput inference</li> <li>Auto-scaling deployment strategies</li> </ul> Level GPU Required Time Intermediate 1\u00d7 A100-80GB (or L40S) 3-6 hours (full pipeline)"},{"location":"LLM/ServerLessFinetuning/#4-multi-gpu-training-with-axolotl","title":"4. Multi-GPU Training with Axolotl","text":"<p>Distributed training for large models (Llama 8 - 70B+)</p> <p>\ud83d\udcc4 View Python Script</p> <p>Advanced distributed training techniques for massive models. You'll learn:</p> <ul> <li>Multi-GPU training with Accelerate and DeepSpeed</li> <li>YAML-based configuration for reproducibility</li> <li>Dataset preprocessing for large-scale training</li> <li>Scaling from 2 to 8 GPUs</li> <li>Cost optimization strategies for expensive training runs</li> </ul> Level GPU Required Time Advanced 2-8\u00d7 A100-80GB 4-12 hours (depends on model size) <p>This is multi-GPU training, and you can run all types of parallelism (data, tensor, pipeline, FSDP) using Modal as well. For the sake of simplicity, I have used Accelerate, but you can go all out with the setup up to 8 GPUs. I have mainly been using Modal for multi-GPU training with a maximum of 8 GPUs. I haven't done multi-node training yet (should be possible with sandboxes, but the setup process might be a bit complex).</p> <p>I think these 3 examples will give you a good picture to replicate the process across multiple things.</p>"},{"location":"LLM/ServerLessFinetuning/#final-thoughts","title":"Final thoughts","text":"<p>As someone working with AI models, infrastructure is crucial to get right as it's expensive and takes a lot of time to set up, especially for individual researchers and small labs who will find it hard to set up and manage infrastructure.</p> <p>With Modal, infrastructure becomes as easy as writing a python script and running it on CPU/GPU, deploying it, scaling it.</p> <p>In this, I go over the details on how to use Modal mainly for running training, eval, and serving scripts for LLM models, but you can do a lot more with Modal.</p> <p>Fun fact: Gitvizz uses modal to run all their backend code and I have been running it for 4 months which cost me less than 4$ and it scales really well. After using modal I completely stopped using k8s and stuff for smaller projects.</p>"},{"location":"LLM/ServerLessFinetuning/#need-help","title":"Need Help?","text":"<p>If your organization needs help with optimally using Modal, we at CognitiveLab can help you set it up and manage it for you.</p> <p>Reach out to us through our website or DM me on twitter @adithya_s_k</p>"},{"location":"LLM/ServerLessFinetuning/#resources","title":"Resources","text":"<ul> <li>Modal Docs</li> <li>Unsloth Docs</li> <li>Axolotl Docs</li> <li>NanoGPT</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModal/","title":"FinetuneGemmaUnslothModal","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport modal\nfrom modal import App, Image as ModalImage, Volume, Secret\n</pre> import os import modal from modal import App, Image as ModalImage, Volume, Secret In\u00a0[\u00a0]: Copied! <pre>app = App(\"Finetuned_Gemma_3_4b_it\")\n</pre> app = App(\"Finetuned_Gemma_3_4b_it\") In\u00a0[\u00a0]: Copied! <pre># Create volumes for persistent storage\nexp_volume = Volume.from_name(\"Finetuned_Gemma_3_4b_it\", create_if_missing=True)\n# Configure volume mounting points\nVOLUME_CONFIG = {\n    \"/data\": exp_volume,\n}\nhuggingface_secret = Secret.from_name(\"secrets-hf-wandb\")\n</pre> # Create volumes for persistent storage exp_volume = Volume.from_name(\"Finetuned_Gemma_3_4b_it\", create_if_missing=True) # Configure volume mounting points VOLUME_CONFIG = {     \"/data\": exp_volume, } huggingface_secret = Secret.from_name(\"secrets-hf-wandb\") In\u00a0[\u00a0]: Copied! <pre># Time constants\nHOURS = 60 * 60\n# Model Configuration\nBASE_MODEL_NAME = \"unsloth/gemma-3-4b-it\"\nWANDB_PROJECT_DEFAULT = \"GemmaFinetuning\"\nOUTPUT_DIR_DEFAULT = \"/data/Finetuned_Gemma_3_4b_it\"\n</pre> # Time constants HOURS = 60 * 60 # Model Configuration BASE_MODEL_NAME = \"unsloth/gemma-3-4b-it\" WANDB_PROJECT_DEFAULT = \"GemmaFinetuning\" OUTPUT_DIR_DEFAULT = \"/data/Finetuned_Gemma_3_4b_it\" In\u00a0[\u00a0]: Copied! <pre># CUDA Configuration for SGLang\nCUDA_VERSION = \"12.8.1\"\nCUDA_FLAVOR = \"devel\"\nCUDA_OS = \"ubuntu24.04\"\nCUDA_TAG = f\"{CUDA_VERSION}-{CUDA_FLAVOR}-{CUDA_OS}\"\n</pre> # CUDA Configuration for SGLang CUDA_VERSION = \"12.8.1\" CUDA_FLAVOR = \"devel\" CUDA_OS = \"ubuntu24.04\" CUDA_TAG = f\"{CUDA_VERSION}-{CUDA_FLAVOR}-{CUDA_OS}\" In\u00a0[\u00a0]: Copied! <pre># Define the GPU image for fine-tuning with Unsloth\nFINETUNING_GPU_IMAGE = (\n    ModalImage.from_registry(f\"nvidia/cuda:{CUDA_TAG}\", add_python=\"3.12\")\n    .apt_install(\n        \"git\",\n        \"build-essential\",\n    )\n    .uv_pip_install(\n        [\n            \"torch\",\n            \"torchvision\",\n            \"torchaudio\",  # optional but often bundled with torch\n        ]\n    )\n    # Install Unsloth and dependencies\n    .uv_pip_install(\n        [\n            # Unsloth core packages\n            \"unsloth\",\n            \"unsloth_zoo\",\n            # Core ML packages\n            \"bitsandbytes\",\n            \"accelerate\",\n            \"xformers\",\n            \"peft\",\n            \"trl\",\n            \"triton\",\n            \"cut_cross_entropy\",\n            # Upgraded packages\n            \"transformers\",\n            \"timm\",\n            # Additional dependencies\n            \"wandb\",\n            \"weave\",\n            \"pillow\",\n            \"opencv-python-headless\",\n            \"deepspeed\",\n            \"pyyaml\",\n            \"packaging\",\n            \"nltk\",\n            \"rouge_score\",\n            \"bert_score\",\n            \"jiwer\",\n            \"scikit-learn\",\n            \"tqdm\",\n            \"pandas\",\n            \"pyarrow\",\n            \"gradio\",\n            \"hf_transfer\",\n        ]\n    )\n    .env(\n        {\n            \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n            \"HF_HOME\": \"/data/.cache\",  # Set HF cache root under /data\n        }\n    )\n)\n</pre> # Define the GPU image for fine-tuning with Unsloth FINETUNING_GPU_IMAGE = (     ModalImage.from_registry(f\"nvidia/cuda:{CUDA_TAG}\", add_python=\"3.12\")     .apt_install(         \"git\",         \"build-essential\",     )     .uv_pip_install(         [             \"torch\",             \"torchvision\",             \"torchaudio\",  # optional but often bundled with torch         ]     )     # Install Unsloth and dependencies     .uv_pip_install(         [             # Unsloth core packages             \"unsloth\",             \"unsloth_zoo\",             # Core ML packages             \"bitsandbytes\",             \"accelerate\",             \"xformers\",             \"peft\",             \"trl\",             \"triton\",             \"cut_cross_entropy\",             # Upgraded packages             \"transformers\",             \"timm\",             # Additional dependencies             \"wandb\",             \"weave\",             \"pillow\",             \"opencv-python-headless\",             \"deepspeed\",             \"pyyaml\",             \"packaging\",             \"nltk\",             \"rouge_score\",             \"bert_score\",             \"jiwer\",             \"scikit-learn\",             \"tqdm\",             \"pandas\",             \"pyarrow\",             \"gradio\",             \"hf_transfer\",         ]     )     .env(         {             \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",             \"HF_HOME\": \"/data/.cache\",  # Set HF cache root under /data         }     ) ) In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=24 * HOURS,\n)\ndef download_datasets(\n    dataset_name: str = \"unsloth/LaTeX_OCR\",\n    split: str = \"train\",\n    cache_dir: str = \"/data/.cache\",\n):\n    \"\"\"\n    Download and cache a dataset from Hugging Face.\n\n    Args:\n        dataset_name: Name of the dataset to download (e.g., 'unsloth/LaTeX_OCR')\n        split: Dataset split to download (e.g., 'train', 'test', 'validation')\n        cache_dir: Directory to cache the dataset\n\n    Returns:\n        dict: Contains status, dataset info, and cache location\n    \"\"\"\n    from datasets import load_dataset\n    import os\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    dataset = load_dataset(dataset_name, split=split, cache_dir=cache_dir)\n\n    print(\"\\n\u2713 Dataset loaded successfully!\")\n    print(f\"  - Name: {dataset_name}\")\n    print(f\"  - Split: {split}\")\n    print(f\"  - Number of samples: {len(dataset)}\")\n    print(f\"  - Cached at: {cache_dir}\")\n    print(\"\\nDataset structure:\")\n    print(dataset)\n\n    exp_volume.commit()\n\n    return {\n        \"status\": \"completed\",\n        \"dataset_name\": dataset_name,\n        \"split\": split,\n        \"num_samples\": len(dataset),\n        \"cache_dir\": cache_dir,\n    }\n</pre> @app.function(     image=FINETUNING_GPU_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=24 * HOURS, ) def download_datasets(     dataset_name: str = \"unsloth/LaTeX_OCR\",     split: str = \"train\",     cache_dir: str = \"/data/.cache\", ):     \"\"\"     Download and cache a dataset from Hugging Face.      Args:         dataset_name: Name of the dataset to download (e.g., 'unsloth/LaTeX_OCR')         split: Dataset split to download (e.g., 'train', 'test', 'validation')         cache_dir: Directory to cache the dataset      Returns:         dict: Contains status, dataset info, and cache location     \"\"\"     from datasets import load_dataset     import os      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      os.makedirs(cache_dir, exist_ok=True)      dataset = load_dataset(dataset_name, split=split, cache_dir=cache_dir)      print(\"\\n\u2713 Dataset loaded successfully!\")     print(f\"  - Name: {dataset_name}\")     print(f\"  - Split: {split}\")     print(f\"  - Number of samples: {len(dataset)}\")     print(f\"  - Cached at: {cache_dir}\")     print(\"\\nDataset structure:\")     print(dataset)      exp_volume.commit()      return {         \"status\": \"completed\",         \"dataset_name\": dataset_name,         \"split\": split,         \"num_samples\": len(dataset),         \"cache_dir\": cache_dir,     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    gpu=\"l40s:1\",\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=24 * HOURS,\n)\ndef download_models(\n    model_name: str = BASE_MODEL_NAME,\n    cache_dir: str = \"/data/.cache\",\n):\n    \"\"\"\n    Download and cache a model from Hugging Face using FastVisionModel.\n    Uses 4-bit quantization for memory efficiency.\n\n    Args:\n        model_name: Name of the model to download (e.g., 'unsloth/gemma-3-4b-it')\n        cache_dir: Base directory to cache the model\n\n    Returns:\n        dict: Contains status and model info\n    \"\"\"\n    from unsloth import FastVisionModel\n    import os\n    import torch\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    model, processor = FastVisionModel.from_pretrained(\n        model_name,  # Can be HF hub ID or local path\n        load_in_4bit=False,\n        use_gradient_checkpointing=\"unsloth\",\n        max_seq_length=8000,\n        dtype=torch.bfloat16,  # Use bfloat16 for better performance\n    )\n    # Commit the volume to persist the cached model\n    exp_volume.commit()\n\n    return {\n        \"status\": \"completed\",\n        \"model_name\": model_name,\n        \"cache_dir\": cache_dir,\n        \"quantization\": \"4-bit\",\n    }\n</pre> @app.function(     image=FINETUNING_GPU_IMAGE,     gpu=\"l40s:1\",     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=24 * HOURS, ) def download_models(     model_name: str = BASE_MODEL_NAME,     cache_dir: str = \"/data/.cache\", ):     \"\"\"     Download and cache a model from Hugging Face using FastVisionModel.     Uses 4-bit quantization for memory efficiency.      Args:         model_name: Name of the model to download (e.g., 'unsloth/gemma-3-4b-it')         cache_dir: Base directory to cache the model      Returns:         dict: Contains status and model info     \"\"\"     from unsloth import FastVisionModel     import os     import torch      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      model, processor = FastVisionModel.from_pretrained(         model_name,  # Can be HF hub ID or local path         load_in_4bit=False,         use_gradient_checkpointing=\"unsloth\",         max_seq_length=8000,         dtype=torch.bfloat16,  # Use bfloat16 for better performance     )     # Commit the volume to persist the cached model     exp_volume.commit()      return {         \"status\": \"completed\",         \"model_name\": model_name,         \"cache_dir\": cache_dir,         \"quantization\": \"4-bit\",     } In\u00a0[\u00a0]: Copied! <pre># GPU Configuration\nTRAIN_GPU = \"a100-80gb\"  # Default GPU for training\nNUM_GPUS = 1\nTRAINING_GPU_CONFIG = f\"{TRAIN_GPU}:{NUM_GPUS}\"\n</pre> # GPU Configuration TRAIN_GPU = \"a100-80gb\"  # Default GPU for training NUM_GPUS = 1 TRAINING_GPU_CONFIG = f\"{TRAIN_GPU}:{NUM_GPUS}\" In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret, Secret.from_dotenv()],\n    gpu=TRAINING_GPU_CONFIG,\n    timeout=24 * HOURS,\n)\ndef fine_tune_unsloth(\n    model_path: str = BASE_MODEL_NAME,  # Can be HF hub ID or local path\n    dataset_name: str = \"unsloth/LaTeX_OCR\",\n    dataset_split: str = \"train\",\n    output_dir: str = OUTPUT_DIR_DEFAULT,\n    hub_id: str = None,\n    max_samples: int = None,  # Maximum number of samples to use from dataset\n    # LoRA parameters\n    lora_r: int = 32,\n    lora_alpha: int = 64,\n    lora_dropout: float = 0.0,\n    # Training hyperparameters\n    per_device_train_batch_size: int = 4,\n    gradient_accumulation_steps: int = 4,\n    num_train_epochs: int = 1,\n    learning_rate: float = 3e-4,\n    warmup_ratio: float = 0.2,\n    max_seq_length: int = 8000,\n    # Checkpoint saving configuration\n    save_strategy: str = \"steps\",\n    save_steps: int = 250,\n    save_total_limit: int = 20,\n    logging_steps: int = 10,\n    # WandB config\n    wandb_project: str = WANDB_PROJECT_DEFAULT,\n    wandb_run_name: str = None,\n):\n    \"\"\"\n    Fine-tune a vision-language model using Unsloth with LoRA.\n\n    Args:\n        model_path: Hugging Face model ID or local path to base model\n        dataset_name: Name of the dataset to use for training\n        dataset_split: Dataset split to use\n        output_dir: Directory to save the fine-tuned model\n        hub_id: Hugging Face Hub ID to push the model to (optional, if None, model won't be pushed)\n        max_samples: Maximum number of samples to use from dataset (if None, use all samples)\n        lora_r: LoRA rank\n        lora_alpha: LoRA alpha parameter\n        lora_dropout: LoRA dropout rate\n        per_device_train_batch_size: Batch size per device\n        gradient_accumulation_steps: Number of gradient accumulation steps\n        num_train_epochs: Number of training epochs\n        learning_rate: Learning rate\n        warmup_ratio: Warmup ratio for learning rate scheduler\n        max_seq_length: Maximum sequence length\n        save_strategy: Checkpoint save strategy ('steps' or 'epoch')\n        save_steps: Save checkpoint every N steps (when save_strategy='steps')\n        save_total_limit: Maximum number of checkpoints to keep\n        logging_steps: Log metrics every N steps\n        wandb_project: Weights &amp; Biases project name\n        wandb_run_name: Weights &amp; Biases run name\n\n    Returns:\n        dict: Contains training statistics and paths\n    \"\"\"\n    from unsloth import FastVisionModel, get_chat_template\n    from unsloth.trainer import UnslothVisionDataCollator\n    from trl import SFTTrainer, SFTConfig\n    import os\n    import torch\n    from datetime import datetime\n    from datasets import load_dataset\n\n    print(f\"\\n{'=' * 80}\")\n    print(\"FINE-TUNING CONFIGURATION\")\n    print(f\"{'=' * 80}\")\n    print(f\"Model: {model_path}\")\n    print(f\"Dataset: {dataset_name} ({dataset_split})\")\n    print(f\"Output: {output_dir}\")\n    print(f\"LoRA: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}\")\n    print(\n        f\"Training: batch_size={per_device_train_batch_size}, \"\n        f\"grad_accum={gradient_accumulation_steps}, epochs={num_train_epochs}\"\n    )\n    print(f\"{'=' * 80}\\n\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Set up environment variables\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n    os.environ[\"WANDB_API_KEY\"] = os.environ[\"WANDB_API_KEY\"]\n    os.environ[\"WANDB_PROJECT\"] = wandb_project\n\n    # Create a meaningful run name if not provided\n    if wandb_run_name is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        model_short = model_path.split(\"/\")[-1]  # Get just the model name part\n        wandb_run_name = f\"finetune_{model_short}_{timestamp}\"\n\n    # Set the W&amp;B run name\n    os.environ[\"WANDB_RUN_NAME\"] = wandb_run_name\n    print(f\"W&amp;B Run Name: {wandb_run_name}\")\n\n    # Swift-compatible memory optimization\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    # Disable dynamo for stable training\n    torch._dynamo.config.disable = True\n\n    print(\"Loading Unsloth model...\")\n\n    # =============================================================================\n    # Load model and add LoRA adapters\n    # =============================================================================\n\n    print(f\"Loading model from: {model_path}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_path,  # Can be HF hub ID or local path\n        load_in_4bit=False,\n        use_gradient_checkpointing=\"unsloth\",\n        max_seq_length=max_seq_length,\n        dtype=torch.bfloat16,  # Use bfloat16 for better performance\n    )\n\n    # Add LoRA adapters\n    print(\n        f\"Adding LoRA adapters (r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout})...\"\n    )\n    model = FastVisionModel.get_peft_model(\n        model,\n        finetune_vision_layers=False,\n        finetune_language_layers=True,\n        finetune_attention_modules=True,\n        finetune_mlp_modules=True,\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        random_state=3407,\n        target_modules=\"all-linear\",\n        modules_to_save=[\n            \"lm_head\",\n            \"embed_tokens\",\n        ],\n    )\n\n    # Set up chat template\n    processor = get_chat_template(processor, \"gemma-3\")\n\n    # =============================================================================\n    # Load and preprocess dataset\n    # =============================================================================\n\n    print(f\"Loading dataset: {dataset_name} (split: {dataset_split})\")\n    dataset = load_dataset(dataset_name, split=dataset_split)\n\n    # Limit dataset to max_samples if specified\n    if max_samples is not None and max_samples &gt; 0:\n        original_size = len(dataset)\n        dataset = dataset.select(range(min(max_samples, len(dataset))))\n        print(f\"Limited dataset from {original_size} to {len(dataset)} samples\")\n\n    print(f\"Using {len(dataset)} samples for training\")\n\n    instruction = \"Write the LaTeX representation for this image.\"\n\n    def convert_to_conversation(sample):\n        conversation = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": instruction},\n                    {\"type\": \"image\", \"image\": sample[\"image\"]},\n                ],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}],\n            },\n        ]\n        return {\"messages\": conversation}\n\n    pass\n    converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n\n    # =============================================================================\n    # Set up trainer and training\n    # =============================================================================\n\n    # Prepare for training\n    FastVisionModel.for_training(model)  # Enable for training!\n\n    # Set up trainer\n    print(\"Setting up trainer...\")\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=converted_dataset,  # Use dataset directly!\n        processing_class=processor.tokenizer,\n        data_collator=UnslothVisionDataCollator(\n            model=model, processor=processor\n        ),  # Use our custom collator\n        args=SFTConfig(\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_ratio=warmup_ratio,\n            num_train_epochs=num_train_epochs,\n            learning_rate=learning_rate,\n            logging_steps=logging_steps,\n            save_strategy=save_strategy,\n            save_steps=save_steps,\n            save_total_limit=save_total_limit,\n            # Additional optimization settings\n            gradient_checkpointing=True,\n            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n            max_grad_norm=0.3,\n            optim=\"adamw_torch_fused\",\n            weight_decay=0.01,\n            lr_scheduler_type=\"linear\",\n            seed=3407,\n            output_dir=output_dir,\n            report_to=\"wandb\",\n            # Vision-specific settings\n            remove_unused_columns=False,\n            dataset_text_field=\"\",\n            dataset_kwargs={\"skip_prepare_dataset\": True},\n            max_length=max_seq_length,\n        ),\n    )\n\n    # Rest of the training code remains the same...\n    # Show memory stats\n    gpu_stats = torch.cuda.get_device_properties(0)\n    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n    print(f\"{start_gpu_memory} GB of memory reserved.\")\n\n    # Train the model\n    trainer_stats = trainer.train()\n\n    # uncomment to resume from last checkpoint\n    # trainer_stats = trainer.train(resume_from_checkpoint=True)\n\n    # Show final memory stats\n    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n    used_percentage = round(used_memory / max_memory * 100, 3)\n    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n    print(\n        f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\"\n    )\n    print(f\"Peak reserved memory = {used_memory} GB.\")\n    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\n    # Save the model\n    # Create final_weights directory within output_dir\n    final_weights_dir = os.path.join(output_dir, \"final_weights\")\n    final_lora_dir = os.path.join(output_dir, \"final_lora\")\n    os.makedirs(final_weights_dir, exist_ok=True)\n    os.makedirs(final_lora_dir, exist_ok=True)\n\n    print(f\"Saving final lora weights to {final_lora_dir}\")\n    model.save_pretrained(final_lora_dir)\n    processor.save_pretrained(final_weights_dir)\n\n    # Only push to hub if hub_id is provided\n    if hub_id:\n        print(f\"Pushing LoRA weights to Hugging Face Hub as: {hub_id}_lora\")\n        model.push_to_hub(f\"{hub_id}_lora\", token=os.environ[\"HUGGINGFACE_TOKEN\"])\n        processor.push_to_hub(f\"{hub_id}_lora\", token=os.environ[\"HUGGINGFACE_TOKEN\"])\n    else:\n        print(\"Skipping LoRA weights push to hub (hub_id not provided)\")\n\n    print(f\"Saving merged model to {final_weights_dir}\")\n    model.save_pretrained_merged(\n        final_weights_dir, processor, save_method=\"merged_16bit\"\n    )\n\n    # Only push merged model if hub_id is provided\n    if hub_id:\n        print(f\"Pushing merged model to Hugging Face Hub as: {hub_id}\")\n        model.push_to_hub_merged(\n            hub_id,\n            processor,\n            token=os.environ[\"HUGGINGFACE_TOKEN\"],\n            save_method=\"merged_16bit\",\n        )\n    else:\n        print(\"Skipping merged model push to hub (hub_id not provided)\")\n\n    # Commit the output to the volume\n    exp_volume.commit()\n\n    print(\"Unsloth fine-tuning completed successfully.\")\n\n    return {\n        \"status\": \"completed\",\n        \"output_dir\": output_dir,\n        \"method\": \"unsloth\",\n        \"training_time\": trainer_stats.metrics[\"train_runtime\"],\n        \"memory_used\": used_memory,\n    }\n</pre> @app.function(     image=FINETUNING_GPU_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret, Secret.from_dotenv()],     gpu=TRAINING_GPU_CONFIG,     timeout=24 * HOURS, ) def fine_tune_unsloth(     model_path: str = BASE_MODEL_NAME,  # Can be HF hub ID or local path     dataset_name: str = \"unsloth/LaTeX_OCR\",     dataset_split: str = \"train\",     output_dir: str = OUTPUT_DIR_DEFAULT,     hub_id: str = None,     max_samples: int = None,  # Maximum number of samples to use from dataset     # LoRA parameters     lora_r: int = 32,     lora_alpha: int = 64,     lora_dropout: float = 0.0,     # Training hyperparameters     per_device_train_batch_size: int = 4,     gradient_accumulation_steps: int = 4,     num_train_epochs: int = 1,     learning_rate: float = 3e-4,     warmup_ratio: float = 0.2,     max_seq_length: int = 8000,     # Checkpoint saving configuration     save_strategy: str = \"steps\",     save_steps: int = 250,     save_total_limit: int = 20,     logging_steps: int = 10,     # WandB config     wandb_project: str = WANDB_PROJECT_DEFAULT,     wandb_run_name: str = None, ):     \"\"\"     Fine-tune a vision-language model using Unsloth with LoRA.      Args:         model_path: Hugging Face model ID or local path to base model         dataset_name: Name of the dataset to use for training         dataset_split: Dataset split to use         output_dir: Directory to save the fine-tuned model         hub_id: Hugging Face Hub ID to push the model to (optional, if None, model won't be pushed)         max_samples: Maximum number of samples to use from dataset (if None, use all samples)         lora_r: LoRA rank         lora_alpha: LoRA alpha parameter         lora_dropout: LoRA dropout rate         per_device_train_batch_size: Batch size per device         gradient_accumulation_steps: Number of gradient accumulation steps         num_train_epochs: Number of training epochs         learning_rate: Learning rate         warmup_ratio: Warmup ratio for learning rate scheduler         max_seq_length: Maximum sequence length         save_strategy: Checkpoint save strategy ('steps' or 'epoch')         save_steps: Save checkpoint every N steps (when save_strategy='steps')         save_total_limit: Maximum number of checkpoints to keep         logging_steps: Log metrics every N steps         wandb_project: Weights &amp; Biases project name         wandb_run_name: Weights &amp; Biases run name      Returns:         dict: Contains training statistics and paths     \"\"\"     from unsloth import FastVisionModel, get_chat_template     from unsloth.trainer import UnslothVisionDataCollator     from trl import SFTTrainer, SFTConfig     import os     import torch     from datetime import datetime     from datasets import load_dataset      print(f\"\\n{'=' * 80}\")     print(\"FINE-TUNING CONFIGURATION\")     print(f\"{'=' * 80}\")     print(f\"Model: {model_path}\")     print(f\"Dataset: {dataset_name} ({dataset_split})\")     print(f\"Output: {output_dir}\")     print(f\"LoRA: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}\")     print(         f\"Training: batch_size={per_device_train_batch_size}, \"         f\"grad_accum={gradient_accumulation_steps}, epochs={num_train_epochs}\"     )     print(f\"{'=' * 80}\\n\")     os.makedirs(output_dir, exist_ok=True)      # Set up environment variables      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]     os.environ[\"WANDB_API_KEY\"] = os.environ[\"WANDB_API_KEY\"]     os.environ[\"WANDB_PROJECT\"] = wandb_project      # Create a meaningful run name if not provided     if wandb_run_name is None:         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")         model_short = model_path.split(\"/\")[-1]  # Get just the model name part         wandb_run_name = f\"finetune_{model_short}_{timestamp}\"      # Set the W&amp;B run name     os.environ[\"WANDB_RUN_NAME\"] = wandb_run_name     print(f\"W&amp;B Run Name: {wandb_run_name}\")      # Swift-compatible memory optimization     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"     # Disable dynamo for stable training     torch._dynamo.config.disable = True      print(\"Loading Unsloth model...\")      # =============================================================================     # Load model and add LoRA adapters     # =============================================================================      print(f\"Loading model from: {model_path}\")     model, processor = FastVisionModel.from_pretrained(         model_path,  # Can be HF hub ID or local path         load_in_4bit=False,         use_gradient_checkpointing=\"unsloth\",         max_seq_length=max_seq_length,         dtype=torch.bfloat16,  # Use bfloat16 for better performance     )      # Add LoRA adapters     print(         f\"Adding LoRA adapters (r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout})...\"     )     model = FastVisionModel.get_peft_model(         model,         finetune_vision_layers=False,         finetune_language_layers=True,         finetune_attention_modules=True,         finetune_mlp_modules=True,         r=lora_r,         lora_alpha=lora_alpha,         lora_dropout=lora_dropout,         bias=\"none\",         random_state=3407,         target_modules=\"all-linear\",         modules_to_save=[             \"lm_head\",             \"embed_tokens\",         ],     )      # Set up chat template     processor = get_chat_template(processor, \"gemma-3\")      # =============================================================================     # Load and preprocess dataset     # =============================================================================      print(f\"Loading dataset: {dataset_name} (split: {dataset_split})\")     dataset = load_dataset(dataset_name, split=dataset_split)      # Limit dataset to max_samples if specified     if max_samples is not None and max_samples &gt; 0:         original_size = len(dataset)         dataset = dataset.select(range(min(max_samples, len(dataset))))         print(f\"Limited dataset from {original_size} to {len(dataset)} samples\")      print(f\"Using {len(dataset)} samples for training\")      instruction = \"Write the LaTeX representation for this image.\"      def convert_to_conversation(sample):         conversation = [             {                 \"role\": \"user\",                 \"content\": [                     {\"type\": \"text\", \"text\": instruction},                     {\"type\": \"image\", \"image\": sample[\"image\"]},                 ],             },             {                 \"role\": \"assistant\",                 \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}],             },         ]         return {\"messages\": conversation}      pass     converted_dataset = [convert_to_conversation(sample) for sample in dataset]      # =============================================================================     # Set up trainer and training     # =============================================================================      # Prepare for training     FastVisionModel.for_training(model)  # Enable for training!      # Set up trainer     print(\"Setting up trainer...\")     trainer = SFTTrainer(         model=model,         train_dataset=converted_dataset,  # Use dataset directly!         processing_class=processor.tokenizer,         data_collator=UnslothVisionDataCollator(             model=model, processor=processor         ),  # Use our custom collator         args=SFTConfig(             per_device_train_batch_size=per_device_train_batch_size,             per_device_eval_batch_size=per_device_train_batch_size,             gradient_accumulation_steps=gradient_accumulation_steps,             warmup_ratio=warmup_ratio,             num_train_epochs=num_train_epochs,             learning_rate=learning_rate,             logging_steps=logging_steps,             save_strategy=save_strategy,             save_steps=save_steps,             save_total_limit=save_total_limit,             # Additional optimization settings             gradient_checkpointing=True,             gradient_checkpointing_kwargs={\"use_reentrant\": False},             max_grad_norm=0.3,             optim=\"adamw_torch_fused\",             weight_decay=0.01,             lr_scheduler_type=\"linear\",             seed=3407,             output_dir=output_dir,             report_to=\"wandb\",             # Vision-specific settings             remove_unused_columns=False,             dataset_text_field=\"\",             dataset_kwargs={\"skip_prepare_dataset\": True},             max_length=max_seq_length,         ),     )      # Rest of the training code remains the same...     # Show memory stats     gpu_stats = torch.cuda.get_device_properties(0)     start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)     max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)     print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")     print(f\"{start_gpu_memory} GB of memory reserved.\")      # Train the model     trainer_stats = trainer.train()      # uncomment to resume from last checkpoint     # trainer_stats = trainer.train(resume_from_checkpoint=True)      # Show final memory stats     used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)     used_memory_for_lora = round(used_memory - start_gpu_memory, 3)     used_percentage = round(used_memory / max_memory * 100, 3)     lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)     print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")     print(         f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\"     )     print(f\"Peak reserved memory = {used_memory} GB.\")     print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")     print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")     print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")      # Save the model     # Create final_weights directory within output_dir     final_weights_dir = os.path.join(output_dir, \"final_weights\")     final_lora_dir = os.path.join(output_dir, \"final_lora\")     os.makedirs(final_weights_dir, exist_ok=True)     os.makedirs(final_lora_dir, exist_ok=True)      print(f\"Saving final lora weights to {final_lora_dir}\")     model.save_pretrained(final_lora_dir)     processor.save_pretrained(final_weights_dir)      # Only push to hub if hub_id is provided     if hub_id:         print(f\"Pushing LoRA weights to Hugging Face Hub as: {hub_id}_lora\")         model.push_to_hub(f\"{hub_id}_lora\", token=os.environ[\"HUGGINGFACE_TOKEN\"])         processor.push_to_hub(f\"{hub_id}_lora\", token=os.environ[\"HUGGINGFACE_TOKEN\"])     else:         print(\"Skipping LoRA weights push to hub (hub_id not provided)\")      print(f\"Saving merged model to {final_weights_dir}\")     model.save_pretrained_merged(         final_weights_dir, processor, save_method=\"merged_16bit\"     )      # Only push merged model if hub_id is provided     if hub_id:         print(f\"Pushing merged model to Hugging Face Hub as: {hub_id}\")         model.push_to_hub_merged(             hub_id,             processor,             token=os.environ[\"HUGGINGFACE_TOKEN\"],             save_method=\"merged_16bit\",         )     else:         print(\"Skipping merged model push to hub (hub_id not provided)\")      # Commit the output to the volume     exp_volume.commit()      print(\"Unsloth fine-tuning completed successfully.\")      return {         \"status\": \"completed\",         \"output_dir\": output_dir,         \"method\": \"unsloth\",         \"training_time\": trainer_stats.metrics[\"train_runtime\"],         \"memory_used\": used_memory,     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    volumes=VOLUME_CONFIG,\n    gpu=TRAINING_GPU_CONFIG,\n    secrets=[huggingface_secret, Secret.from_dotenv()],\n    timeout=2 * HOURS,\n)\ndef export_model(\n    lora_model_path: str = f\"{OUTPUT_DIR_DEFAULT}\",\n    output_path: str = None,\n    hub_model_id: str = None,\n    push_to_hub: bool = True,\n):\n    \"\"\"\n    Export and merge LoRA weights with base model.\n\n    This function loads a LoRA fine-tuned model, merges the LoRA weights with the base model,\n    and optionally pushes to Hugging Face Hub or saves locally.\n\n    Args:\n        lora_model_path: Path to the LoRA weights (can be local path or HF hub ID)\n        output_path: Local path to save the merged model (if not pushing to hub)\n        hub_model_id: Hugging Face Hub ID to push the merged model to\n        push_to_hub: Whether to push the merged model to Hugging Face Hub\n\n    Returns:\n        dict: Contains export status and paths\n    \"\"\"\n    from unsloth import FastVisionModel\n    import os\n    import torch\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Disable dynamo for stable operation\n    torch._dynamo.config.disable = True\n\n    print(f\"Exporting model from {lora_model_path}\")\n\n    # Load the LoRA fine-tuned model\n    model, processor = FastVisionModel.from_pretrained(\n        lora_model_path,\n        load_in_4bit=False,\n    )\n\n    # Prepare for inference (merges LoRA weights)\n    FastVisionModel.for_inference(model)\n\n    if push_to_hub and hub_model_id:\n        print(f\"Pushing to hub: {hub_model_id}\")\n\n        # Save to float16 and push to hub\n        model.push_to_hub_merged(\n            hub_model_id,\n            processor,\n            token=os.environ[\"HUGGINGFACE_TOKEN\"],\n            save_method=\"merged_16bit\",\n        )\n\n        print(f\"\u2713 Pushed to https://huggingface.co/{hub_model_id}\")\n\n        # Commit changes to volume\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"lora_model_path\": lora_model_path,\n            \"hub_model_id\": hub_model_id,\n            \"pushed_to_hub\": True,\n        }\n    else:\n        # Save locally as merged model\n        if output_path is None:\n            output_path = f\"{lora_model_path}_merged\"\n\n        print(f\"Saving to: {output_path}\")\n        os.makedirs(output_path, exist_ok=True)\n\n        model.save_pretrained_merged(output_path, processor, save_method=\"merged_16bit\")\n        print(f\"\u2713 Saved to {output_path}\")\n\n        # Commit changes to volume\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"lora_model_path\": lora_model_path,\n            \"export_path\": output_path,\n            \"pushed_to_hub\": False,\n        }\n</pre> @app.function(     image=FINETUNING_GPU_IMAGE,     volumes=VOLUME_CONFIG,     gpu=TRAINING_GPU_CONFIG,     secrets=[huggingface_secret, Secret.from_dotenv()],     timeout=2 * HOURS, ) def export_model(     lora_model_path: str = f\"{OUTPUT_DIR_DEFAULT}\",     output_path: str = None,     hub_model_id: str = None,     push_to_hub: bool = True, ):     \"\"\"     Export and merge LoRA weights with base model.      This function loads a LoRA fine-tuned model, merges the LoRA weights with the base model,     and optionally pushes to Hugging Face Hub or saves locally.      Args:         lora_model_path: Path to the LoRA weights (can be local path or HF hub ID)         output_path: Local path to save the merged model (if not pushing to hub)         hub_model_id: Hugging Face Hub ID to push the merged model to         push_to_hub: Whether to push the merged model to Hugging Face Hub      Returns:         dict: Contains export status and paths     \"\"\"     from unsloth import FastVisionModel     import os     import torch      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      # Disable dynamo for stable operation     torch._dynamo.config.disable = True      print(f\"Exporting model from {lora_model_path}\")      # Load the LoRA fine-tuned model     model, processor = FastVisionModel.from_pretrained(         lora_model_path,         load_in_4bit=False,     )      # Prepare for inference (merges LoRA weights)     FastVisionModel.for_inference(model)      if push_to_hub and hub_model_id:         print(f\"Pushing to hub: {hub_model_id}\")          # Save to float16 and push to hub         model.push_to_hub_merged(             hub_model_id,             processor,             token=os.environ[\"HUGGINGFACE_TOKEN\"],             save_method=\"merged_16bit\",         )          print(f\"\u2713 Pushed to https://huggingface.co/{hub_model_id}\")          # Commit changes to volume         exp_volume.commit()          return {             \"status\": \"completed\",             \"lora_model_path\": lora_model_path,             \"hub_model_id\": hub_model_id,             \"pushed_to_hub\": True,         }     else:         # Save locally as merged model         if output_path is None:             output_path = f\"{lora_model_path}_merged\"          print(f\"Saving to: {output_path}\")         os.makedirs(output_path, exist_ok=True)          model.save_pretrained_merged(output_path, processor, save_method=\"merged_16bit\")         print(f\"\u2713 Saved to {output_path}\")          # Commit changes to volume         exp_volume.commit()          return {             \"status\": \"completed\",             \"lora_model_path\": lora_model_path,             \"export_path\": output_path,             \"pushed_to_hub\": False,         } In\u00a0[\u00a0]: Copied! <pre># Default serving configuration\nDEFAULT_SERVE_MODEL = \"/data/Finetuned_Gemma_3_4b_it/final_weights\"  # Use the base model by default (change to your hub_id after fine-tuning)\nSERVE_GPU = \"L40S\"  # \"a100-80gb\", \"a100-40gb\", \"l40s\"\nSERVE_NUM_GPUS = 1\nSERVE_GPU_CONFIG = f\"{SERVE_GPU}:{SERVE_NUM_GPUS}\"\nVLLM_PORT = 8000\n</pre> # Default serving configuration DEFAULT_SERVE_MODEL = \"/data/Finetuned_Gemma_3_4b_it/final_weights\"  # Use the base model by default (change to your hub_id after fine-tuning) SERVE_GPU = \"L40S\"  # \"a100-80gb\", \"a100-40gb\", \"l40s\" SERVE_NUM_GPUS = 1 SERVE_GPU_CONFIG = f\"{SERVE_GPU}:{SERVE_NUM_GPUS}\" VLLM_PORT = 8000 In\u00a0[\u00a0]: Copied! <pre># CUDA configuration for vLLM\nVLLM_CUDA_VERSION = \"12.8.1\"\nVLLM_CUDA_FLAVOR = \"devel\"\nVLLM_CUDA_OS = \"ubuntu24.04\"\nVLLM_CUDA_TAG = f\"{VLLM_CUDA_VERSION}-{VLLM_CUDA_FLAVOR}-{VLLM_CUDA_OS}\"\n</pre> # CUDA configuration for vLLM VLLM_CUDA_VERSION = \"12.8.1\" VLLM_CUDA_FLAVOR = \"devel\" VLLM_CUDA_OS = \"ubuntu24.04\" VLLM_CUDA_TAG = f\"{VLLM_CUDA_VERSION}-{VLLM_CUDA_FLAVOR}-{VLLM_CUDA_OS}\" In\u00a0[\u00a0]: Copied! <pre># Build vLLM serving image\nVLLM_GPU_IMAGE = (\n    ModalImage.from_registry(f\"nvidia/cuda:{VLLM_CUDA_TAG}\", add_python=\"3.12\")\n    .apt_install(\"libopenmpi-dev\", \"libnuma-dev\")\n    .run_commands(\"pip install --upgrade pip\")\n    .run_commands(\"pip install uv\")\n    .run_commands(\"uv pip install vllm -U --system\")\n    .pip_install(\n        \"datasets\",\n        \"pillow\",\n        \"huggingface_hub[hf_transfer]\",\n        \"requests\",\n        \"numpy\",\n        \"regex\",\n        \"sentencepiece\",\n    )\n    .run_commands(\n        \"uv pip install 'flash-attn&gt;=2.7.1,&lt;=2.8.0' --no-build-isolation --system\"\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .run_commands(\"python -c 'import torch; print(torch.__version__);'\")\n)\n</pre> # Build vLLM serving image VLLM_GPU_IMAGE = (     ModalImage.from_registry(f\"nvidia/cuda:{VLLM_CUDA_TAG}\", add_python=\"3.12\")     .apt_install(\"libopenmpi-dev\", \"libnuma-dev\")     .run_commands(\"pip install --upgrade pip\")     .run_commands(\"pip install uv\")     .run_commands(\"uv pip install vllm -U --system\")     .pip_install(         \"datasets\",         \"pillow\",         \"huggingface_hub[hf_transfer]\",         \"requests\",         \"numpy\",         \"regex\",         \"sentencepiece\",     )     .run_commands(         \"uv pip install 'flash-attn&gt;=2.7.1,&lt;=2.8.0' --no-build-isolation --system\"     )     .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})     .run_commands(\"python -c 'import torch; print(torch.__version__);'\") ) In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=VLLM_GPU_IMAGE,\n    gpu=SERVE_GPU_CONFIG,\n    scaledown_window=3 * 60,  # how long should we stay up with no requests? 3 minutes\n    secrets=[huggingface_secret],\n    volumes=VOLUME_CONFIG,\n    max_containers=2,\n    timeout=24 * HOURS,\n)\n@modal.concurrent(max_inputs=50)\n@modal.web_server(port=8000, startup_timeout=5 * 60)\ndef serve_vllm():\n    \"\"\"\n    Serve a model using vLLM for fast inference.\n\n    Configuration is controlled via module-level constants:\n    - DEFAULT_SERVE_MODEL: Model to serve (HF hub ID or local path)\n    - VLLM_PORT: Port to serve on\n    - SERVE_NUM_GPUS: Number of GPUs to use for tensor parallelism\n\n    Returns:\n        Web server endpoint\n    \"\"\"\n    import subprocess\n\n    # Set up environment variables\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    cmd = [\n        \"vllm\",\n        \"serve\",\n        \"--uvicorn-log-level=info\",\n        DEFAULT_SERVE_MODEL,\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        str(VLLM_PORT),\n    ]\n\n    # Compilation settings - use enforce-eager for faster boot\n    cmd += [\"--enforce-eager\"]\n\n    # GPU configuration\n    cmd += [\"--tensor-parallel-size\", str(SERVE_NUM_GPUS)]\n    cmd += [\"--gpu-memory-utilization\", \"0.4\"]\n\n    cmd += [\"--trust-remote-code\"]\n\n    print(\"Starting vLLM server with command:\")\n    print(\" \".join(cmd))\n    subprocess.Popen(\" \".join(cmd), shell=True)\n</pre> @app.function(     image=VLLM_GPU_IMAGE,     gpu=SERVE_GPU_CONFIG,     scaledown_window=3 * 60,  # how long should we stay up with no requests? 3 minutes     secrets=[huggingface_secret],     volumes=VOLUME_CONFIG,     max_containers=2,     timeout=24 * HOURS, ) @modal.concurrent(max_inputs=50) @modal.web_server(port=8000, startup_timeout=5 * 60) def serve_vllm():     \"\"\"     Serve a model using vLLM for fast inference.      Configuration is controlled via module-level constants:     - DEFAULT_SERVE_MODEL: Model to serve (HF hub ID or local path)     - VLLM_PORT: Port to serve on     - SERVE_NUM_GPUS: Number of GPUs to use for tensor parallelism      Returns:         Web server endpoint     \"\"\"     import subprocess      # Set up environment variables     os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      cmd = [         \"vllm\",         \"serve\",         \"--uvicorn-log-level=info\",         DEFAULT_SERVE_MODEL,         \"--host\",         \"0.0.0.0\",         \"--port\",         str(VLLM_PORT),     ]      # Compilation settings - use enforce-eager for faster boot     cmd += [\"--enforce-eager\"]      # GPU configuration     cmd += [\"--tensor-parallel-size\", str(SERVE_NUM_GPUS)]     cmd += [\"--gpu-memory-utilization\", \"0.4\"]      cmd += [\"--trust-remote-code\"]      print(\"Starting vLLM server with command:\")     print(\" \".join(cmd))     subprocess.Popen(\" \".join(cmd), shell=True) In\u00a0[\u00a0]: Copied! <pre># Build evaluation image (CPU-based)\nEVAL_IMAGE = (\n    ModalImage.debian_slim(python_version=\"3.12\")\n    .pip_install(\n        \"openai\",\n        \"datasets\",\n        \"pillow\",\n        \"numpy\",\n        \"jiwer\",\n        \"nltk\",\n        \"tqdm\",\n        \"huggingface_hub[hf_transfer]\",\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n)\n</pre> # Build evaluation image (CPU-based) EVAL_IMAGE = (     ModalImage.debian_slim(python_version=\"3.12\")     .pip_install(         \"openai\",         \"datasets\",         \"pillow\",         \"numpy\",         \"jiwer\",         \"nltk\",         \"tqdm\",         \"huggingface_hub[hf_transfer]\",     )     .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"}) ) In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=EVAL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=2 * HOURS,\n)\ndef evaluate_model(\n    endpoint_url: str = None,\n    model_name: str = \"/data/Finetuned_Gemma_3_4b_it/final_weights\",\n    dataset_name: str = \"unsloth/LaTeX_OCR\",\n    dataset_split: str = \"test\",\n    max_samples: int = 100,\n    max_parallel_requests: int = 8,\n    temperature: float = 0.1,\n    max_tokens: int = 512,\n):\n    \"\"\"\n    Evaluate a vision-language model on the LaTeX OCR dataset.\n\n    Args:\n        endpoint_url: URL of the inference endpoint (e.g., \"https://your-endpoint.modal.run/v1\").\n                     If None, automatically retrieves from deployed serve_vllm function.\n        model_name: Model name/path to use for inference\n        dataset_name: Name of the dataset to evaluate on\n        dataset_split: Dataset split to use\n        max_samples: Maximum number of samples to evaluate\n        max_parallel_requests: Number of parallel requests to make\n        temperature: Temperature for inference\n        max_tokens: Maximum tokens to generate\n\n    Returns:\n        dict: Contains evaluation metrics and results\n    \"\"\"\n    import base64\n    import io\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from openai import OpenAI\n    from datasets import load_dataset\n    from jiwer import wer, cer\n    from tqdm import tqdm\n    import time\n\n    # Auto-retrieve endpoint URL if not provided\n    if endpoint_url is None:\n        try:\n            endpoint_url = serve_vllm.get_web_url()\n            if endpoint_url:\n                endpoint_url = endpoint_url.rstrip(\"/\") + \"/v1\"\n                print(f\"Auto-detected endpoint: {endpoint_url}\")\n            else:\n                raise ValueError(\"serve_vllm endpoint URL not available\")\n        except Exception as e:\n            raise ValueError(\n                f\"Could not auto-detect endpoint URL: {e}. \"\n                \"Please provide endpoint_url explicitly or ensure serve_vllm is deployed.\"\n            )\n\n    # Load dataset\n    dataset = load_dataset(dataset_name, split=dataset_split)\n\n    # Limit to max_samples\n    if max_samples and max_samples &lt; len(dataset):\n        dataset = dataset.select(range(max_samples))\n\n    print(f\"Evaluating {len(dataset)} samples from {dataset_name} on {endpoint_url}\")\n\n    # Initialize OpenAI client\n    client = OpenAI(base_url=endpoint_url, api_key=\"EMPTY\")\n\n    # Instruction for the model\n    instruction = \"Write the LaTeX representation for this image.\"\n\n    def encode_image_to_base64(image):\n        \"\"\"Convert PIL Image to base64 string.\"\"\"\n        buffered = io.BytesIO()\n        # Convert to RGB if necessary\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n        image.save(buffered, format=\"JPEG\")\n        return base64.b64encode(buffered.getvalue()).decode()\n\n    def run_inference(sample, idx):\n        \"\"\"Run inference on a single sample.\"\"\"\n        try:\n            # Encode image\n            image_base64 = encode_image_to_base64(sample[\"image\"])\n\n            # Make request\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n                                },\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": instruction,\n                            },\n                        ],\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=max_tokens,\n                top_p=0.95,\n            )\n\n            prediction = response.choices[0].message.content.strip()\n            ground_truth = sample[\"text\"].strip()\n\n            return {\n                \"idx\": idx,\n                \"prediction\": prediction,\n                \"ground_truth\": ground_truth,\n                \"success\": True,\n                \"error\": None,\n            }\n\n        except Exception as e:\n            return {\n                \"idx\": idx,\n                \"prediction\": None,\n                \"ground_truth\": sample[\"text\"].strip(),\n                \"success\": False,\n                \"error\": str(e),\n            }\n\n    # Run parallel inference\n    results = []\n    start_time = time.time()\n\n    with ThreadPoolExecutor(max_workers=max_parallel_requests) as executor:\n        # Submit all tasks\n        future_to_idx = {\n            executor.submit(run_inference, dataset[i], i): i\n            for i in range(len(dataset))\n        }\n\n        # Process completed tasks with progress bar\n        with tqdm(total=len(dataset), desc=\"Evaluating\", unit=\"sample\") as pbar:\n            for future in as_completed(future_to_idx):\n                result = future.result()\n                results.append(result)\n                pbar.update(1)\n\n    end_time = time.time()\n    total_time = end_time - start_time\n\n    # Sort results by index\n    results.sort(key=lambda x: x[\"idx\"])\n\n    successful_results = [r for r in results if r[\"success\"]]\n    failed_count = len(results) - len(successful_results)\n\n    if len(successful_results) == 0:\n        return {\n            \"status\": \"failed\",\n            \"error\": \"All predictions failed\",\n            \"failed_count\": failed_count,\n            \"total_samples\": len(results),\n        }\n\n    predictions = [r[\"prediction\"] for r in successful_results]\n    ground_truths = [r[\"ground_truth\"] for r in successful_results]\n\n    # Calculate WER (Word Error Rate) and CER (Character Error Rate)\n    try:\n        word_error_rate = wer(ground_truths, predictions)\n        char_error_rate = cer(ground_truths, predictions)\n    except Exception:\n        word_error_rate = None\n        char_error_rate = None\n\n    # Calculate exact match accuracy\n    exact_matches = sum(\n        1 for p, g in zip(predictions, ground_truths) if p.strip() == g.strip()\n    )\n    exact_match_accuracy = exact_matches / len(successful_results)\n\n    # Calculate average lengths\n    avg_pred_length = sum(len(p) for p in predictions) / len(predictions)\n    avg_gt_length = sum(len(g) for g in ground_truths) / len(ground_truths)\n\n    # Print concise results\n    print(f\"\\n{'=' * 80}\")\n    print(\n        f\"Results: {len(successful_results)}/{len(results)} successful ({len(successful_results) / len(results) * 100:.1f}%)\"\n    )\n    print(\n        f\"Exact Match: {exact_match_accuracy * 100:.1f}% | CER: {char_error_rate * 100:.1f}% | WER: {word_error_rate * 100:.1f}%\"\n        if char_error_rate and word_error_rate\n        else f\"Exact Match: {exact_match_accuracy * 100:.1f}%\"\n    )\n    print(f\"Time: {total_time:.1f}s ({len(results) / total_time:.1f} samples/s)\")\n    print(f\"{'=' * 80}\")\n\n    return {\n        \"status\": \"completed\",\n        \"endpoint_url\": endpoint_url,\n        \"model_name\": model_name,\n        \"dataset_name\": dataset_name,\n        \"total_samples\": len(results),\n        \"successful_samples\": len(successful_results),\n        \"failed_samples\": failed_count,\n        \"success_rate\": len(successful_results) / len(results),\n        \"metrics\": {\n            \"exact_match_accuracy\": exact_match_accuracy,\n            \"character_error_rate\": char_error_rate,\n            \"word_error_rate\": word_error_rate,\n        },\n        \"statistics\": {\n            \"avg_prediction_length\": avg_pred_length,\n            \"avg_ground_truth_length\": avg_gt_length,\n            \"total_time_seconds\": total_time,\n            \"avg_time_per_sample\": total_time / len(results),\n            \"throughput_samples_per_second\": len(results) / total_time,\n        },\n        \"examples\": [\n            {\n                \"ground_truth\": r[\"ground_truth\"],\n                \"prediction\": r[\"prediction\"],\n                \"match\": r[\"prediction\"].strip() == r[\"ground_truth\"].strip(),\n            }\n            for r in successful_results[:10]\n        ],\n    }\n</pre> @app.function(     image=EVAL_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=2 * HOURS, ) def evaluate_model(     endpoint_url: str = None,     model_name: str = \"/data/Finetuned_Gemma_3_4b_it/final_weights\",     dataset_name: str = \"unsloth/LaTeX_OCR\",     dataset_split: str = \"test\",     max_samples: int = 100,     max_parallel_requests: int = 8,     temperature: float = 0.1,     max_tokens: int = 512, ):     \"\"\"     Evaluate a vision-language model on the LaTeX OCR dataset.      Args:         endpoint_url: URL of the inference endpoint (e.g., \"https://your-endpoint.modal.run/v1\").                      If None, automatically retrieves from deployed serve_vllm function.         model_name: Model name/path to use for inference         dataset_name: Name of the dataset to evaluate on         dataset_split: Dataset split to use         max_samples: Maximum number of samples to evaluate         max_parallel_requests: Number of parallel requests to make         temperature: Temperature for inference         max_tokens: Maximum tokens to generate      Returns:         dict: Contains evaluation metrics and results     \"\"\"     import base64     import io     from concurrent.futures import ThreadPoolExecutor, as_completed     from openai import OpenAI     from datasets import load_dataset     from jiwer import wer, cer     from tqdm import tqdm     import time      # Auto-retrieve endpoint URL if not provided     if endpoint_url is None:         try:             endpoint_url = serve_vllm.get_web_url()             if endpoint_url:                 endpoint_url = endpoint_url.rstrip(\"/\") + \"/v1\"                 print(f\"Auto-detected endpoint: {endpoint_url}\")             else:                 raise ValueError(\"serve_vllm endpoint URL not available\")         except Exception as e:             raise ValueError(                 f\"Could not auto-detect endpoint URL: {e}. \"                 \"Please provide endpoint_url explicitly or ensure serve_vllm is deployed.\"             )      # Load dataset     dataset = load_dataset(dataset_name, split=dataset_split)      # Limit to max_samples     if max_samples and max_samples &lt; len(dataset):         dataset = dataset.select(range(max_samples))      print(f\"Evaluating {len(dataset)} samples from {dataset_name} on {endpoint_url}\")      # Initialize OpenAI client     client = OpenAI(base_url=endpoint_url, api_key=\"EMPTY\")      # Instruction for the model     instruction = \"Write the LaTeX representation for this image.\"      def encode_image_to_base64(image):         \"\"\"Convert PIL Image to base64 string.\"\"\"         buffered = io.BytesIO()         # Convert to RGB if necessary         if image.mode != \"RGB\":             image = image.convert(\"RGB\")         image.save(buffered, format=\"JPEG\")         return base64.b64encode(buffered.getvalue()).decode()      def run_inference(sample, idx):         \"\"\"Run inference on a single sample.\"\"\"         try:             # Encode image             image_base64 = encode_image_to_base64(sample[\"image\"])              # Make request             response = client.chat.completions.create(                 model=model_name,                 messages=[                     {                         \"role\": \"user\",                         \"content\": [                             {                                 \"type\": \"image_url\",                                 \"image_url\": {                                     \"url\": f\"data:image/jpeg;base64,{image_base64}\"                                 },                             },                             {                                 \"type\": \"text\",                                 \"text\": instruction,                             },                         ],                     },                 ],                 temperature=temperature,                 max_tokens=max_tokens,                 top_p=0.95,             )              prediction = response.choices[0].message.content.strip()             ground_truth = sample[\"text\"].strip()              return {                 \"idx\": idx,                 \"prediction\": prediction,                 \"ground_truth\": ground_truth,                 \"success\": True,                 \"error\": None,             }          except Exception as e:             return {                 \"idx\": idx,                 \"prediction\": None,                 \"ground_truth\": sample[\"text\"].strip(),                 \"success\": False,                 \"error\": str(e),             }      # Run parallel inference     results = []     start_time = time.time()      with ThreadPoolExecutor(max_workers=max_parallel_requests) as executor:         # Submit all tasks         future_to_idx = {             executor.submit(run_inference, dataset[i], i): i             for i in range(len(dataset))         }          # Process completed tasks with progress bar         with tqdm(total=len(dataset), desc=\"Evaluating\", unit=\"sample\") as pbar:             for future in as_completed(future_to_idx):                 result = future.result()                 results.append(result)                 pbar.update(1)      end_time = time.time()     total_time = end_time - start_time      # Sort results by index     results.sort(key=lambda x: x[\"idx\"])      successful_results = [r for r in results if r[\"success\"]]     failed_count = len(results) - len(successful_results)      if len(successful_results) == 0:         return {             \"status\": \"failed\",             \"error\": \"All predictions failed\",             \"failed_count\": failed_count,             \"total_samples\": len(results),         }      predictions = [r[\"prediction\"] for r in successful_results]     ground_truths = [r[\"ground_truth\"] for r in successful_results]      # Calculate WER (Word Error Rate) and CER (Character Error Rate)     try:         word_error_rate = wer(ground_truths, predictions)         char_error_rate = cer(ground_truths, predictions)     except Exception:         word_error_rate = None         char_error_rate = None      # Calculate exact match accuracy     exact_matches = sum(         1 for p, g in zip(predictions, ground_truths) if p.strip() == g.strip()     )     exact_match_accuracy = exact_matches / len(successful_results)      # Calculate average lengths     avg_pred_length = sum(len(p) for p in predictions) / len(predictions)     avg_gt_length = sum(len(g) for g in ground_truths) / len(ground_truths)      # Print concise results     print(f\"\\n{'=' * 80}\")     print(         f\"Results: {len(successful_results)}/{len(results)} successful ({len(successful_results) / len(results) * 100:.1f}%)\"     )     print(         f\"Exact Match: {exact_match_accuracy * 100:.1f}% | CER: {char_error_rate * 100:.1f}% | WER: {word_error_rate * 100:.1f}%\"         if char_error_rate and word_error_rate         else f\"Exact Match: {exact_match_accuracy * 100:.1f}%\"     )     print(f\"Time: {total_time:.1f}s ({len(results) / total_time:.1f} samples/s)\")     print(f\"{'=' * 80}\")      return {         \"status\": \"completed\",         \"endpoint_url\": endpoint_url,         \"model_name\": model_name,         \"dataset_name\": dataset_name,         \"total_samples\": len(results),         \"successful_samples\": len(successful_results),         \"failed_samples\": failed_count,         \"success_rate\": len(successful_results) / len(results),         \"metrics\": {             \"exact_match_accuracy\": exact_match_accuracy,             \"character_error_rate\": char_error_rate,             \"word_error_rate\": word_error_rate,         },         \"statistics\": {             \"avg_prediction_length\": avg_pred_length,             \"avg_ground_truth_length\": avg_gt_length,             \"total_time_seconds\": total_time,             \"avg_time_per_sample\": total_time / len(results),             \"throughput_samples_per_second\": len(results) / total_time,         },         \"examples\": [             {                 \"ground_truth\": r[\"ground_truth\"],                 \"prediction\": r[\"prediction\"],                 \"match\": r[\"prediction\"].strip() == r[\"ground_truth\"].strip(),             }             for r in successful_results[:10]         ],     }"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModal/#modal-app-definition-volume-and-secret-setup","title":"============================================================================= MODAL APP DEFINITION , VOLUME AND SECRET SETUP\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModal/#configuration-default-constants","title":"============================================================================= CONFIGURATION DEFAULT CONSTANTS\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModal/#configure-images-and-environments","title":"============================================================================= CONFIGURE IMAGES AND ENVIRONMENTS\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModal/#vllm-serving-configuration","title":"============================================================================= VLLM SERVING CONFIGURATION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModal/#evaluation-configuration","title":"============================================================================= EVALUATION CONFIGURATION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/","title":"Fine-tuning Gemma with Unsloth","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#fine-tuning-gemma-3-4b-with-unsloth-on-modal-production-ready-vision-language-training","title":"Fine-tuning Gemma 3-4B with Unsloth on Modal: Production-Ready Vision-Language Training","text":"<p>\ud83d\udcc4 View Complete Python Script</p> <p>\ud83d\udd17 Original Unsloth Colab Notebook</p> <p>So you've mastered the basics with nanoGPT. Now let's level up and build a production-grade ML pipeline - we're talking dataset management, LoRA fine-tuning, model evaluation, and deployment. All on Modal's serverless infrastructure.</p> <p>We'll fine-tune Google's Gemma 3-4B vision model to read LaTeX equations from images. By the end, you'll have a fully deployed API that can look at a math equation and spit out the LaTeX code for it.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#why-unsloth","title":"Why Unsloth?","text":"<p>Here's the thing - training large language models is expensive and slow. Unsloth changes that game completely.</p> <p>I discovered Unsloth when I was trying to fine-tune Llama models and getting frustrated with how slow everything was. Then I found this library that claimed \"2x faster training\" and I was skeptical. But holy shit, it actually delivers.</p> <p>What makes Unsloth special: - 2-5x faster training than standard Hugging Face Transformers (no joke, you'll see the difference) - 60-80% less memory usage - fits bigger models on smaller GPUs - Built-in LoRA and QLoRA support - efficient fine-tuning out of the box - Optimized kernels for vision-language models like Gemma, Llama, Qwen - Drop-in replacement for Hugging Face - same API, just faster</p> <p>The original Colab notebook from Unsloth shows you how to do this on a single GPU. We're taking that exact workflow and making it run on Modal, so you can: - Train on any GPU type (A100-80GB? Sure!) - Separate data prep from training (save money) - Deploy with vLLM for high-throughput inference - Scale to production without changing your code</p> <p>Think of this as \"the Unsloth Colab notebook, but productionized\".</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#what-were-building","title":"What We're Building","text":"<p>This isn't just a training script. We're building a complete ML pipeline that handles everything from data to deployment:</p> <ol> <li>Download datasets (on CPU, because why waste GPU money?)</li> <li>Download and cache models (one time cost, reuse forever)</li> <li>Fine-tune with LoRA (the actual training)</li> <li>Evaluate performance (with real metrics, not vibes)</li> <li>Deploy with vLLM (production-ready serving with auto-scaling)</li> </ol> <p>The cool part? Each stage is independent. Screw up training? Just re-run that step. Want to evaluate a different checkpoint? Easy.</p> <p>Here's what the flow looks like:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Download Data  \u2502  (CPU - $0.00001/hr)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Download Model  \u2502  (L40S - $1/hr, one time)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Fine-tune     \u2502  (A100-80GB - $3.50/hr)\n\u2502   with LoRA     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Export/Merge   \u2502  (A100-80GB - ~10 min)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n\u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Serve \u2502 \u2502 Evaluate\u2502  (Both use the deployed model)\n\u2502 vLLM  \u2502 \u2502  Model  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#getting-started","title":"Getting Started","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#install-modal","title":"Install Modal","text":"<p>Same as before:</p> <pre><code>pip install modal\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#authenticate","title":"Authenticate","text":"<pre><code>modal setup\n</code></pre> <p>Or use API keys:</p> <pre><code>export MODAL_TOKEN_ID=&lt;your_token_id&gt;\nexport MODAL_TOKEN_SECRET=&lt;your_token_secret&gt;\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#set-up-your-secrets","title":"Set Up Your Secrets","text":"<p>This time we actually need some secrets because we're downloading from Hugging Face and (optionally) logging to Weights &amp; Biases.</p> <p>You'll need: - A Hugging Face token (get it from hf.co/settings/tokens) - A Weights &amp; Biases API key (optional but highly recommended - get it from wandb.ai/authorize)</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#option-1-env-file-easiest-for-local-development","title":"Option 1: .env file (easiest for local development)","text":"<p>Create a <code>.env</code> file in your project:</p> <pre><code>HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxx\nWANDB_API_KEY=xxxxxxxxxxxxx\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#option-2-modal-secrets-better-for-production","title":"Option 2: Modal Secrets (better for production)","text":"<pre><code>modal secret create secrets-hf-wandb \\\n  HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxx \\\n  WANDB_API_KEY=xxxxxxxxxxxxx\n</code></pre> <p>Note: The script looks for a secret named <code>secrets-hf-wandb</code>. If you use a different name, just update the code where it says <code>Secret.from_name(\"secrets-hf-wandb\")</code>.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#project-structure","title":"Project Structure","text":"<p>Beautiful thing about this? It's just one file:</p> <pre><code>ServerLessFinetuning/\n\u251c\u2500\u2500 FinetuneGemmaUnslothModal.py    # Everything lives here\n\u2514\u2500\u2500 .env                             # Optional: your secrets\n</code></pre> <p>No cloning repos, no juggling dependencies. Just one Python file that does it all.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#understanding-the-pipeline","title":"Understanding the Pipeline","text":"<p>Let's break down what we're building. This is a production-grade ML pipeline with 6 independent stages. You can run any stage separately, which is huge for development and debugging.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-overview","title":"Stage Overview","text":"<ol> <li>Dataset Download - Grab the LaTeX OCR dataset (images of equations + LaTeX code)</li> <li>Model Download - Download Gemma 3-4B and cache it (so we don't re-download every time)</li> <li>LoRA Fine-tuning - Train adapters to teach Gemma to read equations</li> <li>Model Export - Merge LoRA adapters into the base model (makes deployment easier)</li> <li>vLLM Serving - Deploy as an OpenAI-compatible API with auto-scaling</li> <li>Evaluation - Measure accuracy with real metrics (character error rate, exact match, etc.)</li> </ol> <p>Each stage saves its outputs to a Modal volume, so the next stage can pick up where the last one left off.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#configuration-and-setup","title":"Configuration and Setup","text":"<p>Alright, let's dive into the code. I'll walk you through each piece and explain why it matters.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#app-volume-and-secrets","title":"App, Volume, and Secrets","text":"<pre><code>from modal import App, Image as ModalImage, Volume, Secret\n\n# Create the Modal app - this is our project namespace\napp = App(\"Finetuned_Gemma_3_4b_it\")\n\n# Create persistent storage - everything goes here\n# Models, datasets, checkpoints, evaluation results - all in one volume\nexp_volume = Volume.from_name(\"Finetuned_Gemma_3_4b_it\", create_if_missing=True)\n\n# Mount the volume at /data in all our containers\nVOLUME_CONFIG = {\n    \"/data\": exp_volume,  # Single volume for the entire experiment\n}\n\n# Load secrets for Hugging Face and Weights &amp; Biases\n# This injects HUGGINGFACE_TOKEN and WANDB_API_KEY as environment variables\nhuggingface_secret = Secret.from_name(\"secrets-hf-wandb\")\n</code></pre> <p>What's happening here:</p> <ul> <li>Volume strategy: I use a single volume for the entire experiment. Models in <code>/data/.cache</code>, checkpoints in <code>/data/Finetuned_Gemma_3_4b_it</code>, datasets in <code>/data/.cache</code>. Keeps everything organized and makes debugging easier.</li> <li>Secrets: Modal injects these as environment variables. So inside our functions, we can just do <code>os.environ[\"HUGGINGFACE_TOKEN\"]</code>.</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#configuration-constants","title":"Configuration Constants","text":"<pre><code># Time constants\nHOURS = 60 * 60  # Makes timeouts more readable\n\n# Model configuration\nBASE_MODEL_NAME = \"unsloth/gemma-3-4b-it\"  # Unsloth's optimized Gemma\nWANDB_PROJECT_DEFAULT = \"GemmaFinetuning\"   # W&amp;B project name\nOUTPUT_DIR_DEFAULT = \"/data/Finetuned_Gemma_3_4b_it\"  # Where to save checkpoints\n</code></pre> <p>These constants make it easy to swap models or change output directories. Want to try Llama instead? Just change <code>BASE_MODEL_NAME</code>.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#building-the-training-image","title":"Building the Training Image","text":"<p>This is where things get interesting. We need a container with CUDA, PyTorch, Unsloth, and a bunch of other stuff.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#why-cuda-devel","title":"Why CUDA \"devel\"?","text":"<pre><code>CUDA_VERSION = \"12.8.1\"     # Latest CUDA version\nCUDA_FLAVOR = \"devel\"        # \"devel\" includes nvcc compiler\nCUDA_OS = \"ubuntu24.04\"      # Ubuntu 24.04 LTS\nCUDA_TAG = f\"{CUDA_VERSION}-{CUDA_FLAVOR}-{CUDA_OS}\"\n</code></pre> <p>Here's the deal: some packages like <code>flash-attn</code> and <code>triton</code> need to compile CUDA code during installation. If you use the <code>runtime</code> image, you'll get cryptic errors about missing <code>nvcc</code>. Trust me, I learned this the hard way.</p> <p>The <code>devel</code> image includes the full CUDA toolkit with the compiler. It's bigger, but it Just Works\u2122.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#complete-image-definition","title":"Complete Image Definition","text":"<pre><code>FINETUNING_GPU_IMAGE = (\n    # Start with NVIDIA's official CUDA image\n    ModalImage.from_registry(f\"nvidia/cuda:{CUDA_TAG}\", add_python=\"3.12\")\n\n    # Install system dependencies\n    # git: for cloning repos if needed\n    # build-essential: gcc, make, etc. for compiling Python extensions\n    .apt_install(\"git\", \"build-essential\")\n\n    # Install PyTorch first (required by most other packages)\n    # Using uv for faster installs (it's like pip but 10-100x faster)\n    .uv_pip_install([\"torch\", \"torchvision\", \"torchaudio\"])\n\n    # Now install the ML ecosystem\n    .uv_pip_install([\n        # === Unsloth core ===\n        \"unsloth\",              # The star of the show - optimized training\n        \"unsloth_zoo\",          # Pre-configured models\n\n        # === Quantization and efficiency ===\n        \"bitsandbytes\",         # 8-bit optimizers, quantization\n        \"accelerate\",           # Multi-GPU support, mixed precision\n        \"xformers\",             # Memory-efficient attention\n        \"peft\",                 # LoRA and other parameter-efficient methods\n        \"trl\",                  # Transformer Reinforcement Learning\n        \"triton\",               # GPU kernel language (used by flash-attn)\n        \"cut_cross_entropy\",    # Optimized loss computation\n\n        # === Transformers ecosystem ===\n        \"transformers\",         # Hugging Face transformers\n        \"timm\",                 # Vision model utilities\n\n        # === Training tools ===\n        \"wandb\",                # Experiment tracking (highly recommend!)\n        \"weave\",                # W&amp;B's LLM eval framework\n        \"deepspeed\",            # For multi-GPU training (optional here)\n\n        # === Evaluation metrics ===\n        \"nltk\",                 # NLP toolkit\n        \"rouge_score\",          # ROUGE metrics\n        \"bert_score\",           # BERTScore\n        \"jiwer\",                # Word/Character Error Rate\n        \"scikit-learn\",         # General ML utilities\n\n        # === Utilities ===\n        \"pillow\",               # Image processing\n        \"opencv-python-headless\",  # More image processing\n        \"gradio\",               # Quick UI demos\n        \"hf_transfer\",          # Faster Hugging Face downloads\n    ])\n\n    # Set environment variables\n    .env({\n        # Enable fast multi-threaded downloads from Hugging Face\n        # This can be 5-10x faster for large models!\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n\n        # Cache everything in the volume (so it persists)\n        # This means we download models once, use them forever\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n</code></pre> <p>Key points:</p> <ol> <li> <p>uv_pip_install: Modal uses <code>uv</code> under the hood, which is stupid fast. Installing 20+ packages takes like 2 minutes instead of 10.</p> </li> <li> <p>HF_HUB_ENABLE_HF_TRANSFER: This enables Hugging Face's <code>hf_transfer</code> library which downloads models in parallel. For a 16GB model, this can cut download time from 10 minutes to 2 minutes.</p> </li> <li> <p>HF_HOME in volume: By setting this to <code>/data/.cache</code>, all Hugging Face downloads get cached in our volume. Download a model once, use it in all future runs.</p> </li> </ol> <p>\u23f0 Build time warning: The first time you run this, Modal will build the image. It takes 10-15 minutes because of all the compilation (flash-attn especially). Grab a coffee. But here's the magic - Modal caches the image. Every subsequent run? Instant.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-1-downloading-datasets","title":"Stage 1: Downloading Datasets","text":"<p>Let's start with data. We're using Unsloth's LaTeX OCR dataset - images of math equations paired with their LaTeX code.</p> <pre><code>@app.function(\n    image=FINETUNING_GPU_IMAGE,     # Our big image with all dependencies\n    volumes=VOLUME_CONFIG,           # Mount /data volume\n    secrets=[huggingface_secret],   # Inject HF token\n    timeout=24 * HOURS,              # Give it up to 24 hours (large datasets)\n    # Notice: No GPU! This runs on CPU to save money\n)\ndef download_datasets(\n    dataset_name: str = \"unsloth/LaTeX_OCR\",  # HuggingFace dataset ID\n    split: str = \"train\",                      # Which split to download\n    cache_dir: str = \"/data/.cache\",           # Where to cache it\n):\n    \"\"\"\n    Download and cache a dataset from Hugging Face.\n\n    Runs on CPU (no GPU wasted on downloading files).\n    Dataset gets cached in the volume, so we only download once.\n    \"\"\"\n    from datasets import load_dataset\n    import os\n\n    # Set HF token from our secret\n    # Modal injects HUGGINGFACE_TOKEN from the secret we passed in\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Make sure cache directory exists\n    os.makedirs(cache_dir, exist_ok=True)\n\n    print(f\"Downloading {dataset_name} ({split} split)...\")\n    print(f\"Cache dir: {cache_dir}\")\n\n    # Download the dataset\n    # cache_dir tells it to save in our volume (persists across runs)\n    dataset = load_dataset(dataset_name, split=split, cache_dir=cache_dir)\n\n    # Print some info\n    print(\"\\n\u2713 Dataset loaded successfully!\")\n    print(f\"  - Name: {dataset_name}\")\n    print(f\"  - Split: {split}\")\n    print(f\"  - Number of samples: {len(dataset)}\")\n    print(f\"  - Features: {dataset.features}\")\n\n    # CRITICAL: Commit changes to the volume\n    # This persists the downloaded data\n    exp_volume.commit()\n\n    # Return metadata\n    return {\n        \"status\": \"completed\",\n        \"dataset_name\": dataset_name,\n        \"num_samples\": len(dataset),\n    }\n</code></pre> <p>Why download separately?</p> <p>You might be thinking \"why not just download during training?\" Here's why this is better:</p> <ol> <li>No GPU waste: Downloading files doesn't need a GPU. Why pay $3.50/hr for an A100 when a CPU costs pennies?</li> <li>Faster iteration: Download once, train many times with different hyperparameters</li> <li>Debugging: If download fails, you know immediately. Not after 10 minutes of training setup.</li> </ol> <p>Running it:</p> <pre><code># Download the default dataset (LaTeX OCR)\nmodal run FinetuneGemmaUnslothModal.py::download_datasets\n\n# Or download a custom dataset\nmodal run FinetuneGemmaUnslothModal.py::download_datasets \\\n  --dataset-name=\"your-username/your-dataset\" \\\n  --split=\"train\"\n</code></pre> <p>The first time you run this, it downloads and caches the dataset. Second time? Instant, because it's already in the volume.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-2-downloading-models","title":"Stage 2: Downloading Models","text":"<p>Same idea as datasets - download once, use forever.</p> <pre><code>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    gpu=\"l40s:1\",                   # Use a cheap GPU (L40S is ~$1/hr)\n    volumes=VOLUME_CONFIG,           # Mount our volume\n    secrets=[huggingface_secret],   # Need HF token for model access\n    timeout=24 * HOURS,\n)\ndef download_models(\n    model_name: str = BASE_MODEL_NAME,      # \"unsloth/gemma-3-4b-it\"\n    cache_dir: str = \"/data/.cache\",        # Cache in volume\n):\n    \"\"\"\n    Download and cache the base model using Unsloth's FastVisionModel.\n\n    Why L40S GPU? Some models need a GPU just to load (for safety checks, etc.)\n    L40S is cheaper than A100, perfect for this one-time download.\n    \"\"\"\n    from unsloth import FastVisionModel\n    import os\n    import torch\n\n    # Set HF token\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    print(f\"Downloading model: {model_name}\")\n    print(f\"Cache dir: {cache_dir}\")\n\n    # Load the model with Unsloth's optimized loader\n    # This downloads and caches the model weights\n    model, processor = FastVisionModel.from_pretrained(\n        model_name,\n        load_in_4bit=False,                    # Full precision for now\n        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n        max_seq_length=8000,                    # Max context length\n        dtype=torch.bfloat16,                   # Use bfloat16 (good balance)\n    )\n\n    print(f\"\\n\u2713 Model downloaded and cached!\")\n    print(f\"  - Model: {model_name}\")\n    print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"  - Cache: {cache_dir}\")\n\n    # Commit to volume\n    exp_volume.commit()\n\n    return {\n        \"status\": \"completed\",\n        \"model_name\": model_name,\n        \"cache_dir\": cache_dir,\n    }\n</code></pre> <p>Why use a GPU for downloading?</p> <p>Some models (especially gated ones like Gemma) run initialization code that requires a GPU. It's annoying, but that's how it is. We use an L40S because it's cheap (~$1/hr) and we only do this once.</p> <p>Run it:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::download_models\n</code></pre> <p>First run downloads ~16GB (takes a few minutes with <code>hf_transfer</code>). Every subsequent run? Instant.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-3-fine-tuning-with-lora","title":"Stage 3: Fine-tuning with LoRA","text":"<p>Alright, here's where the magic happens. We're going to fine-tune Gemma 3-4B to read LaTeX equations from images.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#gpu-configuration","title":"GPU Configuration","text":"<pre><code>TRAIN_GPU = \"a100-80gb\"    # For 4B vision models, A100-80GB is ideal\nNUM_GPUS = 1                # Unsloth is optimized for single-GPU\nTRAINING_GPU_CONFIG = f\"{TRAIN_GPU}:{NUM_GPUS}\"\n</code></pre> <p>Why A100-80GB? - Vision-language models are memory-hungry (images take a lot of VRAM) - 4B model + images + gradients = needs ~40-60GB - A100-40GB might OOM, A100-80GB is comfortable</p> <p>Why single GPU? - Unsloth is insanely optimized for single-GPU training - Multi-GPU adds communication overhead - For most fine-tuning, single A100 is faster than 2-4 smaller GPUs</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#the-training-function","title":"The Training Function","text":"<p>This is a big one, so I'll break it into pieces:</p> <pre><code>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret, Secret.from_dotenv()],  # Both Modal secrets and .env\n    gpu=TRAINING_GPU_CONFIG,                              # \"a100-80gb:1\"\n    timeout=24 * HOURS,                                   # Long timeout for big datasets\n)\ndef fine_tune_unsloth(\n    # Model and dataset config\n    model_path: str = BASE_MODEL_NAME,                 # Which model to fine-tune\n    dataset_name: str = \"unsloth/LaTeX_OCR\",          # Which dataset to use\n    dataset_split: str = \"train\",                      # Which split\n    output_dir: str = OUTPUT_DIR_DEFAULT,              # Where to save checkpoints\n    hub_id: str = None,                                # Push to HF Hub? (optional)\n    max_samples: int = None,                           # Limit dataset (for testing)\n\n    # LoRA hyperparameters\n    lora_r: int = 32,                                  # LoRA rank (higher = more capacity)\n    lora_alpha: int = 64,                              # LoRA scaling (usually 2x rank)\n    lora_dropout: float = 0.0,                         # Dropout in LoRA layers\n\n    # Training hyperparameters\n    per_device_train_batch_size: int = 4,              # Batch size per GPU\n    gradient_accumulation_steps: int = 4,              # Effective batch = 4 * 4 = 16\n    num_train_epochs: int = 1,                         # How many epochs\n    learning_rate: float = 3e-4,                       # Learning rate\n    warmup_ratio: float = 0.2,                         # Warmup 20% of steps\n    max_seq_length: int = 8000,                        # Max tokens per sample\n\n    # Checkpointing\n    save_strategy: str = \"steps\",                      # Save by steps (not epochs)\n    save_steps: int = 250,                             # Save every 250 steps\n    save_total_limit: int = 20,                        # Keep only 20 checkpoints\n    logging_steps: int = 10,                           # Log every 10 steps\n\n    # Weights &amp; Biases\n    wandb_project: str = WANDB_PROJECT_DEFAULT,        # W&amp;B project name\n    wandb_run_name: str = None,                        # W&amp;B run name (auto-generated)\n):\n    \"\"\"\n    Fine-tune Gemma 3-4B vision model with LoRA using Unsloth.\n\n    This is based on Unsloth's Colab notebook but productionized for Modal.\n    \"\"\"\n    from unsloth import FastVisionModel, get_chat_template\n    from unsloth.trainer import UnslothVisionDataCollator\n    from datasets import load_dataset\n    from trl import SFTTrainer, SFTConfig\n    import torch\n    import os\n    from datetime import datetime\n</code></pre> <p>Let me continue with the rest of the training function with detailed comments:</p> <pre><code>    # === Environment Setup ===\n    print(\"=\" * 80)\n    print(\"SETTING UP TRAINING ENVIRONMENT\")\n    print(\"=\" * 80)\n\n    # Set up authentication tokens\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n    os.environ[\"WANDB_API_KEY\"] = os.environ[\"WANDB_API_KEY\"]\n    os.environ[\"WANDB_PROJECT\"] = wandb_project\n\n    # Auto-generate W&amp;B run name if not provided\n    # Format: finetune_gemma-3-4b-it_20250110_143022\n    if wandb_run_name is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        model_short = model_path.split(\"/\")[-1]  # Extract \"gemma-3-4b-it\" from path\n        wandb_run_name = f\"finetune_{model_short}_{timestamp}\"\n\n    os.environ[\"WANDB_RUN_NAME\"] = wandb_run_name\n\n    # Memory optimization: only use GPU 0\n    # (In single-GPU setup, this prevents memory fragmentation)\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n    # Disable torch compile (can cause issues with some models)\n    torch._dynamo.config.disable = True\n\n    print(f\"Model: {model_path}\")\n    print(f\"Dataset: {dataset_name} ({dataset_split})\")\n    print(f\"Output: {output_dir}\")\n    print(f\"W&amp;B: {wandb_project}/{wandb_run_name}\")\n    print(\"\")\n\n    # === Load Model with LoRA ===\n    print(\"=\" * 80)\n    print(\"LOADING MODEL AND ADDING LORA ADAPTERS\")\n    print(\"=\" * 80)\n\n    # Load base model\n    # Unsloth's FastVisionModel is a drop-in replacement for HF's model\n    # but with optimized kernels and memory usage\n    model, processor = FastVisionModel.from_pretrained(\n        model_path,\n        load_in_4bit=False,                    # Use full precision (more accurate)\n        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n        max_seq_length=max_seq_length,         # Context window\n        dtype=torch.bfloat16,                   # bfloat16 is great for training\n    )\n\n    # Add LoRA adapters\n    # LoRA (Low-Rank Adaptation) trains small adapter layers instead of the full model\n    # This is WAY more efficient - we only train ~1% of parameters!\n    model = FastVisionModel.get_peft_model(\n        model,\n        finetune_vision_layers=False,      # Keep vision encoder frozen\n        finetune_language_layers=True,     # Train the language model part\n        finetune_attention_modules=True,   # Add LoRA to attention\n        finetune_mlp_modules=True,         # Add LoRA to MLPs\n\n        # LoRA config\n        r=lora_r,                          # Rank (32 is a good default)\n        lora_alpha=lora_alpha,             # Scaling (usually 2x rank)\n        lora_dropout=lora_dropout,         # Dropout (0.0 often works fine)\n        bias=\"none\",                       # Don't train bias terms\n        random_state=3407,                 # For reproducibility\n        target_modules=\"all-linear\",       # Apply to all linear layers\n        modules_to_save=[\"lm_head\", \"embed_tokens\"],  # Also train these\n    )\n\n    # Set up chat template for the model\n    # This formats inputs/outputs correctly for Gemma\n    processor = get_chat_template(processor, \"gemma-3\")\n\n    print(f\"\u2713 Model loaded with LoRA adapters\")\n    print(f\"  - Base model: {model_path}\")\n    print(f\"  - LoRA rank: {lora_r}\")\n    print(f\"  - Trainable params: ~1-2% of total\")\n    print(\"\")\n\n    # === Load and Prepare Dataset ===\n    print(\"=\" * 80)\n    print(\"LOADING DATASET\")\n    print(\"=\" * 80)\n\n    # Load dataset from cache (downloaded in Stage 1)\n    dataset = load_dataset(dataset_name, split=dataset_split)\n\n    # Limit dataset size if specified (useful for testing)\n    if max_samples is not None and max_samples &gt; 0:\n        dataset = dataset.select(range(min(max_samples, len(dataset))))\n        print(f\"\u26a0\ufe0f  Limited to {len(dataset)} samples for testing\")\n\n    print(f\"\u2713 Dataset loaded: {len(dataset)} samples\")\n    print(\"\")\n\n    # === Format Dataset ===\n    # Convert dataset to chat format that Gemma expects\n    # Each sample has an image and corresponding LaTeX code\n\n    instruction = \"Write the LaTeX representation for this image.\"\n\n    def convert_to_conversation(sample):\n        \"\"\"\n        Convert a dataset sample to chat format.\n\n        Input sample has:\n          - \"image\": PIL Image of equation\n          - \"text\": LaTeX code for that equation\n\n        Output format:\n          - User message: instruction + image\n          - Assistant message: LaTeX code\n        \"\"\"\n        conversation = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": instruction},\n                    {\"type\": \"image\", \"image\": sample[\"image\"]},\n                ],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}],\n            },\n        ]\n        return {\"messages\": conversation}\n\n    # Convert all samples\n    print(\"Converting dataset to chat format...\")\n    converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n    print(f\"\u2713 Converted {len(converted_dataset)} samples\")\n    print(\"\")\n\n    # === Training Setup ===\n    print(\"=\" * 80)\n    print(\"STARTING TRAINING\")\n    print(\"=\" * 80)\n\n    # Enable training mode (sets up gradient computation)\n    FastVisionModel.for_training(model)\n\n    # Create trainer\n    # SFTTrainer is from TRL library - supervised fine-tuning trainer\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=converted_dataset,\n        processing_class=processor.tokenizer,\n\n        # Data collator handles batching images + text\n        # Unsloth's collator is optimized for vision-language models\n        data_collator=UnslothVisionDataCollator(\n            model=model,\n            processor=processor\n        ),\n\n        # Training arguments\n        args=SFTConfig(\n            # === Batch size config ===\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            # Effective batch size = 4 * 4 = 16\n\n            # === Learning rate schedule ===\n            warmup_ratio=warmup_ratio,         # Warm up for 20% of training\n            num_train_epochs=num_train_epochs,\n            learning_rate=learning_rate,\n            lr_scheduler_type=\"linear\",        # Linear decay after warmup\n\n            # === Logging ===\n            logging_steps=logging_steps,       # Log every 10 steps\n            report_to=\"wandb\",                 # Log to W&amp;B\n\n            # === Checkpointing ===\n            save_strategy=save_strategy,       # Save by steps\n            save_steps=save_steps,             # Every 250 steps\n            save_total_limit=save_total_limit, # Keep only 20 checkpoints\n            output_dir=output_dir,             # Where to save\n\n            # === Optimization ===\n            gradient_checkpointing=True,       # Trade compute for memory\n            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n            max_grad_norm=0.3,                 # Gradient clipping\n            optim=\"adamw_torch_fused\",         # Fastest AdamW implementation\n            weight_decay=0.01,                 # L2 regularization\n\n            # === Precision ===\n            bf16=True,                         # Use bfloat16 (faster + stable)\n            tf32=False,                        # Don't need TF32\n\n            # === Vision-specific settings ===\n            remove_unused_columns=False,       # Keep all columns (need images!)\n            dataset_text_field=\"\",             # We handle formatting ourselves\n            dataset_kwargs={\"skip_prepare_dataset\": True},\n            max_length=max_seq_length,\n        ),\n    )\n\n    print(f\"Training config:\")\n    print(f\"  - Effective batch size: {per_device_train_batch_size * gradient_accumulation_steps}\")\n    print(f\"  - Learning rate: {learning_rate}\")\n    print(f\"  - Epochs: {num_train_epochs}\")\n    print(f\"  - Total steps: ~{len(converted_dataset) // (per_device_train_batch_size * gradient_accumulation_steps) * num_train_epochs}\")\n    print(\"\")\n\n    # === TRAIN! ===\n    print(\"\ud83d\ude80 Starting training...\")\n    print(\"=\" * 80)\n    trainer_stats = trainer.train()\n    print(\"=\" * 80)\n    print(\"\u2713 Training completed!\")\n    print(\"\")\n\n    # === Save Model ===\n    print(\"=\" * 80)\n    print(\"SAVING MODEL\")\n    print(\"=\" * 80)\n\n    # Create output directories\n    final_weights_dir = os.path.join(output_dir, \"final_weights\")  # Merged model\n    final_lora_dir = os.path.join(output_dir, \"final_lora\")        # LoRA adapters only\n\n    os.makedirs(final_weights_dir, exist_ok=True)\n    os.makedirs(final_lora_dir, exist_ok=True)\n\n    # Save LoRA adapters (small, ~100MB)\n    print(\"Saving LoRA adapters...\")\n    model.save_pretrained(final_lora_dir)\n    processor.save_pretrained(final_lora_dir)\n    print(f\"  \u2713 LoRA adapters saved to {final_lora_dir}\")\n\n    # Optionally push LoRA to Hugging Face Hub\n    if hub_id:\n        print(f\"Pushing LoRA to Hub: {hub_id}_lora\")\n        model.push_to_hub(\n            f\"{hub_id}_lora\",\n            token=os.environ[\"HUGGINGFACE_TOKEN\"]\n        )\n        processor.push_to_hub(\n            f\"{hub_id}_lora\",\n            token=os.environ[\"HUGGINGFACE_TOKEN\"]\n        )\n        print(f\"  \u2713 Pushed to {hub_id}_lora\")\n\n    # Save merged model (base + LoRA combined, ready to deploy)\n    print(\"Saving merged model (this takes a few minutes)...\")\n    model.save_pretrained_merged(\n        final_weights_dir,\n        processor,\n        save_method=\"merged_16bit\"  # Save in 16-bit precision\n    )\n    print(f\"  \u2713 Merged model saved to {final_weights_dir}\")\n\n    # Optionally push merged model to Hub\n    if hub_id:\n        print(f\"Pushing merged model to Hub: {hub_id}\")\n        model.push_to_hub_merged(\n            hub_id,\n            processor,\n            token=os.environ[\"HUGGINGFACE_TOKEN\"],\n            save_method=\"merged_16bit\"\n        )\n        print(f\"  \u2713 Pushed to {hub_id}\")\n\n    # CRITICAL: Commit everything to the volume\n    # This persists checkpoints, final models, everything\n    print(\"\\nCommitting to volume...\")\n    exp_volume.commit()\n    print(\"\u2713 Volume committed\")\n\n    print(\"\")\n    print(\"=\" * 80)\n    print(\"\ud83c\udf89 FINE-TUNING COMPLETE!\")\n    print(\"=\" * 80)\n    print(f\"LoRA adapters: {final_lora_dir}\")\n    print(f\"Merged model: {final_weights_dir}\")\n    if hub_id:\n        print(f\"Hugging Face: {hub_id} and {hub_id}_lora\")\n    print(\"\")\n\n    return {\n        \"status\": \"completed\",\n        \"output_dir\": output_dir,\n        \"lora_dir\": final_lora_dir,\n        \"merged_dir\": final_weights_dir,\n        \"hub_id\": hub_id,\n    }\n</code></pre> <p>Phew! That's a lot of code, but it's all there for a reason. Let me highlight the key points:</p> <p>LoRA Strategy: - We freeze the vision encoder (it's already good at seeing images) - We only train LoRA adapters on the language model - This trains ~1-2% of parameters instead of 100% - Massively faster and more memory efficient</p> <p>Batch Size Math: </p><pre><code>Effective batch size = per_device_batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus\n                    = 4 \u00d7 4 \u00d7 1\n                    = 16\n</code></pre> <p>Two Save Formats: 1. LoRA adapters (~100MB): Just the trained adapters. Requires base model to use. 2. Merged model (full size): Base model + adapters combined. Ready to deploy.</p> <p>For serving, we use the merged model. For sharing or storage, LoRA adapters are more efficient.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#running-training","title":"Running Training","text":"<p>Basic run (test on small subset):</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --max-samples=100 \\\n  --num-train-epochs=1\n</code></pre> <p>This trains on 100 samples for 1 epoch - great for making sure everything works.</p> <p>Full training run:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --num-train-epochs=3 \\\n  --learning-rate=0.0003\n</code></pre> <p>Train and push to Hugging Face:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --hub-id=\"your-username/gemma-latex-ocr\" \\\n  --num-train-epochs=3\n</code></pre> <p>This pushes both the LoRA adapters and merged model to your HF account.</p> <p>Custom hyperparameters:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --lora-r=64 \\\n  --lora-alpha=128 \\\n  --per-device-train-batch-size=2 \\\n  --gradient-accumulation-steps=8\n</code></pre> <p>While training runs, you'll see logs streaming in real-time. And if you set up W&amp;B, check <code>wandb.ai/&lt;your-username&gt;/GemmaFinetuning</code> to see beautiful charts of loss curves, learning rate schedules, GPU utilization, everything.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-4-export-and-merge-model-optional","title":"Stage 4: Export and Merge Model (Optional)","text":"<p>Okay, so after training, you have LoRA adapters saved. The training function already saves both LoRA adapters AND the merged model. But let's say you only saved LoRA adapters (to save space), and now you want to create a standalone merged model. That's what this stage is for.</p> <pre><code>@app.function(\n    image=FINETUNING_GPU_IMAGE,\n    volumes=VOLUME_CONFIG,\n    gpu=TRAINING_GPU_CONFIG,              # Need same GPU as training\n    secrets=[huggingface_secret, Secret.from_dotenv()],\n    timeout=2 * HOURS,                    # Merging takes ~10-30 minutes\n)\ndef export_model(\n    lora_model_path: str = f\"{OUTPUT_DIR_DEFAULT}\",  # Where LoRA adapters are\n    output_path: str = None,                          # Where to save merged model\n    hub_model_id: str = None,                         # Optional: push to HF Hub\n    push_to_hub: bool = True,                         # Whether to push\n):\n    \"\"\"\n    Export LoRA adapters and merge them with base model.\n\n    Why? Two reasons:\n    1. Merged models are easier to deploy (no need to load base + adapters separately)\n    2. Merged models can be quantized for faster inference\n    \"\"\"\n    from unsloth import FastVisionModel\n    import os\n\n    # Set HF token for pushing to Hub\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    print(\"=\" * 80)\n    print(\"LOADING LORA MODEL AND MERGING\")\n    print(\"=\" * 80)\n    print(f\"LoRA path: {lora_model_path}\")\n\n    # Load the LoRA model\n    # This loads base model + LoRA adapters\n    model, processor = FastVisionModel.from_pretrained(\n        lora_model_path,           # Path to LoRA adapters\n        load_in_4bit=False,        # Load in full precision\n    )\n\n    # Prepare for inference\n    # This merges the LoRA weights into the base model\n    FastVisionModel.for_inference(model)\n\n    print(\"\u2713 Model loaded and LoRA weights merged\")\n    print(\"\")\n\n    # === Save or Push ===\n    if push_to_hub and hub_model_id:\n        # Push merged model to Hugging Face Hub\n        print(f\"Pushing merged model to Hub: {hub_model_id}\")\n        model.push_to_hub_merged(\n            hub_model_id,\n            processor,\n            token=os.environ[\"HUGGINGFACE_TOKEN\"],\n            save_method=\"merged_16bit\",  # Save in 16-bit (good balance)\n        )\n        print(f\"\u2713 Pushed to https://huggingface.co/{hub_model_id}\")\n    else:\n        # Save locally to volume\n        if output_path is None:\n            output_path = f\"{lora_model_path}_merged\"\n\n        print(f\"Saving merged model to: {output_path}\")\n        model.save_pretrained_merged(\n            output_path,\n            processor,\n            save_method=\"merged_16bit\"\n        )\n        print(f\"\u2713 Saved to {output_path}\")\n\n    # Commit to volume\n    exp_volume.commit()\n\n    print(\"\")\n    print(\"=\" * 80)\n    print(\"\u2713 EXPORT COMPLETE!\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"lora_path\": lora_model_path,\n        \"merged_path\": output_path if not push_to_hub else hub_model_id,\n    }\n</code></pre> <p>When to use this: - You only saved LoRA adapters during training (to save disk space) - You want to create a standalone model for deployment - You want to push to HuggingFace Hub after training</p> <p>Run it:</p> <pre><code># Export and save to volume\nmodal run FinetuneGemmaUnslothModal.py::export_model \\\n  --lora-model-path=\"/data/Finetuned_Gemma_3_4b_it/final_lora\"\n\n# Export and push to HuggingFace\nmodal run FinetuneGemmaUnslothModal.py::export_model \\\n  --lora-model-path=\"/data/Finetuned_Gemma_3_4b_it/final_lora\" \\\n  --hub-model-id=\"your-username/gemma-latex-merged\" \\\n  --push-to-hub=True\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-5-serving-with-vllm","title":"Stage 5: Serving with vLLM","text":"<p>Alright, now let's deploy our model for real-time inference. We're using vLLM, which is basically the industry standard for serving LLMs at scale.</p> <p>Why vLLM? - Fast: Optimized attention kernels, continuous batching - Scalable: Handles thousands of requests per second - Compatible: OpenAI-compatible API (drop-in replacement) - Auto-scaling: Modal handles spinning up/down instances based on load</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#vllm-image-separate-from-training","title":"vLLM Image (Separate from Training)","text":"<p>We use a different image for serving because vLLM has different dependencies than training.</p> <pre><code>VLLM_CUDA_VERSION = \"12.8.1\"\nVLLM_CUDA_TAG = f\"{VLLM_CUDA_VERSION}-devel-ubuntu24.04\"\n\nVLLM_GPU_IMAGE = (\n    # Start with CUDA base\n    ModalImage.from_registry(f\"nvidia/cuda:{VLLM_CUDA_TAG}\", add_python=\"3.12\")\n\n    # Install system dependencies for vLLM\n    .apt_install(\"libopenmpi-dev\", \"libnuma-dev\")  # For distributed inference\n\n    # Upgrade pip and install uv\n    .run_commands(\"pip install --upgrade pip\")\n    .run_commands(\"pip install uv\")\n\n    # Install vLLM (latest version)\n    .run_commands(\"uv pip install vllm -U --system\")\n\n    # Install supporting packages\n    .pip_install(\n        \"datasets\",                       # For eval/testing\n        \"pillow\",                         # Image handling\n        \"huggingface_hub[hf_transfer]\",  # Fast model downloads\n        \"requests\",                       # HTTP requests\n        \"numpy\",                          # Numerical ops\n    )\n\n    # Install flash-attention (required for vLLM)\n    # Must be installed separately with --no-build-isolation\n    .run_commands(\n        \"uv pip install 'flash-attn&gt;=2.7.1,&lt;=2.8.0' --no-build-isolation --system\"\n    )\n\n    # Enable fast HF downloads\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n)\n</code></pre> <p>Why separate image? - vLLM and training have overlapping dependencies that can conflict - vLLM image is lighter (no training frameworks) - Faster to build and deploy</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#serving-configuration","title":"Serving Configuration","text":"<pre><code># Which model to serve (path on volume)\nDEFAULT_SERVE_MODEL = \"/data/Finetuned_Gemma_3_4b_it/final_weights\"\n\n# GPU for serving (can be different from training!)\nSERVE_GPU = \"L40S\"        # L40S is great for inference (~$1/hr)\nSERVE_NUM_GPUS = 1\nSERVE_GPU_CONFIG = f\"{SERVE_GPU}:{SERVE_NUM_GPUS}\"\n\nVLLM_PORT = 8000          # Internal port\n</code></pre> <p>GPU choice for serving: - L40S: Best price/performance for inference (\\(1/hr) - **A100-40GB**: If you need higher throughput (\\)2.50/hr) - A100-80GB: For very large models or high batch sizes ($3.50/hr)</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#the-serve-function","title":"The Serve Function","text":"<pre><code>@app.function(\n    image=VLLM_GPU_IMAGE,\n    gpu=SERVE_GPU_CONFIG,                # L40S for serving\n    scaledown_window=3 * 60,             # Scale to 0 after 3 min idle (saves $$$)\n    secrets=[huggingface_secret],        # Need HF token\n    volumes=VOLUME_CONFIG,                # Mount our volume (has the model)\n    max_containers=2,                     # Auto-scale up to 2 instances\n    timeout=24 * HOURS,\n)\n@modal.concurrent(max_inputs=50)         # Handle 50 concurrent requests per instance\n@modal.web_server(port=8000, startup_timeout=5 * 60)  # Expose as web server\ndef serve_vllm():\n    \"\"\"\n    Serve the fine-tuned model using vLLM.\n\n    This creates an OpenAI-compatible API endpoint that:\n    - Auto-scales from 0 to max_containers based on load\n    - Shuts down after 3 minutes of inactivity (cost optimization!)\n    - Handles up to 50 concurrent requests per container\n    \"\"\"\n    import subprocess\n    import os\n\n    # Set HF token (might need to download model files)\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    print(\"=\" * 80)\n    print(\"STARTING VLLM SERVER\")\n    print(\"=\" * 80)\n    print(f\"Model: {DEFAULT_SERVE_MODEL}\")\n    print(f\"Port: {VLLM_PORT}\")\n    print(f\"GPU: {SERVE_GPU_CONFIG}\")\n    print(\"\")\n\n    # Build vLLM command\n    cmd = [\n        \"vllm\", \"serve\",                           # vLLM serve command\n        \"--uvicorn-log-level=info\",                # Logging level\n        DEFAULT_SERVE_MODEL,                       # Path to model\n        \"--host\", \"0.0.0.0\",                       # Listen on all interfaces\n        \"--port\", str(VLLM_PORT),                  # Port to serve on\n        \"--enforce-eager\",                         # Faster startup (skip torch.compile)\n        \"--tensor-parallel-size\", str(SERVE_NUM_GPUS),  # How many GPUs to use\n        \"--gpu-memory-utilization\", \"0.4\",         # Use 40% of GPU memory (be conservative)\n        \"--trust-remote-code\",                     # Allow custom model code\n    ]\n\n    print(f\"Command: {' '.join(cmd)}\")\n    print(\"\")\n    print(\"\ud83d\ude80 Starting vLLM server...\")\n    print(\"=\" * 80)\n\n    # Start vLLM in background\n    # Popen returns immediately, server keeps running\n    subprocess.Popen(\" \".join(cmd), shell=True)\n</code></pre> <p>Key configuration options:</p> <ol> <li> <p><code>scaledown_window=3*60</code>: This is HUGE for cost savings. If there are no requests for 3 minutes, Modal shuts down the container. You pay $0 when idle!</p> </li> <li> <p><code>max_containers=2</code>: Modal will automatically spin up a second instance if the first one gets too many requests. Load balancing happens automatically.</p> </li> <li> <p><code>@modal.concurrent(max_inputs=50)</code>: Each instance can handle 50 concurrent requests. If you get more than 50, Modal queues them or spins up instance #2.</p> </li> <li> <p><code>gpu-memory-utilization=0.4</code>: Use only 40% of GPU memory. vLLM is memory-efficient, and this leaves headroom for request spikes.</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#deploying-the-server","title":"Deploying the Server","text":"<p>To deploy and keep it running:</p> <pre><code>modal deploy FinetuneGemmaUnslothModal.py\n</code></pre> <p>This creates a persistent deployment that stays alive (but auto-scales to 0 when idle).</p> <p>Get the URL:</p> <p>After deploying, Modal prints the URL. Or find it with:</p> <pre><code>modal app list\n</code></pre> <p>You'll get something like: <code>https://your-username--finetuned-gemma-3-4b-it-serve-vllm.modal.run</code></p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#using-the-api","title":"Using the API","text":"<p>The server exposes an OpenAI-compatible API. Here's how to use it:</p> <pre><code>from openai import OpenAI\nimport base64\n\n# Create client pointing to your Modal endpoint\nclient = OpenAI(\n    base_url=\"https://your-endpoint.modal.run/v1\",  # Your Modal URL + /v1\n    api_key=\"EMPTY\"  # Modal doesn't require API key (it's behind Modal auth)\n)\n\n# Encode image to base64\nwith open(\"equation.jpg\", \"rb\") as f:\n    image_b64 = base64.b64encode(f.read()).decode()\n\n# Make request (just like OpenAI!)\nresponse = client.chat.completions.create(\n    model=\"/data/Finetuned_Gemma_3_4b_it/final_weights\",  # Model path\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                # Send image as base64\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"}\n                },\n                # Send text prompt\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Write the LaTeX representation for this image.\"\n                },\n            ],\n        },\n    ],\n    temperature=0.1,      # Low temp for deterministic output\n    max_tokens=512,       # Max length of response\n)\n\n# Print the LaTeX code\nprint(response.choices[0].message.content)\n</code></pre> <p>Example output: </p><pre><code>\\frac{d}{dx} \\left( x^2 + 2x + 1 \\right) = 2x + 2\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#testing-the-deployment","title":"Testing the Deployment","text":"<p>Quick test script:</p> <pre><code>import requests\nimport base64\n\n# Your Modal endpoint\nurl = \"https://your-endpoint.modal.run/v1/chat/completions\"\n\n# Load and encode image\nwith open(\"test_equation.jpg\", \"rb\") as f:\n    img_b64 = base64.b64encode(f.read()).decode()\n\n# Make request\nresponse = requests.post(\n    url,\n    json={\n        \"model\": \"/data/Finetuned_Gemma_3_4b_it/final_weights\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}},\n                    {\"type\": \"text\", \"text\": \"Write the LaTeX representation for this image.\"}\n                ]\n            }\n        ],\n        \"temperature\": 0.1,\n        \"max_tokens\": 512\n    }\n)\n\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <p>Pro tip: The first request after the server scales from 0 will take 30-60 seconds (model loading). Subsequent requests are instant.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#stage-6-evaluation","title":"Stage 6: Evaluation","text":"<p>Alright, let's measure how good our model actually is. We'll use real metrics: exact match accuracy, character error rate, and word error rate.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#evaluation-image-lightweight-cpu-only","title":"Evaluation Image (Lightweight, CPU-only)","text":"<pre><code>EVAL_IMAGE = (\n    # Lightweight Debian base (no CUDA needed for eval)\n    ModalImage.debian_slim(python_version=\"3.12\")\n\n    # Install evaluation dependencies\n    .pip_install(\n        \"openai\",                         # To call our vLLM endpoint\n        \"datasets\",                       # Load test dataset\n        \"pillow\",                         # Image processing\n        \"numpy\",                          # Numerical ops\n        \"jiwer\",                          # Word/Character Error Rate metrics\n        \"nltk\",                           # NLP utilities\n        \"tqdm\",                           # Progress bars\n        \"huggingface_hub[hf_transfer]\",  # Fast dataset downloads\n    )\n\n    # Enable fast downloads\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n)\n</code></pre> <p>Why CPU for evaluation? - Evaluation just calls our API endpoint (which has the GPU) - Processing responses doesn't need GPU - Saves money!</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#the-evaluation-function","title":"The Evaluation Function","text":"<pre><code>@app.function(\n    image=EVAL_IMAGE,                # Lightweight CPU image\n    volumes=VOLUME_CONFIG,            # Access cached datasets\n    secrets=[huggingface_secret],    # HF token for datasets\n    timeout=2 * HOURS,                # Eval can take a while\n    # No GPU! Runs on CPU\n)\ndef evaluate_model(\n    endpoint_url: str = None,                                  # vLLM endpoint (auto-detected)\n    model_name: str = \"/data/Finetuned_Gemma_3_4b_it/final_weights\",\n    dataset_name: str = \"unsloth/LaTeX_OCR\",                  # Test dataset\n    dataset_split: str = \"test\",                              # Use test split\n    max_samples: int = 100,                                    # How many to evaluate\n    max_parallel_requests: int = 8,                           # Concurrent requests\n    temperature: float = 0.1,                                 # Low temp for consistency\n    max_tokens: int = 512,                                    # Max response length\n):\n    \"\"\"\n    Evaluate the fine-tuned model on LaTeX OCR test set.\n\n    Metrics:\n    - Exact Match Accuracy: % of perfect predictions\n    - Character Error Rate (CER): Edit distance at character level\n    - Word Error Rate (WER): Edit distance at word level\n    \"\"\"\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from openai import OpenAI\n    from datasets import load_dataset\n    from jiwer import wer, cer\n    from tqdm import tqdm\n    import base64\n    from io import BytesIO\n    import os\n\n    print(\"=\" * 80)\n    print(\"EVALUATING MODEL\")\n    print(\"=\" * 80)\n\n    # === Get endpoint URL ===\n    if endpoint_url is None:\n        # Auto-retrieve the vLLM endpoint URL\n        print(\"Auto-detecting vLLM endpoint...\")\n        endpoint_url = serve_vllm.get_web_url().rstrip(\"/\") + \"/v1\"\n\n    print(f\"Endpoint: {endpoint_url}\")\n    print(f\"Model: {model_name}\")\n    print(f\"Dataset: {dataset_name} ({dataset_split})\")\n    print(f\"Max samples: {max_samples}\")\n    print(\"\")\n\n    # === Load test dataset ===\n    print(\"Loading test dataset...\")\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    dataset = load_dataset(dataset_name, split=dataset_split)\n\n    # Limit to max_samples\n    if max_samples and max_samples &lt; len(dataset):\n        dataset = dataset.select(range(max_samples))\n\n    print(f\"\u2713 Loaded {len(dataset)} samples\")\n    print(\"\")\n\n    # === Set up OpenAI client ===\n    client = OpenAI(\n        base_url=endpoint_url,\n        api_key=\"EMPTY\"  # Modal doesn't require API key\n    )\n\n    # === Helper function to encode images ===\n    def encode_image_to_base64(image):\n        \"\"\"Convert PIL Image to base64 string.\"\"\"\n        buffered = BytesIO()\n        image.save(buffered, format=\"PNG\")\n        img_bytes = buffered.getvalue()\n        return base64.b64encode(img_bytes).decode()\n\n    # === Run inference on all samples (in parallel) ===\n    def run_inference(sample, idx):\n        \"\"\"\n        Run inference on a single sample.\n\n        Returns:\n            dict with \"prediction\" and \"ground_truth\"\n        \"\"\"\n        try:\n            # Encode image\n            image_base64 = encode_image_to_base64(sample[\"image\"])\n\n            # Call API\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": \"Write the LaTeX representation for this image.\"\n                            },\n                        ],\n                    },\n                ],\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n\n            # Extract prediction\n            prediction = response.choices[0].message.content.strip()\n            ground_truth = sample[\"text\"].strip()\n\n            return {\n                \"prediction\": prediction,\n                \"ground_truth\": ground_truth,\n            }\n        except Exception as e:\n            print(f\"Error on sample {idx}: {e}\")\n            return {\n                \"prediction\": \"\",\n                \"ground_truth\": sample[\"text\"].strip(),\n            }\n\n    # Run evaluation with parallel requests\n    print(f\"Running inference on {len(dataset)} samples...\")\n    print(f\"Parallelism: {max_parallel_requests} concurrent requests\")\n    print(\"\")\n\n    results = []\n    with ThreadPoolExecutor(max_workers=max_parallel_requests) as executor:\n        # Submit all tasks\n        futures = [\n            executor.submit(run_inference, dataset[i], i)\n            for i in range(len(dataset))\n        ]\n\n        # Collect results with progress bar\n        for future in tqdm(as_completed(futures), total=len(dataset), desc=\"Evaluating\"):\n            results.append(future.result())\n\n    # === Calculate metrics ===\n    print(\"\")\n    print(\"=\" * 80)\n    print(\"CALCULATING METRICS\")\n    print(\"=\" * 80)\n\n    predictions = [r[\"prediction\"] for r in results]\n    ground_truths = [r[\"ground_truth\"] for r in results]\n\n    # Exact match accuracy\n    exact_matches = sum(p == g for p, g in zip(predictions, ground_truths))\n    exact_match_accuracy = exact_matches / len(results)\n\n    # Character Error Rate (CER)\n    # Lower is better, 0 = perfect\n    character_error_rate = cer(ground_truths, predictions)\n\n    # Word Error Rate (WER)\n    # Lower is better, 0 = perfect\n    word_error_rate = wer(ground_truths, predictions)\n\n    # === Print results ===\n    print(\"\")\n    print(\"\ud83d\udcca EVALUATION RESULTS\")\n    print(\"=\" * 80)\n    print(f\"Samples evaluated: {len(results)}\")\n    print(f\"\")\n    print(f\"Exact Match Accuracy:  {exact_match_accuracy:.2%}  ({exact_matches}/{len(results)})\")\n    print(f\"Character Error Rate:  {character_error_rate:.2%}  (lower is better)\")\n    print(f\"Word Error Rate:       {word_error_rate:.2%}  (lower is better)\")\n    print(\"=\" * 80)\n\n    # === Print example predictions ===\n    print(\"\")\n    print(\"\ud83d\udcdd EXAMPLE PREDICTIONS (first 5)\")\n    print(\"=\" * 80)\n    for i in range(min(5, len(results))):\n        print(f\"\\nSample {i+1}:\")\n        print(f\"  Ground Truth: {results[i]['ground_truth']}\")\n        print(f\"  Prediction:   {results[i]['prediction']}\")\n        print(f\"  Match: {'\u2713' if results[i]['prediction'] == results[i]['ground_truth'] else '\u2717'}\")\n    print(\"=\" * 80)\n\n    # Save full results to volume\n    results_file = f\"/data/Finetuned_Gemma_3_4b_it/eval_results_{dataset_split}.json\"\n    import json\n    with open(results_file, \"w\") as f:\n        json.dump({\n            \"metrics\": {\n                \"exact_match_accuracy\": exact_match_accuracy,\n                \"character_error_rate\": character_error_rate,\n                \"word_error_rate\": word_error_rate,\n            },\n            \"num_samples\": len(results),\n            \"examples\": results[:20],  # Save first 20 examples\n        }, f, indent=2)\n\n    exp_volume.commit()\n    print(f\"\\n\u2713 Full results saved to {results_file}\")\n\n    return {\n        \"status\": \"completed\",\n        \"metrics\": {\n            \"exact_match_accuracy\": exact_match_accuracy,\n            \"character_error_rate\": character_error_rate,\n            \"word_error_rate\": word_error_rate,\n        },\n        \"num_samples\": len(results),\n        \"examples\": results[:10],  # Return first 10 examples\n    }\n</code></pre> <p>What the metrics mean:</p> <ol> <li> <p>Exact Match Accuracy: The gold standard. Did we get it 100% right? For LaTeX, even a missing space matters.</p> </li> <li> <p>Character Error Rate (CER): How many character edits (insert/delete/replace) to go from prediction to ground truth? Lower is better. 0% = perfect, 100% = complete garbage.</p> </li> <li> <p>Word Error Rate (WER): Same as CER but at word level. More forgiving for LaTeX because <code>\\frac{a}{b}</code> has multiple \"words\".</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#running-evaluation","title":"Running Evaluation","text":"<p>Basic run:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::evaluate_model\n</code></pre> <p>This auto-detects your deployed vLLM endpoint and evaluates on 100 samples.</p> <p>Evaluate more samples:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::evaluate_model \\\n  --max-samples=500 \\\n  --max-parallel-requests=16\n</code></pre> <p>Custom endpoint:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::evaluate_model \\\n  --endpoint-url=\"https://your-custom-endpoint.modal.run/v1\" \\\n  --max-samples=1000\n</code></pre> <p>Example output:</p> <pre><code>\ud83d\udcca EVALUATION RESULTS\n================================================================================\nSamples evaluated: 100\n\nExact Match Accuracy:  78.00%  (78/100)\nCharacter Error Rate:  5.23%  (lower is better)\nWord Error Rate:       8.45%  (lower is better)\n================================================================================\n\n\ud83d\udcdd EXAMPLE PREDICTIONS (first 5)\n================================================================================\n\nSample 1:\n  Ground Truth: \\frac{d}{dx} \\left( x^2 + 2x + 1 \\right) = 2x + 2\n  Prediction:   \\frac{d}{dx} \\left( x^2 + 2x + 1 \\right) = 2x + 2\n  Match: \u2713\n\nSample 2:\n  Ground Truth: \\int_{0}^{1} x^2 dx = \\frac{1}{3}\n  Prediction:   \\int_{0}^{1} x^2 dx = \\frac{1}{3}\n  Match: \u2713\n\n...\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Let me show you how I'd actually use this end-to-end:</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#1-download-everything-one-time","title":"1. Download Everything (One Time)","text":"<pre><code># Download dataset (CPU, cheap)\nmodal run FinetuneGemmaUnslothModal.py::download_datasets\n\n# Download model (L40S, ~$1 for 10 minutes)\nmodal run FinetuneGemmaUnslothModal.py::download_models\n</code></pre> <p>Cost so far: ~$1 Time: ~15 minutes</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#2-quick-test-run-make-sure-it-works","title":"2. Quick Test Run (Make Sure It Works)","text":"<pre><code># Train on 100 samples for 1 epoch\nmodal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --max-samples=100 \\\n  --num-train-epochs=1 \\\n  --save-steps=50\n</code></pre> <p>Cost: ~$3-5 (A100-80GB for 30-60 minutes) Time: 30-60 minutes</p> <p>If this works, you know your pipeline is solid.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#3-full-training-run","title":"3. Full Training Run","text":"<pre><code># Production training with HF Hub push\nmodal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --hub-id=\"your-username/gemma-latex-ocr\" \\\n  --num-train-epochs=3 \\\n  --learning-rate=0.0003 \\\n  --per-device-train-batch-size=4 \\\n  --gradient-accumulation-steps=4\n</code></pre> <p>Cost: ~$20-40 (A100-80GB for 4-8 hours depending on dataset size) Time: 4-8 hours</p> <p>While this runs, go touch grass. Check W&amp;B dashboard occasionally to make sure loss is going down.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#4-deploy-for-serving","title":"4. Deploy for Serving","text":"<pre><code>modal deploy FinetuneGemmaUnslothModal.py\n</code></pre> <p>Cost: \\(0 when idle, ~\\)1/hr when active (L40S)</p> <p>Modal gives you a URL. Save it.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#5-evaluate","title":"5. Evaluate","text":"<pre><code>modal run FinetuneGemmaUnslothModal.py::evaluate_model \\\n  --max-samples=500\n</code></pre> <p>Cost: ~$0.10 (CPU for 10-20 minutes) Time: 10-20 minutes</p> <p>Check your metrics. If accuracy is good (&gt;75%), you're golden. If not, tweak hyperparameters and go back to step 3.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#6-use-in-production","title":"6. Use in Production","text":"<pre><code># In your application\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://your-endpoint.modal.run/v1\",\n    api_key=\"EMPTY\"\n)\n\n# Your app can now read LaTeX from images!\n</code></pre> <p>Total cost for full pipeline: ~$25-50 Time: 1 day (mostly waiting for training)</p> <p>Compare this to managing your own GPU infrastructure... yeah, Modal wins.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#hyperparameter-tuning-tips","title":"Hyperparameter Tuning Tips","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#for-better-accuracy","title":"For Better Accuracy","text":"<pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --lora-r=64 \\              # Higher rank = more capacity\n  --lora-alpha=128 \\         # Scale accordingly\n  --learning-rate=0.0001 \\   # Lower LR = more stable\n  --num-train-epochs=5       # More epochs\n</code></pre> <p>Trade-off: Slower training, higher cost, but better results.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#for-faster-iteration","title":"For Faster Iteration","text":"<pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --lora-r=16 \\              # Lower rank = faster\n  --lora-alpha=32 \\\n  --learning-rate=0.0005 \\   # Higher LR = faster convergence\n  --num-train-epochs=2 \\\n  --max-samples=5000         # Smaller dataset\n</code></pre> <p>Trade-off: Lower accuracy, but 2-3x faster training.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#for-memory-issues","title":"For Memory Issues","text":"<p>If you get OOM errors:</p> <pre><code>modal run FinetuneGemmaUnslothModal.py::fine_tune_unsloth \\\n  --per-device-train-batch-size=2 \\     # Smaller batches\n  --gradient-accumulation-steps=8 \\     # Maintain effective batch size\n  --max-seq-length=4096                  # Shorter sequences\n</code></pre> <p>Or switch to A100-80GB if you're on A100-40GB.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#secret-not-found","title":"\"Secret not found\"","text":"<p>Error: <code>Modal Secret \"secrets-hf-wandb\" not found</code></p> <p>Fix: </p><pre><code>modal secret create secrets-hf-wandb \\\n  HUGGINGFACE_TOKEN=hf_xxx \\\n  WANDB_API_KEY=xxx\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Fixes: 1. Reduce batch size: <code>--per-device-train-batch-size=2</code> 2. Reduce sequence length: <code>--max-seq-length=4096</code> 3. Use smaller LoRA rank: <code>--lora-r=16 --lora-alpha=32</code> 4. Switch to A100-80GB</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#image-build-timeout","title":"Image Build Timeout","text":"<p>Error: Image build exceeds timeout</p> <p>Fix: First build takes 15-20 minutes. This is normal. Modal caches it. Grab a coffee.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#vllm-server-not-responding","title":"vLLM Server Not Responding","text":"<p>Error: <code>Could not connect to endpoint</code></p> <p>Fix: </p><pre><code># Make sure it's deployed\nmodal app list\n\n# If not running, deploy it\nmodal deploy FinetuneGemmaUnslothModal.py\n</code></pre> <p>The first request after deploy takes 30-60 seconds (cold start). Be patient.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#evaluation-fails","title":"Evaluation Fails","text":"<p>Error: Various errors during eval</p> <p>Checks: 1. Is vLLM running? <code>modal app list</code> 2. Is the endpoint URL correct? 3. Is the model path correct in the eval function?</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#cost-breakdown","title":"Cost Breakdown","text":"<p>Based on Modal pricing (approximate):</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#training","title":"Training","text":"<ul> <li>Download dataset: $0.001 (CPU, 5 min)</li> <li>Download model: $1 (L40S, 10 min)</li> <li>Test training: $5 (A100-80GB, 1 hour)</li> <li>Full training: $25-40 (A100-80GB, 6-10 hours)</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#serving-pay-per-use","title":"Serving (pay per use)","text":"<ul> <li>Idle: $0/month (auto-scales to 0)</li> <li>Active: ~$1/hour (L40S)</li> <li>Typical monthly cost: $5-20 (depends on usage)</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#evaluation","title":"Evaluation","text":"<ul> <li>CPU cost: ~$0.10 per eval run</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#storage","title":"Storage","text":"<ul> <li>Volumes: Free up to 50GB</li> <li>This project: ~15GB = $0/month</li> </ul> <p>Total for complete pipeline: $30-50 one-time + $5-20/month for serving</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#whats-next","title":"What's Next?","text":"<p>You've built a complete production ML pipeline. Here's what you can do next:</p> <ol> <li> <p>Try different models: Replace Gemma with Llama, Qwen, or any other vision-language model. Just change <code>BASE_MODEL_NAME</code>.</p> </li> <li> <p>Use your own dataset: Got images + text pairs? Upload to HuggingFace, point the script at it.</p> </li> <li> <p>Optimize serving: Experiment with different GPUs, batch sizes, quantization.</p> </li> <li> <p>Add more metrics: BLEU score, semantic similarity, whatever matters for your use case.</p> </li> <li> <p>Build an app: You have an API. Now build a web app that uses it!</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#resources","title":"Resources","text":"<ul> <li>Original Unsloth Colab - Where this all started</li> <li>Unsloth Documentation - Deep dive into Unsloth</li> <li>Modal Documentation - Everything about Modal</li> <li>vLLM Documentation - Serving optimization</li> <li>Gemma Model Card - About the base model</li> <li>LoRA Paper - The theory behind it</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/#wrapping-up","title":"Wrapping Up","text":"<p>You just built what most companies would consider their \"production ML infrastructure\": - Dataset management - Distributed training - Model versioning - API deployment - Evaluation pipelines</p> <p>All in one Python file, running on Modal. No Kubernetes, no Docker nightmares, no infrastructure headaches.</p> <p>The Unsloth Colab notebook showed you how to train on a single GPU. This tutorial showed you how to take that exact workflow and productionize it - separate stages, proper caching, auto-scaling deployment, real evaluation metrics.</p> <p>This is how I actually do ML work nowadays. Write code locally, run on Modal's GPUs, deploy with one command.</p> <p>Got questions? Hit me up on Twitter @adithya_s_k!</p> <p>Now go build something cool with this. \ud83d\ude80</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/","title":"FinetuneLlamaAxolotlGPUModal","text":"In\u00a0[\u00a0]: Copied! <pre>from modal import App, Image as ModalImage, Volume, Secret\n</pre> from modal import App, Image as ModalImage, Volume, Secret In\u00a0[\u00a0]: Copied! <pre># Time constants\nHOURS = 60 * 60\n# GPU Configuration\nGPU_TYPE = \"a100-80gb\"  # Default GPU type (can be: a100-80gb, a100-40gb, l40s, etc.)\n# Training Configuration\nWANDB_PROJECT_DEFAULT = \"Llama-70b-MultiGPU-finetune\"\n</pre> # Time constants HOURS = 60 * 60 # GPU Configuration GPU_TYPE = \"a100-80gb\"  # Default GPU type (can be: a100-80gb, a100-40gb, l40s, etc.) # Training Configuration WANDB_PROJECT_DEFAULT = \"Llama-70b-MultiGPU-finetune\" In\u00a0[\u00a0]: Copied! <pre>app = App(\"Finetuned_Llama_70b_Axolotl_MultiGPU\")\n# Create volumes for persistent storage\nexp_volume = Volume.from_name(\"Finetuned_Llama_70b_Axolotl\", create_if_missing=True)\n# Configure volume mounting points\nVOLUME_CONFIG = {\n    \"/data\": exp_volume,\n}\nhuggingface_secret = Secret.from_name(\"secrets-hf-wandb\")\n</pre> app = App(\"Finetuned_Llama_70b_Axolotl_MultiGPU\") # Create volumes for persistent storage exp_volume = Volume.from_name(\"Finetuned_Llama_70b_Axolotl\", create_if_missing=True) # Configure volume mounting points VOLUME_CONFIG = {     \"/data\": exp_volume, } huggingface_secret = Secret.from_name(\"secrets-hf-wandb\") <p>This is the original Axolotl image, it can be used but it opens JupyterLab by default AXOLOTL_IMAGE = ModalImage.from_registry( \"axolotlai/axolotl-cloud:main-latest\", add_python=\"3.12\" ).env( { \"JUPYTER_ENABLE_LAB\": \"no\",  # Disable JupyterLab auto-start \"JUPYTER_TOKEN\": \"\",  # Disable Jupyter token requirement \"HF_HOME\": \"/data/.cache\",  # Set HF cache root under /data } )</p> In\u00a0[\u00a0]: Copied! <pre># Custom CUDA image with Axolotl and dependencies pre-installed\nCUDA_VERSION = \"12.8.1\"\nCUDA_FLAVOR = \"devel\"\nCUDA_OS = \"ubuntu24.04\"\nCUDA_TAG = f\"{CUDA_VERSION}-{CUDA_FLAVOR}-{CUDA_OS}\"\n</pre> # Custom CUDA image with Axolotl and dependencies pre-installed CUDA_VERSION = \"12.8.1\" CUDA_FLAVOR = \"devel\" CUDA_OS = \"ubuntu24.04\" CUDA_TAG = f\"{CUDA_VERSION}-{CUDA_FLAVOR}-{CUDA_OS}\" In\u00a0[\u00a0]: Copied! <pre># Define the GPU image for fine-tuning with Unsloth\nAXOLOTL_IMAGE = (\n    ModalImage.from_registry(f\"nvidia/cuda:{CUDA_TAG}\", add_python=\"3.12\")\n    .apt_install(\n        \"git\",\n        \"build-essential\",\n    )\n    .uv_pip_install(\n        [\n            \"torch\",\n            \"torchvision\",\n            \"torchaudio\",  # optional but often bundled with torch\n        ]\n    )\n    .run_commands(\n        \"uv pip install --no-deps -U packaging setuptools wheel ninja --system\"\n    )\n    .run_commands(\"uv pip install --no-build-isolation axolotl[deepspeed] --system\")\n    .run_commands(\n        \"UV_NO_BUILD_ISOLATION=1 uv pip install flash-attn --no-build-isolation --system\"\n    )\n    .env(\n        {\n            \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n            \"HF_HOME\": \"/data/.cache\",  # Set HF cache root under /data\n        }\n    )\n)\n</pre> # Define the GPU image for fine-tuning with Unsloth AXOLOTL_IMAGE = (     ModalImage.from_registry(f\"nvidia/cuda:{CUDA_TAG}\", add_python=\"3.12\")     .apt_install(         \"git\",         \"build-essential\",     )     .uv_pip_install(         [             \"torch\",             \"torchvision\",             \"torchaudio\",  # optional but often bundled with torch         ]     )     .run_commands(         \"uv pip install --no-deps -U packaging setuptools wheel ninja --system\"     )     .run_commands(\"uv pip install --no-build-isolation axolotl[deepspeed] --system\")     .run_commands(         \"UV_NO_BUILD_ISOLATION=1 uv pip install flash-attn --no-build-isolation --system\"     )     .env(         {             \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",             \"HF_HOME\": \"/data/.cache\",  # Set HF cache root under /data         }     ) ) In\u00a0[\u00a0]: Copied! <pre># =============================================================================\n# HELPER FUNCTIONS\n# =============================================================================\ndef write_config_to_volume(\n    train_config_yaml: str,\n    config_path: str = \"/data/config.yml\",\n    update_paths: bool = True,\n) -&gt; dict:\n    \"\"\"Write YAML configuration to volume with optional path updates.\"\"\"\n    import os\n    import yaml\n\n    config_dict = yaml.safe_load(train_config_yaml)\n\n    if update_paths and \"output_dir\" in config_dict:\n        config_dict[\"output_dir\"] = config_dict[\"output_dir\"].replace(\n            \"./outputs\", \"/data/outputs\"\n        )\n\n    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n\n    with open(config_path, \"w\") as f:\n        yaml.dump(config_dict, f, default_flow_style=False)\n\n    exp_volume.commit()\n    return config_dict\n</pre> # ============================================================================= # HELPER FUNCTIONS # ============================================================================= def write_config_to_volume(     train_config_yaml: str,     config_path: str = \"/data/config.yml\",     update_paths: bool = True, ) -&gt; dict:     \"\"\"Write YAML configuration to volume with optional path updates.\"\"\"     import os     import yaml      config_dict = yaml.safe_load(train_config_yaml)      if update_paths and \"output_dir\" in config_dict:         config_dict[\"output_dir\"] = config_dict[\"output_dir\"].replace(             \"./outputs\", \"/data/outputs\"         )      os.makedirs(os.path.dirname(config_path), exist_ok=True)      with open(config_path, \"w\") as f:         yaml.dump(config_dict, f, default_flow_style=False)      exp_volume.commit()     return config_dict In\u00a0[\u00a0]: Copied! <pre>TRAIN_CONFIG_YAML = f\"\"\"\nbase_model: NousResearch/Meta-Llama-3-8B-Instruct\n# optionally might have model_type or tokenizer_type\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n# Automatically upload checkpoint and final model to HF\n# hub_model_id: username/custom_model_name\n\nload_in_8bit: true\nload_in_4bit: false\n\nchat_template: llama3\ndatasets:\n  - path: fozziethebeat/alpaca_messages_2k_test\n    type: chat_template\ndataset_prepared_path: /data/prepared_datasets/alpaca_2k\nval_set_size: 0.05\noutput_dir: /data/outputs/lora-out\n\nsequence_len: 4096\nsample_packing: false\n\nadapter: lora\nlora_model_dir:\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\n\nwandb_project: {WANDB_PROJECT_DEFAULT}\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 8\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\nbf16: auto\ntf32: false\n\ngradient_checkpointing: true\nresume_from_checkpoint:\nlogging_steps: 1\nflash_attention: true\n\nwarmup_ratio: 0.1\nevals_per_epoch: 4\nsaves_per_epoch: 4\nweight_decay: 0.0\nspecial_tokens:\n   pad_token: &lt;|end_of_text|&gt;\n\"\"\"\n</pre> TRAIN_CONFIG_YAML = f\"\"\" base_model: NousResearch/Meta-Llama-3-8B-Instruct # optionally might have model_type or tokenizer_type model_type: LlamaForCausalLM tokenizer_type: AutoTokenizer # Automatically upload checkpoint and final model to HF # hub_model_id: username/custom_model_name  load_in_8bit: true load_in_4bit: false  chat_template: llama3 datasets:   - path: fozziethebeat/alpaca_messages_2k_test     type: chat_template dataset_prepared_path: /data/prepared_datasets/alpaca_2k val_set_size: 0.05 output_dir: /data/outputs/lora-out  sequence_len: 4096 sample_packing: false  adapter: lora lora_model_dir: lora_r: 32 lora_alpha: 16 lora_dropout: 0.05 lora_target_linear: true  wandb_project: {WANDB_PROJECT_DEFAULT} wandb_entity: wandb_watch: wandb_name: wandb_log_model:  gradient_accumulation_steps: 4 micro_batch_size: 8 num_epochs: 4 optimizer: adamw_bnb_8bit lr_scheduler: cosine learning_rate: 0.0002  bf16: auto tf32: false  gradient_checkpointing: true resume_from_checkpoint: logging_steps: 1 flash_attention: true  warmup_ratio: 0.1 evals_per_epoch: 4 saves_per_epoch: 4 weight_decay: 0.0 special_tokens:    pad_token: &lt;|end_of_text|&gt; \"\"\" In\u00a0[\u00a0]: Copied! <pre># GPU Configuration for preprocessing (single GPU)\nPREPROCESS_NUM_GPUS = 1\nPREPROCESS_GPU_CONFIG = f\"{GPU_TYPE}:{PREPROCESS_NUM_GPUS}\"\n</pre> # GPU Configuration for preprocessing (single GPU) PREPROCESS_NUM_GPUS = 1 PREPROCESS_GPU_CONFIG = f\"{GPU_TYPE}:{PREPROCESS_NUM_GPUS}\" In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=24 * HOURS,\n    gpu=PREPROCESS_GPU_CONFIG,\n)\ndef process_datasets(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n):\n    \"\"\"Preprocess and tokenize dataset before training using Axolotl.\"\"\"\n    import os\n    import subprocess\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    config_dict = write_config_to_volume(train_config_yaml, config_path, True)\n    exp_volume.commit()\n\n    print(\"Starting dataset preprocessing...\")\n    try:\n        subprocess.run([\"axolotl\", \"preprocess\", config_path], check=True)\n        print(\"\u2713 Preprocessing completed\")\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"config_path\": config_path,\n            \"preprocessed_data_path\": config_dict.get(\"dataset_prepared_path\"),\n            \"output_dir\": config_dict.get(\"output_dir\"),\n        }\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Preprocessing failed: {e}\")\n</pre> @app.function(     image=AXOLOTL_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=24 * HOURS,     gpu=PREPROCESS_GPU_CONFIG, ) def process_datasets(     train_config_yaml: str = TRAIN_CONFIG_YAML,     config_path: str = \"/data/config.yml\", ):     \"\"\"Preprocess and tokenize dataset before training using Axolotl.\"\"\"     import os     import subprocess      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      config_dict = write_config_to_volume(train_config_yaml, config_path, True)     exp_volume.commit()      print(\"Starting dataset preprocessing...\")     try:         subprocess.run([\"axolotl\", \"preprocess\", config_path], check=True)         print(\"\u2713 Preprocessing completed\")         exp_volume.commit()          return {             \"status\": \"completed\",             \"config_path\": config_path,             \"preprocessed_data_path\": config_dict.get(\"dataset_prepared_path\"),             \"output_dir\": config_dict.get(\"output_dir\"),         }     except subprocess.CalledProcessError as e:         raise RuntimeError(f\"Preprocessing failed: {e}\") In\u00a0[\u00a0]: Copied! <pre># GPU Configuration for training (2-8 GPUs for multi-GPU training)\nTRAIN_NUM_GPUS = 4  # Can be adjusted from 2 to 8\nTRAIN_GPU_CONFIG = f\"{GPU_TYPE}:{TRAIN_NUM_GPUS}\"\n</pre> # GPU Configuration for training (2-8 GPUs for multi-GPU training) TRAIN_NUM_GPUS = 4  # Can be adjusted from 2 to 8 TRAIN_GPU_CONFIG = f\"{GPU_TYPE}:{TRAIN_NUM_GPUS}\" In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=24 * HOURS,\n    gpu=TRAIN_GPU_CONFIG,\n)\ndef train_model(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n):\n    \"\"\"\n    Train or fine-tune a model using Axolotl with multi-GPU support.\n    All configuration is defined in the YAML file.\n    Uses accelerate for multi-GPU training.\n\n    Args:\n        train_config_yaml: YAML configuration content as string\n        config_path: Path where config will be written on the volume\n\n    Returns:\n        dict: Contains training status and output paths\n    \"\"\"\n    import os\n    import subprocess\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n    os.environ[\"WANDB_API_KEY\"] = os.environ[\"WANDB_API_KEY\"]\n    os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_DEFAULT\n\n    # Write config to volume using global helper function\n    config_dict = write_config_to_volume(\n        train_config_yaml=train_config_yaml,\n        config_path=config_path,\n        update_paths=True,\n    )\n\n    exp_volume.commit()\n\n    # Run Axolotl training with accelerate for multi-GPU support\n    print(f\"Starting training with {TRAIN_NUM_GPUS} GPUs...\")\n    cmd = [\n        \"accelerate\",\n        \"launch\",\n        \"--multi_gpu\",\n        \"--num_processes\",\n        str(TRAIN_NUM_GPUS),\n        \"--num_machines\",\n        \"1\",\n        \"--mixed_precision\",\n        \"bf16\",\n        \"--dynamo_backend\",\n        \"no\",\n        \"-m\",\n        \"axolotl.cli.train\",\n        config_path,\n    ]\n\n    try:\n        subprocess.run(cmd, check=True)\n        print(\"\u2713 Training completed\")\n\n        # Commit trained model to volume\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"config_path\": config_path,\n            \"output_dir\": config_dict.get(\"output_dir\"),\n            \"base_model\": config_dict.get(\"base_model\"),\n            \"num_gpus\": TRAIN_NUM_GPUS,\n        }\n\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Training failed: {e}\")\n</pre> @app.function(     image=AXOLOTL_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=24 * HOURS,     gpu=TRAIN_GPU_CONFIG, ) def train_model(     train_config_yaml: str = TRAIN_CONFIG_YAML,     config_path: str = \"/data/config.yml\", ):     \"\"\"     Train or fine-tune a model using Axolotl with multi-GPU support.     All configuration is defined in the YAML file.     Uses accelerate for multi-GPU training.      Args:         train_config_yaml: YAML configuration content as string         config_path: Path where config will be written on the volume      Returns:         dict: Contains training status and output paths     \"\"\"     import os     import subprocess      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]     os.environ[\"WANDB_API_KEY\"] = os.environ[\"WANDB_API_KEY\"]     os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_DEFAULT      # Write config to volume using global helper function     config_dict = write_config_to_volume(         train_config_yaml=train_config_yaml,         config_path=config_path,         update_paths=True,     )      exp_volume.commit()      # Run Axolotl training with accelerate for multi-GPU support     print(f\"Starting training with {TRAIN_NUM_GPUS} GPUs...\")     cmd = [         \"accelerate\",         \"launch\",         \"--multi_gpu\",         \"--num_processes\",         str(TRAIN_NUM_GPUS),         \"--num_machines\",         \"1\",         \"--mixed_precision\",         \"bf16\",         \"--dynamo_backend\",         \"no\",         \"-m\",         \"axolotl.cli.train\",         config_path,     ]      try:         subprocess.run(cmd, check=True)         print(\"\u2713 Training completed\")          # Commit trained model to volume         exp_volume.commit()          return {             \"status\": \"completed\",             \"config_path\": config_path,             \"output_dir\": config_dict.get(\"output_dir\"),             \"base_model\": config_dict.get(\"base_model\"),             \"num_gpus\": TRAIN_NUM_GPUS,         }      except subprocess.CalledProcessError as e:         raise RuntimeError(f\"Training failed: {e}\") In\u00a0[\u00a0]: Copied! <pre># GPU Configuration for merging LoRA (single GPU)\nMERGE_NUM_GPUS = 1\nMERGE_GPU_CONFIG = f\"{GPU_TYPE}:{MERGE_NUM_GPUS}\"\n</pre> # GPU Configuration for merging LoRA (single GPU) MERGE_NUM_GPUS = 1 MERGE_GPU_CONFIG = f\"{GPU_TYPE}:{MERGE_NUM_GPUS}\" In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=4 * HOURS,\n    gpu=MERGE_GPU_CONFIG,\n)\ndef merge_lora(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n    lora_model_dir: str = None,\n):\n    \"\"\"\n    Merge trained LoRA adapters into the base model.\n\n    Args:\n        train_config_yaml: YAML configuration content as string\n        config_path: Path where config will be written on the volume\n        lora_model_dir: Path to LoRA adapter directory (optional, uses config if not provided)\n\n    Returns:\n        dict: Contains merge status and output paths\n    \"\"\"\n    import os\n    import subprocess\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Write config to volume\n    config_dict = write_config_to_volume(\n        train_config_yaml=train_config_yaml,\n        config_path=config_path,\n        update_paths=True,\n    )\n\n    exp_volume.commit()\n\n    # Build merge command\n    print(\"Starting LoRA merge...\")\n    cmd = [\"axolotl\", \"merge-lora\", config_path]\n\n    if lora_model_dir:\n        cmd.extend([\"--lora-model-dir\", lora_model_dir])\n\n    try:\n        subprocess.run(cmd, check=True)\n        print(\"\u2713 LoRA merge completed\")\n\n        # Commit merged model to volume\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"config_path\": config_path,\n            \"output_dir\": config_dict.get(\"output_dir\"),\n            \"lora_model_dir\": lora_model_dir or config_dict.get(\"lora_model_dir\"),\n        }\n\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"LoRA merge failed: {e}\")\n</pre> @app.function(     image=AXOLOTL_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=4 * HOURS,     gpu=MERGE_GPU_CONFIG, ) def merge_lora(     train_config_yaml: str = TRAIN_CONFIG_YAML,     config_path: str = \"/data/config.yml\",     lora_model_dir: str = None, ):     \"\"\"     Merge trained LoRA adapters into the base model.      Args:         train_config_yaml: YAML configuration content as string         config_path: Path where config will be written on the volume         lora_model_dir: Path to LoRA adapter directory (optional, uses config if not provided)      Returns:         dict: Contains merge status and output paths     \"\"\"     import os     import subprocess      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      # Write config to volume     config_dict = write_config_to_volume(         train_config_yaml=train_config_yaml,         config_path=config_path,         update_paths=True,     )      exp_volume.commit()      # Build merge command     print(\"Starting LoRA merge...\")     cmd = [\"axolotl\", \"merge-lora\", config_path]      if lora_model_dir:         cmd.extend([\"--lora-model-dir\", lora_model_dir])      try:         subprocess.run(cmd, check=True)         print(\"\u2713 LoRA merge completed\")          # Commit merged model to volume         exp_volume.commit()          return {             \"status\": \"completed\",             \"config_path\": config_path,             \"output_dir\": config_dict.get(\"output_dir\"),             \"lora_model_dir\": lora_model_dir or config_dict.get(\"lora_model_dir\"),         }      except subprocess.CalledProcessError as e:         raise RuntimeError(f\"LoRA merge failed: {e}\") In\u00a0[\u00a0]: Copied! <pre># GPU Configuration for inference (single GPU)\nINFERENCE_NUM_GPUS = 1\nINFERENCE_GPU_CONFIG = f\"{GPU_TYPE}:{INFERENCE_NUM_GPUS}\"\n</pre> # GPU Configuration for inference (single GPU) INFERENCE_NUM_GPUS = 1 INFERENCE_GPU_CONFIG = f\"{GPU_TYPE}:{INFERENCE_NUM_GPUS}\" In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=1 * HOURS,\n    gpu=INFERENCE_GPU_CONFIG,\n)\ndef run_inference(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n    prompt: str = \"Hello, how are you?\",\n    lora_model_dir: str = None,\n    base_model: str = None,\n):\n    \"\"\"\n    Run inference using the trained model.\n\n    Args:\n        train_config_yaml: YAML configuration content as string\n        config_path: Path where config will be written on the volume\n        prompt: Input prompt for inference\n        lora_model_dir: Path to LoRA adapter directory (optional)\n        base_model: Path to base or merged model (optional)\n\n    Returns:\n        dict: Contains inference output and metadata\n    \"\"\"\n    import os\n    import subprocess\n    import tempfile\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Write config to volume\n    config_dict = write_config_to_volume(\n        train_config_yaml=train_config_yaml,\n        config_path=config_path,\n        update_paths=True,\n    )\n\n    # Build inference command\n    print(\"Starting inference...\")\n    print(f\"Prompt: {prompt}\")\n    print(\"-\" * 80)\n\n    cmd = [\"axolotl\", \"inference\", config_path]\n\n    if lora_model_dir:\n        cmd.extend([\"--lora-model-dir\", lora_model_dir])\n    if base_model:\n        cmd.extend([\"--base-model\", base_model])\n\n    # Write prompt to temp file and pipe it\n    try:\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".txt\") as f:\n            f.write(prompt)\n            prompt_file = f.name\n\n        # Run inference with prompt piped from file\n        with open(prompt_file, \"r\") as f:\n            result = subprocess.run(\n                cmd,\n                stdin=f,\n                capture_output=True,\n                text=True,\n                check=True,\n            )\n\n        print(\"\u2713 Inference completed\")\n        print(\"\\n\" + \"=\" * 80)\n        print(\"MODEL OUTPUT:\")\n        print(\"=\" * 80)\n        print(result.stdout)\n        print(\"=\" * 80)\n\n        if result.stderr:\n            print(\"\\nSTDERR:\")\n            print(result.stderr)\n\n        response_dict = {\n            \"status\": \"completed\",\n            \"prompt\": prompt,\n            \"output\": result.stdout,\n            \"model\": base_model or config_dict.get(\"base_model\"),\n        }\n\n        return response_dict\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error output: {e.stderr}\")\n        print(f\"Command output: {e.stdout}\")\n        raise RuntimeError(f\"Inference failed: {e}\")\n    finally:\n        # Clean up temp file\n        import os as os_module\n\n        if \"prompt_file\" in locals():\n            os_module.unlink(prompt_file)\n</pre> @app.function(     image=AXOLOTL_IMAGE,     volumes=VOLUME_CONFIG,     secrets=[huggingface_secret],     timeout=1 * HOURS,     gpu=INFERENCE_GPU_CONFIG, ) def run_inference(     train_config_yaml: str = TRAIN_CONFIG_YAML,     config_path: str = \"/data/config.yml\",     prompt: str = \"Hello, how are you?\",     lora_model_dir: str = None,     base_model: str = None, ):     \"\"\"     Run inference using the trained model.      Args:         train_config_yaml: YAML configuration content as string         config_path: Path where config will be written on the volume         prompt: Input prompt for inference         lora_model_dir: Path to LoRA adapter directory (optional)         base_model: Path to base or merged model (optional)      Returns:         dict: Contains inference output and metadata     \"\"\"     import os     import subprocess     import tempfile      os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]      # Write config to volume     config_dict = write_config_to_volume(         train_config_yaml=train_config_yaml,         config_path=config_path,         update_paths=True,     )      # Build inference command     print(\"Starting inference...\")     print(f\"Prompt: {prompt}\")     print(\"-\" * 80)      cmd = [\"axolotl\", \"inference\", config_path]      if lora_model_dir:         cmd.extend([\"--lora-model-dir\", lora_model_dir])     if base_model:         cmd.extend([\"--base-model\", base_model])      # Write prompt to temp file and pipe it     try:         with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".txt\") as f:             f.write(prompt)             prompt_file = f.name          # Run inference with prompt piped from file         with open(prompt_file, \"r\") as f:             result = subprocess.run(                 cmd,                 stdin=f,                 capture_output=True,                 text=True,                 check=True,             )          print(\"\u2713 Inference completed\")         print(\"\\n\" + \"=\" * 80)         print(\"MODEL OUTPUT:\")         print(\"=\" * 80)         print(result.stdout)         print(\"=\" * 80)          if result.stderr:             print(\"\\nSTDERR:\")             print(result.stderr)          response_dict = {             \"status\": \"completed\",             \"prompt\": prompt,             \"output\": result.stdout,             \"model\": base_model or config_dict.get(\"base_model\"),         }          return response_dict      except subprocess.CalledProcessError as e:         print(f\"Error output: {e.stderr}\")         print(f\"Command output: {e.stdout}\")         raise RuntimeError(f\"Inference failed: {e}\")     finally:         # Clean up temp file         import os as os_module          if \"prompt_file\" in locals():             os_module.unlink(prompt_file)"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#configuration-constants","title":"============================================================================= CONFIGURATION CONSTANTS\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#modal-app-volume-and-secret-setup","title":"============================================================================= MODAL APP VOLUME AND SECRET SETUP\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#model-image-setup","title":"============================================================================= MODEL IMAGE SETUP\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#training-configuration-you-can-fine-more-configuration-options-here-httpsgithubcomaxolotl-ai-cloudaxolotltreemainexamples","title":"============================================================================= TRAINING CONFIGURATION You can fine more Configuration options here: https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#preprocessing-function","title":"============================================================================= PREPROCESSING FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#training-function","title":"============================================================================= TRAINING FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#merge-lora-function","title":"============================================================================= MERGE LORA FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal/#inference-function","title":"============================================================================= INFERENCE FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/","title":"Multi-GPU Training with Axolotl","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#fine-tuning-llama-8-70b-with-axolotl-on-modal-multi-gpu-training-made-simple","title":"Fine-tuning Llama 8-70B with Axolotl on Modal: Multi-GPU Training Made Simple","text":"<p>\ud83d\udcc4 View Complete Python Script</p> <p>So you've trained nanoGPT from scratch and fine-tuned Gemma with Unsloth. Now let's go full beast mode - we're training Llama 8-70B across multiple GPUs. We're talking real production ML infrastructure here</p> <p>And the crazy part? We're doing it all with Axolotl on Modal. No Kubernetes cluster to manage, no infrastructure nightmares. Just distributed training power.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#why-axolotl","title":"Why Axolotl?","text":"<p>I discovered Axolotl when I needed to fine-tune a 8-70B model and realized Unsloth wasn't built for multi-GPU setups. That's where Axolotl shines.</p> <p>What makes Axolotl special: - Production-grade multi-GPU support - Train across 2, 4, or even 8 GPUs without writing custom distributed code - YAML-based configs - All your hyperparameters in one readable file. No more scattered parameters across Python code - Built-in DeepSpeed and FSDP - The same tech Microsoft uses to train massive models, just works out of the box - Extensive model support - Llama, Mistral, Qwen, you name it. Pre-configured recipes for all major models - Battle-tested - Used by companies to train production models, not just a research toy</p> <p>The thing is, when you're training a 8-70B model, you physically can't fit it on a single GPU. Even an A100-80GB can barely hold the model weights, let alone gradients and optimizer states. You NEED multi-GPU training.</p> <p>Axolotl handles all the complexity: splitting the model across GPUs, synchronizing gradients, managing checkpoints. You just write a YAML file and hit run.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#what-were-building","title":"What We're Building","text":"<p>This is a complete multi-GPU training pipeline with four independent stages:</p> <ol> <li>Preprocess datasets - Tokenize and cache (on 1 GPU, because why waste money?)</li> <li>Multi-GPU training - Fine-tune Llama 8-70B across 4 GPUs with LoRA</li> <li>Merge LoRA adapters - Combine adapters with base model for deployment</li> <li>Inference - Test your fine-tuned model</li> </ol> <p>Here's the mental model:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Preprocess Data     \u2502  (1 GPU - $3.50/hr, run once)\n\u2502  Tokenize &amp; Cache    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Multi-GPU Training  \u2502  (4\u00d7 A100-80GB - $14/hr)\n\u2502  Llama 8-70B + LoRA    \u2502  \u2190 The beast mode part\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Merge LoRA         \u2502  (1 GPU - $3.50/hr, ~30 min)\n\u2502  Adapters \u2192 Model    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Inference         \u2502  (1 GPU - test your model)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each stage is independent. Screw up training? Just re-run that step. Want to test different hyperparameters? Preprocessing is already cached.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#getting-started","title":"Getting Started","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#install-modal","title":"Install Modal","text":"<p>You know the drill by now:</p> <pre><code>pip install modal\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#authenticate","title":"Authenticate","text":"<pre><code>modal setup\n</code></pre> <p>Or with API keys:</p> <pre><code>export MODAL_TOKEN_ID=&lt;your_token_id&gt;\nexport MODAL_TOKEN_SECRET=&lt;your_token_secret&gt;\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#set-up-your-secrets","title":"Set Up Your Secrets","text":"<p>For this, you'll need: - Hugging Face token - Required for Llama models (they're gated) - Weights &amp; Biases API key - Optional but highly recommended for tracking training</p> <p>Create the Modal secret:</p> <pre><code>modal secret create secrets-hf-wandb \\\n  HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxx \\\n  WANDB_API_KEY=xxxxxxxxxxxxx\n</code></pre> <p>Note: The script looks for a secret named <code>secrets-hf-wandb</code>. If you use a different name, update the code where it says <code>Secret.from_name(\"secrets-hf-wandb\")</code>.</p> <p>Get your tokens: - HF token: hf.co/settings/tokens - W&amp;B key: wandb.ai/authorize</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#project-structure","title":"Project Structure","text":"<p>This is even simpler than the previous tutorials:</p> <pre><code>ServerLessFinetuning/\n\u251c\u2500\u2500 FinetuneLlamaAxolotlGPUModal.py    # Everything lives here\n\u2514\u2500\u2500 .env                                # Optional: local secrets\n</code></pre> <p>One file. That's it. All your configuration, all your training stages, all your pipeline logic - in one clean Python file.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#understanding-multi-gpu-training","title":"Understanding Multi-GPU Training","text":"<p>Before we dive into code, let's understand why we even need multiple GPUs.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#the-memory-problem","title":"The Memory Problem","text":"<p>Here's the brutal math:</p> Model Size Parameters FP16 Weights Training Memory Fits on Single GPU? Llama 3-8B 8 billion ~16GB ~40GB \u2713 (A100-80GB) Llama 3-8-70B 70 billion ~140GB ~280GB \u2717 (Impossible!) Llama 3-405B 405 billion ~810GB ~1.6TB \u2717 (Very impossible!) <p>Training memory = model weights + gradients + optimizer states + activations. Roughly 2-3x the model size.</p> <p>A single A100-80GB has... 80GB. You literally cannot fit a 8-70B model for training, even with quantization.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#the-multi-gpu-solution","title":"The Multi-GPU Solution","text":"<p>There are several strategies to distribute a model across GPUs:</p> <ol> <li> <p>Data Parallelism (DP): Copy the full model to each GPU, split the batch across them. Great for small models, useless for 8-70B.</p> </li> <li> <p>Tensor Parallelism (TP): Split individual layers across GPUs. Each GPU has part of each attention head, part of each MLP. Complex but efficient.</p> </li> <li> <p>Pipeline Parallelism (PP): Different GPUs process different layers. GPU 0 has layers 1-20, GPU 1 has 21-40, etc. Simple but can have bubble time.</p> </li> <li> <p>FSDP (Fully Sharded Data Parallel): The modern approach. Shard everything - model parameters, gradients, optimizer states. Each GPU only keeps what it needs, fetches the rest when needed.</p> </li> </ol> <p>The beauty of Axolotl? You don't have to think about this. It uses HuggingFace Accelerate under the hood, which figures out the best strategy automatically. You just say \"use 4 GPUs\" and it handles the rest.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#configuration-overview","title":"Configuration Overview","text":"<p>Let's start with the basics:</p> <pre><code>from modal import App, Image as ModalImage, Volume, Secret\n\n# Create the Modal app\napp = App(\"Finetuned_Llama_70b_Axolotl_MultiGPU\")\n\n# Create persistent storage\nexp_volume = Volume.from_name(\"Finetuned_Llama_70b_Axolotl\", create_if_missing=True)\n\n# Mount the volume at /data in all containers\nVOLUME_CONFIG = {\n    \"/data\": exp_volume,\n}\n\n# Load secrets\nhuggingface_secret = Secret.from_name(\"secrets-hf-wandb\")\n</code></pre> <p>What's happening: - Volume: All our data lives here - preprocessed datasets, checkpoints, final models. Persists across runs. - Secrets: Injected as environment variables in our containers. Clean and secure.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#configuration-constants","title":"Configuration Constants","text":"<pre><code># Time constants\nHOURS = 60 * 60  # Makes timeouts readable\n\n# GPU Configuration\nGPU_TYPE = \"a100-80gb\"  # Can be: a100-80gb, a100-40gb, l40s, etc.\n\n# Training Configuration\nWANDB_PROJECT_DEFAULT = \"Llama-70b-MultiGPU-finetune\"\n</code></pre> <p>GPU type considerations:</p> <p>For 8B models: - L40S works great (~\\(1/hr) - A100-40GB for comfort (\\)2.50/hr)</p> <p>For 8-70B models: - A100-80GB is required (\\(3.50/hr per GPU) - You'll need 4-8 of them (\\)14-28/hr total)</p> <p>For 405B models: - 8\u00d7 A100-80GB minimum ($28/hr) - Or use H100s, B100s</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#building-the-axolotl-image","title":"Building the Axolotl Image","text":"<p>This is where things get interesting. We need a container with CUDA, PyTorch, Axolotl, DeepSpeed, Flash Attention... the works.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#cuda-base-configuration","title":"CUDA Base Configuration","text":"<pre><code>CUDA_VERSION = \"12.8.1\"\nCUDA_FLAVOR = \"devel\"\nCUDA_OS = \"ubuntu24.04\"\nCUDA_TAG = f\"{CUDA_VERSION}-{CUDA_FLAVOR}-{CUDA_OS}\"\n</code></pre> <p>Why \"devel\"?</p> <p>Flash Attention (which makes training way faster) needs to compile CUDA code during installation. The <code>runtime</code> image doesn't include the CUDA compiler (<code>nvcc</code>), so you'll get cryptic errors.</p> <p>The <code>devel</code> image includes the full CUDA toolkit. It's bigger, but it Just Works\u2122.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#complete-image-definition","title":"Complete Image Definition","text":"<pre><code>AXOLOTL_IMAGE = (\n    # Start with NVIDIA's official CUDA image\n    ModalImage.from_registry(f\"nvidia/cuda:{CUDA_TAG}\", add_python=\"3.12\")\n\n    # Install system dependencies\n    .apt_install(\"git\", \"build-essential\")\n\n    # Install PyTorch first\n    .uv_pip_install([\n        \"torch\",\n        \"torchvision\",\n        \"torchaudio\",\n    ])\n\n    # Install base dependencies for Axolotl\n    .run_commands(\n        \"uv pip install --no-deps -U packaging setuptools wheel ninja --system\"\n    )\n\n    # Install Axolotl with DeepSpeed support\n    .run_commands(\"uv pip install --no-build-isolation axolotl[deepspeed] --system\")\n\n    # Install Flash Attention (this takes a while to compile)\n    .run_commands(\n        \"UV_NO_BUILD_ISOLATION=1 uv pip install flash-attn --no-build-isolation --system\"\n    )\n\n    # Set environment variables\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",  # Fast model downloads\n        \"HF_HOME\": \"/data/.cache\",          # Cache in volume\n    })\n)\n</code></pre> <p>Key points:</p> <ol> <li> <p>Installation order matters: Base deps \u2192 Axolotl \u2192 Flash Attention. Do it wrong and you'll have dependency hell.</p> </li> <li> <p><code>--no-build-isolation</code>: Required for Flash Attention. Don't ask me why, it's just how flash-attn works.</p> </li> <li> <p><code>HF_HUB_ENABLE_HF_TRANSFER</code>: Enables parallel downloads from HuggingFace. Can be 5-10x faster for large models.</p> </li> <li> <p>Cache in volume: All HuggingFace downloads go to <code>/data/.cache</code>, which persists. Download once, use forever.</p> </li> </ol> <p>\u23f0 Build time warning: The first build takes 20-30 minutes because Flash Attention compiles from source. It's compiling optimized CUDA kernels for your GPU architecture. Go grab lunch. But here's the magic - Modal caches this image. Every subsequent run? Instant startup.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#alternative-official-axolotl-image","title":"Alternative: Official Axolotl Image","text":"<p>Axolotl provides an official Docker image, but I don't use it:</p> <pre><code># This works, but I don't recommend it:\n# AXOLOTL_IMAGE = ModalImage.from_registry(\n#     \"axolotlai/axolotl-cloud:main-latest\", add_python=\"3.12\"\n# ).env({\n#     \"JUPYTER_ENABLE_LAB\": \"no\",\n#     \"JUPYTER_TOKEN\": \"\",\n#     \"HF_HOME\": \"/data/.cache\",\n# })\n</code></pre> <p>Why custom image? - Official image auto-starts JupyterLab, which we don't need on Modal - Custom image is lighter and more predictable - Full control over versions (important for reproducibility)</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#training-configuration-with-yaml","title":"Training Configuration with YAML","text":"<p>Here's where Axolotl really shines. All your configuration goes in a YAML file. No scattered parameters, no magic constants buried in code. Everything in one place.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#complete-training-config","title":"Complete Training Config","text":"<pre><code>TRAIN_CONFIG_YAML = f\"\"\"\nbase_model: NousResearch/Meta-Llama-3-8B-Instruct\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\n# Optional: Automatically upload to HuggingFace Hub\n# hub_model_id: username/custom_model_name\n\nload_in_8bit: true\nload_in_4bit: false\n\nchat_template: llama3\ndatasets:\n  - path: fozziethebeat/alpaca_messages_2k_test\n    type: chat_template\n\ndataset_prepared_path: /data/prepared_datasets/alpaca_2k\nval_set_size: 0.05\noutput_dir: /data/outputs/lora-out\n\nsequence_len: 4096\nsample_packing: false\n\n# LoRA Configuration\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\n\n# Weights &amp; Biases\nwandb_project: {WANDB_PROJECT_DEFAULT}\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\n# Training Hyperparameters\ngradient_accumulation_steps: 4\nmicro_batch_size: 8\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\nbf16: auto\ntf32: false\n\n# Memory Optimizations\ngradient_checkpointing: true\nresume_from_checkpoint:\nlogging_steps: 1\nflash_attention: true\n\nwarmup_ratio: 0.1\nevals_per_epoch: 4\nsaves_per_epoch: 4\nweight_decay: 0.0\n\nspecial_tokens:\n   pad_token: &lt;|end_of_text|&gt;\n\"\"\"\n</code></pre> <p>Let me break down the important parts:</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#model-settings","title":"Model Settings","text":"<pre><code>base_model: NousResearch/Meta-Llama-3-8B-Instruct\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n</code></pre> <p>This is configured for Llama 3-8B (for testing). When you're ready for the real deal:</p> <pre><code>base_model: meta-llama/Meta-Llama-3-8-70B-Instruct\n</code></pre> <p>For other models: </p><pre><code># Mistral 7B\nbase_model: mistralai/Mistral-7B-Instruct-v0.2\nmodel_type: MistralForCausalLM\n\n# Qwen 72B\nbase_model: Qwen/Qwen2.5-72B-Instruct\nmodel_type: Qwen2ForCausalLM\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#quantization-settings","title":"Quantization Settings","text":"<pre><code>load_in_8bit: true   # Reduces memory by ~50%\nload_in_4bit: false  # Reduces memory by ~75%\n</code></pre> <p>My recommendations: - 8B models: <code>load_in_8bit: false</code> (use full precision, you have the memory) - 8-70B models: <code>load_in_8bit: true</code> (essential to fit on 4 GPUs) - 405B models: <code>load_in_4bit: true</code> (required, even with 8 GPUs)</p> <p>The quality hit from 8-bit is minimal. The quality hit from 4-bit is noticeable but acceptable.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#dataset-configuration","title":"Dataset Configuration","text":"<pre><code>datasets:\n  - path: fozziethebeat/alpaca_messages_2k_test\n    type: chat_template\n\ndataset_prepared_path: /data/prepared_datasets/alpaca_2k\nval_set_size: 0.05  # 5% for validation\n</code></pre> <p>Multiple datasets: </p><pre><code>datasets:\n  - path: dataset1/name\n    type: chat_template\n  - path: dataset2/name\n    type: alpaca\n    split: train\n</code></pre> <p>Axolotl supports many formats: <code>chat_template</code>, <code>alpaca</code>, <code>sharegpt</code>, <code>completion</code>, etc. Check the Axolotl docs for the full list.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#lora-parameters","title":"LoRA Parameters","text":"<pre><code>adapter: lora\nlora_r: 32           # Rank (higher = more capacity, slower training)\nlora_alpha: 16       # Scaling factor (usually r/2 or r)\nlora_dropout: 0.05   # Regularization\nlora_target_linear: true  # Apply to all linear layers\n</code></pre> <p>LoRA rank guidelines:</p> <p>For 8B models: </p><pre><code>lora_r: 32\nlora_alpha: 64\n</code></pre> <p>For 8-70B models (high quality): </p><pre><code>lora_r: 64\nlora_alpha: 128\n</code></pre> <p>For faster training: </p><pre><code>lora_r: 16\nlora_alpha: 32\n</code></pre> <p>Higher rank = more capacity to learn, but slower training and larger adapters.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#training-hyperparameters","title":"Training Hyperparameters","text":"<pre><code>micro_batch_size: 8              # Batch size per GPU\ngradient_accumulation_steps: 4   # Steps before updating weights\nnum_epochs: 4\nlearning_rate: 0.0002\noptimizer: adamw_bnb_8bit        # 8-bit Adam (saves memory!)\nlr_scheduler: cosine\n</code></pre> <p>Effective batch size calculation: </p><pre><code>Effective Batch = micro_batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus\n                = 8 \u00d7 4 \u00d7 4\n                = 128\n</code></pre> <p>With 4 GPUs, each does batch size 8, accumulates over 4 steps, so effectively you're training with batch size 128.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#memory-optimizations","title":"Memory Optimizations","text":"<pre><code>gradient_checkpointing: true  # Trade compute for memory (essential!)\nflash_attention: true         # Faster, more memory-efficient attention\nbf16: auto                    # Use bfloat16 if GPU supports it\n</code></pre> <p>Gradient checkpointing is critical for large models. It recomputes activations during backward pass instead of storing them. Uses ~40% less memory at the cost of ~20% slower training. Totally worth it.</p> <p>Flash Attention is a must-have. It's an optimized attention implementation that's both faster AND uses less memory. Win-win.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#helper-function-write-config-to-volume","title":"Helper Function: Write Config to Volume","text":"<p>Before we get to the training stages, here's a helper function that all stages use:</p> <pre><code>def write_config_to_volume(\n    train_config_yaml: str,\n    config_path: str = \"/data/config.yml\",\n    update_paths: bool = True,\n) -&gt; dict:\n    \"\"\"Write YAML configuration to volume with optional path updates.\"\"\"\n    import os\n    import yaml\n\n    # Parse YAML string into dict\n    config_dict = yaml.safe_load(train_config_yaml)\n\n    # Update paths to use volume instead of local dirs\n    if update_paths and \"output_dir\" in config_dict:\n        config_dict[\"output_dir\"] = config_dict[\"output_dir\"].replace(\n            \"./outputs\", \"/data/outputs\"\n        )\n\n    # Ensure directory exists\n    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n\n    # Write to volume\n    with open(config_path, \"w\") as f:\n        yaml.dump(config_dict, f, default_flow_style=False)\n\n    # Commit so it persists\n    exp_volume.commit()\n\n    return config_dict\n</code></pre> <p>What it does: 1. Converts YAML string to dict (for inspection) 2. Updates paths to use <code>/data</code> volume (so outputs persist) 3. Writes config to volume 4. Commits volume (critical!)</p> <p>This keeps our config in one place and ensures all stages use the same configuration.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#stage-1-dataset-preprocessing","title":"Stage 1: Dataset Preprocessing","text":"<p>Alright, let's get to the actual pipeline. First stage: preprocessing.</p> <pre><code># GPU Configuration for preprocessing (single GPU is fine)\nPREPROCESS_NUM_GPUS = 1\nPREPROCESS_GPU_CONFIG = f\"{GPU_TYPE}:{PREPROCESS_NUM_GPUS}\"\n\n@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=24 * HOURS,\n    gpu=PREPROCESS_GPU_CONFIG,  # Just 1 GPU\n)\ndef process_datasets(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n):\n    \"\"\"Preprocess and tokenize dataset before training using Axolotl.\"\"\"\n    import os\n    import subprocess\n\n    # Set HF token\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Write config to volume\n    config_dict = write_config_to_volume(train_config_yaml, config_path, True)\n    exp_volume.commit()\n\n    print(\"Starting dataset preprocessing...\")\n\n    try:\n        # Run Axolotl preprocessing\n        subprocess.run([\"axolotl\", \"preprocess\", config_path], check=True)\n        print(\"\u2713 Preprocessing completed\")\n\n        # Commit preprocessed data\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"config_path\": config_path,\n            \"preprocessed_data_path\": config_dict.get(\"dataset_prepared_path\"),\n            \"output_dir\": config_dict.get(\"output_dir\"),\n        }\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Preprocessing failed: {e}\")\n</code></pre> <p>What happens during preprocessing:</p> <ol> <li>Download dataset from HuggingFace (or load from cache)</li> <li>Apply chat template to format conversations correctly</li> <li>Tokenize everything using the model's tokenizer</li> <li>Save to disk at <code>dataset_prepared_path</code></li> <li>Split train/val based on <code>val_set_size</code></li> </ol> <p>Why preprocess separately?</p> <p>You might think \"why not just preprocess during training?\" Here's why this is better:</p> <ol> <li> <p>Cost savings: Preprocessing doesn't need 4 GPUs. Why pay $14/hr when you can pay $3.50/hr?</p> </li> <li> <p>Reusability: Preprocess once, train multiple times with different hyperparameters. Huge time saver when experimenting.</p> </li> <li> <p>Debugging: If preprocessing fails, you know immediately. Not after 3 hours of training setup.</p> </li> <li> <p>Visibility: You can inspect the preprocessed data to make sure it looks right.</p> </li> </ol> <p>Run it:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::process_datasets\n</code></pre> <p>First run downloads the dataset and tokenizes everything. Takes 10-30 minutes depending on dataset size. Subsequent runs? Instant, because it's cached.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#stage-2-multi-gpu-training","title":"Stage 2: Multi-GPU Training","text":"<p>Here's where the magic happens. We're training Llama across multiple GPUs with Accelerate.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#gpu-configuration","title":"GPU Configuration","text":"<pre><code># GPU Configuration for training (2-8 GPUs)\nTRAIN_NUM_GPUS = 4  # Can be adjusted from 2 to 8\nTRAIN_GPU_CONFIG = f\"{GPU_TYPE}:{TRAIN_NUM_GPUS}\"\n</code></pre> <p>Scaling guidelines:</p> Model Min GPUs Recommended GPU Type Cost/hr Llama 3-8B 1 1 A100-40GB $2.50 Llama 3-13B 1 2 A100-40GB $5.00 Llama 3-8-70B 2 4 A100-80GB $14.00 Llama 3-405B 8 8 A100-80GB $28.00"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#training-function","title":"Training Function","text":"<pre><code>@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=24 * HOURS,\n    gpu=TRAIN_GPU_CONFIG,  # e.g., \"a100-80gb:4\"\n)\ndef train_model(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n):\n    \"\"\"\n    Train or fine-tune a model using Axolotl with multi-GPU support.\n    Uses accelerate for multi-GPU training.\n    \"\"\"\n    import os\n    import subprocess\n\n    # Set up environment\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n    os.environ[\"WANDB_API_KEY\"] = os.environ[\"WANDB_API_KEY\"]\n    os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_DEFAULT\n\n    # Write config to volume\n    config_dict = write_config_to_volume(\n        train_config_yaml=train_config_yaml,\n        config_path=config_path,\n        update_paths=True,\n    )\n\n    exp_volume.commit()\n\n    # Build accelerate command for multi-GPU training\n    print(f\"Starting training with {TRAIN_NUM_GPUS} GPUs...\")\n\n    cmd = [\n        \"accelerate\",\n        \"launch\",\n        \"--multi_gpu\",                        # Enable multi-GPU mode\n        \"--num_processes\", str(TRAIN_NUM_GPUS),  # Number of GPUs\n        \"--num_machines\", \"1\",                # Single machine (Modal handles this)\n        \"--mixed_precision\", \"bf16\",          # Use bfloat16 for speed\n        \"--dynamo_backend\", \"no\",             # Disable torch.compile (stability)\n        \"-m\", \"axolotl.cli.train\",            # Run Axolotl training\n        config_path,                          # Path to our YAML config\n    ]\n\n    try:\n        subprocess.run(cmd, check=True)\n        print(\"\u2713 Training completed\")\n\n        # Commit trained model to volume\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"config_path\": config_path,\n            \"output_dir\": config_dict.get(\"output_dir\"),\n            \"base_model\": config_dict.get(\"base_model\"),\n            \"num_gpus\": TRAIN_NUM_GPUS,\n        }\n\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Training failed: {e}\")\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#understanding-the-accelerate-command","title":"Understanding the Accelerate Command","text":"<p>Let me break down what this command does:</p> <pre><code>accelerate launch \\\n  --multi_gpu \\                     # Enable multi-GPU distributed training\n  --num_processes 4 \\               # Use 4 GPUs (one process per GPU)\n  --num_machines 1 \\                # Single machine (Modal provides this)\n  --mixed_precision bf16 \\          # Use bfloat16 for memory and speed\n  --dynamo_backend no \\             # Disable torch.compile (causes issues)\n  -m axolotl.cli.train \\            # Run Axolotl's training module\n  /data/config.yml                  # Our YAML configuration\n</code></pre> <p>What Accelerate does for you:</p> <ol> <li>Spawns one process per GPU - Each GPU gets its own Python process</li> <li>Initializes distributed backend - Sets up NCCL for GPU communication</li> <li>Shards the model - Splits model parameters across GPUs based on available memory</li> <li>Synchronizes gradients - All-reduce operation after backward pass</li> <li>Manages checkpointing - Handles saving/loading distributed checkpoints</li> </ol> <p>You write normal PyTorch code (which Axolotl already did), Accelerate makes it distributed. It's magical.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#run-training","title":"Run Training","text":"<p>Basic run (test with 8B model first):</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::train_model\n</code></pre> <p>For actual 8-70B training, edit the YAML:</p> <pre><code>TRAIN_CONFIG_YAML = f\"\"\"\nbase_model: meta-llama/Meta-Llama-3-8-70B-Instruct\nload_in_8bit: true\nlora_r: 64\nlora_alpha: 128\nmicro_batch_size: 4\ngradient_accumulation_steps: 4\nnum_epochs: 3\n# ... rest of config\n\"\"\"\n</code></pre> <p>Then run:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::train_model\n</code></pre> <p>Monitor your training:</p> <ol> <li>Modal Dashboard: Click the URL in the terminal for real-time logs and GPU utilization</li> <li>Weights &amp; Biases: Go to <code>wandb.ai/&lt;username&gt;/Llama-70b-MultiGPU-finetune</code> for beautiful charts</li> <li>Check GPU usage: All 4 GPUs should be near 100% utilization</li> </ol> <p>\ud83d\udcb0 Cost Alert: 4\u00d7 A100-80GB costs ~$14/hour. A 3-hour training run = $42. A 10-hour run = $140. This is why we preprocess separately and test with small models first!</p> <p>Training checkpoints:</p> <p>Axolotl automatically saves checkpoints to <code>/data/outputs/lora-out/checkpoint-{step}</code>. If training crashes, resume with:</p> <pre><code>resume_from_checkpoint: /data/outputs/lora-out/checkpoint-1000\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#stage-3-merge-lora-adapters","title":"Stage 3: Merge LoRA Adapters","text":"<p>After training, you have LoRA adapters (~100MB) that work with the base model. For deployment, it's easier to merge them into a single model.</p> <pre><code># GPU Configuration for merging (single GPU is fine)\nMERGE_NUM_GPUS = 1\nMERGE_GPU_CONFIG = f\"{GPU_TYPE}:{MERGE_NUM_GPUS}\"\n\n@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=4 * HOURS,\n    gpu=MERGE_GPU_CONFIG,\n)\ndef merge_lora(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n    lora_model_dir: str = None,\n):\n    \"\"\"Merge trained LoRA adapters into the base model.\"\"\"\n    import os\n    import subprocess\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Write config\n    config_dict = write_config_to_volume(\n        train_config_yaml=train_config_yaml,\n        config_path=config_path,\n        update_paths=True,\n    )\n\n    exp_volume.commit()\n\n    # Build merge command\n    print(\"Starting LoRA merge...\")\n    cmd = [\"axolotl\", \"merge-lora\", config_path]\n\n    if lora_model_dir:\n        cmd.extend([\"--lora-model-dir\", lora_model_dir])\n\n    try:\n        subprocess.run(cmd, check=True)\n        print(\"\u2713 LoRA merge completed\")\n\n        # Commit merged model\n        exp_volume.commit()\n\n        return {\n            \"status\": \"completed\",\n            \"config_path\": config_path,\n            \"output_dir\": config_dict.get(\"output_dir\"),\n            \"lora_model_dir\": lora_model_dir or config_dict.get(\"lora_model_dir\"),\n        }\n\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"LoRA merge failed: {e}\")\n</code></pre> <p>What merging does:</p> <ol> <li>Loads base model weights</li> <li>Loads LoRA adapter weights</li> <li>Applies the LoRA transformations to base model</li> <li>Saves the combined model</li> </ol> <p>Why only 1 GPU?</p> <p>Merging is sequential - you're just doing matrix operations to combine weights. Doesn't benefit from multiple GPUs. Save the money.</p> <p>Run it:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::merge_lora\n</code></pre> <p>Specify custom LoRA directory:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::merge_lora \\\n  --lora-model-dir=\"/data/outputs/lora-out\"\n</code></pre> <p>Merging takes 15-45 minutes depending on model size. For 8-70B, expect ~30 minutes.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#stage-4-inference","title":"Stage 4: Inference","text":"<p>Let's test your fine-tuned model!</p> <pre><code># GPU Configuration for inference (single GPU)\nINFERENCE_NUM_GPUS = 1\nINFERENCE_GPU_CONFIG = f\"{GPU_TYPE}:{INFERENCE_NUM_GPUS}\"\n\n@app.function(\n    image=AXOLOTL_IMAGE,\n    volumes=VOLUME_CONFIG,\n    secrets=[huggingface_secret],\n    timeout=1 * HOURS,\n    gpu=INFERENCE_GPU_CONFIG,\n)\ndef run_inference(\n    train_config_yaml: str = TRAIN_CONFIG_YAML,\n    config_path: str = \"/data/config.yml\",\n    prompt: str = \"Hello, how are you?\",\n    lora_model_dir: str = None,\n    base_model: str = None,\n):\n    \"\"\"Run inference using the trained model.\"\"\"\n    import os\n    import subprocess\n    import tempfile\n\n    os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n\n    # Write config\n    config_dict = write_config_to_volume(\n        train_config_yaml=train_config_yaml,\n        config_path=config_path,\n        update_paths=True,\n    )\n\n    print(\"Starting inference...\")\n    print(f\"Prompt: {prompt}\")\n    print(\"-\" * 80)\n\n    # Build inference command\n    cmd = [\"axolotl\", \"inference\", config_path]\n\n    if lora_model_dir:\n        cmd.extend([\"--lora-model-dir\", lora_model_dir])\n    if base_model:\n        cmd.extend([\"--base-model\", base_model])\n\n    # Write prompt to temp file and pipe it\n    try:\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".txt\") as f:\n            f.write(prompt)\n            prompt_file = f.name\n\n        # Run inference with prompt from file\n        with open(prompt_file, \"r\") as f:\n            result = subprocess.run(\n                cmd,\n                stdin=f,\n                capture_output=True,\n                text=True,\n                check=True,\n            )\n\n        print(\"\u2713 Inference completed\")\n        print(\"\\n\" + \"=\" * 80)\n        print(\"MODEL OUTPUT:\")\n        print(\"=\" * 80)\n        print(result.stdout)\n        print(\"=\" * 80)\n\n        if result.stderr:\n            print(\"\\nSTDERR:\")\n            print(result.stderr)\n\n        return {\n            \"status\": \"completed\",\n            \"prompt\": prompt,\n            \"output\": result.stdout,\n            \"model\": base_model or config_dict.get(\"base_model\"),\n        }\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error output: {e.stderr}\")\n        print(f\"Command output: {e.stdout}\")\n        raise RuntimeError(f\"Inference failed: {e}\")\n    finally:\n        # Clean up temp file\n        import os as os_module\n        if \"prompt_file\" in locals():\n            os_module.unlink(prompt_file)\n</code></pre> <p>Run inference:</p> <p>With LoRA adapters (before merging):</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::run_inference \\\n  --prompt=\"Explain quantum computing in simple terms.\" \\\n  --lora-model-dir=\"/data/outputs/lora-out\"\n</code></pre> <p>With merged model:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::run_inference \\\n  --prompt=\"Write a poem about machine learning.\" \\\n  --base-model=\"/data/outputs/lora-out-merged\"\n</code></pre> <p>Test multiple prompts:</p> <p>Create a Python script to batch test:</p> <pre><code>import modal\n\napp = modal.App.lookup(\"Finetuned_Llama_70b_Axolotl_MultiGPU\")\nrun_inference = modal.Function.lookup(app.name, \"run_inference\")\n\nprompts = [\n    \"Explain gradient descent.\",\n    \"Write Python code to implement binary search.\",\n    \"What are the benefits of transformer architecture?\",\n]\n\nfor prompt in prompts:\n    result = run_inference.remote(prompt=prompt)\n    print(f\"\\nPrompt: {prompt}\")\n    print(f\"Output: {result['output']}\\n\")\n    print(\"-\" * 80)\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Let me walk you through how I actually use this for a real project.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#1-customize-configuration","title":"1. Customize Configuration","text":"<p>First, I edit the YAML config in the script:</p> <pre><code>TRAIN_CONFIG_YAML = f\"\"\"\nbase_model: meta-llama/Meta-Llama-3-8-70B-Instruct\nmodel_type: LlamaForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: true\n\nchat_template: llama3\ndatasets:\n  - path: my-username/my-custom-dataset\n    type: chat_template\n\ndataset_prepared_path: /data/prepared_datasets/my_data\noutput_dir: /data/outputs/llama70b-custom\n\nsequence_len: 8192  # Longer context\nlora_r: 64\nlora_alpha: 128\n\ngradient_accumulation_steps: 2\nmicro_batch_size: 4\nnum_epochs: 3\nlearning_rate: 0.0001\n\nwandb_project: {WANDB_PROJECT_DEFAULT}\n\n# Everything else stays the same\ngradient_checkpointing: true\nflash_attention: true\nbf16: auto\n\"\"\"\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#2-preprocess-dataset","title":"2. Preprocess Dataset","text":"<pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::process_datasets\n</code></pre> <p>Expected time: 30 min - 2 hours (depends on dataset size) Cost: ~$2-5 (1 GPU for preprocessing)</p> <p>Grab a coffee while this runs.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#3-test-training-small-sanity-check","title":"3. Test Training (Small Sanity Check)","text":"<p>Before burning $100 on a full training run, I test with 1 epoch:</p> <pre><code># Temporarily edit YAML:\nnum_epochs: 1\n</code></pre> <p>Then:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::train_model\n</code></pre> <p>Expected time: 30-60 minutes Cost: ~$7-14</p> <p>This catches configuration errors, memory issues, etc. If this works, the full run will work.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#4-full-training","title":"4. Full Training","text":"<p>Restore the full config and run:</p> <pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::train_model\n</code></pre> <p>Expected time: 3-10 hours (depends on dataset size) Cost: $40-150</p> <p>Now you wait. Monitor W&amp;B to make sure loss is going down. Check Modal dashboard to verify all GPUs are utilized.</p> <p>Go touch grass. This is running on Modal's infrastructure, not your machine.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#5-merge-lora","title":"5. Merge LoRA","text":"<pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::merge_lora\n</code></pre> <p>Expected time: 30-45 minutes Cost: ~$2-3</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#6-test-inference","title":"6. Test Inference","text":"<pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::run_inference \\\n  --prompt=\"Test my fine-tuned 8-70B model with this prompt.\"\n</code></pre> <p>Expected time: 1-2 minutes Cost: ~$0.10</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#7-push-to-hub-optional","title":"7. Push to Hub (Optional)","text":"<p>Want to share your model? Add this to the YAML config:</p> <pre><code>hub_model_id: your-username/llama-70b-custom-finetuned\n</code></pre> <p>Axolotl automatically pushes to HuggingFace Hub during training.</p> <p>Total cost for full pipeline: $50-200 depending on dataset size and number of epochs.</p> <p>Total time: 1 day (mostly waiting for training)</p> <p>Compare this to managing your own 4\u00d7 A100-80GB cluster... yeah, Modal wins.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#advanced-tips-and-tricks","title":"Advanced Tips and Tricks","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#multi-dataset-training","title":"Multi-Dataset Training","text":"<p>Train on multiple datasets simultaneously:</p> <pre><code>datasets:\n  - path: dataset1/name\n    type: chat_template\n  - path: dataset2/name\n    type: alpaca\n  - path: dataset3/name\n    type: sharegpt\n</code></pre> <p>Axolotl combines them automatically.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#checkpoint-management","title":"Checkpoint Management","text":"<pre><code>saves_per_epoch: 4              # Save 4 times per epoch\nsave_total_limit: 10            # Keep only 10 most recent checkpoints\nresume_from_checkpoint: /data/outputs/lora-out/checkpoint-1000\n</code></pre> <p>Pro tip: If training crashes or you want to tweak learning rate halfway through, just resume from a checkpoint.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#custom-evaluation","title":"Custom Evaluation","text":"<pre><code>val_set_size: 0.1           # 10% for validation\nevals_per_epoch: 10         # Evaluate 10 times per epoch\n</code></pre> <p>Watch validation loss in W&amp;B to catch overfitting.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#optimizer-tuning","title":"Optimizer Tuning","text":"<pre><code>optimizer: adamw_bnb_8bit      # Options: adamw_torch, adamw_bnb_8bit, adafactor\nlr_scheduler: cosine           # Options: linear, cosine, constant\nwarmup_ratio: 0.1\nweight_decay: 0.01\nmax_grad_norm: 1.0\n</code></pre> <p>I usually stick with <code>adamw_bnb_8bit</code> (saves memory) and <code>cosine</code> scheduler (smooth learning rate decay).</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#hyperparameter-tuning-guide","title":"Hyperparameter Tuning Guide","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#for-llama-3-8b-single-gpu","title":"For Llama 3-8B (Single GPU)","text":"<pre><code>micro_batch_size: 16\ngradient_accumulation_steps: 2\nlearning_rate: 0.0003\nlora_r: 32\nlora_alpha: 64\nnum_epochs: 3\nload_in_8bit: false  # Can use full precision\n</code></pre> <p>GPU: 1\u00d7 A100-40GB Cost: ~$2.50/hr Training time: 2-4 hours</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#for-llama-3-8-70b-multi-gpu","title":"For Llama 3-8-70B (Multi-GPU)","text":"<pre><code>micro_batch_size: 4\ngradient_accumulation_steps: 4\nlearning_rate: 0.0001\nlora_r: 64\nlora_alpha: 128\nnum_epochs: 2-3\nload_in_8bit: true  # Essential\n</code></pre> <p>GPU: 4\u00d7 A100-80GB Cost: ~$14/hr Training time: 4-10 hours</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#for-maximum-speed","title":"For Maximum Speed","text":"<pre><code>flash_attention: true\nbf16: auto\ngradient_checkpointing: true\nsample_packing: true              # Pack multiple samples per sequence\npad_to_sequence_len: true\noptimizer: adamw_bnb_8bit\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#for-maximum-quality","title":"For Maximum Quality","text":"<pre><code>learning_rate: 0.00005            # Lower LR = more stable\nnum_epochs: 5                     # More epochs\nwarmup_ratio: 0.2                 # Longer warmup\nlora_r: 128                       # Higher capacity\nlora_alpha: 256\nmicro_batch_size: 2               # Smaller batches if you have memory\n</code></pre> <p>Quality vs. Speed is always a tradeoff. Experiment!</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#out-of-memory-during-training","title":"\"Out of Memory\" During Training","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solutions (in order of preference):</p> <ol> <li> <p>Reduce batch size: </p><pre><code>micro_batch_size: 2\ngradient_accumulation_steps: 8  # Keep effective batch size the same\n</code></pre> </li> <li> <p>Enable gradient checkpointing (if not already):    </p><pre><code>gradient_checkpointing: true\n</code></pre> </li> <li> <p>Use quantization: </p><pre><code>load_in_8bit: true\n</code></pre> </li> <li> <p>Add more GPUs: </p><pre><code>TRAIN_NUM_GPUS = 8\n</code></pre> </li> <li> <p>Reduce sequence length: </p><pre><code>sequence_len: 2048\n</code></pre> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#training-loss-not-decreasing","title":"Training Loss Not Decreasing","text":"<p>Symptoms: Loss stays flat or increases</p> <p>Solutions:</p> <ol> <li> <p>Check your learning rate - might be too low:    </p><pre><code>learning_rate: 0.0003\n</code></pre> </li> <li> <p>Verify data quality:</p> </li> <li>Load a few samples from preprocessed data</li> <li> <p>Make sure they look correct</p> </li> <li> <p>Increase LoRA rank: </p><pre><code>lora_r: 64\nlora_alpha: 128\n</code></pre> </li> <li> <p>Train longer: </p><pre><code>num_epochs: 5\n</code></pre> </li> <li> <p>Check for data leakage - validation set might be in training set</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#preprocessing-hangs-or-fails","title":"Preprocessing Hangs or Fails","text":"<p>Solution:</p> <p>Test dataset access locally first:</p> <pre><code>from datasets import load_dataset\ndataset = load_dataset(\"your/dataset\", split=\"train\")\nprint(f\"Loaded {len(dataset)} samples\")\nprint(dataset[0])\n</code></pre> <p>If that works, ensure your HF token is in Modal secrets.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#multi-gpu-not-working","title":"Multi-GPU Not Working","text":"<p>Symptoms: Training only uses 1 GPU</p> <p>Debug steps:</p> <ol> <li> <p>Verify GPU count: </p><pre><code>TRAIN_NUM_GPUS = 4  # Check this!\n</code></pre> </li> <li> <p>Check GPU utilization in Modal dashboard - should see all 4 GPUs active</p> </li> <li> <p>Add debug prints: </p><pre><code>import torch\nprint(f\"GPUs available: {torch.cuda.device_count()}\")\n</code></pre> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#secret-not-found","title":"\"Secret not found\"","text":"<p>Error: <code>Modal Secret \"secrets-hf-wandb\" not found</code></p> <p>Solution:</p> <pre><code>modal secret create secrets-hf-wandb \\\n  HUGGINGFACE_TOKEN=hf_xxx \\\n  WANDB_API_KEY=xxx\n</code></pre> <p>Or update the script to use your secret name:</p> <pre><code>huggingface_secret = Secret.from_name(\"my-secret-name\")\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#1-always-preprocess-separately","title":"1. Always Preprocess Separately","text":"<p>Bad (expensive): </p><pre><code># This runs preprocessing on 4 GPUs!\nmodal run script.py::train_model\n</code></pre> <p>Good (cheap): </p><pre><code># Preprocess on 1 GPU\nmodal run script.py::process_datasets  # $3.50/hr\n\n# Train on 4 GPUs\nmodal run script.py::train_model      # $14/hr\n</code></pre> <p>Savings: ~$10/hr during preprocessing (which can take 1-2 hours)</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#2-test-with-smaller-models-first","title":"2. Test with Smaller Models First","text":"<pre><code># Test with 8B first\nbase_model: NousResearch/Meta-Llama-3-8B-Instruct\nTRAIN_NUM_GPUS = 1\n\n# Then scale to 8-70B\nbase_model: meta-llama/Meta-Llama-3-8-70B-Instruct\nTRAIN_NUM_GPUS = 4\n</code></pre> <p>Catch bugs on the cheap model, then run the expensive one.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#3-use-smaller-gpus-for-testing","title":"3. Use Smaller GPUs for Testing","text":"<pre><code># Testing phase\nGPU_TYPE = \"l40s\"  # ~$1/hr\n\n# Production phase\nGPU_TYPE = \"a100-80gb\"  # ~$3.50/hr\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#4-limit-test-datasets","title":"4. Limit Test Datasets","text":"<p>For testing, use a small subset:</p> <pre><code>datasets:\n  - path: dataset/name\n    type: chat_template\n    num_samples: 100  # Just 100 samples for testing\n</code></pre> <p>Or edit the dataset on HuggingFace to create a \"mini\" version.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#5-smart-checkpointing","title":"5. Smart Checkpointing","text":"<pre><code>saves_per_epoch: 2           # Don't checkpoint too often\nsave_total_limit: 5          # Delete old checkpoints\n</code></pre> <p>Volume storage is free up to 50GB, but good practice for huge models.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#6-resume-from-checkpoint","title":"6. Resume from Checkpoint","text":"<p>If training fails or you want to try different hyperparameters:</p> <pre><code>resume_from_checkpoint: /data/outputs/lora-out/checkpoint-1000\n</code></pre> <p>Don't start from scratch! You've already paid for those first 1000 steps.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#real-time-logs","title":"Real-time Logs","text":"<pre><code>modal run FinetuneLlamaAxolotlGPUModal.py::train_model\n# Click the URL in output to open Modal dashboard\n</code></pre> <p>Dashboard shows: - Real-time logs (stdout/stderr) - GPU utilization per GPU (should be ~95-100%) - Memory usage per GPU - Cost accumulation ($$$ ticking up)</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#weights-biases","title":"Weights &amp; Biases","text":"<p>Go to <code>wandb.ai/&lt;username&gt;/Llama-70b-MultiGPU-finetune</code></p> <p>Charts to watch: - Training loss - should decrease smoothly - Validation loss - should decrease, but slower than training - Learning rate - should follow the schedule (cosine decay) - GPU utilization - should be high</p> <p>If training loss decreases but validation loss increases: You're overfitting. Reduce epochs or add regularization.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#check-preprocessed-data","title":"Check Preprocessed Data","text":"<pre><code>modal volume ls Finetuned_Llama_70b_Axolotl /data/prepared_datasets\n</code></pre> <p>Lists preprocessed files. You can download and inspect:</p> <pre><code>modal volume get Finetuned_Llama_70b_Axolotl \\\n  /data/prepared_datasets/alpaca_2k \\\n  ./local_data\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#download-checkpoints","title":"Download Checkpoints","text":"<pre><code># List checkpoints\nmodal volume ls Finetuned_Llama_70b_Axolotl /data/outputs/lora-out\n\n# Download specific checkpoint\nmodal volume get Finetuned_Llama_70b_Axolotl \\\n  /data/outputs/lora-out/checkpoint-1000 \\\n  ./local_checkpoint\n</code></pre> <p>Useful for local testing or pushing to HuggingFace manually.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#scaling-to-8-gpus-for-the-brave","title":"Scaling to 8 GPUs (For the Brave)","text":"<p>For massive models like Llama 3-405B or Mixtral 8\u00d722B, you need 8 GPUs.</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#update-configuration","title":"Update Configuration","text":"<pre><code>TRAIN_NUM_GPUS = 8\nGPU_TYPE = \"a100-80gb\"\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#update-yaml-for-memory","title":"Update YAML for Memory","text":"<pre><code>micro_batch_size: 2\ngradient_accumulation_steps: 2\nload_in_8bit: true  # Or even 4bit\n\n# Effective batch: 2 \u00d7 2 \u00d7 8 = 32\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#enable-deepspeed-zero-stage-3-optional","title":"Enable DeepSpeed ZeRO Stage 3 (Optional)","text":"<p>For maximum memory efficiency, create <code>deepspeed_zero3.json</code>:</p> <pre><code>{\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \"offload_optimizer\": {\"device\": \"cpu\"},\n    \"offload_param\": {\"device\": \"cpu\"}\n  },\n  \"fp16\": {\"enabled\": true},\n  \"train_micro_batch_size_per_gpu\": 2\n}\n</code></pre> <p>Add to YAML:</p> <pre><code>deepspeed: /path/to/deepspeed_zero3.json\n</code></pre> <p>DeepSpeed ZeRO Stage 3 shards everything - parameters, gradients, optimizer states - across GPUs. Even offloads to CPU when needed. Insanely memory efficient but adds communication overhead.</p> <p>Cost: 8\u00d7 A100-80GB = ~$28/hour</p> <p>A 10-hour training run = $280. Make sure you really need 8 GPUs!</p>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#whats-next","title":"What's Next?","text":"<p>You've built a production-grade multi-GPU training pipeline. Here's what you can do next:</p> <ol> <li> <p>Custom datasets: Format your own data for Axolotl (see Axolotl dataset docs)</p> </li> <li> <p>Advanced LoRA variants: Try QLoRA (4-bit quantized LoRA) or DoRA (decomposed LoRA)</p> </li> <li> <p>Full fine-tuning: Remove <code>adapter: lora</code> and train all parameters (requires way more memory)</p> </li> <li> <p>Evaluation: Add custom evaluation metrics beyond just loss</p> </li> <li> <p>Deployment: Serve your model with vLLM (see the Gemma tutorial for details)</p> </li> <li> <p>Multi-node training: Scale beyond 8 GPUs using Modal's multi-node support (advanced!)</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#resources","title":"Resources","text":"<ul> <li>Axolotl Documentation - Official docs with dataset formats and advanced configs</li> <li>Axolotl GitHub - Source code and issue tracker</li> <li>Axolotl Examples - Pre-built configs for tons of models</li> <li>Modal Documentation - Everything about Modal</li> <li>HuggingFace Accelerate - Deep dive into distributed training</li> <li>DeepSpeed - For ZeRO optimization details</li> </ul>"},{"location":"LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/#wrapping-up","title":"Wrapping Up","text":"<p>You just built what most companies call \"ML infrastructure\": - Multi-GPU distributed training - Automatic checkpointing and resumption - Experiment tracking with W&amp;B - Model versioning on HuggingFace - Cost-optimized preprocessing pipeline</p> <p>All in one Python file, running on Modal. No Kubernetes, no Docker nightmares, no spending weeks setting up infrastructure.</p> <p>The Unsloth tutorial showed you highly optimized single-GPU training. This tutorial showed you how to scale beyond that - training models that literally don't fit on a single GPU.</p> <p>This is how real ML teams train models nowadays. Not on local machines, not on hand-managed clusters. On serverless infrastructure like Modal, where you pay by the second and scale infinitely.</p> <p>The YAML configuration approach is especially powerful. Need to try different learning rates? Just edit one line and re-run. Want to train on a different dataset? Change one parameter. Everything is reproducible, everything is versioned, everything is clean.</p> <p>Got questions? Hit me up on Twitter @adithya_s_k!</p> <p>Now go train that 8-70B model and show the world what you built. \ud83d\ude80</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/","title":"TrainNanoGPTModal","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nSimple Modal script to run nanoGPT training on serverless GPUs.\n\nThis demonstrates how you can take a local repository (nanoGPT) and run it\non Modal with minimal changes - just copy the code into the image and run!\n\nUsage:\n    # Prepare Shakespeare dataset and train a small GPT\n    modal run FinetuneNanoGPT.py\n\n    # Or run individual steps:\n    modal run FinetuneNanoGPT.py::prepare_data\n    modal run FinetuneNanoGPT.py::train\n    modal run FinetuneNanoGPT.py::sample\n\"\"\"\n</pre> \"\"\" Simple Modal script to run nanoGPT training on serverless GPUs.  This demonstrates how you can take a local repository (nanoGPT) and run it on Modal with minimal changes - just copy the code into the image and run!  Usage:     # Prepare Shakespeare dataset and train a small GPT     modal run FinetuneNanoGPT.py      # Or run individual steps:     modal run FinetuneNanoGPT.py::prepare_data     modal run FinetuneNanoGPT.py::train     modal run FinetuneNanoGPT.py::sample \"\"\" In\u00a0[\u00a0]: Copied! <pre>from modal import App, Image as ModalImage, Volume\n</pre> from modal import App, Image as ModalImage, Volume In\u00a0[\u00a0]: Copied! <pre>HOURS = 60 * 60\nGPU_TYPE = \"a100-40gb\"  # Can be: a100-40gb, a100-80gb, l40s, t4, etc.\n</pre> HOURS = 60 * 60 GPU_TYPE = \"a100-40gb\"  # Can be: a100-40gb, a100-80gb, l40s, t4, etc. In\u00a0[\u00a0]: Copied! <pre>app = App(\"nanogpt-training\")\nvolume = Volume.from_name(\"nanogpt-outputs\", create_if_missing=True)\n</pre> app = App(\"nanogpt-training\") volume = Volume.from_name(\"nanogpt-outputs\", create_if_missing=True) In\u00a0[\u00a0]: Copied! <pre>VOLUME_CONFIG = {\n    \"/data\": volume,\n}\n</pre> VOLUME_CONFIG = {     \"/data\": volume, } In\u00a0[\u00a0]: Copied! <pre># Simple approach: copy the entire nanoGPT directory into the image\nNANOGPT_IMAGE = (\n    ModalImage.debian_slim(python_version=\"3.11\")\n    .pip_install(\n        \"torch\",\n        \"numpy\",\n        \"transformers\",\n        \"datasets\",\n        \"tiktoken\",\n        \"tqdm\",\n    )\n    # Copy the nanoGPT directory from local filesystem into the image\n    # copy=True because we have .workdir() after this\n    .add_local_dir(local_path=\"nanoGPT\", remote_path=\"/root/nanoGPT\", copy=True)\n    .workdir(\"/root/nanoGPT\")\n)\n</pre> # Simple approach: copy the entire nanoGPT directory into the image NANOGPT_IMAGE = (     ModalImage.debian_slim(python_version=\"3.11\")     .pip_install(         \"torch\",         \"numpy\",         \"transformers\",         \"datasets\",         \"tiktoken\",         \"tqdm\",     )     # Copy the nanoGPT directory from local filesystem into the image     # copy=True because we have .workdir() after this     .add_local_dir(local_path=\"nanoGPT\", remote_path=\"/root/nanoGPT\", copy=True)     .workdir(\"/root/nanoGPT\") ) In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOGPT_IMAGE,\n    timeout=10 * 60,  # 10 minutes\n)\ndef prepare_data():\n    \"\"\"\n    Prepare the Shakespeare dataset for character-level training.\n    This downloads the data and creates train.bin and val.bin files.\n    \"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(\"PREPARING SHAKESPEARE DATASET\")\n    print(\"=\" * 80)\n\n    # Run the prepare script\n    result = subprocess.run(\n        [\"python\", \"data/shakespeare_char/prepare.py\"], capture_output=True, text=True\n    )\n\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Data preparation failed with code {result.returncode}\")\n\n    print(\"\u2713 Data preparation completed!\")\n    return {\"status\": \"completed\", \"dataset\": \"shakespeare_char\"}\n</pre> @app.function(     image=NANOGPT_IMAGE,     timeout=10 * 60,  # 10 minutes ) def prepare_data():     \"\"\"     Prepare the Shakespeare dataset for character-level training.     This downloads the data and creates train.bin and val.bin files.     \"\"\"     import subprocess      print(\"=\" * 80)     print(\"PREPARING SHAKESPEARE DATASET\")     print(\"=\" * 80)      # Run the prepare script     result = subprocess.run(         [\"python\", \"data/shakespeare_char/prepare.py\"], capture_output=True, text=True     )      print(result.stdout)     if result.stderr:         print(\"STDERR:\", result.stderr)      if result.returncode != 0:         raise RuntimeError(f\"Data preparation failed with code {result.returncode}\")      print(\"\u2713 Data preparation completed!\")     return {\"status\": \"completed\", \"dataset\": \"shakespeare_char\"} In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOGPT_IMAGE,\n    gpu=GPU_TYPE,\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n)\ndef train(\n    max_iters: int = 1000,\n    eval_interval: int = 500,\n    batch_size: int = 64,\n    block_size: int = 256,\n    n_layer: int = 6,\n    n_head: int = 6,\n    n_embd: int = 384,\n    learning_rate: float = 1e-3,\n):\n    \"\"\"\n    Train a character-level GPT on Shakespeare data.\n\n    This runs the nanoGPT training script with customizable hyperparameters.\n    The trained model checkpoint will be saved to the Modal volume.\n\n    Args:\n        max_iters: Number of training iterations\n        eval_interval: How often to evaluate\n        batch_size: Batch size for training\n        block_size: Context length\n        n_layer: Number of transformer layers\n        n_head: Number of attention heads\n        n_embd: Embedding dimension\n        learning_rate: Learning rate\n    \"\"\"\n    import subprocess\n    import os\n\n    print(\"=\" * 80)\n    print(\"TRAINING NANOGPT ON SHAKESPEARE\")\n    print(\"=\" * 80)\n    print(f\"Max iterations: {max_iters}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Block size: {block_size}\")\n    print(f\"Layers: {n_layer}, Heads: {n_head}, Embedding: {n_embd}\")\n    print(\"=\" * 80)\n\n    # Make sure data is prepared\n    if not os.path.exists(\"data/shakespeare_char/train.bin\"):\n        print(\"Data not found, preparing it first...\")\n        prepare_data.local()\n\n    # Build training command with arguments\n    cmd = [\n        \"python\",\n        \"train.py\",\n        \"config/train_shakespeare_char.py\",\n        f\"--max_iters={max_iters}\",\n        f\"--eval_interval={eval_interval}\",\n        f\"--batch_size={batch_size}\",\n        f\"--block_size={block_size}\",\n        f\"--n_layer={n_layer}\",\n        f\"--n_head={n_head}\",\n        f\"--n_embd={n_embd}\",\n        f\"--learning_rate={learning_rate}\",\n        \"--out_dir=/data/out\",  # Save outputs to volume\n        \"--dataset=shakespeare_char\",  # Important: tells sample.py where to find meta.pkl\n        \"--compile=False\",  # Disable compilation for faster startup\n    ]\n\n    print(f\"Running: {' '.join(cmd)}\")\n    print()\n\n    # Run training\n    result = subprocess.run(cmd, capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Training failed with code {result.returncode}\")\n\n    # Copy meta.pkl to output directory for sampling\n    import shutil\n\n    meta_src = \"data/shakespeare_char/meta.pkl\"\n    meta_dst = \"/data/out/meta.pkl\"\n    if os.path.exists(meta_src):\n        shutil.copy(meta_src, meta_dst)\n        print(f\"\u2713 Copied {meta_src} to {meta_dst}\")\n\n    # Commit the volume to save the checkpoint\n    volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\u2713 Training completed! Model saved to /data/out\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"max_iters\": max_iters,\n        \"output_dir\": \"/data/out\",\n    }\n</pre> @app.function(     image=NANOGPT_IMAGE,     gpu=GPU_TYPE,     volumes=VOLUME_CONFIG,     timeout=2 * HOURS, ) def train(     max_iters: int = 1000,     eval_interval: int = 500,     batch_size: int = 64,     block_size: int = 256,     n_layer: int = 6,     n_head: int = 6,     n_embd: int = 384,     learning_rate: float = 1e-3, ):     \"\"\"     Train a character-level GPT on Shakespeare data.      This runs the nanoGPT training script with customizable hyperparameters.     The trained model checkpoint will be saved to the Modal volume.      Args:         max_iters: Number of training iterations         eval_interval: How often to evaluate         batch_size: Batch size for training         block_size: Context length         n_layer: Number of transformer layers         n_head: Number of attention heads         n_embd: Embedding dimension         learning_rate: Learning rate     \"\"\"     import subprocess     import os      print(\"=\" * 80)     print(\"TRAINING NANOGPT ON SHAKESPEARE\")     print(\"=\" * 80)     print(f\"Max iterations: {max_iters}\")     print(f\"Batch size: {batch_size}\")     print(f\"Block size: {block_size}\")     print(f\"Layers: {n_layer}, Heads: {n_head}, Embedding: {n_embd}\")     print(\"=\" * 80)      # Make sure data is prepared     if not os.path.exists(\"data/shakespeare_char/train.bin\"):         print(\"Data not found, preparing it first...\")         prepare_data.local()      # Build training command with arguments     cmd = [         \"python\",         \"train.py\",         \"config/train_shakespeare_char.py\",         f\"--max_iters={max_iters}\",         f\"--eval_interval={eval_interval}\",         f\"--batch_size={batch_size}\",         f\"--block_size={block_size}\",         f\"--n_layer={n_layer}\",         f\"--n_head={n_head}\",         f\"--n_embd={n_embd}\",         f\"--learning_rate={learning_rate}\",         \"--out_dir=/data/out\",  # Save outputs to volume         \"--dataset=shakespeare_char\",  # Important: tells sample.py where to find meta.pkl         \"--compile=False\",  # Disable compilation for faster startup     ]      print(f\"Running: {' '.join(cmd)}\")     print()      # Run training     result = subprocess.run(cmd, capture_output=False, text=True)      if result.returncode != 0:         raise RuntimeError(f\"Training failed with code {result.returncode}\")      # Copy meta.pkl to output directory for sampling     import shutil      meta_src = \"data/shakespeare_char/meta.pkl\"     meta_dst = \"/data/out/meta.pkl\"     if os.path.exists(meta_src):         shutil.copy(meta_src, meta_dst)         print(f\"\u2713 Copied {meta_src} to {meta_dst}\")      # Commit the volume to save the checkpoint     volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"\u2713 Training completed! Model saved to /data/out\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"max_iters\": max_iters,         \"output_dir\": \"/data/out\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOGPT_IMAGE,\n    gpu=GPU_TYPE,\n    volumes=VOLUME_CONFIG,\n    timeout=10 * 60,\n)\ndef sample(\n    num_samples: int = 5,\n    max_new_tokens: int = 500,\n    temperature: float = 0.8,\n    start: str = \"\\n\",\n):\n    \"\"\"\n    Generate text samples from the trained model.\n\n    Args:\n        num_samples: Number of samples to generate\n        max_new_tokens: Length of each sample\n        temperature: Sampling temperature (higher = more random)\n        start: Starting prompt for generation\n    \"\"\"\n    import subprocess\n    import os\n\n    os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n\n    print(\"=\" * 80)\n    print(\"GENERATING SAMPLES FROM TRAINED MODEL\")\n    print(\"=\" * 80)\n    print(f\"Num samples: {num_samples}\")\n    print(f\"Max tokens: {max_new_tokens}\")\n    print(f\"Temperature: {temperature}\")\n    print(f\"Start prompt: {repr(start)}\")\n    print(\"=\" * 80)\n\n    # Check if model files exist\n    if os.path.exists(\"/data/out/ckpt.pt\"):\n        print(\"\u2713 Found checkpoint: /data/out/ckpt.pt\")\n    else:\n        print(\"\u2717 Checkpoint not found: /data/out/ckpt.pt\")\n\n    if os.path.exists(\"/data/out/meta.pkl\"):\n        print(\"\u2713 Found meta file: /data/out/meta.pkl\")\n    else:\n        print(\"\u2717 Meta file not found: /data/out/meta.pkl\")\n        print(\"  Sampling will use GPT-2 encoding which will fail!\")\n\n    print()\n\n    # Ensure meta.pkl exists in the data directory for sample.py to find\n    # sample.py looks for meta.pkl in data/{dataset}/meta.pkl first, then falls back to out_dir\n    import shutil\n\n    os.makedirs(\"data/shakespeare_char\", exist_ok=True)\n\n    # Copy meta.pkl from volume to data directory if it exists\n    if os.path.exists(\"/data/out/meta.pkl\") and not os.path.exists(\n        \"data/shakespeare_char/meta.pkl\"\n    ):\n        shutil.copy(\"/data/out/meta.pkl\", \"data/shakespeare_char/meta.pkl\")\n        print(\"\u2713 Copied meta.pkl to data/shakespeare_char/\")\n\n    # Build sampling command\n    cmd = [\n        \"python\",\n        \"sample.py\",\n        \"--out_dir=/data/out\",  # Read model from volume\n        f\"--num_samples={num_samples}\",\n        f\"--max_new_tokens={max_new_tokens}\",\n        f\"--temperature={temperature}\",\n        f\"--start={start}\",\n        \"--compile=False\",\n    ]\n\n    print(f\"Running: {' '.join(cmd)}\")\n    print()\n\n    # Run sampling\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Sampling failed with code {result.returncode}\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\u2713 Sampling completed!\")\n    print(\"=\" * 80)\n\n    return {\"status\": \"completed\", \"samples\": result.stdout}\n</pre> @app.function(     image=NANOGPT_IMAGE,     gpu=GPU_TYPE,     volumes=VOLUME_CONFIG,     timeout=10 * 60, ) def sample(     num_samples: int = 5,     max_new_tokens: int = 500,     temperature: float = 0.8,     start: str = \"\\n\", ):     \"\"\"     Generate text samples from the trained model.      Args:         num_samples: Number of samples to generate         max_new_tokens: Length of each sample         temperature: Sampling temperature (higher = more random)         start: Starting prompt for generation     \"\"\"     import subprocess     import os      os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"      print(\"=\" * 80)     print(\"GENERATING SAMPLES FROM TRAINED MODEL\")     print(\"=\" * 80)     print(f\"Num samples: {num_samples}\")     print(f\"Max tokens: {max_new_tokens}\")     print(f\"Temperature: {temperature}\")     print(f\"Start prompt: {repr(start)}\")     print(\"=\" * 80)      # Check if model files exist     if os.path.exists(\"/data/out/ckpt.pt\"):         print(\"\u2713 Found checkpoint: /data/out/ckpt.pt\")     else:         print(\"\u2717 Checkpoint not found: /data/out/ckpt.pt\")      if os.path.exists(\"/data/out/meta.pkl\"):         print(\"\u2713 Found meta file: /data/out/meta.pkl\")     else:         print(\"\u2717 Meta file not found: /data/out/meta.pkl\")         print(\"  Sampling will use GPT-2 encoding which will fail!\")      print()      # Ensure meta.pkl exists in the data directory for sample.py to find     # sample.py looks for meta.pkl in data/{dataset}/meta.pkl first, then falls back to out_dir     import shutil      os.makedirs(\"data/shakespeare_char\", exist_ok=True)      # Copy meta.pkl from volume to data directory if it exists     if os.path.exists(\"/data/out/meta.pkl\") and not os.path.exists(         \"data/shakespeare_char/meta.pkl\"     ):         shutil.copy(\"/data/out/meta.pkl\", \"data/shakespeare_char/meta.pkl\")         print(\"\u2713 Copied meta.pkl to data/shakespeare_char/\")      # Build sampling command     cmd = [         \"python\",         \"sample.py\",         \"--out_dir=/data/out\",  # Read model from volume         f\"--num_samples={num_samples}\",         f\"--max_new_tokens={max_new_tokens}\",         f\"--temperature={temperature}\",         f\"--start={start}\",         \"--compile=False\",     ]      print(f\"Running: {' '.join(cmd)}\")     print()      # Run sampling     result = subprocess.run(cmd, capture_output=True, text=True)      print(result.stdout)     if result.stderr:         print(\"STDERR:\", result.stderr)      if result.returncode != 0:         raise RuntimeError(f\"Sampling failed with code {result.returncode}\")      print(\"\\n\" + \"=\" * 80)     print(\"\u2713 Sampling completed!\")     print(\"=\" * 80)      return {\"status\": \"completed\", \"samples\": result.stdout} In\u00a0[\u00a0]: Copied! <pre>@app.local_entrypoint()\ndef main():\n    \"\"\"Run the complete pipeline: prepare data -&gt; train -&gt; sample\"\"\"\n    print(\"\ud83d\ude80 Starting nanoGPT pipeline...\")\n\n    # Prepare data\n    print(\"\ud83d\udcc1 Preparing dataset...\")\n    prepare_data.remote()\n\n    # Train model\n    print(\"\ud83c\udfcb\ufe0f Training model...\")\n    train.remote(max_iters=1000, eval_interval=250, batch_size=64)\n\n    # Generate samples\n    print(\"\u2728 Generating samples...\")\n    sample.remote(num_samples=3, max_new_tokens=300)\n\n    print(\"\ud83c\udf89 Pipeline completed!\")\n</pre> @app.local_entrypoint() def main():     \"\"\"Run the complete pipeline: prepare data -&gt; train -&gt; sample\"\"\"     print(\"\ud83d\ude80 Starting nanoGPT pipeline...\")      # Prepare data     print(\"\ud83d\udcc1 Preparing dataset...\")     prepare_data.remote()      # Train model     print(\"\ud83c\udfcb\ufe0f Training model...\")     train.remote(max_iters=1000, eval_interval=250, batch_size=64)      # Generate samples     print(\"\u2728 Generating samples...\")     sample.remote(num_samples=3, max_new_tokens=300)      print(\"\ud83c\udf89 Pipeline completed!\")"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#configuration","title":"============================================================================= CONFIGURATION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#modal-app-and-volume-setup","title":"============================================================================= MODAL APP AND VOLUME SETUP\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#image-setup-copy-local-nanogpt-repo-into-the-image","title":"============================================================================= IMAGE SETUP - Copy local nanoGPT repo into the image\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#data-preparation-function","title":"============================================================================= DATA PREPARATION FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#training-function","title":"============================================================================= TRAINING FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#sampling-function","title":"============================================================================= SAMPLING FUNCTION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModal/#local-entrypoint-run-everything-in-sequence","title":"============================================================================= LOCAL ENTRYPOINT - Run everything in sequence\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/","title":"Training NanoGPT on Modal","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#training-nanogpt-on-modal-your-first-gpu-training-pipeline","title":"Training NanoGPT on Modal: Your First GPU Training Pipeline","text":"<p>\ud83d\udcc4 View Complete Python Script</p> <p>So you want to train a GPT model but don't want to deal with the headache of setting up infrastructure? Let me show you how I do it with Modal and nanoGPT.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#why-start-with-nanogpt","title":"Why Start with NanoGPT?","text":"<p>Here's the thing about nanoGPT - if you've ever wondered how GPT actually works under the hood, this is the best place to start. Andrej Karpathy built this as an educational implementation that's simple enough to understand but powerful enough to actually train real models.</p> <p>It's only ~300 lines of clean PyTorch code. No abstractions hiding what's really happening. Just pure, understandable transformer training.</p> <p>And honestly? It's become the go-to for anyone learning how to train language models from scratch. Plus, since it's just a regular Python repo, it's perfect for showing you how to take any existing codebase and run it on Modal's GPUs.</p> <p>Think of this as your \"Hello World\" for GPU training on Modal. Once you get this working, you'll know how to run pretty much anything on serverless GPUs.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#what-were-building","title":"What We're Building","text":"<p>We'll train a character-level GPT on Shakespeare's collected works (because why not make our model speak like the Bard?). The pipeline has three stages:</p> <ol> <li>Prep the data - Download and tokenize Shakespeare (runs on CPU, saves money)</li> <li>Train the model - Fire up a GPU and train our tiny GPT</li> <li>Generate text - Watch our model write Shakespeare-esque text</li> </ol> <p>The best part? You write all this code locally, and with one command, it runs on a beefy A100 GPU in the cloud. No SSH, no Docker, no infrastructure headaches.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#getting-set-up","title":"Getting Set Up","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#install-modal","title":"Install Modal","text":"<p>First things first:</p> <pre><code>pip install modal\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#authenticate","title":"Authenticate","text":"<p>Then authenticate (only need to do this once):</p> <pre><code>modal setup\n</code></pre> <p>This opens your browser and handles the OAuth flow. If you're running this in CI/CD or prefer API keys:</p> <pre><code>export MODAL_TOKEN_ID=&lt;your_token_id&gt;\nexport MODAL_TOKEN_SECRET=&lt;your_token_secret&gt;\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#clone-nanogpt","title":"Clone NanoGPT","text":"<p>Now grab nanoGPT. We need it locally because we're going to copy it into our Modal container:</p> <pre><code>cd /path/to/your/project\ngit clone https://github.com/karpathy/nanoGPT.git\n</code></pre> <p>Your folder should look like this:</p> <pre><code>ServerLessFinetuning/\n\u251c\u2500\u2500 TrainNanoGPTModal.py    # Your Modal script (we'll create this)\n\u2514\u2500\u2500 nanoGPT/                 # The cloned repo\n    \u251c\u2500\u2500 train.py\n    \u251c\u2500\u2500 sample.py\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 config/\n</code></pre> <p>Important: The Modal script needs to see the <code>nanoGPT/</code> folder in the same directory. When Modal builds your container image, it'll copy this entire directory into it.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#understanding-the-modal-script","title":"Understanding the Modal Script","text":"<p>Alright, let me walk you through how this works. I'll explain each piece and why it matters.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#app-and-volume-setup","title":"App and Volume Setup","text":"<pre><code>from modal import App, Image as ModalImage, Volume\n\n# Create the Modal app - this is your project's namespace\napp = App(\"nanogpt-training\")\n\n# Create or get existing volume for persistent storage\n# If \"nanogpt-outputs\" doesn't exist, Modal creates it for you\nvolume = Volume.from_name(\"nanogpt-outputs\", create_if_missing=True)\n\n# Define where to mount the volume in our containers\n# This dict maps container paths to Modal volumes\nVOLUME_CONFIG = {\n    \"/data\": volume,  # Mount 'volume' at /data inside the container\n}\n</code></pre> <p>So here's what's happening:</p> <ul> <li>App: Every Modal project needs an app. Think of it as your project container - all your functions live under this app.</li> <li>Volume: This is persistent storage that survives across runs. When your GPU instance shuts down (and it will, to save you money), you need somewhere to keep your model checkpoints. Volumes stick around even after your functions finish.</li> <li>VOLUME_CONFIG: This dict tells Modal \"hey, mount this volume at <code>/data</code> in my containers\". You can mount multiple volumes at different paths if you want.</li> </ul> <p>The cool thing about volumes? They persist across function calls. So when you train your model and save it to <code>/data/out</code>, you can load it later in a completely different function. It just works.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#the-container-image","title":"The Container Image","text":"<p>This is crucial - we need to tell Modal what our container should look like:</p> <pre><code>NANOGPT_IMAGE = (\n    # Start with a lightweight Debian base + Python 3.11\n    ModalImage.debian_slim(python_version=\"3.11\")\n\n    # Install all the Python packages nanoGPT needs\n    # Modal uses pip under the hood\n    .pip_install(\n        \"torch\",          # PyTorch for the model\n        \"numpy\",          # Numerical operations\n        \"transformers\",   # Hugging Face utilities\n        \"datasets\",       # For loading datasets\n        \"tiktoken\",       # OpenAI's tokenizer\n        \"tqdm\",          # Progress bars\n    )\n\n    # Copy your local nanoGPT directory into the container\n    # local_path: where it is on your machine\n    # remote_path: where it goes in the container\n    # copy=True: actually copy the files (vs just mounting)\n    .add_local_dir(local_path=\"nanoGPT\", remote_path=\"/root/nanoGPT\", copy=True)\n\n    # Set the working directory - all commands run from here\n    .workdir(\"/root/nanoGPT\")\n)\n</code></pre> <p>Let me break this down:</p> <ol> <li>Start with a minimal base - Debian slim keeps things lightweight and fast</li> <li>Install dependencies - Everything nanoGPT needs to run</li> <li>Copy your local code - This is the magic! <code>.add_local_dir()</code> takes the nanoGPT repo from your machine and bakes it into the container image</li> <li>Set working directory - So when we run <code>python train.py</code>, we're already in <code>/root/nanoGPT</code></li> </ol> <p>The first time Modal builds this, it'll take a few minutes (installing PyTorch takes time). But Modal caches the entire image, so every subsequent run is instant. You only rebuild when you change the image definition - like adding a new package or updating nanoGPT.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#the-three-stage-pipeline","title":"The Three-Stage Pipeline","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#stage-1-preparing-the-data","title":"Stage 1: Preparing the Data","text":"<pre><code>@app.function(\n    image=NANOGPT_IMAGE,      # Use the image we defined above\n    timeout=10 * 60,           # 10 minutes timeout (in seconds)\n    # Notice: No GPU specified! This runs on CPU to save money\n)\ndef prepare_data():\n    \"\"\"\n    Download and prep the Shakespeare dataset.\n    Creates train.bin and val.bin files.\n    \"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(\"PREPARING SHAKESPEARE DATASET\")\n    print(\"=\" * 80)\n\n    # Run nanoGPT's data preparation script\n    # This downloads Shakespeare text and tokenizes it\n    result = subprocess.run(\n        [\"python\", \"data/shakespeare_char/prepare.py\"],\n        capture_output=True,  # Capture output so we can print it\n        text=True             # Get output as string, not bytes\n    )\n\n    # Print what happened\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    # If the script failed, raise an error\n    if result.returncode != 0:\n        raise RuntimeError(f\"Data preparation failed with code {result.returncode}\")\n\n    print(\"\u2713 Data preparation completed!\")\n\n    # Return a dict with status info\n    # (Modal functions can return JSON-serializable data)\n    return {\"status\": \"completed\", \"dataset\": \"shakespeare_char\"}\n</code></pre> <p>Notice what we're NOT specifying? A GPU. This function runs on CPU because we don't need a GPU just to download and tokenize text. Why pay for GPU time when we don't need it?</p> <p>This function: 1. Downloads the Shakespeare text (~1MB) 2. Tokenizes it at the character level 3. Creates <code>train.bin</code> and <code>val.bin</code> files</p> <p>These files stay in the container's filesystem (not the volume) since they're small and the training function needs them.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#stage-2-training","title":"Stage 2: Training","text":"<p>Now here's where the fun begins:</p> <pre><code>@app.function(\n    image=NANOGPT_IMAGE,           # Our container image with nanoGPT\n    gpu=GPU_TYPE,                   # \"a100-40gb\" by default - NOW we need a GPU!\n    volumes=VOLUME_CONFIG,          # Mount our volume at /data\n    timeout=2 * HOURS,              # Give it 2 hours max\n)\ndef train(\n    # All hyperparameters as function arguments - makes experimenting easy!\n    max_iters: int = 1000,          # How many training steps\n    eval_interval: int = 500,       # Check validation loss every N steps\n    batch_size: int = 64,           # Samples per batch\n    block_size: int = 256,          # Context length (characters)\n    n_layer: int = 6,               # Number of transformer layers\n    n_head: int = 6,                # Attention heads per layer\n    n_embd: int = 384,              # Hidden dimension size\n    learning_rate: float = 1e-3,    # Optimizer learning rate\n):\n    \"\"\"Train a character-level GPT on Shakespeare.\"\"\"\n    import subprocess\n    import os\n    import shutil\n\n    print(\"=\" * 80)\n    print(\"TRAINING NANOGPT ON SHAKESPEARE\")\n    print(\"=\" * 80)\n\n    # Safety check: make sure data is prepared\n    # If not, run prepare_data locally in this container\n    if not os.path.exists(\"data/shakespeare_char/train.bin\"):\n        print(\"Data not found, preparing it first...\")\n        prepare_data.local()  # .local() runs in this same container\n\n    # Build the training command with all our hyperparameters\n    # We're basically calling: python train.py config.py --max_iters=1000 ...\n    cmd = [\n        \"python\",\n        \"train.py\",                              # nanoGPT's training script\n        \"config/train_shakespeare_char.py\",      # Base config file\n        f\"--max_iters={max_iters}\",              # Override config with our params\n        f\"--eval_interval={eval_interval}\",\n        f\"--batch_size={batch_size}\",\n        f\"--block_size={block_size}\",\n        f\"--n_layer={n_layer}\",\n        f\"--n_head={n_head}\",\n        f\"--n_embd={n_embd}\",\n        f\"--learning_rate={learning_rate}\",\n        \"--out_dir=/data/out\",                   # Save to volume (persists!)\n        \"--dataset=shakespeare_char\",            # Which dataset to use\n        \"--compile=False\",                       # Skip torch.compile for faster startup\n    ]\n\n    print(f\"Running: {' '.join(cmd)}\")\n    # Run the training - output streams to console in real-time\n    result = subprocess.run(cmd, capture_output=False, text=True)\n\n    # Check if training succeeded\n    if result.returncode != 0:\n        raise RuntimeError(f\"Training failed with code {result.returncode}\")\n\n    # Copy meta.pkl (character encoding info) to output dir\n    # We'll need this for sampling later\n    meta_src = \"data/shakespeare_char/meta.pkl\"\n    meta_dst = \"/data/out/meta.pkl\"\n    if os.path.exists(meta_src):\n        shutil.copy(meta_src, meta_dst)\n\n    # THIS IS CRITICAL - persist everything to the volume!\n    # Without this, your checkpoint disappears when the container shuts down\n    volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\u2713 Training completed! Model saved to /data/out\")\n    print(\"=\" * 80)\n\n    # Return info about the training run\n    return {\n        \"status\": \"completed\",\n        \"max_iters\": max_iters,\n        \"output_dir\": \"/data/out\",\n    }\n</code></pre> <p>Few things to note here:</p> <ol> <li>GPU specification: We're requesting an A100-40GB. Modal spins one up just for this function.</li> <li>Hyperparameters as arguments: Makes it super easy to experiment - just pass different values when you call the function.</li> <li><code>volume.commit()</code>: This is crucial! It persists everything you wrote to <code>/data</code> back to the volume. Forget this and your checkpoint disappears when the container shuts down.</li> <li>Fallback data prep: If the data isn't ready, we call <code>prepare_data.local()</code> to run it first.</li> </ol> <p>The training runs just like it would locally, except it's happening on a beefy GPU in the cloud.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#stage-3-generating-samples","title":"Stage 3: Generating Samples","text":"<p>Now let's see what our model learned:</p> <pre><code>@app.function(\n    image=NANOGPT_IMAGE,      # Same image as training\n    gpu=GPU_TYPE,              # Need GPU for inference\n    volumes=VOLUME_CONFIG,     # Mount volume to access saved checkpoint\n    timeout=10 * 60,           # 10 minutes should be plenty\n)\ndef sample(\n    num_samples: int = 5,           # How many texts to generate\n    max_new_tokens: int = 500,      # Length of each sample\n    temperature: float = 0.8,        # Randomness (0.1=boring, 1.5=wild)\n    start: str = \"\\n\",               # Starting prompt\n):\n    \"\"\"Generate text samples from our trained model.\"\"\"\n    import subprocess\n    import os\n    import shutil\n\n    print(\"=\" * 80)\n    print(\"GENERATING SAMPLES FROM TRAINED MODEL\")\n    print(\"=\" * 80)\n\n    # Sanity check: make sure the checkpoint exists\n    # (It should be in the volume from training)\n    if os.path.exists(\"/data/out/ckpt.pt\"):\n        print(\"\u2713 Found checkpoint: /data/out/ckpt.pt\")\n    else:\n        print(\"\u2717 Checkpoint not found: /data/out/ckpt.pt\")\n        # Could raise an error here, but we'll let sample.py handle it\n\n    # nanoGPT's sample.py looks for meta.pkl in the data directory\n    # So we need to copy it from the volume to where it expects it\n    os.makedirs(\"data/shakespeare_char\", exist_ok=True)\n    if os.path.exists(\"/data/out/meta.pkl\") and not os.path.exists(\n        \"data/shakespeare_char/meta.pkl\"\n    ):\n        shutil.copy(\"/data/out/meta.pkl\", \"data/shakespeare_char/meta.pkl\")\n        print(\"\u2713 Copied meta.pkl to expected location\")\n\n    # Build the sampling command\n    cmd = [\n        \"python\",\n        \"sample.py\",                           # nanoGPT's sampling script\n        \"--out_dir=/data/out\",                 # Where to find the checkpoint\n        f\"--num_samples={num_samples}\",        # How many samples to generate\n        f\"--max_new_tokens={max_new_tokens}\",  # Length of each sample\n        f\"--temperature={temperature}\",         # Sampling temperature\n        f\"--start={start}\",                    # Starting prompt\n        \"--compile=False\",                     # Skip compilation\n    ]\n\n    print(f\"Running: {' '.join(cmd)}\")\n    # Run sampling and capture output (we want to return it)\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    # Print the generated text\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    # Check if sampling succeeded\n    if result.returncode != 0:\n        raise RuntimeError(f\"Sampling failed with code {result.returncode}\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\u2713 Sampling completed!\")\n    print(\"=\" * 80)\n\n    # Return the generated samples\n    return {\"status\": \"completed\", \"samples\": result.stdout}\n</code></pre> <p>This loads the checkpoint from our volume and generates text. The <code>temperature</code> parameter controls creativity - higher values mean more random (and often more interesting) outputs. At 0.1, the model plays it safe and picks the most likely next character. At 1.5, it gets wild and experimental.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#running-the-pipeline","title":"Running the Pipeline","text":"<p>There are two ways to run this:</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#option-1-run-everything-at-once","title":"Option 1: Run Everything at Once","text":"<pre><code>@app.local_entrypoint()  # This decorator makes it the main entry point\ndef main():\n    \"\"\"Run the complete pipeline: data prep -&gt; train -&gt; sample\"\"\"\n    print(\"\ud83d\ude80 Starting nanoGPT pipeline...\")\n\n    # Step 1: Prepare data (runs on CPU)\n    print(\"\ud83d\udcc1 Preparing dataset...\")\n    prepare_data.remote()  # .remote() runs this on Modal's infrastructure\n\n    # Step 2: Train model (runs on GPU)\n    print(\"\ud83c\udfcb\ufe0f Training model...\")\n    train.remote(\n        max_iters=1000,      # Override default params\n        eval_interval=250,\n        batch_size=64\n    )\n\n    # Step 3: Generate samples (runs on GPU)\n    print(\"\u2728 Generating samples...\")\n    sample.remote(\n        num_samples=3,        # Just 3 samples\n        max_new_tokens=300    # 300 characters each\n    )\n\n    print(\"\ud83c\udf89 Pipeline completed!\")\n</code></pre> <p>Then just:</p> <pre><code>modal run TrainNanoGPTModal.py\n</code></pre> <p>The <code>.remote()</code> calls tell Modal to run these functions on their infrastructure, not locally. Modal handles spinning up containers, mounting volumes, and tearing everything down when done.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#option-2-run-steps-individually","title":"Option 2: Run Steps Individually","text":"<p>Sometimes you want more control:</p> <pre><code># Just prepare the data\nmodal run TrainNanoGPTModal.py::prepare_data\n\n# Train with custom parameters\nmodal run TrainNanoGPTModal.py::train --max-iters=2000 --batch-size=128\n\n# Generate samples\nmodal run TrainNanoGPTModal.py::sample\n</code></pre> <p>This is great for experimentation. You can prepare data once, then train multiple times with different hyperparameters.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#playing-with-configuration","title":"Playing with Configuration","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#gpu-types","title":"GPU Types","text":"<pre><code>GPU_TYPE = \"a100-40gb\"  # Default - fast and powerful\n</code></pre> <p>Your options: - T4: ~\\(0.50/hr - Great for testing, slower training - **L40S**: ~\\)1/hr - Good price/performance balance - A100-40GB: ~\\(2.50/hr - Fast training - **A100-80GB**: ~\\)3.50/hr - For larger models</p> <p>For testing nanoGPT, honestly a T4 is fine. Switch to A100 when you're doing real runs.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#training-hyperparameters","title":"Training Hyperparameters","text":"Parameter Default What it does <code>max_iters</code> 1000 How many training steps <code>eval_interval</code> 500 Check validation loss every N steps <code>batch_size</code> 64 Samples per batch <code>block_size</code> 256 Context length (chars the model sees) <code>n_layer</code> 6 Number of transformer layers <code>n_head</code> 6 Attention heads per layer <code>n_embd</code> 384 Hidden dimension size <code>learning_rate</code> 1e-3 Step size for optimizer <p>Want to train faster? Reduce <code>max_iters</code> to 100 while testing. Want better results? Increase <code>n_layer</code> and <code>n_embd</code> (but you'll need more memory).</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#sampling-parameters","title":"Sampling Parameters","text":"Parameter Default What it does <code>num_samples</code> 5 How many texts to generate <code>max_new_tokens</code> 500 Length of each sample <code>temperature</code> 0.8 Randomness (0.1=boring, 1.5=wild) <code>start</code> \"\\n\" Starting prompt"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#local-vs-remote-execution","title":"Local vs Remote Execution","text":"<p>Inside your functions, you can choose where things run:</p> <pre><code># Run locally on your machine\nprepare_data.local()\n\n# Run remotely on Modal\nprepare_data.remote()\n</code></pre> <p>Use local when: - Debugging - Testing small changes - You have a GPU locally and want to use it</p> <p>Use remote when: - Production training - You need specific GPU types - You don't want to manage infrastructure (most of the time!)</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#adding-secrets","title":"Adding Secrets","text":"<p>Need Hugging Face tokens or WandB API keys?</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#option-1-env-file","title":"Option 1: .env file","text":"<p>Create a <code>.env</code> file:</p> <pre><code>HF_TOKEN=your_huggingface_token\nWANDB_API_KEY=your_wandb_key\n</code></pre> <p>Then update your function:</p> <pre><code>@app.function(\n    image=NANOGPT_IMAGE,\n    gpu=GPU_TYPE,\n    volumes=VOLUME_CONFIG,\n    secrets=[modal.Secret.from_dotenv()],  # Add this\n    timeout=2 * HOURS,\n)\ndef train(...):\n    import os\n    hf_token = os.environ[\"HF_TOKEN\"]\n    ...\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#option-2-modal-secrets","title":"Option 2: Modal Secrets","text":"<p>More secure for production:</p> <pre><code>modal secret create my-secrets HF_TOKEN=xxx WANDB_API_KEY=yyy\n</code></pre> <pre><code>secrets=[modal.Secret.from_name(\"my-secrets\")]\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#when-things-go-wrong","title":"When Things Go Wrong","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#nanogpt-directory-not-found","title":"\"nanoGPT directory not found\"","text":"<p>Make sure you cloned it in the right place:</p> <pre><code>git clone https://github.com/karpathy/nanoGPT.git\nls  # Should show: nanoGPT/  TrainNanoGPTModal.py\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#checkpoint-not-found-during-sampling","title":"\"Checkpoint not found during sampling\"","text":"<p>Training didn't complete or you forgot <code>volume.commit()</code>. Check your training logs.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Your batch size is too big for the GPU:</p> <pre><code>modal run TrainNanoGPTModal.py::train --batch-size=32\n</code></pre> <p>Or switch to a bigger GPU by changing <code>GPU_TYPE = \"a100-80gb\"</code>.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#training-taking-forever","title":"Training Taking Forever","text":"<p>For testing, reduce iterations:</p> <pre><code>modal run TrainNanoGPTModal.py::train --max-iters=100\n</code></pre> <p>100 iterations won't give you great results, but it'll let you verify everything works.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#monitoring-your-training","title":"Monitoring Your Training","text":"<p>When you run <code>modal run</code>, you'll get a URL like:</p> <pre><code>View run at https://modal.com/apps/...\n</code></pre> <p>Click it to see: - Real-time logs streaming - GPU utilization graphs - How much you're spending - Function status</p> <p>It's actually a really nice dashboard. I keep it open while training to make sure my GPU utilization is high (means I'm not wasting money).</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#cost-optimization-tips","title":"Cost Optimization Tips","text":"<ol> <li> <p>Use CPU for data prep: We already do this! Data preparation on CPU, training on GPU.</p> </li> <li> <p>Start with cheap GPUs: Use T4 for testing, A100 for real runs.</p> </li> <li> <p>Set timeouts: Don't let a buggy script run forever:    </p><pre><code>timeout=1 * HOURS  # Kill it after an hour\n</code></pre> </li> <li> <p>Clean up old checkpoints: Volumes are free up to 50GB, but still, no need to hoard.</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#whats-next","title":"What's Next?","text":"<p>Now that you've got the basics down with nanoGPT, you can:</p> <ul> <li>Experiment with hyperparameters: Try different learning rates, model sizes</li> <li>Use your own data: Replace Shakespeare with your favorite books, code, whatever</li> <li>Add WandB tracking: Log your experiments properly</li> <li>Try the other tutorials: The Gemma tutorial shows production-scale fine-tuning with LoRA, and the Llama tutorial covers multi-GPU training</li> </ul> <p>The pattern is always the same: 1. Write your code locally 2. Define your Modal image 3. Wrap your functions with <code>@app.function()</code> 4. Run with <code>modal run</code></p> <p>That's it. No Docker, no Kubernetes, no infrastructure headaches. Just write Python and run it on GPUs.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanoGPTModalTutorial/#resources","title":"Resources","text":"<ul> <li>NanoGPT GitHub - The repo we're using</li> <li>Modal Documentation - When you want to dig deeper</li> <li>Modal GPU Types - All available GPUs and pricing</li> <li>Andrej Karpathy's Tutorial - Watch him build nanoGPT from scratch</li> </ul> <p>Got questions? Hit me up on Twitter @adithya_s_k or check out the other tutorials in this series!</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/","title":"TrainNanochatModal","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nTrain nanochat on Modal: Build Your Own ChatGPT from Scratch\n\nComplete nanochat pipeline - from tokenizer training to a functional ChatGPT clone.\n\nPipeline stages:\n1. Tokenizer Training - Custom BPE tokenizer (65K vocab)\n2. Base Model Pretraining - GPT on FineWeb dataset\n3. Midtraining - Conversation tokens and tool use\n4. Supervised Fine-tuning - Task-specific training\n5. Reinforcement Learning - Optional RL on GSM8K\n6. Comprehensive Evaluation - CORE, ARC, GSM8K, HumanEval, MMLU\n7. Inference - Chat CLI and Web UI\n\nGPU Requirements:\n- Recommended: 4-8x A100 80GB (~4 hours, ~$96)\n- Minimum: 1x A100 80GB (8x longer)\n- Testing: 1x A100 40GB (reduced batch sizes)\n\nSetup:\n    1. Clone nanochat: git clone https://github.com/karpathy/nanochat.git\n\n    2. Optional - Set up secrets for WandB:\n       modal secret create nanochat-secrets WANDB_API_KEY=your_key HUGGINGFACE_TOKEN=your_token\n\nUsage:\n    modal run TrainNanochatModal.py  # Full pipeline\n    modal run TrainNanochatModal.py::main --num-data-shards=8 --depth=12  # Quick test\n    modal run TrainNanochatModal.py::chat_cli --source=sft --prompt=\"Why is the sky blue?\"\n    modal run TrainNanochatModal.py::chat_web --source=sft  # Web UI\n\"\"\"\n</pre> \"\"\" Train nanochat on Modal: Build Your Own ChatGPT from Scratch  Complete nanochat pipeline - from tokenizer training to a functional ChatGPT clone.  Pipeline stages: 1. Tokenizer Training - Custom BPE tokenizer (65K vocab) 2. Base Model Pretraining - GPT on FineWeb dataset 3. Midtraining - Conversation tokens and tool use 4. Supervised Fine-tuning - Task-specific training 5. Reinforcement Learning - Optional RL on GSM8K 6. Comprehensive Evaluation - CORE, ARC, GSM8K, HumanEval, MMLU 7. Inference - Chat CLI and Web UI  GPU Requirements: - Recommended: 4-8x A100 80GB (~4 hours, ~$96) - Minimum: 1x A100 80GB (8x longer) - Testing: 1x A100 40GB (reduced batch sizes)  Setup:     1. Clone nanochat: git clone https://github.com/karpathy/nanochat.git      2. Optional - Set up secrets for WandB:        modal secret create nanochat-secrets WANDB_API_KEY=your_key HUGGINGFACE_TOKEN=your_token  Usage:     modal run TrainNanochatModal.py  # Full pipeline     modal run TrainNanochatModal.py::main --num-data-shards=8 --depth=12  # Quick test     modal run TrainNanochatModal.py::chat_cli --source=sft --prompt=\"Why is the sky blue?\"     modal run TrainNanochatModal.py::chat_web --source=sft  # Web UI \"\"\" In\u00a0[\u00a0]: Copied! <pre>from modal import App, Image as ModalImage, Volume, Secret\n</pre> from modal import App, Image as ModalImage, Volume, Secret In\u00a0[\u00a0]: Copied! <pre>MINUTES = 60\nHOURS = 60 * 60\n</pre> MINUTES = 60 HOURS = 60 * 60 In\u00a0[\u00a0]: Copied! <pre>GPU_TYPE = \"a100-80gb\"\n</pre> GPU_TYPE = \"a100-80gb\" In\u00a0[\u00a0]: Copied! <pre># Multi-GPU configuration (nanochat supports 1-8 GPUs)\nNUM_GPUS_BASE = 4\nNUM_GPUS_MID = 4\nNUM_GPUS_SFT = 4\nNUM_GPUS_RL = 4\nNUM_GPUS_EVAL = 4\nNUM_GPUS_TOKENIZER = 1\nNUM_GPUS_INFERENCE = 1\n</pre> # Multi-GPU configuration (nanochat supports 1-8 GPUs) NUM_GPUS_BASE = 4 NUM_GPUS_MID = 4 NUM_GPUS_SFT = 4 NUM_GPUS_RL = 4 NUM_GPUS_EVAL = 4 NUM_GPUS_TOKENIZER = 1 NUM_GPUS_INFERENCE = 1 In\u00a0[\u00a0]: Copied! <pre>WANDB_PROJECT_DEFAULT = \"nanochat-modal\"\nBASE_DIR = \"/data/.cache/nanochat\"\n</pre> WANDB_PROJECT_DEFAULT = \"nanochat-modal\" BASE_DIR = \"/data/.cache/nanochat\" In\u00a0[\u00a0]: Copied! <pre>app = App(\"nanochat-training\")\n</pre> app = App(\"nanochat-training\") In\u00a0[\u00a0]: Copied! <pre>data_volume = Volume.from_name(\"nanochat-data\", create_if_missing=True)\ncheckpoint_volume = Volume.from_name(\"nanochat-checkpoints\", create_if_missing=True)\n</pre> data_volume = Volume.from_name(\"nanochat-data\", create_if_missing=True) checkpoint_volume = Volume.from_name(\"nanochat-checkpoints\", create_if_missing=True) In\u00a0[\u00a0]: Copied! <pre>VOLUME_CONFIG = {\n    \"/data\": data_volume,\n    \"/checkpoints\": checkpoint_volume,\n}\n</pre> VOLUME_CONFIG = {     \"/data\": data_volume,     \"/checkpoints\": checkpoint_volume, } In\u00a0[\u00a0]: Copied! <pre>try:\n    nanochat_secret = Secret.from_dotenv()\n    print(\"Loaded secrets from .env file\")\nexcept Exception:\n    try:\n        nanochat_secret = Secret.from_name(\"nanochat-secrets\")\n        print(\"Loaded secrets from Modal\")\n    except Exception:\n        nanochat_secret = None\n        print(\"No secrets found - WandB logging disabled\")\n</pre> try:     nanochat_secret = Secret.from_dotenv()     print(\"Loaded secrets from .env file\") except Exception:     try:         nanochat_secret = Secret.from_name(\"nanochat-secrets\")         print(\"Loaded secrets from Modal\")     except Exception:         nanochat_secret = None         print(\"No secrets found - WandB logging disabled\") In\u00a0[\u00a0]: Copied! <pre>NANOCHAT_IMAGE = (\n    ModalImage.from_registry(\"nvidia/cuda:12.8.1-devel-ubuntu24.04\", add_python=\"3.11\")\n    .apt_install(\"git\", \"build-essential\", \"curl\", \"wget\", \"unzip\")\n    .run_commands(\n        \"curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\",\n        \"echo 'source $HOME/.cargo/env' &gt;&gt; $HOME/.bashrc\",\n    )\n    .run_commands(\n        \"curl -LsSf https://astral.sh/uv/install.sh | sh\",\n        \"echo 'export PATH=\\\"$HOME/.cargo/bin:$PATH\\\"' &gt;&gt; $HOME/.bashrc\",\n    )\n    .add_local_dir(local_path=\"nanochat\", remote_path=\"/root/nanochat\", copy=True)\n    .workdir(\"/root/nanochat\")\n    .run_commands(\n        \"bash -c 'source $HOME/.cargo/env &amp;&amp; uv sync &amp;&amp; uv run maturin develop --release --manifest-path rustbpe/Cargo.toml'\"\n    )\n    .env(\n        {\n            \"OMP_NUM_THREADS\": \"1\",\n            \"NANOCHAT_BASE_DIR\": \"/data/.cache/nanochat\",\n            \"HF_HOME\": \"/data/.cache/huggingface\",\n        }\n    )\n)\n</pre> NANOCHAT_IMAGE = (     ModalImage.from_registry(\"nvidia/cuda:12.8.1-devel-ubuntu24.04\", add_python=\"3.11\")     .apt_install(\"git\", \"build-essential\", \"curl\", \"wget\", \"unzip\")     .run_commands(         \"curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\",         \"echo 'source $HOME/.cargo/env' &gt;&gt; $HOME/.bashrc\",     )     .run_commands(         \"curl -LsSf https://astral.sh/uv/install.sh | sh\",         \"echo 'export PATH=\\\"$HOME/.cargo/bin:$PATH\\\"' &gt;&gt; $HOME/.bashrc\",     )     .add_local_dir(local_path=\"nanochat\", remote_path=\"/root/nanochat\", copy=True)     .workdir(\"/root/nanochat\")     .run_commands(         \"bash -c 'source $HOME/.cargo/env &amp;&amp; uv sync &amp;&amp; uv run maturin develop --release --manifest-path rustbpe/Cargo.toml'\"     )     .env(         {             \"OMP_NUM_THREADS\": \"1\",             \"NANOCHAT_BASE_DIR\": \"/data/.cache/nanochat\",             \"HF_HOME\": \"/data/.cache/huggingface\",         }     ) ) In\u00a0[\u00a0]: Copied! <pre>def setup_base_dir():\n    \"\"\"Create base directory structure.\"\"\"\n    import os\n\n    os.makedirs(BASE_DIR, exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/base_data\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/tokenizer\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/checkpoints\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/eval_bundle\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/report\", exist_ok=True)\n</pre> def setup_base_dir():     \"\"\"Create base directory structure.\"\"\"     import os      os.makedirs(BASE_DIR, exist_ok=True)     os.makedirs(f\"{BASE_DIR}/base_data\", exist_ok=True)     os.makedirs(f\"{BASE_DIR}/tokenizer\", exist_ok=True)     os.makedirs(f\"{BASE_DIR}/checkpoints\", exist_ok=True)     os.makedirs(f\"{BASE_DIR}/eval_bundle\", exist_ok=True)     os.makedirs(f\"{BASE_DIR}/report\", exist_ok=True) In\u00a0[\u00a0]: Copied! <pre>def setup_secrets():\n    \"\"\"Set up environment variables from secrets.\"\"\"\n    import os\n\n    if \"WANDB_API_KEY\" in os.environ:\n        print(\"WandB API key found\")\n    else:\n        print(\"WandB API key not found - logging disabled\")\n\n    if \"HUGGINGFACE_TOKEN\" in os.environ:\n        os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n        print(\"HuggingFace token found\")\n    else:\n        print(\"HuggingFace token not found\")\n</pre> def setup_secrets():     \"\"\"Set up environment variables from secrets.\"\"\"     import os      if \"WANDB_API_KEY\" in os.environ:         print(\"WandB API key found\")     else:         print(\"WandB API key not found - logging disabled\")      if \"HUGGINGFACE_TOKEN\" in os.environ:         os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]         print(\"HuggingFace token found\")     else:         print(\"HuggingFace token not found\") In\u00a0[\u00a0]: Copied! <pre>def run_torchrun_command(script: str, num_gpus: int, extra_args: list = None):\n    \"\"\"Run nanochat script with torchrun for multi-GPU training.\"\"\"\n    import subprocess\n\n    if extra_args is None:\n        extra_args = []\n\n    extra_args_str = \" \".join(extra_args) if extra_args else \"\"\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run torchrun --standalone --nproc_per_node={num_gpus} -m {script}\"\n\n    if extra_args:\n        cmd += f\" -- {extra_args_str}\"\n\n    print(f\"Running: {cmd}\")\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Command failed with code {result.returncode}\")\n\n    return result\n</pre> def run_torchrun_command(script: str, num_gpus: int, extra_args: list = None):     \"\"\"Run nanochat script with torchrun for multi-GPU training.\"\"\"     import subprocess      if extra_args is None:         extra_args = []      extra_args_str = \" \".join(extra_args) if extra_args else \"\"     cmd = f\"cd /root/nanochat &amp;&amp; uv run torchrun --standalone --nproc_per_node={num_gpus} -m {script}\"      if extra_args:         cmd += f\" -- {extra_args_str}\"      print(f\"Running: {cmd}\")     result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)      if result.returncode != 0:         raise RuntimeError(f\"Command failed with code {result.returncode}\")      return result In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n)\ndef download_dataset(num_shards: int = 240):\n    \"\"\"\n    Download FineWeb dataset shards from HuggingFace.\n\n    Each shard is ~250M characters (~100MB compressed).\n    - Full speedrun: 240 shards (~60B characters, ~24GB)\n    - Testing: 8 shards (~2B characters, ~800MB)\n    \"\"\"\n    import subprocess\n\n    setup_base_dir()\n\n    print(\"=\" * 80)\n    print(f\"DOWNLOADING FINEWEB DATASET - {num_shards} SHARDS\")\n    print(\"=\" * 80)\n    print(f\"Total data: ~{num_shards * 250 / 1000:.1f}B characters (~{num_shards * 100 / 1024:.1f}GB)\")\n    print()\n\n    result = subprocess.run(\n        [\"bash\", \"-c\", f\"cd /root/nanochat &amp;&amp; uv run python -m nanochat.dataset -n {num_shards}\"],\n        capture_output=True,\n        text=True,\n    )\n\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Dataset download failed with code {result.returncode}\")\n\n    data_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Downloaded {num_shards} shards successfully\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"num_shards\": num_shards,\n        \"data_dir\": f\"{BASE_DIR}/base_data\",\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     volumes=VOLUME_CONFIG,     timeout=2 * HOURS, ) def download_dataset(num_shards: int = 240):     \"\"\"     Download FineWeb dataset shards from HuggingFace.      Each shard is ~250M characters (~100MB compressed).     - Full speedrun: 240 shards (~60B characters, ~24GB)     - Testing: 8 shards (~2B characters, ~800MB)     \"\"\"     import subprocess      setup_base_dir()      print(\"=\" * 80)     print(f\"DOWNLOADING FINEWEB DATASET - {num_shards} SHARDS\")     print(\"=\" * 80)     print(f\"Total data: ~{num_shards * 250 / 1000:.1f}B characters (~{num_shards * 100 / 1024:.1f}GB)\")     print()      result = subprocess.run(         [\"bash\", \"-c\", f\"cd /root/nanochat &amp;&amp; uv run python -m nanochat.dataset -n {num_shards}\"],         capture_output=True,         text=True,     )      print(result.stdout)     if result.stderr:         print(\"STDERR:\", result.stderr)      if result.returncode != 0:         raise RuntimeError(f\"Dataset download failed with code {result.returncode}\")      data_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(f\"Downloaded {num_shards} shards successfully\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"num_shards\": num_shards,         \"data_dir\": f\"{BASE_DIR}/base_data\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_TOKENIZER}\",\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n)\ndef train_tokenizer(\n    max_chars: int = 2_000_000_000,\n    vocab_size: int = 65536,\n    doc_cap: int = 10000,\n):\n    \"\"\"\n    Train a custom BPE tokenizer on FineWeb data.\n    Training takes 30-60 minutes on a single GPU.\n    \"\"\"\n    import subprocess\n\n    setup_base_dir()\n\n    print(\"=\" * 80)\n    print(\"TRAINING CUSTOM BPE TOKENIZER\")\n    print(\"=\" * 80)\n    print(f\"Max characters: {max_chars:,}\")\n    print(f\"Vocabulary size: {vocab_size:,}\")\n    print(f\"Document cap: {doc_cap:,}\")\n    print()\n\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.tok_train --max_chars={max_chars} --vocab_size={vocab_size} --doc_cap={doc_cap}\"\n\n    print(f\"Running: {cmd}\")\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Tokenizer training failed with code {result.returncode}\")\n\n    data_volume.commit()\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Tokenizer training completed\")\n    print(f\"Tokenizer saved to {BASE_DIR}/tokenizer/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"max_chars\": max_chars,\n        \"vocab_size\": vocab_size,\n        \"tokenizer_dir\": f\"{BASE_DIR}/tokenizer\",\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_TOKENIZER}\",     volumes=VOLUME_CONFIG,     timeout=2 * HOURS, ) def train_tokenizer(     max_chars: int = 2_000_000_000,     vocab_size: int = 65536,     doc_cap: int = 10000, ):     \"\"\"     Train a custom BPE tokenizer on FineWeb data.     Training takes 30-60 minutes on a single GPU.     \"\"\"     import subprocess      setup_base_dir()      print(\"=\" * 80)     print(\"TRAINING CUSTOM BPE TOKENIZER\")     print(\"=\" * 80)     print(f\"Max characters: {max_chars:,}\")     print(f\"Vocabulary size: {vocab_size:,}\")     print(f\"Document cap: {doc_cap:,}\")     print()      cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.tok_train --max_chars={max_chars} --vocab_size={vocab_size} --doc_cap={doc_cap}\"      print(f\"Running: {cmd}\")     result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)      if result.returncode != 0:         raise RuntimeError(f\"Tokenizer training failed with code {result.returncode}\")      data_volume.commit()     checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"Tokenizer training completed\")     print(f\"Tokenizer saved to {BASE_DIR}/tokenizer/\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"max_chars\": max_chars,         \"vocab_size\": vocab_size,         \"tokenizer_dir\": f\"{BASE_DIR}/tokenizer\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_TOKENIZER}\",\n    volumes=VOLUME_CONFIG,\n    timeout=30 * MINUTES,\n)\ndef evaluate_tokenizer():\n    \"\"\"Evaluate the trained tokenizer.\"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(\"EVALUATING TOKENIZER\")\n    print(\"=\" * 80)\n\n    result = subprocess.run(\n        [\"bash\", \"-c\", \"cd /root/nanochat &amp;&amp; uv run python -m scripts.tok_eval\"],\n        capture_output=False,\n        text=True\n    )\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Tokenizer evaluation failed with code {result.returncode}\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Tokenizer evaluation completed\")\n    print(\"=\" * 80)\n\n    return {\"status\": \"completed\"}\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_TOKENIZER}\",     volumes=VOLUME_CONFIG,     timeout=30 * MINUTES, ) def evaluate_tokenizer():     \"\"\"Evaluate the trained tokenizer.\"\"\"     import subprocess      print(\"=\" * 80)     print(\"EVALUATING TOKENIZER\")     print(\"=\" * 80)      result = subprocess.run(         [\"bash\", \"-c\", \"cd /root/nanochat &amp;&amp; uv run python -m scripts.tok_eval\"],         capture_output=False,         text=True     )      if result.returncode != 0:         raise RuntimeError(f\"Tokenizer evaluation failed with code {result.returncode}\")      print(\"\\n\" + \"=\" * 80)     print(\"Tokenizer evaluation completed\")     print(\"=\" * 80)      return {\"status\": \"completed\"} In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_BASE}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=8 * HOURS,\n)\ndef train_base_model(\n    depth: int = 20,\n    device_batch_size: int = 32,\n    max_iterations: int = -1,\n    wandb_run: str = \"dummy\",\n):\n    \"\"\"\n    Pretrain the base GPT model on FineWeb.\n\n    Model sizes: depth=20 (561M params), depth=26 (1B params)\n    Training duration: ~2-3 hours on 8 GPUs, ~16-24 hours on 1 GPU\n    \"\"\"\n    import subprocess\n    import os\n\n    setup_base_dir()\n    setup_secrets()\n\n    eval_bundle_path = f\"{BASE_DIR}/eval_bundle\"\n    if not os.path.exists(eval_bundle_path):\n        print(\"Downloading eval bundle...\")\n        subprocess.run(\n            [\n                \"curl\",\n                \"-L\",\n                \"-o\",\n                \"eval_bundle.zip\",\n                \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\",\n            ],\n            check=True,\n        )\n        subprocess.run([\"unzip\", \"-q\", \"eval_bundle.zip\"], check=True)\n        subprocess.run([\"mv\", \"eval_bundle\", eval_bundle_path], check=True)\n        subprocess.run([\"rm\", \"eval_bundle.zip\"], check=True)\n\n    print(\"=\" * 80)\n    print(\"PRETRAINING BASE MODEL ON FINEWEB\")\n    print(\"=\" * 80)\n    print(f\"Model depth: {depth}\")\n    print(f\"Estimated parameters: {depth * depth * 64 * 12 // 1_000_000}M\")\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_BASE}\")\n    print(f\"WandB run: {wandb_run}\")\n    print()\n\n    extra_args = [\n        f\"--depth={depth}\",\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--run={wandb_run}\",\n    ]\n\n    if max_iterations &gt; 0:\n        extra_args.append(f\"--num_iterations={max_iterations}\")\n\n    run_torchrun_command(\"scripts.base_train\", NUM_GPUS_BASE, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Base model training completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/base/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"depth\": depth,\n        \"device_batch_size\": device_batch_size,\n        \"num_gpus\": NUM_GPUS_BASE,\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/base\",\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_BASE}\",     volumes=VOLUME_CONFIG,     secrets=[nanochat_secret] if nanochat_secret else [],     timeout=8 * HOURS, ) def train_base_model(     depth: int = 20,     device_batch_size: int = 32,     max_iterations: int = -1,     wandb_run: str = \"dummy\", ):     \"\"\"     Pretrain the base GPT model on FineWeb.      Model sizes: depth=20 (561M params), depth=26 (1B params)     Training duration: ~2-3 hours on 8 GPUs, ~16-24 hours on 1 GPU     \"\"\"     import subprocess     import os      setup_base_dir()     setup_secrets()      eval_bundle_path = f\"{BASE_DIR}/eval_bundle\"     if not os.path.exists(eval_bundle_path):         print(\"Downloading eval bundle...\")         subprocess.run(             [                 \"curl\",                 \"-L\",                 \"-o\",                 \"eval_bundle.zip\",                 \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\",             ],             check=True,         )         subprocess.run([\"unzip\", \"-q\", \"eval_bundle.zip\"], check=True)         subprocess.run([\"mv\", \"eval_bundle\", eval_bundle_path], check=True)         subprocess.run([\"rm\", \"eval_bundle.zip\"], check=True)      print(\"=\" * 80)     print(\"PRETRAINING BASE MODEL ON FINEWEB\")     print(\"=\" * 80)     print(f\"Model depth: {depth}\")     print(f\"Estimated parameters: {depth * depth * 64 * 12 // 1_000_000}M\")     print(f\"Device batch size: {device_batch_size}\")     print(f\"Number of GPUs: {NUM_GPUS_BASE}\")     print(f\"WandB run: {wandb_run}\")     print()      extra_args = [         f\"--depth={depth}\",         f\"--device_batch_size={device_batch_size}\",         f\"--run={wandb_run}\",     ]      if max_iterations &gt; 0:         extra_args.append(f\"--num_iterations={max_iterations}\")      run_torchrun_command(\"scripts.base_train\", NUM_GPUS_BASE, extra_args)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"Base model training completed\")     print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/base/\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"depth\": depth,         \"device_batch_size\": device_batch_size,         \"num_gpus\": NUM_GPUS_BASE,         \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/base\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",\n    volumes=VOLUME_CONFIG,\n    timeout=1 * HOURS,\n)\ndef evaluate_base_model(max_per_task: int = 500):\n    \"\"\"Evaluate base model on CORE benchmark.\"\"\"\n    print(\"=\" * 80)\n    print(\"EVALUATING BASE MODEL - CORE METRIC\")\n    print(\"=\" * 80)\n\n    extra_args = []\n    if max_per_task &gt; 0:\n        extra_args.append(f\"--core_metric_max_per_task={max_per_task}\")\n\n    run_torchrun_command(\"scripts.base_eval\", NUM_GPUS_EVAL, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Base model evaluation completed\")\n    print(\"=\" * 80)\n\n    return {\"status\": \"completed\"}\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",     volumes=VOLUME_CONFIG,     timeout=1 * HOURS, ) def evaluate_base_model(max_per_task: int = 500):     \"\"\"Evaluate base model on CORE benchmark.\"\"\"     print(\"=\" * 80)     print(\"EVALUATING BASE MODEL - CORE METRIC\")     print(\"=\" * 80)      extra_args = []     if max_per_task &gt; 0:         extra_args.append(f\"--core_metric_max_per_task={max_per_task}\")      run_torchrun_command(\"scripts.base_eval\", NUM_GPUS_EVAL, extra_args)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"Base model evaluation completed\")     print(\"=\" * 80)      return {\"status\": \"completed\"} In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",\n    volumes=VOLUME_CONFIG,\n    timeout=1 * HOURS,\n)\ndef evaluate_base_loss():\n    \"\"\"Evaluate base model validation loss (bits per byte).\"\"\"\n    print(\"=\" * 80)\n    print(\"EVALUATING BASE MODEL - VALIDATION LOSS\")\n    print(\"=\" * 80)\n\n    run_torchrun_command(\"scripts.base_loss\", NUM_GPUS_EVAL)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Base loss evaluation completed\")\n    print(\"=\" * 80)\n\n    return {\"status\": \"completed\"}\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",     volumes=VOLUME_CONFIG,     timeout=1 * HOURS, ) def evaluate_base_loss():     \"\"\"Evaluate base model validation loss (bits per byte).\"\"\"     print(\"=\" * 80)     print(\"EVALUATING BASE MODEL - VALIDATION LOSS\")     print(\"=\" * 80)      run_torchrun_command(\"scripts.base_loss\", NUM_GPUS_EVAL)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"Base loss evaluation completed\")     print(\"=\" * 80)      return {\"status\": \"completed\"} In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_MID}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=2 * HOURS,\n)\ndef train_mid_model(\n    device_batch_size: int = 32,\n    wandb_run: str = \"dummy\",\n):\n    \"\"\"\n    Midtrain the model on conversation data.\n\n    Teaches conversation tokens, tool use, and multiple choice format.\n    Duration: ~30-45 minutes on 8 GPUs\n    \"\"\"\n    setup_secrets()\n\n    print(\"=\" * 80)\n    print(\"MIDTRAINING - TEACHING CONVERSATION TOKENS\")\n    print(\"=\" * 80)\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_MID}\")\n    print()\n\n    extra_args = [\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--run={wandb_run}\",\n    ]\n\n    run_torchrun_command(\"scripts.mid_train\", NUM_GPUS_MID, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Midtraining completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/mid/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/mid\",\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_MID}\",     volumes=VOLUME_CONFIG,     secrets=[nanochat_secret] if nanochat_secret else [],     timeout=2 * HOURS, ) def train_mid_model(     device_batch_size: int = 32,     wandb_run: str = \"dummy\", ):     \"\"\"     Midtrain the model on conversation data.      Teaches conversation tokens, tool use, and multiple choice format.     Duration: ~30-45 minutes on 8 GPUs     \"\"\"     setup_secrets()      print(\"=\" * 80)     print(\"MIDTRAINING - TEACHING CONVERSATION TOKENS\")     print(\"=\" * 80)     print(f\"Device batch size: {device_batch_size}\")     print(f\"Number of GPUs: {NUM_GPUS_MID}\")     print()      extra_args = [         f\"--device_batch_size={device_batch_size}\",         f\"--run={wandb_run}\",     ]      run_torchrun_command(\"scripts.mid_train\", NUM_GPUS_MID, extra_args)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"Midtraining completed\")     print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/mid/\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/mid\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_SFT}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=2 * HOURS,\n)\ndef train_sft_model(\n    device_batch_size: int = 4,\n    num_epochs: int = 1,\n    wandb_run: str = \"dummy\",\n    source: str = \"mid\",\n):\n    \"\"\"\n    Supervised fine-tuning on task-specific data.\n\n    Trains on MMLU, ARC, GSM8K, HumanEval, and SmolTalk.\n    Duration: ~30-45 minutes on 8 GPUs\n    \"\"\"\n    setup_secrets()\n\n    print(\"=\" * 80)\n    print(\"SUPERVISED FINE-TUNING\")\n    print(\"=\" * 80)\n    print(f\"Source: {source}\")\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_SFT}\")\n    print(f\"Epochs: {num_epochs}\")\n    print()\n\n    extra_args = [\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--num_epochs={num_epochs}\",\n        f\"--run={wandb_run}\",\n        f\"--source={source}\",\n    ]\n\n    run_torchrun_command(\"scripts.chat_sft\", NUM_GPUS_SFT, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SFT completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/sft/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/sft\",\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_SFT}\",     volumes=VOLUME_CONFIG,     secrets=[nanochat_secret] if nanochat_secret else [],     timeout=2 * HOURS, ) def train_sft_model(     device_batch_size: int = 4,     num_epochs: int = 1,     wandb_run: str = \"dummy\",     source: str = \"mid\", ):     \"\"\"     Supervised fine-tuning on task-specific data.      Trains on MMLU, ARC, GSM8K, HumanEval, and SmolTalk.     Duration: ~30-45 minutes on 8 GPUs     \"\"\"     setup_secrets()      print(\"=\" * 80)     print(\"SUPERVISED FINE-TUNING\")     print(\"=\" * 80)     print(f\"Source: {source}\")     print(f\"Device batch size: {device_batch_size}\")     print(f\"Number of GPUs: {NUM_GPUS_SFT}\")     print(f\"Epochs: {num_epochs}\")     print()      extra_args = [         f\"--device_batch_size={device_batch_size}\",         f\"--num_epochs={num_epochs}\",         f\"--run={wandb_run}\",         f\"--source={source}\",     ]      run_torchrun_command(\"scripts.chat_sft\", NUM_GPUS_SFT, extra_args)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"SFT completed\")     print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/sft/\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/sft\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_RL}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=2 * HOURS,\n)\ndef train_rl_model(\n    device_batch_size: int = 8,\n    num_epochs: int = 1,\n    wandb_run: str = \"dummy\",\n    source: str = \"sft\",\n):\n    \"\"\"\n    Reinforcement learning on GSM8K (optional).\n\n    Uses GRPO/REINFORCE to improve math reasoning.\n    Duration: ~30-45 minutes on 8 GPUs\n    \"\"\"\n    setup_secrets()\n\n    print(\"=\" * 80)\n    print(\"REINFORCEMENT LEARNING ON GSM8K\")\n    print(\"=\" * 80)\n    print(f\"Source: {source}\")\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_RL}\")\n    print(f\"Epochs: {num_epochs}\")\n    print()\n\n    extra_args = [\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--num_epochs={num_epochs}\",\n        f\"--run={wandb_run}\",\n        f\"--source={source}\",\n    ]\n\n    run_torchrun_command(\"scripts.chat_rl\", NUM_GPUS_RL, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RL training completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/rl/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/rl\",\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_RL}\",     volumes=VOLUME_CONFIG,     secrets=[nanochat_secret] if nanochat_secret else [],     timeout=2 * HOURS, ) def train_rl_model(     device_batch_size: int = 8,     num_epochs: int = 1,     wandb_run: str = \"dummy\",     source: str = \"sft\", ):     \"\"\"     Reinforcement learning on GSM8K (optional).      Uses GRPO/REINFORCE to improve math reasoning.     Duration: ~30-45 minutes on 8 GPUs     \"\"\"     setup_secrets()      print(\"=\" * 80)     print(\"REINFORCEMENT LEARNING ON GSM8K\")     print(\"=\" * 80)     print(f\"Source: {source}\")     print(f\"Device batch size: {device_batch_size}\")     print(f\"Number of GPUs: {NUM_GPUS_RL}\")     print(f\"Epochs: {num_epochs}\")     print()      extra_args = [         f\"--device_batch_size={device_batch_size}\",         f\"--num_epochs={num_epochs}\",         f\"--run={wandb_run}\",         f\"--source={source}\",     ]      run_torchrun_command(\"scripts.chat_rl\", NUM_GPUS_RL, extra_args)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(\"RL training completed\")     print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/rl/\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/rl\",     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n)\ndef evaluate_chat_model(\n    source: str = \"sft\",\n    tasks: str = \"all\",\n):\n    \"\"\"\n    Evaluate the chat model on benchmark tasks.\n\n    Available tasks: ARC-Easy, ARC-Challenge, GSM8K, HumanEval, MMLU, ChatCORE\n    \"\"\"\n    print(\"=\" * 80)\n    print(f\"EVALUATING CHAT MODEL - {source.upper()}\")\n    print(\"=\" * 80)\n    print(f\"Tasks: {tasks}\")\n    print()\n\n    extra_args = [\"-i\", source]\n\n    if tasks != \"all\":\n        extra_args.extend([\"-a\", tasks])\n\n    run_torchrun_command(\"scripts.chat_eval\", NUM_GPUS_EVAL, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Evaluation of {source} model completed\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"source\": source,\n        \"tasks\": tasks,\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",     volumes=VOLUME_CONFIG,     timeout=2 * HOURS, ) def evaluate_chat_model(     source: str = \"sft\",     tasks: str = \"all\", ):     \"\"\"     Evaluate the chat model on benchmark tasks.      Available tasks: ARC-Easy, ARC-Challenge, GSM8K, HumanEval, MMLU, ChatCORE     \"\"\"     print(\"=\" * 80)     print(f\"EVALUATING CHAT MODEL - {source.upper()}\")     print(\"=\" * 80)     print(f\"Tasks: {tasks}\")     print()      extra_args = [\"-i\", source]      if tasks != \"all\":         extra_args.extend([\"-a\", tasks])      run_torchrun_command(\"scripts.chat_eval\", NUM_GPUS_EVAL, extra_args)      checkpoint_volume.commit()      print(\"\\n\" + \"=\" * 80)     print(f\"Evaluation of {source} model completed\")     print(\"=\" * 80)      return {         \"status\": \"completed\",         \"source\": source,         \"tasks\": tasks,     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_INFERENCE}\",\n    volumes=VOLUME_CONFIG,\n    timeout=1 * HOURS,\n)\ndef chat_cli(\n    source: str = \"sft\",\n    prompt: str = \"\",\n    temperature: float = 0.6,\n    top_k: int = 50,\n):\n    \"\"\"Chat with the model via command line interface.\"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(f\"CHAT CLI - {source.upper()} MODEL\")\n    print(\"=\" * 80)\n\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.chat_cli -i {source} -t {temperature} -k {top_k}\"\n\n    if prompt:\n        escaped_prompt = prompt.replace('\"', '\\\\\"')\n        cmd += f' -p \"{escaped_prompt}\"'\n\n    print(f\"Running: {cmd}\")\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Chat CLI failed with code {result.returncode}\")\n\n    return {\n        \"status\": \"completed\",\n        \"source\": source,\n        \"prompt\": prompt,\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_INFERENCE}\",     volumes=VOLUME_CONFIG,     timeout=1 * HOURS, ) def chat_cli(     source: str = \"sft\",     prompt: str = \"\",     temperature: float = 0.6,     top_k: int = 50, ):     \"\"\"Chat with the model via command line interface.\"\"\"     import subprocess      print(\"=\" * 80)     print(f\"CHAT CLI - {source.upper()} MODEL\")     print(\"=\" * 80)      cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.chat_cli -i {source} -t {temperature} -k {top_k}\"      if prompt:         escaped_prompt = prompt.replace('\"', '\\\\\"')         cmd += f' -p \"{escaped_prompt}\"'      print(f\"Running: {cmd}\")     result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)      if result.returncode != 0:         raise RuntimeError(f\"Chat CLI failed with code {result.returncode}\")      return {         \"status\": \"completed\",         \"source\": source,         \"prompt\": prompt,     } In\u00a0[\u00a0]: Copied! <pre>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_INFERENCE}\",\n    volumes=VOLUME_CONFIG,\n    timeout=4 * HOURS,\n    max_containers=2,\n)\ndef chat_web(\n    source: str = \"sft\",\n    port: int = 8000,\n    temperature: float = 0.8,\n    top_k: int = 50,\n    max_tokens: int = 512,\n):\n    \"\"\"Serve the chat model via a web UI.\"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(f\"STARTING WEB UI - {source.upper()} MODEL\")\n    print(\"=\" * 80)\n    print(f\"Port: {port}\")\n    print(f\"Temperature: {temperature}\")\n    print(f\"Top-k: {top_k}\")\n    print(f\"Max tokens: {max_tokens}\")\n    print()\n\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.chat_web -i {source} -p {port} -t {temperature} -k {top_k} -m {max_tokens} --host 0.0.0.0\"\n\n    print(f\"Running: {cmd}\")\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Web UI will be available at: http://localhost:{port}\")\n    print(\"=\" * 80)\n    print()\n\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Web server failed with code {result.returncode}\")\n\n    return {\n        \"status\": \"completed\",\n        \"source\": source,\n        \"port\": port,\n    }\n</pre> @app.function(     image=NANOCHAT_IMAGE,     gpu=f\"{GPU_TYPE}:{NUM_GPUS_INFERENCE}\",     volumes=VOLUME_CONFIG,     timeout=4 * HOURS,     max_containers=2, ) def chat_web(     source: str = \"sft\",     port: int = 8000,     temperature: float = 0.8,     top_k: int = 50,     max_tokens: int = 512, ):     \"\"\"Serve the chat model via a web UI.\"\"\"     import subprocess      print(\"=\" * 80)     print(f\"STARTING WEB UI - {source.upper()} MODEL\")     print(\"=\" * 80)     print(f\"Port: {port}\")     print(f\"Temperature: {temperature}\")     print(f\"Top-k: {top_k}\")     print(f\"Max tokens: {max_tokens}\")     print()      cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.chat_web -i {source} -p {port} -t {temperature} -k {top_k} -m {max_tokens} --host 0.0.0.0\"      print(f\"Running: {cmd}\")     print(\"\\n\" + \"=\" * 80)     print(f\"Web UI will be available at: http://localhost:{port}\")     print(\"=\" * 80)     print()      result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)      if result.returncode != 0:         raise RuntimeError(f\"Web server failed with code {result.returncode}\")      return {         \"status\": \"completed\",         \"source\": source,         \"port\": port,     } In\u00a0[\u00a0]: Copied! <pre>@app.local_entrypoint()\ndef main(\n    run_download: bool = True,\n    run_tokenizer: bool = True,\n    run_base: bool = True,\n    run_mid: bool = True,\n    run_sft: bool = True,\n    run_rl: bool = False,\n    run_eval: bool = True,\n    run_inference: bool = True,\n    num_data_shards: int = 240,\n    depth: int = 20,\n    device_batch_size_base: int = 32,\n    device_batch_size_sft: int = 4,\n    wandb_run: str = \"dummy\",\n):\n    \"\"\"\n    Run the complete nanochat pipeline from scratch.\n\n    Pipeline stages:\n    1. Download FineWeb dataset\n    2. Train + evaluate tokenizer\n    3. Train + evaluate base model\n    4. Train + evaluate mid model\n    5. Train + evaluate SFT model\n    6. (Optional) Train + evaluate RL model\n    7. Run final inference test\n\n    Configuration modes:\n    - Full Speedrun (4h, $96): num_data_shards=240, depth=20\n    - Quick Test (1h, $24): num_data_shards=8, depth=12\n    - GPT-2 Grade (12h, $288): num_data_shards=450, depth=26\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"NANOCHAT TRAINING PIPELINE\")\n    print(\"=\" * 80)\n    print(f\"Mode: {'Speedrun' if num_data_shards &gt;= 240 else 'Quick Test'}\")\n    print(f\"Data shards: {num_data_shards}\")\n    print(f\"Model depth: {depth}\")\n    print(f\"WandB run: {wandb_run}\")\n    print(\"=\" * 80)\n    print()\n\n    if run_download:\n        print(\"Stage 1/8: Downloading dataset...\")\n        download_dataset.remote(num_shards=num_data_shards)\n\n    if run_tokenizer:\n        print(\"\\nStage 2/8: Training tokenizer...\")\n        train_tokenizer.remote()\n        print(\"Evaluating tokenizer...\")\n        evaluate_tokenizer.remote()\n\n    if run_base:\n        print(\"\\nStage 3/8: Training base model...\")\n        train_base_model.remote(\n            depth=depth, device_batch_size=device_batch_size_base, wandb_run=wandb_run\n        )\n        if run_eval:\n            print(\"Evaluating base model (CORE)...\")\n            evaluate_base_model.remote()\n            print(\"Evaluating base model (loss)...\")\n            evaluate_base_loss.remote()\n\n    if run_mid:\n        print(\"\\nStage 4/8: Midtraining (conversation tokens)...\")\n        train_mid_model.remote(\n            device_batch_size=device_batch_size_base, wandb_run=wandb_run\n        )\n        if run_eval:\n            print(\"Evaluating mid model...\")\n            evaluate_chat_model.remote(source=\"mid\")\n\n    if run_sft:\n        print(\"\\nStage 5/8: Supervised fine-tuning...\")\n        train_sft_model.remote(\n            device_batch_size=device_batch_size_sft, wandb_run=wandb_run, source=\"mid\"\n        )\n        if run_eval:\n            print(\"Evaluating SFT model...\")\n            evaluate_chat_model.remote(source=\"sft\")\n\n    if run_rl:\n        print(\"\\nStage 6/8: Reinforcement learning...\")\n        train_rl_model.remote(wandb_run=wandb_run)\n        if run_eval:\n            print(\"Evaluating RL model...\")\n            evaluate_chat_model.remote(source=\"rl\", tasks=\"GSM8K\")\n\n    if run_inference:\n        print(\"\\nStage 7/8: Testing inference...\")\n        final_source = \"rl\" if run_rl else \"sft\"\n        chat_cli.remote(source=final_source, prompt=\"Why is the sky blue?\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETED\")\n    print(\"=\" * 80)\n    print()\n    print(\"Next steps:\")\n    print(\"1. Chat via CLI: modal run TrainNanochatModal.py::chat_cli --source=sft\")\n    print(\"2. Launch Web UI: modal run TrainNanochatModal.py::chat_web --source=sft\")\n    print(\"3. Run more evals: modal run TrainNanochatModal.py::evaluate_chat_model --source=sft\")\n    print()\n</pre> @app.local_entrypoint() def main(     run_download: bool = True,     run_tokenizer: bool = True,     run_base: bool = True,     run_mid: bool = True,     run_sft: bool = True,     run_rl: bool = False,     run_eval: bool = True,     run_inference: bool = True,     num_data_shards: int = 240,     depth: int = 20,     device_batch_size_base: int = 32,     device_batch_size_sft: int = 4,     wandb_run: str = \"dummy\", ):     \"\"\"     Run the complete nanochat pipeline from scratch.      Pipeline stages:     1. Download FineWeb dataset     2. Train + evaluate tokenizer     3. Train + evaluate base model     4. Train + evaluate mid model     5. Train + evaluate SFT model     6. (Optional) Train + evaluate RL model     7. Run final inference test      Configuration modes:     - Full Speedrun (4h, $96): num_data_shards=240, depth=20     - Quick Test (1h, $24): num_data_shards=8, depth=12     - GPT-2 Grade (12h, $288): num_data_shards=450, depth=26     \"\"\"     print(\"=\" * 80)     print(\"NANOCHAT TRAINING PIPELINE\")     print(\"=\" * 80)     print(f\"Mode: {'Speedrun' if num_data_shards &gt;= 240 else 'Quick Test'}\")     print(f\"Data shards: {num_data_shards}\")     print(f\"Model depth: {depth}\")     print(f\"WandB run: {wandb_run}\")     print(\"=\" * 80)     print()      if run_download:         print(\"Stage 1/8: Downloading dataset...\")         download_dataset.remote(num_shards=num_data_shards)      if run_tokenizer:         print(\"\\nStage 2/8: Training tokenizer...\")         train_tokenizer.remote()         print(\"Evaluating tokenizer...\")         evaluate_tokenizer.remote()      if run_base:         print(\"\\nStage 3/8: Training base model...\")         train_base_model.remote(             depth=depth, device_batch_size=device_batch_size_base, wandb_run=wandb_run         )         if run_eval:             print(\"Evaluating base model (CORE)...\")             evaluate_base_model.remote()             print(\"Evaluating base model (loss)...\")             evaluate_base_loss.remote()      if run_mid:         print(\"\\nStage 4/8: Midtraining (conversation tokens)...\")         train_mid_model.remote(             device_batch_size=device_batch_size_base, wandb_run=wandb_run         )         if run_eval:             print(\"Evaluating mid model...\")             evaluate_chat_model.remote(source=\"mid\")      if run_sft:         print(\"\\nStage 5/8: Supervised fine-tuning...\")         train_sft_model.remote(             device_batch_size=device_batch_size_sft, wandb_run=wandb_run, source=\"mid\"         )         if run_eval:             print(\"Evaluating SFT model...\")             evaluate_chat_model.remote(source=\"sft\")      if run_rl:         print(\"\\nStage 6/8: Reinforcement learning...\")         train_rl_model.remote(wandb_run=wandb_run)         if run_eval:             print(\"Evaluating RL model...\")             evaluate_chat_model.remote(source=\"rl\", tasks=\"GSM8K\")      if run_inference:         print(\"\\nStage 7/8: Testing inference...\")         final_source = \"rl\" if run_rl else \"sft\"         chat_cli.remote(source=final_source, prompt=\"Why is the sky blue?\")      print(\"\\n\" + \"=\" * 80)     print(\"PIPELINE COMPLETED\")     print(\"=\" * 80)     print()     print(\"Next steps:\")     print(\"1. Chat via CLI: modal run TrainNanochatModal.py::chat_cli --source=sft\")     print(\"2. Launch Web UI: modal run TrainNanochatModal.py::chat_web --source=sft\")     print(\"3. Run more evals: modal run TrainNanochatModal.py::evaluate_chat_model --source=sft\")     print()"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#configuration","title":"============================================================================= CONFIGURATION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#modal-app-and-volumes","title":"============================================================================= MODAL APP AND VOLUMES\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#secrets-setup","title":"============================================================================= SECRETS SETUP\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#container-image","title":"============================================================================= CONTAINER IMAGE\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#helper-functions","title":"============================================================================= HELPER FUNCTIONS\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-1-dataset-download","title":"============================================================================= STAGE 1: DATASET DOWNLOAD\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-2-tokenizer-training","title":"============================================================================= STAGE 2: TOKENIZER TRAINING\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-3-base-model-pretraining","title":"============================================================================= STAGE 3: BASE MODEL PRETRAINING\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-4-midtraining","title":"============================================================================= STAGE 4: MIDTRAINING\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-5-supervised-fine-tuning-sft","title":"============================================================================= STAGE 5: SUPERVISED FINE-TUNING (SFT)\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-6-reinforcement-learning-optional","title":"============================================================================= STAGE 6: REINFORCEMENT LEARNING (OPTIONAL)\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-7-evaluation","title":"============================================================================= STAGE 7: EVALUATION\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#stage-8-inference","title":"============================================================================= STAGE 8: INFERENCE\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModal/#main-pipeline","title":"============================================================================= MAIN PIPELINE\u00b6","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/","title":"Training Nanochat on Modal","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#training-nanochat-on-modal-build-your-own-chatgpt-from-scratch","title":"Training Nanochat on Modal: Build Your Own ChatGPT from Scratch","text":"<p>\ud83d\udcc4 View Complete Python Script</p> <p>So you've fine-tuned models with LoRA, trained small GPTs, even tackled multi-GPU training. Now let's go full circle and build ChatGPT from absolute scratch - tokenizer training, base model pretraining, conversation fine-tuning, the works.</p> <p>We're talking about the complete Andrej Karpathy nanochat speedrun. Everything from \"raw text on the internet\" to \"functioning ChatGPT clone that can have conversations and use tools.\"</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#why-nanochat","title":"Why Nanochat?","text":"<p>Here's the thing - if you want to truly understand how ChatGPT works, you can't just fine-tune a pre-trained model. You need to see the entire pipeline.</p> <p>Andrej Karpathy built nanochat as the most comprehensive educational implementation of modern LLM training. It's not a toy - it's the real deal, just scaled down to be understandable and trainable on reasonable hardware.</p> <p>What makes nanochat special: - Complete pipeline - Every single step from tokenizer to deployment - Production techniques - Same methods used by OpenAI, Anthropic, etc. - Educational focus - Clean, readable code with excellent documentation - Proven results - Actually produces working chat models with tool use - Reasonable scale - Train on 4-8 GPUs instead of thousands</p> <p>This is basically \"here's how we built ChatGPT\" but in a form you can actually run and understand. The original repo assumes you have a local GPU cluster. We're taking that exact pipeline and making it run on Modal's serverless infrastructure.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#what-were-building","title":"What We're Building","text":"<p>This isn't just training a model. We're building the entire stack:</p> <p>Stage 1: Download Dataset - Grab FineWeb-edu (100B tokens of high-quality text) Stage 2: Train Tokenizer - Build a custom BPE tokenizer (like GPT-4 uses) Stage 3: Base Pretraining - Train a GPT on raw internet text Stage 4: Midtraining - Teach conversation format and tool use Stage 5: Supervised Fine-tuning - Train on specific tasks (code, math, chat) Stage 6: Reinforcement Learning - Optional GRPO on math problems Stage 7: Evaluation - Measure performance on real benchmarks Stage 8: Inference - Chat with your model (CLI and web UI)</p> <p>The beauty of this pipeline? Each stage is independent. Screw up fine-tuning? Just re-run that step. Want to try different hyperparameters? Preprocessing is already done.</p> <p>Here's what the complete flow looks like:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Download FineWeb   \u2502  (CPU - cheap, run once)\n\u2502  100B tokens        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Train Tokenizer    \u2502  (1 GPU - 30-60 min)\n\u2502  Custom BPE 65K     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Pretrain Base      \u2502  (4-8 GPUs - 2-4 hours)\n\u2502  Language Model     \u2502  \u2190 The big one\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Midtrain           \u2502  (4 GPUs - 30-45 min)\n\u2502  Conversation       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SFT                \u2502  (4 GPUs - 30-45 min)\n\u2502  Task-specific      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  RL (Optional)      \u2502  (4 GPUs - 30-45 min)\n\u2502  Math reasoning     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n      \u2502         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Chat   \u2502 \u2502  Eval   \u2502\n\u2502  CLI/Web\u2502 \u2502  Metrics\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Total time for full speedrun: ~4-5 hours on 8\u00d7 A100-80GB Total cost: ~$100-150 for the complete pipeline</p> <p>Compare this to OpenAI spending millions... yeah, we're doing pretty well.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#getting-started","title":"Getting Started","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#install-modal","title":"Install Modal","text":"<p>You know the drill:</p> <pre><code>pip install modal\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#authenticate","title":"Authenticate","text":"<pre><code>modal setup\n</code></pre> <p>Or with API keys:</p> <pre><code>export MODAL_TOKEN_ID=&lt;your_token_id&gt;\nexport MODAL_TOKEN_SECRET=&lt;your_token_secret&gt;\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#set-up-your-secrets","title":"Set Up Your Secrets","text":"<p>For this pipeline, you'll want: - Hugging Face token - For downloading datasets - Weights &amp; Biases API key - For tracking training (highly recommended!)</p> <p>Create the Modal secret:</p> <pre><code>modal secret create nanochat-secrets \\\n  WANDB_API_KEY=xxxxxxxxxxxxx \\\n  HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxx\n</code></pre> <p>Get your tokens: - HF token: hf.co/settings/tokens - W&amp;B key: wandb.ai/authorize</p> <p>Or use a .env file (local development):</p> <pre><code>WANDB_API_KEY=your_key\nHUGGINGFACE_TOKEN=hf_your_token\n</code></pre> <p>The script tries <code>.env</code> first, then falls back to Modal secrets.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#clone-nanochat","title":"Clone Nanochat","text":"<p>This is important - we need the nanochat repo locally:</p> <pre><code>cd /path/to/ServerLessFinetuning\ngit clone https://github.com/karpathy/nanochat.git\n</code></pre> <p>Your folder structure should be:</p> <pre><code>ServerLessFinetuning/\n\u251c\u2500\u2500 TrainNanochatModal.py    # Your Modal script\n\u2514\u2500\u2500 nanochat/                 # Cloned repo\n    \u251c\u2500\u2500 scripts/\n    \u251c\u2500\u2500 nanochat/\n    \u2514\u2500\u2500 rustbpe/\n</code></pre> <p>Why clone it? Modal copies this entire directory into the container image. This way we have all of nanochat's scripts and utilities available.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#understanding-the-pipeline","title":"Understanding the Pipeline","text":"<p>Let me walk you through what makes this different from the other tutorials. This isn't fine-tuning - this is building everything from scratch.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#the-full-speedrun","title":"The Full Speedrun","text":"<p>Nanochat is designed as a \"speedrun\" - train a working ChatGPT in one go. Here's what each stage does:</p> <p>Stage 1: Dataset Download - Downloads FineWeb-edu-100B from HuggingFace - 240 shards \u00d7 250M characters = ~60B characters - Enough to train a 561M parameter model (Chinchilla optimal)</p> <p>Stage 2: Tokenizer Training - Trains a BPE tokenizer on 2B characters - Creates a 65K vocab (2^16 tokens) - Same approach as GPT-4</p> <p>Stage 3: Base Pretraining - Trains a GPT from random initialization - Uses Muon optimizer (better than Adam for LLMs) - Learns language from raw text</p> <p>Stage 4: Midtraining - Teaches conversation format (user/assistant turns) - Adds tool use (calculator) - Trains on SmolTalk + MMLU + GSM8K</p> <p>Stage 5: Supervised Fine-tuning - Task-specific training - MMLU (knowledge), ARC (reasoning), GSM8K (math), HumanEval (code)</p> <p>Stage 6: Reinforcement Learning (Optional) - GRPO/REINFORCE on math problems - Improves reasoning through self-play</p> <p>Stage 7: Evaluation - CORE metric (comprehensive benchmark) - Task-specific evals (ARC, GSM8K, HumanEval, MMLU)</p> <p>Stage 8: Inference - Chat CLI for interactive testing - FastAPI web UI for demos</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#configuration-and-setup","title":"Configuration and Setup","text":"<p>Alright, let's dive into the code. The configuration is straightforward but important.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#app-and-volumes","title":"App and Volumes","text":"<pre><code>from modal import App, Image as ModalImage, Volume, Secret\n\napp = App(\"nanochat-training\")\n\n# Two volumes: one for data, one for checkpoints\ndata_volume = Volume.from_name(\"nanochat-data\", create_if_missing=True)\ncheckpoint_volume = Volume.from_name(\"nanochat-checkpoints\", create_if_missing=True)\n\nVOLUME_CONFIG = {\n    \"/data\": data_volume,          # Dataset and cache\n    \"/checkpoints\": checkpoint_volume,  # Model checkpoints\n}\n</code></pre> <p>Why two volumes? - Data volume: FineWeb shards, tokenizer, eval data (~30GB) - Checkpoint volume: Model weights, training state (~20GB) - Separation makes it easier to manage and debug</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#configuration-constants","title":"Configuration Constants","text":"<pre><code>MINUTES = 60\nHOURS = 60 * 60\n\n# GPU type - can be changed based on your needs\nGPU_TYPE = \"a100-80gb\"\n\n# Multi-GPU configuration (nanochat supports 1-8 GPUs)\nNUM_GPUS_BASE = 4        # Base pretraining\nNUM_GPUS_MID = 4         # Midtraining\nNUM_GPUS_SFT = 4         # Supervised fine-tuning\nNUM_GPUS_RL = 4          # Reinforcement learning\nNUM_GPUS_EVAL = 4        # Evaluation\nNUM_GPUS_TOKENIZER = 1   # Tokenizer (single GPU)\nNUM_GPUS_INFERENCE = 1   # Inference (single GPU)\n\nBASE_DIR = \"/data/.cache/nanochat\"  # Everything goes here\n</code></pre> <p>GPU scaling: - 1 GPU: Full pipeline takes ~24 hours (~\\(84) - 4 GPUs: Full pipeline takes ~6 hours (~\\)96) - 8 GPUs: Full pipeline takes ~4 hours (~$112)</p> <p>The sweet spot is 4 GPUs - good parallelism without too much communication overhead.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#secrets-setup","title":"Secrets Setup","text":"<pre><code># Try .env first, then Modal secrets\ntry:\n    nanochat_secret = Secret.from_dotenv()\n    print(\"Loaded secrets from .env file\")\nexcept Exception:\n    try:\n        nanochat_secret = Secret.from_name(\"nanochat-secrets\")\n        print(\"Loaded secrets from Modal\")\n    except Exception:\n        nanochat_secret = None\n        print(\"No secrets found - WandB logging disabled\")\n</code></pre> <p>This is graceful - works with or without secrets. If you don't have W&amp;B, training still works, you just don't get the nice dashboards.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#building-the-container-image","title":"Building the Container Image","text":"<p>This is the most complex image we've built yet. We need CUDA, PyTorch, Rust (for the tokenizer), and all of nanochat's dependencies.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#why-this-image-is-different","title":"Why This Image is Different","text":"<pre><code>NANOCHAT_IMAGE = (\n    # NVIDIA CUDA 12.8 with Python 3.11\n    ModalImage.from_registry(\"nvidia/cuda:12.8.1-devel-ubuntu24.04\", add_python=\"3.11\")\n\n    # System dependencies\n    .apt_install(\"git\", \"build-essential\", \"curl\", \"wget\", \"unzip\")\n\n    # Install Rust (needed for tokenizer)\n    .run_commands(\n        \"curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\",\n        \"echo 'source $HOME/.cargo/env' &gt;&gt; $HOME/.bashrc\",\n    )\n\n    # Install uv (fast Python package installer)\n    .run_commands(\n        \"curl -LsSf https://astral.sh/uv/install.sh | sh\",\n        \"echo 'export PATH=\\\"$HOME/.cargo/bin:$PATH\\\"' &gt;&gt; $HOME/.bashrc\",\n    )\n\n    # Copy nanochat repo into the image\n    .add_local_dir(local_path=\"nanochat\", remote_path=\"/root/nanochat\", copy=True)\n\n    # Set working directory\n    .workdir(\"/root/nanochat\")\n\n    # Install Python dependencies AND build Rust tokenizer\n    .run_commands(\n        \"bash -c 'source $HOME/.cargo/env &amp;&amp; uv sync &amp;&amp; uv run maturin develop --release --manifest-path rustbpe/Cargo.toml'\"\n    )\n\n    # Environment variables\n    .env({\n        \"OMP_NUM_THREADS\": \"1\",\n        \"NANOCHAT_BASE_DIR\": \"/data/.cache/nanochat\",\n        \"HF_HOME\": \"/data/.cache/huggingface\",\n    })\n)\n</code></pre> <p>Key points:</p> <ol> <li> <p>Rust installation: Nanochat's tokenizer is written in Rust for speed. We need the full Rust toolchain.</p> </li> <li> <p>uv sync: This reads <code>pyproject.toml</code> and installs all dependencies in a virtual environment. Much faster than pip.</p> </li> <li> <p>maturin develop: Builds the Rust tokenizer and makes it importable from Python. This is the magic that makes nanochat's tokenizer so fast.</p> </li> <li> <p>add_local_dir: Copies your local nanochat clone into the image. This is why you need to clone it first.</p> </li> </ol> <p>\u23f0 Build time warning: First build takes 15-20 minutes. The Rust tokenizer compilation is the slow part. But Modal caches everything - subsequent runs are instant.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#helper-functions","title":"Helper Functions","text":"<p>Before we get to the stages, let's look at the helper functions that make everything work.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#setup-functions","title":"Setup Functions","text":"<pre><code>def setup_base_dir():\n    \"\"\"Create directory structure for nanochat artifacts.\"\"\"\n    import os\n\n    os.makedirs(BASE_DIR, exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/base_data\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/tokenizer\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/checkpoints\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/eval_bundle\", exist_ok=True)\n    os.makedirs(f\"{BASE_DIR}/report\", exist_ok=True)\n\ndef setup_secrets():\n    \"\"\"Set up environment variables from secrets.\"\"\"\n    import os\n\n    if \"WANDB_API_KEY\" in os.environ:\n        print(\"WandB API key found\")\n    else:\n        print(\"WandB API key not found - logging disabled\")\n\n    if \"HUGGINGFACE_TOKEN\" in os.environ:\n        os.environ[\"HF_TOKEN\"] = os.environ[\"HUGGINGFACE_TOKEN\"]\n        print(\"HuggingFace token found\")\n    else:\n        print(\"HuggingFace token not found\")\n</code></pre> <p>Simple but important - creates all the directories nanochat expects and sets up authentication.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#torchrun-helper","title":"Torchrun Helper","text":"<pre><code>def run_torchrun_command(script: str, num_gpus: int, extra_args: list = None):\n    \"\"\"Run nanochat script with torchrun for multi-GPU training.\"\"\"\n    import subprocess\n\n    if extra_args is None:\n        extra_args = []\n\n    extra_args_str = \" \".join(extra_args) if extra_args else \"\"\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run torchrun --standalone --nproc_per_node={num_gpus} -m {script}\"\n\n    if extra_args:\n        cmd += f\" -- {extra_args_str}\"\n\n    print(f\"Running: {cmd}\")\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Command failed with code {result.returncode}\")\n\n    return result\n</code></pre> <p>What torchrun does: - Spawns one process per GPU - Sets up distributed training (NCCL backend) - Handles rank assignment and synchronization - Makes multi-GPU training \"just work\"</p> <p>This is how nanochat does distributed training without writing custom distributed code.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-1-dataset-download","title":"Stage 1: Dataset Download","text":"<p>Let's start with the data. We're downloading FineWeb-edu, which is basically \"the good parts of the internet.\"</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n    # No GPU - saves money!\n)\ndef download_dataset(num_shards: int = 240):\n    \"\"\"\n    Download FineWeb dataset shards from HuggingFace.\n\n    Each shard is ~250M characters (~100MB compressed).\n    - Full speedrun: 240 shards (~60B characters, ~24GB)\n    - Testing: 8 shards (~2B characters, ~800MB)\n    \"\"\"\n    import subprocess\n\n    setup_base_dir()\n\n    print(\"=\" * 80)\n    print(f\"DOWNLOADING FINEWEB DATASET - {num_shards} SHARDS\")\n    print(\"=\" * 80)\n    print(f\"Total data: ~{num_shards * 250 / 1000:.1f}B characters (~{num_shards * 100 / 1024:.1f}GB)\")\n    print()\n\n    result = subprocess.run(\n        [\"bash\", \"-c\", f\"cd /root/nanochat &amp;&amp; uv run python -m nanochat.dataset -n {num_shards}\"],\n        capture_output=True,\n        text=True,\n    )\n\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Dataset download failed with code {result.returncode}\")\n\n    data_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Downloaded {num_shards} shards successfully\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"num_shards\": num_shards,\n        \"data_dir\": f\"{BASE_DIR}/base_data\",\n    }\n</code></pre> <p>How many shards do you need?</p> Model Size Parameters Chinchilla Tokens Characters Needed Shards Tiny test ~100M 2B ~10B 40 Quick test ~200M 4B ~20B 80 Speedrun 561M (d20) 11B ~54B 240 GPT-2 grade ~1B (d26) 20B ~96B 400 <p>For testing, use 8 shards. For the real speedrun, use 240.</p> <p>Run it:</p> <pre><code># Full speedrun dataset\nmodal run TrainNanochatModal.py::download_dataset\n\n# Quick test dataset\nmodal run TrainNanochatModal.py::download_dataset --num-shards=8\n</code></pre> <p>First download takes 30-60 minutes depending on internet speed. Cached after that.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-2-tokenizer-training","title":"Stage 2: Tokenizer Training","text":"<p>Now we train a custom tokenizer on the FineWeb data. This is similar to how GPT-4's tokenizer was trained.</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_TOKENIZER}\",\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n)\ndef train_tokenizer(\n    max_chars: int = 2_000_000_000,  # 2 billion characters\n    vocab_size: int = 65536,          # 65K vocab (2^16)\n    doc_cap: int = 10000,              # Max chars per document\n):\n    \"\"\"\n    Train a custom BPE tokenizer on FineWeb data.\n    Training takes 30-60 minutes on a single GPU.\n    \"\"\"\n    import subprocess\n\n    setup_base_dir()\n\n    print(\"=\" * 80)\n    print(\"TRAINING CUSTOM BPE TOKENIZER\")\n    print(\"=\" * 80)\n    print(f\"Max characters: {max_chars:,}\")\n    print(f\"Vocabulary size: {vocab_size:,}\")\n    print(f\"Document cap: {doc_cap:,}\")\n    print()\n\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.tok_train --max_chars={max_chars} --vocab_size={vocab_size} --doc_cap={doc_cap}\"\n\n    print(f\"Running: {cmd}\")\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Tokenizer training failed with code {result.returncode}\")\n\n    data_volume.commit()\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Tokenizer training completed\")\n    print(f\"Tokenizer saved to {BASE_DIR}/tokenizer/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"max_chars\": max_chars,\n        \"vocab_size\": vocab_size,\n        \"tokenizer_dir\": f\"{BASE_DIR}/tokenizer\",\n    }\n</code></pre> <p>Why train a custom tokenizer?</p> <p>You might wonder \"why not use GPT-2's tokenizer?\" Here's why:</p> <ol> <li>Domain-specific: FineWeb has different token distributions than GPT-2's training data</li> <li>Efficiency: Custom tokenizers compress better (lower bits per byte)</li> <li>Control: You decide vocab size, special tokens, etc.</li> <li>Learning: Understanding tokenization is crucial for understanding LLMs</li> </ol> <p>The training process: - Samples 2B characters from FineWeb - Runs Byte Pair Encoding (BPE) to find common subwords - Creates a 65K vocabulary - Saves tokenizer model and merges</p> <p>Run it:</p> <pre><code>modal run TrainNanochatModal.py::train_tokenizer\n</code></pre> <p>Takes 30-60 minutes on a single GPU. You can also evaluate it:</p> <pre><code>modal run TrainNanochatModal.py::evaluate_tokenizer\n</code></pre> <p>This shows you the compression ratio and other metrics.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-3-base-model-pretraining","title":"Stage 3: Base Model Pretraining","text":"<p>Here's the big one. We're training a GPT from scratch on internet text. This is where most of the compute goes.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#model-architecture","title":"Model Architecture","text":"<p>Nanochat uses a simple but effective architecture:</p> <pre><code>depth: int = 20  # Model depth parameter\n\n# Derived dimensions:\nmodel_dim = depth * 64      # 20 * 64 = 1280\nnum_heads = model_dim / 128 # 1280 / 128 = 10\nnum_layers = depth          # 20 layers\n\n# Total parameters: ~561M\n</code></pre> <p>Model sizes: - depth=12: ~200M params (quick test) - depth=20: ~561M params (default speedrun) - depth=26: ~1B params (GPT-2 grade)</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#the-training-function","title":"The Training Function","text":"<pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_BASE}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=8 * HOURS,\n)\ndef train_base_model(\n    depth: int = 20,\n    device_batch_size: int = 32,\n    max_iterations: int = -1,  # -1 = auto from Chinchilla\n    wandb_run: str = \"dummy\",\n):\n    \"\"\"\n    Pretrain the base GPT model on FineWeb.\n\n    Model sizes: depth=20 (561M params), depth=26 (1B params)\n    Training duration: ~2-3 hours on 8 GPUs, ~16-24 hours on 1 GPU\n    \"\"\"\n    import subprocess\n    import os\n\n    setup_base_dir()\n    setup_secrets()\n\n    # Download eval bundle if needed (for CORE metric)\n    eval_bundle_path = f\"{BASE_DIR}/eval_bundle\"\n    if not os.path.exists(eval_bundle_path):\n        print(\"Downloading eval bundle...\")\n        subprocess.run([\n            \"curl\", \"-L\", \"-o\", \"eval_bundle.zip\",\n            \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\",\n        ], check=True)\n        subprocess.run([\"unzip\", \"-q\", \"eval_bundle.zip\"], check=True)\n        subprocess.run([\"mv\", \"eval_bundle\", eval_bundle_path], check=True)\n        subprocess.run([\"rm\", \"eval_bundle.zip\"], check=True)\n\n    print(\"=\" * 80)\n    print(\"PRETRAINING BASE MODEL ON FINEWEB\")\n    print(\"=\" * 80)\n    print(f\"Model depth: {depth}\")\n    print(f\"Estimated parameters: {depth * depth * 64 * 12 // 1_000_000}M\")\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_BASE}\")\n    print(f\"WandB run: {wandb_run}\")\n    print()\n\n    extra_args = [\n        f\"--depth={depth}\",\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--run={wandb_run}\",\n    ]\n\n    if max_iterations &gt; 0:\n        extra_args.append(f\"--num_iterations={max_iterations}\")\n\n    run_torchrun_command(\"scripts.base_train\", NUM_GPUS_BASE, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Base model training completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/base/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"depth\": depth,\n        \"device_batch_size\": device_batch_size,\n        \"num_gpus\": NUM_GPUS_BASE,\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/base\",\n    }\n</code></pre> <p>What happens during training:</p> <ol> <li>Initialization: Random weights, Muon optimizer setup</li> <li>Training loop: Read batches, forward pass, backward pass, update weights</li> <li>Evaluation: Every N steps, compute validation loss and CORE metric</li> <li>Checkpointing: Save model state every N steps</li> <li>Logging: Send metrics to W&amp;B (if configured)</li> </ol> <p>The Muon Optimizer:</p> <p>Nanochat uses Muon for linear layers and AdamW for embeddings. Muon is a new optimizer that: - Converges faster than Adam for transformers - Uses less memory (no momentum for linear layers) - More stable for large learning rates</p> <p>It's basically \"what if we applied optimizer improvements from recent research?\"</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#training-duration-and-cost","title":"Training Duration and Cost","text":"Setup GPUs Time Cost Quick test (d12, 8 shards) 1 ~2 hours ~$7 Quick test (d12, 8 shards) 4 ~30 min ~$7 Full speedrun (d20, 240 shards) 1 ~20 hours ~$70 Full speedrun (d20, 240 shards) 4 ~5 hours ~$70 Full speedrun (d20, 240 shards) 8 ~3 hours ~$84 GPT-2 grade (d26, 400 shards) 8 ~10 hours ~$280 <p>Run it:</p> <pre><code># Quick test (make sure it works)\nmodal run TrainNanochatModal.py::train_base_model \\\n  --depth=12 \\\n  --device-batch-size=32 \\\n  --max-iterations=1000\n\n# Full speedrun\nmodal run TrainNanochatModal.py::train_base_model \\\n  --depth=20 \\\n  --device-batch-size=32 \\\n  --wandb-run=\"my-speedrun-$(date +%Y%m%d)\"\n</code></pre> <p>Monitor training: - Modal dashboard shows real-time logs and GPU utilization - W&amp;B shows loss curves, learning rate schedule, CORE metric - All 4 (or 8) GPUs should be at ~95-100% utilization</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#evaluation","title":"Evaluation","text":"<p>After training (or during), you can evaluate:</p> <pre><code># CORE metric (comprehensive benchmark)\nmodal run TrainNanochatModal.py::evaluate_base_model\n\n# Validation loss (bits per byte)\nmodal run TrainNanochatModal.py::evaluate_base_loss\n</code></pre> <p>CORE metric measures general language understanding. A good d20 model gets ~0.35-0.40. For comparison: - Random: 0.0 - Llama 3 8B: ~0.50 - GPT-4: ~0.60</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-4-midtraining","title":"Stage 4: Midtraining","text":"<p>Now we teach the model how to have conversations. This is where it learns special tokens like <code>&lt;|user_start|&gt;</code>, <code>&lt;|assistant_end|&gt;</code>, etc.</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_MID}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=2 * HOURS,\n)\ndef train_mid_model(\n    device_batch_size: int = 32,\n    wandb_run: str = \"dummy\",\n):\n    \"\"\"\n    Midtrain the model on conversation data.\n\n    Teaches conversation tokens, tool use, and multiple choice format.\n    Duration: ~30-45 minutes on 8 GPUs\n    \"\"\"\n    setup_secrets()\n\n    print(\"=\" * 80)\n    print(\"MIDTRAINING - TEACHING CONVERSATION TOKENS\")\n    print(\"=\" * 80)\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_MID}\")\n    print()\n\n    extra_args = [\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--run={wandb_run}\",\n    ]\n\n    run_torchrun_command(\"scripts.mid_train\", NUM_GPUS_MID, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Midtraining completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/mid/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/mid\",\n    }\n</code></pre> <p>What midtraining teaches:</p> <ol> <li> <p>Conversation format: </p><pre><code>&lt;|user_start|&gt;What is 2+2?&lt;|user_end|&gt;\n&lt;|assistant_start|&gt;2+2 equals 4.&lt;|assistant_end|&gt;\n</code></pre> </li> <li> <p>Tool use: </p><pre><code>&lt;|user_start|&gt;What is 123 * 456?&lt;|user_end|&gt;\n&lt;|assistant_start|&gt;&lt;calculator&gt;123*456&lt;/calculator&gt;\nThe answer is 56088.&lt;|assistant_end|&gt;\n</code></pre> </li> <li> <p>Multiple choice: </p><pre><code>Question: What is the capital of France?\nA) London  B) Paris  C) Berlin  D) Madrid\nAnswer: B\n</code></pre> </li> </ol> <p>Training data: - SmolTalk: 460K conversations - MMLU auxiliary: 100K multiple choice - GSM8K: 8K math problems with calculator</p> <p>Run it:</p> <pre><code>modal run TrainNanochatModal.py::train_mid_model\n</code></pre> <p>Takes 30-45 minutes on 4-8 GPUs, costs ~$5-7.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-5-supervised-fine-tuning","title":"Stage 5: Supervised Fine-tuning","text":"<p>Now we specialize the model on specific tasks: knowledge, reasoning, math, code, and chat.</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_SFT}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=2 * HOURS,\n)\ndef train_sft_model(\n    device_batch_size: int = 4,\n    num_epochs: int = 1,\n    wandb_run: str = \"dummy\",\n    source: str = \"mid\",\n):\n    \"\"\"\n    Supervised fine-tuning on task-specific data.\n\n    Trains on MMLU, ARC, GSM8K, HumanEval, and SmolTalk.\n    Duration: ~30-45 minutes on 8 GPUs\n    \"\"\"\n    setup_secrets()\n\n    print(\"=\" * 80)\n    print(\"SUPERVISED FINE-TUNING\")\n    print(\"=\" * 80)\n    print(f\"Source: {source}\")\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_SFT}\")\n    print(f\"Epochs: {num_epochs}\")\n    print()\n\n    extra_args = [\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--num_epochs={num_epochs}\",\n        f\"--run={wandb_run}\",\n        f\"--source={source}\",\n    ]\n\n    run_torchrun_command(\"scripts.chat_sft\", NUM_GPUS_SFT, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"SFT completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/sft/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/sft\",\n    }\n</code></pre> <p>Training mixture:</p> Dataset Examples Task Weight MMLU ~14K General knowledge 25% ARC-Easy ~2.4K Science reasoning 15% ARC-Challenge ~1.2K Hard science 15% GSM8K ~7.5K Math with tools 20% HumanEval ~164 Code generation 5% SmolTalk ~50K Conversations 20% <p>The mixture is designed to produce a well-rounded model that can chat, reason, do math, and write code.</p> <p>Run it:</p> <pre><code># Start from midtrained model (recommended)\nmodal run TrainNanochatModal.py::train_sft_model --source=mid\n\n# Or start from base model (skip midtraining)\nmodal run TrainNanochatModal.py::train_sft_model --source=base\n</code></pre> <p>Takes 30-45 minutes on 4-8 GPUs, costs ~$5-7.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-6-reinforcement-learning-optional","title":"Stage 6: Reinforcement Learning (Optional)","text":"<p>This is optional but improves math reasoning significantly.</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_RL}\",\n    volumes=VOLUME_CONFIG,\n    secrets=[nanochat_secret] if nanochat_secret else [],\n    timeout=2 * HOURS,\n)\ndef train_rl_model(\n    device_batch_size: int = 8,\n    num_epochs: int = 1,\n    wandb_run: str = \"dummy\",\n    source: str = \"sft\",\n):\n    \"\"\"\n    Reinforcement learning on GSM8K (optional).\n\n    Uses GRPO/REINFORCE to improve math reasoning.\n    Duration: ~30-45 minutes on 8 GPUs\n    \"\"\"\n    setup_secrets()\n\n    print(\"=\" * 80)\n    print(\"REINFORCEMENT LEARNING ON GSM8K\")\n    print(\"=\" * 80)\n    print(f\"Source: {source}\")\n    print(f\"Device batch size: {device_batch_size}\")\n    print(f\"Number of GPUs: {NUM_GPUS_RL}\")\n    print(f\"Epochs: {num_epochs}\")\n    print()\n\n    extra_args = [\n        f\"--device_batch_size={device_batch_size}\",\n        f\"--num_epochs={num_epochs}\",\n        f\"--run={wandb_run}\",\n        f\"--source={source}\",\n    ]\n\n    run_torchrun_command(\"scripts.chat_rl\", NUM_GPUS_RL, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RL training completed\")\n    print(f\"Checkpoints saved to {BASE_DIR}/checkpoints/rl/\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"checkpoint_dir\": f\"{BASE_DIR}/checkpoints/rl\",\n    }\n</code></pre> <p>How RL works here:</p> <ol> <li>Model generates multiple answers to a math problem</li> <li>Answers that get the right solution get positive reward</li> <li>Answers that get the wrong solution get negative reward</li> <li>Update model to generate more correct answers</li> </ol> <p>This is simplified GRPO (Group Relative Policy Optimization). It's like PPO but simpler and works well for math.</p> <p>Expected improvement: - GSM8K accuracy: 60% \u2192 75% after RL - ARC/MMLU: Stays about the same (RL only trains on math)</p> <p>Run it:</p> <pre><code>modal run TrainNanochatModal.py::train_rl_model\n</code></pre> <p>Takes 30-45 minutes on 4-8 GPUs, costs ~$5-7.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-7-evaluation","title":"Stage 7: Evaluation","text":"<p>Now let's measure how good our model actually is. Nanochat includes comprehensive evaluation on real benchmarks.</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_EVAL}\",\n    volumes=VOLUME_CONFIG,\n    timeout=2 * HOURS,\n)\ndef evaluate_chat_model(\n    source: str = \"sft\",\n    tasks: str = \"all\",\n):\n    \"\"\"\n    Evaluate the chat model on benchmark tasks.\n\n    Available tasks: ARC-Easy, ARC-Challenge, GSM8K, HumanEval, MMLU, ChatCORE\n    \"\"\"\n    print(\"=\" * 80)\n    print(f\"EVALUATING CHAT MODEL - {source.upper()}\")\n    print(\"=\" * 80)\n    print(f\"Tasks: {tasks}\")\n    print()\n\n    extra_args = [\"-i\", source]\n\n    if tasks != \"all\":\n        extra_args.extend([\"-a\", tasks])\n\n    run_torchrun_command(\"scripts.chat_eval\", NUM_GPUS_EVAL, extra_args)\n\n    checkpoint_volume.commit()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Evaluation of {source} model completed\")\n    print(\"=\" * 80)\n\n    return {\n        \"status\": \"completed\",\n        \"source\": source,\n        \"tasks\": tasks,\n    }\n</code></pre> <p>Available benchmarks:</p> Benchmark What it measures Good score ARC-Easy Elementary science &gt;60% ARC-Challenge Hard science &gt;35% GSM8K Grade school math &gt;60% HumanEval Code generation &gt;20% MMLU General knowledge &gt;50% ChatCORE Conversation quality &gt;0.40 <p>Run evaluation:</p> <pre><code># Evaluate all tasks\nmodal run TrainNanochatModal.py::evaluate_chat_model --source=sft\n\n# Evaluate specific tasks\nmodal run TrainNanochatModal.py::evaluate_chat_model \\\n  --source=rl \\\n  --tasks=\"GSM8K,ARC-Challenge\"\n\n# Compare mid vs sft vs rl\nmodal run TrainNanochatModal.py::evaluate_chat_model --source=mid\nmodal run TrainNanochatModal.py::evaluate_chat_model --source=sft\nmodal run TrainNanochatModal.py::evaluate_chat_model --source=rl\n</code></pre> <p>Takes 30-60 minutes depending on which tasks you run. Results are saved to the volume and logged to W&amp;B.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#stage-8-inference","title":"Stage 8: Inference","text":"<p>Finally, let's chat with our model! Nanochat provides both a CLI and a web UI.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#chat-cli","title":"Chat CLI","text":"<pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_INFERENCE}\",\n    volumes=VOLUME_CONFIG,\n    timeout=1 * HOURS,\n)\ndef chat_cli(\n    source: str = \"sft\",\n    prompt: str = \"\",\n    temperature: float = 0.6,\n    top_k: int = 50,\n):\n    \"\"\"Chat with the model via command line interface.\"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(f\"CHAT CLI - {source.upper()} MODEL\")\n    print(\"=\" * 80)\n\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.chat_cli -i {source} -t {temperature} -k {top_k}\"\n\n    if prompt:\n        escaped_prompt = prompt.replace('\"', '\\\\\"')\n        cmd += f' -p \"{escaped_prompt}\"'\n\n    print(f\"Running: {cmd}\")\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Chat CLI failed with code {result.returncode}\")\n\n    return {\n        \"status\": \"completed\",\n        \"source\": source,\n        \"prompt\": prompt,\n    }\n</code></pre> <p>Use it:</p> <pre><code># Single question\nmodal run TrainNanochatModal.py::chat_cli \\\n  --source=sft \\\n  --prompt=\"Why is the sky blue?\"\n\n# Interactive mode (empty prompt)\nmodal run TrainNanochatModal.py::chat_cli --source=sft\n\n# With different sampling\nmodal run TrainNanochatModal.py::chat_cli \\\n  --source=rl \\\n  --temperature=0.8 \\\n  --top-k=100\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#chat-web-ui","title":"Chat Web UI","text":"<p>For a more user-friendly interface:</p> <pre><code>@app.function(\n    image=NANOCHAT_IMAGE,\n    gpu=f\"{GPU_TYPE}:{NUM_GPUS_INFERENCE}\",\n    volumes=VOLUME_CONFIG,\n    timeout=4 * HOURS,\n    max_containers=2,\n)\ndef chat_web(\n    source: str = \"sft\",\n    port: int = 8000,\n    temperature: float = 0.8,\n    top_k: int = 50,\n    max_tokens: int = 512,\n):\n    \"\"\"Serve the chat model via a web UI.\"\"\"\n    import subprocess\n\n    print(\"=\" * 80)\n    print(f\"STARTING WEB UI - {source.upper()} MODEL\")\n    print(\"=\" * 80)\n    print(f\"Port: {port}\")\n    print(f\"Temperature: {temperature}\")\n    print(f\"Top-k: {top_k}\")\n    print(f\"Max tokens: {max_tokens}\")\n    print()\n\n    cmd = f\"cd /root/nanochat &amp;&amp; uv run python -m scripts.chat_web -i {source} -p {port} -t {temperature} -k {top_k} -m {max_tokens} --host 0.0.0.0\"\n\n    print(f\"Running: {cmd}\")\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Web UI will be available at: http://localhost:{port}\")\n    print(\"=\" * 80)\n    print()\n\n    result = subprocess.run([\"bash\", \"-c\", cmd], capture_output=False, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"Web server failed with code {result.returncode}\")\n\n    return {\n        \"status\": \"completed\",\n        \"source\": source,\n        \"port\": port,\n    }\n</code></pre> <p>Deploy it:</p> <pre><code>modal deploy TrainNanochatModal.py\n</code></pre> <p>Modal gives you a URL. Open it in your browser and you have a ChatGPT-like interface!</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#running-the-complete-pipeline","title":"Running the Complete Pipeline","text":"<p>Alright, let's put it all together. Here's how to run the full speedrun from scratch.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#the-main-pipeline","title":"The Main Pipeline","text":"<pre><code>@app.local_entrypoint()\ndef main(\n    run_download: bool = True,\n    run_tokenizer: bool = True,\n    run_base: bool = True,\n    run_mid: bool = True,\n    run_sft: bool = True,\n    run_rl: bool = False,\n    run_eval: bool = True,\n    run_inference: bool = True,\n    num_data_shards: int = 240,\n    depth: int = 20,\n    device_batch_size_base: int = 32,\n    device_batch_size_sft: int = 4,\n    wandb_run: str = \"dummy\",\n):\n    \"\"\"\n    Run the complete nanochat pipeline from scratch.\n\n    Configuration modes:\n    - Full Speedrun (4h, $96): num_data_shards=240, depth=20\n    - Quick Test (1h, $24): num_data_shards=8, depth=12\n    - GPT-2 Grade (12h, $288): num_data_shards=450, depth=26\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"NANOCHAT TRAINING PIPELINE\")\n    print(\"=\" * 80)\n    print(f\"Mode: {'Speedrun' if num_data_shards &gt;= 240 else 'Quick Test'}\")\n    print(f\"Data shards: {num_data_shards}\")\n    print(f\"Model depth: {depth}\")\n    print(f\"WandB run: {wandb_run}\")\n    print(\"=\" * 80)\n    print()\n\n    if run_download:\n        print(\"Stage 1/8: Downloading dataset...\")\n        download_dataset.remote(num_shards=num_data_shards)\n\n    if run_tokenizer:\n        print(\"\\nStage 2/8: Training tokenizer...\")\n        train_tokenizer.remote()\n        print(\"Evaluating tokenizer...\")\n        evaluate_tokenizer.remote()\n\n    if run_base:\n        print(\"\\nStage 3/8: Training base model...\")\n        train_base_model.remote(\n            depth=depth, device_batch_size=device_batch_size_base, wandb_run=wandb_run\n        )\n        if run_eval:\n            print(\"Evaluating base model (CORE)...\")\n            evaluate_base_model.remote()\n            print(\"Evaluating base model (loss)...\")\n            evaluate_base_loss.remote()\n\n    if run_mid:\n        print(\"\\nStage 4/8: Midtraining (conversation tokens)...\")\n        train_mid_model.remote(\n            device_batch_size=device_batch_size_base, wandb_run=wandb_run\n        )\n        if run_eval:\n            print(\"Evaluating mid model...\")\n            evaluate_chat_model.remote(source=\"mid\")\n\n    if run_sft:\n        print(\"\\nStage 5/8: Supervised fine-tuning...\")\n        train_sft_model.remote(\n            device_batch_size=device_batch_size_sft, wandb_run=wandb_run, source=\"mid\"\n        )\n        if run_eval:\n            print(\"Evaluating SFT model...\")\n            evaluate_chat_model.remote(source=\"sft\")\n\n    if run_rl:\n        print(\"\\nStage 6/8: Reinforcement learning...\")\n        train_rl_model.remote(wandb_run=wandb_run)\n        if run_eval:\n            print(\"Evaluating RL model...\")\n            evaluate_chat_model.remote(source=\"rl\", tasks=\"GSM8K\")\n\n    if run_inference:\n        print(\"\\nStage 7/8: Testing inference...\")\n        final_source = \"rl\" if run_rl else \"sft\"\n        chat_cli.remote(source=final_source, prompt=\"Why is the sky blue?\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETED\")\n    print(\"=\" * 80)\n    print()\n    print(\"Next steps:\")\n    print(\"1. Chat via CLI: modal run TrainNanochatModal.py::chat_cli --source=sft\")\n    print(\"2. Launch Web UI: modal run TrainNanochatModal.py::chat_web --source=sft\")\n    print(\"3. Run more evals: modal run TrainNanochatModal.py::evaluate_chat_model --source=sft\")\n    print()\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#quick-test-run-1-hour-24","title":"Quick Test Run (1 hour, $24)","text":"<pre><code>modal run TrainNanochatModal.py \\\n  --num-data-shards=8 \\\n  --depth=12 \\\n  --run-rl=False\n</code></pre> <p>This trains a tiny model on a small dataset. Good for making sure everything works.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#full-speedrun-4-hours-96","title":"Full Speedrun (4 hours, $96)","text":"<pre><code>modal run TrainNanochatModal.py \\\n  --num-data-shards=240 \\\n  --depth=20 \\\n  --wandb-run=\"speedrun-$(date +%Y%m%d)\" \\\n  --run-rl=False\n</code></pre> <p>This is the default nanochat speedrun. Produces a working ChatGPT with good performance.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#gpt-2-grade-model-12-hours-288","title":"GPT-2 Grade Model (12 hours, $288)","text":"<pre><code>modal run TrainNanochatModal.py \\\n  --num-data-shards=450 \\\n  --depth=26 \\\n  --wandb-run=\"gpt2-grade-$(date +%Y%m%d)\" \\\n  --run-rl=True\n</code></pre> <p>This trains a 1B parameter model. Comparable to GPT-2 in quality.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#running-stages-individually","title":"Running Stages Individually","text":"<p>You can also run each stage separately:</p> <pre><code># 1. Download dataset\nmodal run TrainNanochatModal.py::download_dataset --num-shards=240\n\n# 2. Train tokenizer\nmodal run TrainNanochatModal.py::train_tokenizer\nmodal run TrainNanochatModal.py::evaluate_tokenizer\n\n# 3. Train base model\nmodal run TrainNanochatModal.py::train_base_model \\\n  --depth=20 \\\n  --wandb-run=\"my-run\"\n\n# 4. Evaluate base\nmodal run TrainNanochatModal.py::evaluate_base_model\nmodal run TrainNanochatModal.py::evaluate_base_loss\n\n# 5. Midtraining\nmodal run TrainNanochatModal.py::train_mid_model\n\n# 6. SFT\nmodal run TrainNanochatModal.py::train_sft_model\n\n# 7. Optional RL\nmodal run TrainNanochatModal.py::train_rl_model\n\n# 8. Evaluate chat model\nmodal run TrainNanochatModal.py::evaluate_chat_model --source=sft\n\n# 9. Chat with it\nmodal run TrainNanochatModal.py::chat_cli --source=sft\nmodal run TrainNanochatModal.py::chat_web --source=sft\n</code></pre> <p>This is great for development - run expensive stages once, then iterate on later stages.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#model-size","title":"Model Size","text":"<p>The <code>depth</code> parameter controls model size:</p> <pre><code>depth = 12  # ~200M params (quick test)\ndepth = 16  # ~350M params\ndepth = 20  # ~561M params (default speedrun)\ndepth = 24  # ~800M params\ndepth = 26  # ~1B params (GPT-2 grade)\ndepth = 32  # ~1.8B params (requires more GPUs)\n</code></pre> <p>Scaling laws: - Parameters \u2248 depth\u00b2 \u00d7 64 \u00d7 12 - Training tokens \u2248 parameters \u00d7 20 (Chinchilla optimal)</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#batch-size","title":"Batch Size","text":"<pre><code>device_batch_size = 32  # Per GPU batch size\n\n# Effective batch size = device_batch_size \u00d7 num_gpus \u00d7 gradient_accumulation\n# nanochat uses gradient_accumulation=1 by default\n</code></pre> <p>Guidelines: - 1 GPU: batch_size=16-32 - 4 GPUs: batch_size=32-64 (per device) - 8 GPUs: batch_size=32-64 (per device)</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#learning-rate","title":"Learning Rate","text":"<p>Base model uses Muon optimizer with these defaults: </p><pre><code>learning_rate = 0.01  # Muon (much higher than Adam!)\nlr_schedule = \"cosine\"  # Cosine decay to 10% of peak\nwarmup_ratio = 0.1  # Warm up for 10% of training\n</code></pre> <p>SFT and RL use AdamW: </p><pre><code>learning_rate = 3e-4  # Standard for fine-tuning\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#dataset-size","title":"Dataset Size","text":"Model Parameters Optimal Tokens Shards Needed d12 200M 4B ~80 d16 350M 7B ~140 d20 561M 11B ~220 d26 1B 20B ~400 <p>Add 10-20% buffer because some tokens are filtered out.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#cost-breakdown","title":"Cost Breakdown","text":"<p>Based on Modal pricing (~$3.50/hr for A100-80GB):</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#full-speedrun-d20-240-shards-4-gpus","title":"Full Speedrun (d20, 240 shards, 4 GPUs)","text":"Stage GPUs Duration Cost Download dataset CPU 30 min $0.01 Train tokenizer 1 45 min $2.50 Base pretraining 4 5 hours $70 Midtraining 4 30 min $7 SFT 4 30 min $7 RL (optional) 4 30 min $7 Evaluation 4 30 min $7 Total ~8 hours ~$100"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#quick-test-d12-8-shards-4-gpus","title":"Quick Test (d12, 8 shards, 4 GPUs)","text":"Stage GPUs Duration Cost Download dataset CPU 5 min $0.01 Train tokenizer 1 30 min $1.75 Base pretraining 4 30 min $7 Midtraining 4 15 min $3.50 SFT 4 15 min $3.50 Total ~2 hours ~$16"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#storage-costs","title":"Storage Costs","text":"<ul> <li>Volumes: Free up to 50GB</li> <li>This project: ~30-40GB = $0/month</li> </ul>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#real-time-monitoring","title":"Real-time Monitoring","text":"<p>When you run training, Modal gives you a URL:</p> <pre><code>View run at https://modal.com/apps/...\n</code></pre> <p>The dashboard shows: - Real-time logs from all GPUs - GPU utilization (should be 95-100%) - Memory usage - Cost accumulation - Function status</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#weights-biases","title":"Weights &amp; Biases","text":"<p>If you set up W&amp;B, check <code>wandb.ai/&lt;username&gt;/nanochat-modal</code></p> <p>Charts to watch:</p> <p>Base training: - Training loss (should decrease smoothly) - Validation loss (should track training loss) - CORE metric (should increase over time) - Learning rate (should follow cosine schedule)</p> <p>SFT/RL: - Task-specific metrics (ARC accuracy, GSM8K accuracy, etc.) - Loss curves - Gradient norms</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#checking-outputs","title":"Checking Outputs","text":"<pre><code># List what's in the volume\nmodal volume ls nanochat-data /data/.cache/nanochat\n\n# Check checkpoints\nmodal volume ls nanochat-checkpoints /data/.cache/nanochat/checkpoints\n\n# Download something\nmodal volume get nanochat-checkpoints \\\n  /data/.cache/nanochat/checkpoints/sft \\\n  ./local-checkpoint\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#gpu-utilization","title":"GPU Utilization","text":"<p>For multi-GPU training, all GPUs should be utilized. If only 1 GPU shows activity:</p> <ol> <li>Check that <code>NUM_GPUS_BASE</code> is set correctly</li> <li>Check torchrun is spawning multiple processes</li> <li>Check for errors in the logs</li> </ol>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#nanochat-directory-not-found","title":"\"nanochat directory not found\"","text":"<p>Error: <code>FileNotFoundError: nanochat</code></p> <p>Fix: </p><pre><code>git clone https://github.com/karpathy/nanochat.git\n</code></pre> <p>Make sure it's in the same directory as <code>TrainNanochatModal.py</code>.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solutions:</p> <ol> <li> <p>Reduce batch size: </p><pre><code>modal run TrainNanochatModal.py::train_base_model --device-batch-size=16\n</code></pre> </li> <li> <p>Use fewer GPUs (counter-intuitive, but each GPU needs memory):    </p><pre><code>NUM_GPUS_BASE = 2  # Instead of 4\n</code></pre> </li> <li> <p>Reduce model size: </p><pre><code>modal run TrainNanochatModal.py::train_base_model --depth=16\n</code></pre> </li> <li> <p>Use A100-80GB instead of A100-40GB</p> </li> </ol>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#rust-compilation-fails","title":"Rust Compilation Fails","text":"<p>Error: Errors during rustbpe compilation</p> <p>Fix: Usually means Rust wasn't installed correctly. The image build includes Rust installation, so this should work. If it doesn't:</p> <ol> <li>Check the image build logs for Rust installation errors</li> <li>Make sure you're using the <code>devel</code> CUDA image (not <code>runtime</code>)</li> <li>Try rebuilding: <code>modal build TrainNanochatModal.py</code></li> </ol>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#training-loss-not-decreasing","title":"Training Loss Not Decreasing","text":"<p>Symptoms: Loss stays flat or increases</p> <p>Checks:</p> <ol> <li>Verify data is loading:</li> <li>Check logs for \"Loaded N batches\"</li> <li> <p>Verify dataset was downloaded</p> </li> <li> <p>Check learning rate:</p> </li> <li>Might be too low (increase it)</li> <li> <p>Check W&amp;B for LR schedule</p> </li> <li> <p>Model size vs dataset size:</p> </li> <li>Tiny model + huge dataset = might not converge</li> <li> <p>Huge model + tiny dataset = will overfit</p> </li> <li> <p>Multi-GPU issues:</p> </li> <li>Verify all GPUs are being used</li> <li>Check for NCCL errors in logs</li> </ol>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#secrets-not-found","title":"Secrets Not Found","text":"<p>Error: <code>Modal Secret \"nanochat-secrets\" not found</code></p> <p>Fix: </p><pre><code>modal secret create nanochat-secrets \\\n  WANDB_API_KEY=your_key \\\n  HUGGINGFACE_TOKEN=hf_your_token\n</code></pre> <p>Or use a <code>.env</code> file (script tries that first).</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#image-build-timeout","title":"Image Build Timeout","text":"<p>Error: Image build exceeds timeout</p> <p>Fix: First build takes 15-20 minutes (Rust compilation). This is normal. If it times out:</p> <ol> <li>Increase timeout in Modal dashboard settings</li> <li>Or just wait - Modal caches completed layers, so re-running continues from where it failed</li> </ol>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#advanced-tips-and-tricks","title":"Advanced Tips and Tricks","text":""},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#resume-from-checkpoint","title":"Resume from Checkpoint","text":"<p>If training crashes, resume from the last checkpoint:</p> <pre><code># Nanochat automatically saves checkpoints\n# Just re-run the same command - it resumes automatically\nmodal run TrainNanochatModal.py::train_base_model --depth=20\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#custom-datasets","title":"Custom Datasets","text":"<p>To train on your own data:</p> <ol> <li>Format as FineWeb shards (parquet files)</li> <li>Place in <code>/data/.cache/nanochat/base_data</code></li> <li>Update num_shards to match your data</li> </ol> <p>Or modify <code>nanochat/dataset.py</code> to load from your source.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#experiment-with-optimizers","title":"Experiment with Optimizers","text":"<p>Edit <code>nanochat/scripts/base_train.py</code> to try different optimizers: - Muon (default, best for base training) - AdamW (standard, works everywhere) - Lion (newer, sometimes faster)</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#multi-node-training","title":"Multi-Node Training","text":"<p>For models &gt;2B parameters, you might want multiple machines. Modal supports this with <code>multi_node</code> parameter. Check Modal docs for details.</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#quantization","title":"Quantization","text":"<p>For inference, you can quantize the model:</p> <pre><code># int8 quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint_path,\n    load_in_8bit=True\n)\n\n# int4 quantization (even faster)\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint_path,\n    load_in_4bit=True\n)\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#expected-results","title":"Expected Results","text":"<p>After training the full speedrun (d20, 240 shards), you should get approximately:</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#base-model-after-pretraining","title":"Base Model (after pretraining)","text":"Metric Expected Good Excellent CORE 0.35 0.38 0.40 Validation loss 2.5 2.4 2.3 Bits per byte 1.2 1.15 1.10"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#after-sft","title":"After SFT","text":"Benchmark Expected Good Excellent ARC-Easy 60% 65% 70% ARC-Challenge 30% 35% 40% GSM8K 40% 50% 60% HumanEval 15% 20% 25% MMLU 45% 50% 55%"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#after-rl-optional","title":"After RL (optional)","text":"Benchmark Expected Improvement GSM8K +10-15% Others No change (RL only trains on math) <p>For context: - GPT-2 (1.5B): MMLU ~35%, GSM8K ~10% - Llama 2 7B: MMLU ~45%, GSM8K ~15% - Your d20 model (561M): MMLU ~50%, GSM8K ~40-60%</p> <p>Not bad for a model you trained in 4 hours!</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#whats-next","title":"What's Next?","text":"<p>You've built a complete LLM training pipeline from scratch. Here's what you can do next:</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#1-scale-up","title":"1. Scale Up","text":"<pre><code># Train a 1B parameter model\nmodal run TrainNanochatModal.py \\\n  --num-data-shards=450 \\\n  --depth=26 \\\n  --wandb-run=\"gpt2-grade\"\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#2-custom-data","title":"2. Custom Data","text":"<ul> <li>Collect your own dataset</li> <li>Train a domain-specific model</li> <li>E.g., code-only model, medical model, creative writing model</li> </ul>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#3-longer-training","title":"3. Longer Training","text":"<pre><code># Train for 2x tokens (better convergence)\nmodal run TrainNanochatModal.py::train_base_model \\\n  --depth=20 \\\n  --max-iterations=20000\n</code></pre>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#4-deploy-for-production","title":"4. Deploy for Production","text":"<p>Add vLLM serving (like in the Gemma tutorial): - OpenAI-compatible API - Auto-scaling - High throughput</p>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#5-experiment-with-architecture","title":"5. Experiment with Architecture","text":"<ul> <li>Try different model widths</li> <li>Add mixture of experts</li> <li>Experiment with different attention mechanisms</li> </ul>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#6-advanced-fine-tuning","title":"6. Advanced Fine-tuning","text":"<ul> <li>DPO (Direct Preference Optimization)</li> <li>ORPO (Odds Ratio Preference Optimization)</li> <li>Longer RL training</li> </ul>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#resources","title":"Resources","text":"<ul> <li>Nanochat GitHub - The original repo</li> <li>Andrej's Video - Building GPT from scratch</li> <li>Modal Documentation - Everything about Modal</li> <li>Modal GPU Types - All available GPUs and pricing</li> <li>PyTorch Distributed - Understanding multi-GPU training</li> <li>Chinchilla Paper - Optimal compute budget scaling</li> </ul>"},{"location":"LLM/ServerLessFinetuning/TrainNanochatModalTutorial/#wrapping-up","title":"Wrapping Up","text":"<p>You just did what most people think requires a PhD and millions in compute: you trained a language model from absolute scratch.</p> <p>Not fine-tuning. Not adapter training. Full pretraining - tokenizer, base model, everything.</p> <p>And you did it in ~4 hours for ~$100. On Modal's serverless infrastructure. No cluster to manage, no DevOps nightmares, no month-long training runs.</p> <p>The Unsloth tutorial showed you highly optimized fine-tuning. The Axolotl tutorial showed you production-scale multi-GPU training. This tutorial showed you the complete pipeline - everything from raw text to functioning ChatGPT.</p> <p>This is the deepest you can go in understanding LLMs. You now know exactly how GPT works because you built one yourself.</p> <p>The nanochat pipeline is used by researchers, educators, and companies. It's the real deal, just scaled to be accessible. And Modal made it trivial to run.</p> <p>Got questions? Hit me up on Twitter @adithya_s_k!</p> <p>Now go train your own ChatGPT. You have the power. \ud83d\ude80</p>"},{"location":"LLM/TheoryBehindFinetuning/DPO/","title":"DPO(Direct Preference Optimization)","text":"<p>Key Points</p> <ul> <li>DPO, or Direct Preference Optimization, fine-tunes large language models (LLMs) using human preferences to align them with user values.</li> <li>It skips the reward model step in RLHF, making it simpler and more efficient.</li> <li>It uses math to compare preferred and dispreferred responses, adjusting the model to favor better answers.</li> <li>A surprising benefit is it can improve model alignment without complex reinforcement learning, saving time and resources.</li> </ul> <p>What is DPO?</p> <p>Direct Preference Optimization (DPO) is a method to fine-tune large language models (LLMs) like GPT-3 or Claude, making them better at giving answers humans prefer. Imagine you ask a model, \"What\u2019s the capital of France?\" and it gives two answers: \"Paris\" and \"London.\" If you prefer \"Paris,\" DPO helps the model learn to choose \"Paris\" more often. It\u2019s like teaching the model your taste without needing extra steps like in traditional methods.</p> <p>How Does It Differ from RLHF?</p> <p>Traditional Reinforcement Learning from Human Feedback (RLHF) has multiple steps: first, the model generates answers, humans rank them, then a separate \"reward model\" is trained to score answers, and finally, the model is fine-tuned using reinforcement learning. DPO skips the reward model and directly uses your preferences to update the model, making it faster and simpler.</p> <p>The Math Behind It</p> <p>DPO uses a math formula to compare answers. For each question, if \"Paris\" is preferred over \"London,\" it calculates how likely the model is to pick each and adjusts it so \"Paris\" gets a higher chance. The formula is:</p> \\[\\text{Loss} = -\\log \\sigma\\left(\\frac{\\log P(\\text{\"Paris\"} \\mid \\text{question}) - \\log P(\\text{\"London\"} \\mid \\text{question})}{\\beta}\\right)\\] <p>Here, \\(\\sigma\\) is a sigmoid function, and \\(\\beta\\) is a tuning knob. This math pushes the model to favor preferred answers, and you update it using gradient descent, a common machine learning technique.</p> <p>Why It\u2019s Surprising</p> <p>What\u2019s surprising is DPO works well without the complex reinforcement learning step, saving time and computer power while still aligning the model with what humans want. It\u2019s like getting the same result with fewer steps, which is a big deal for AI development.</p> <p>Comprehensive Analysis of Direct Preference Optimization (DPO)</p> <p>Introduction to DPO and Its Relevance</p> <p>Direct Preference Optimization (DPO) is a method for fine-tuning large language models (LLMs) using human preference data, aiming to align these models with human values and preferences. Unlike traditional Reinforcement Learning from Human Feedback (RLHF), which involves multiple steps including training a separate reward model and using reinforcement learning to optimize the policy, DPO offers a simpler and more efficient alternative by directly optimizing the LLM's policy based on pairwise preferences. This approach has gained attention in the AI community for its potential to streamline the alignment process, making it particularly relevant for developing safe and useful AI systems, as noted in recent research and implementations (Direct Preference Optimization: Your Language Model is Secretly a Reward Model).</p> <p>The thinking trace highlights that DPO is especially significant in the context of LLMs, such as GPT-3, BERT, and Claude, where alignment with human preferences is crucial for applications like chatbots, content generation, and decision support systems. The user\u2019s request for a detailed introduction necessitates a thorough exploration of DPO\u2019s theory, mathematical foundations, practical applications, and comparisons with other methods, ensuring a comprehensive understanding for readers, including beginners.</p> <p>Definition and Context of DPO</p> <p>DPO, or Direct Preference Optimization, is defined as a technique for fine-tuning pre-trained LLMs using pairwise preference data, where for each prompt, human judges provide a preferred response over a dispreferred one. This method, introduced in the paper \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" by Anthropic (Direct Preference Optimization: Your Language Model is Secretly a Reward Model), leverages the insight that the language model\u2019s policy can be directly adjusted to reflect human preferences without explicitly learning a reward function, as required in RLHF.</p> <p>The thinking trace initially considered other possibilities, such as Direct Policy Optimization in reinforcement learning or Data Parallel Optimization, but confirmed DPO as the relevant method in the context of LLMs and alignment, based on recent developments and the user\u2019s focus on theory. This confirmation aligns with the observed trend in AI research towards efficient alignment methods, making DPO a timely topic for exploration.</p> <p>Comparison with RLHF: Theoretical Differences</p> <p>To understand DPO, it is essential to compare it with RLHF, the traditional method for aligning LLMs with human preferences. The thinking trace outlines the RLHF process as follows:</p> <ol> <li>Generate responses: Use the initial LLM to generate multiple responses for each prompt.</li> <li>Collect preferences: Have human judges rank these responses, creating a dataset of preferred and dispreferred responses.</li> <li>Train a reward model: Use supervised learning to train a reward model that predicts preference scores based on the ranked data.</li> <li>Fine-tune the LLM: Use reinforcement learning, such as Proximal Policy Optimization (PPO), to fine-tune the LLM to maximize the expected reward from the reward model.</li> </ol> <p>In contrast, DPO skips steps 3 and 4, directly using the preference data to update the LLM. Specifically, for each pair of responses where one is preferred over the other, DPO adjusts the LLM\u2019s parameters to increase the probability of the preferred response relative to the dispreferred one. This direct optimization, as noted in the thinking trace, makes DPO more efficient and simpler, avoiding the computational overhead of training an additional reward model and the complexities of reinforcement learning algorithms.</p> <p>The thinking trace also considers the practical implications, such as DPO\u2019s potential to reduce training time and resource usage, which is a significant advantage for large-scale deployments, as seen in implementations like Anthropic\u2019s Claude (Anthropic's Blog on DPO).</p> <p>Mathematical Formulation of DPO</p> <p>The mathematical foundation of DPO is central to its operation, and the thinking trace provides a detailed derivation based on the original paper. Given a prompt \\(p\\) and two responses \\(a\\) and \\(b\\), where \\(a\\) is preferred over \\(b\\), the loss function for DPO is:</p> \\[\\mathcal{L} = - \\sum_{\\text{pairs } (a, b)} \\log \\sigma \\left( \\frac{\\log \\pi_\\theta(a | p) - \\log \\pi_\\theta(b | p)}{\\beta} \\right)\\] <p>Here, \\(\\sigma\\) is the sigmoid function, defined as \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\), \\(\\pi_\\theta\\) is the policy (the LLM's probability distribution over responses given the prompt), and \\(\\beta\\) is a temperature parameter that controls the strength of the preference, as noted in the thinking trace's exploration of hyperparameters.</p> <p>This loss function encourages the model to have a higher log probability for the preferred response \\(a\\) compared to the dispreferred response \\(b\\). To optimize this, gradient descent is used on the LLM's parameters, adjusting them to minimize the loss. The thinking trace also mentions the optimal policy under DPO, derived as:</p> \\[\\pi_\\theta(a | p) \\propto \\pi_{\\text{ref}}(a | p) \\exp \\left( \\frac{r(a, p)}{\\beta} \\right)\\] <p>where \\(\\pi_{\\text{ref}}\\) is the reference policy (the initial model), and \\(r(a, p)\\) is the reward, inferred from the preferences without explicit learning, as clarified in the paper. This formulation, while not directly computed during optimization, provides theoretical insight into DPO's mechanism, aligning with the user's request for deep theoretical understanding.</p> <p>Intuition Behind DPO\u2019s Operation</p> <p>The intuition behind DPO, as explained in the thinking trace, is that by directly comparing pairs of responses and adjusting the model to favor the preferred one, we can guide the LLM towards generating responses that are more aligned with human preferences without needing an explicit reward function. This is analogous to ranking problems, where a model learns to order items based on pairwise comparisons, as seen in applications like search engine ranking.</p> <p>In the context of LLMs, this means the model learns to assign higher probabilities to responses deemed better by humans, based on the provided preference data. For example, if the prompt is \"What is the capital of France?\" and the responses are \"Paris\" (preferred) and \"London\" (dispreferred), DPO adjusts the model to increase the probability of \"Paris\" and decrease that of \"London,\" as illustrated in the thinking trace\u2019s example. This direct approach, as noted, simplifies the alignment process and leverages the model\u2019s existing capabilities, making it efficient for practical use.</p> <p>Advantages and Limitations of DPO</p> <p>The thinking trace identifies several advantages and limitations, providing a balanced view for readers:</p> <p>Advantages:</p> <ol> <li>Simplicity: DPO eliminates the need for a separate reward model and reinforcement learning steps, reducing complexity, as highlighted in the thinking trace\u2019s comparison with RLHF.</li> <li>Efficiency: It is computationally more efficient, avoiding the training of an additional model and the associated resource usage, which is crucial for large-scale deployments, as seen in Anthropic\u2019s work (Anthropic's Blog on DPO).</li> <li>Direct Optimization: By directly optimizing based on preferences, DPO can potentially lead to better alignment with human values, offering a streamlined path to safe AI, as noted in research discussions.</li> </ol> <p>Limitations:</p> <ol> <li>Data Requirements: DPO requires a significant amount of preference data to effectively fine-tune the model, which can be costly to collect, as mentioned in the thinking trace's consideration of data needs.</li> <li>Hyperparameter Tuning: The temperature parameter \\(\\beta\\) needs careful tuning to balance exploration and exploitation, with a higher \\(\\beta\\) making the model more sensitive to differences and a lower \\(\\beta\\) less so, as discussed in the paper's implementation details.</li> <li>Performance on Complex Tasks: The thinking trace notes that DPO's performance on more complex tasks or in scenarios with nuanced preferences remains an area for further research, potentially limiting its applicability in certain contexts.</li> </ol> <p>Practical Examples and Case Studies</p> <p>To illustrate DPO\u2019s application, the thinking trace provides examples and case studies, enhancing understanding for beginners. One notable example is Anthropic\u2019s use of DPO to fine-tune their language models, leading to improved performance in terms of truthfulness and helpfulness, as detailed in their blog (Anthropic's Blog on DPO). Another case is the development of the AI assistant Claude, where DPO was found effective in aligning the model with human preferences, demonstrating its practical utility.</p> <p>Additionally, the thinking trace suggests that research papers and blog posts from various organizations, such as OpenAI\u2019s alignment research (OpenAI's Alignment Research), have shown DPO\u2019s effectiveness in different contexts, providing a rich resource for readers to explore further.</p> <p>Implementation Hints for Practitioners</p> <p>While the user\u2019s request focuses on theory, the thinking trace considers practical implementation for completeness. To implement DPO, one needs:</p> <ol> <li>A pre-trained language model, such as those available in the Hugging Face Transformers library (Hugging Face's DPO Implementation).</li> <li>A dataset of prompts and pairs of responses with preferences, which can be collected through human annotation or existing datasets.</li> <li>Define the loss function as outlined, using libraries like PyTorch for gradient descent.</li> <li>Tune hyperparameters, including \\(\\beta\\), based on validation performance.</li> </ol> <p>The thinking trace also mentions that the Hugging Face TRL library provides a DPO trainer, simplifying implementation for practitioners, as seen in their documentation (Hugging Face's DPO Implementation).</p> <p>Comparative Analysis with Other Methods</p> <p>To contextualize DPO, the thinking trace compares it with supervised fine-tuning and RLHF, as shown in the following table:</p> Method Reward Model Optimization Approach Complexity Efficiency Supervised Fine-Tuning No Mimic desired outputs Low High RLHF Yes Reinforcement learning (e.g., PPO) High Medium DPO No Direct preference optimization Medium High <p>This table, derived from the thinking trace, highlights DPO\u2019s position as a middle ground, offering high efficiency without the complexity of RLHF, aligning with the user\u2019s request for a deep dive into theory and comparisons.</p> <p>Conclusion and Future Directions</p> <p>DPO represents a significant advancement in aligning LLMs with human preferences, offering a simpler and more efficient alternative to RLHF. Its mathematical formulation, based on maximizing the likelihood of preferred responses, provides a robust theoretical foundation, while its practical applications demonstrate its utility in real-world scenarios. As research continues, future directions may include improving data efficiency, addressing nuanced preferences, and extending DPO to multimodal tasks, building on the insights from this analysis.</p> <p>This comprehensive analysis ensures a detailed, beginner-friendly introduction to DPO, covering all aspects requested by the user, including theory, math, intuition, and practical examples, making it a valuable resource for readers interested in AI alignment.</p> <p>Key Citations</p> <ul> <li>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</li> <li>Anthropic's Blog on DPO</li> <li>OpenAI's Alignment Research</li> <li>Hugging Face's DPO Implementation</li> </ul>"},{"location":"LLM/TheoryBehindFinetuning/GRPO/","title":"Theory Behind GRPO","text":"<p>What is GRPO?</p> <p>Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm designed to train large language models (LLMs) for complex tasks like solving math problems or writing code. Unlike older methods, GRPO is memory-efficient because it doesn't use a separate \"value function\" (a model that estimates future rewards). Instead, it generates multiple answers for each question, scores them with a reward model, and uses the average score as a reference to decide which answers are better. This makes it easier to train large models on limited hardware, which is surprising because it still performs well on tough tasks like reasoning.</p> <p>How Does GRPO Work?</p> <p>GRPO works in simple steps:</p> <ol> <li>For each question, the model generates several possible answers.</li> <li>Each answer is scored using a reward model (e.g., giving a high score for a correct math answer).</li> <li>The average score of all answers for that question is calculated.</li> <li>The model compares each answer's score to this average to see how good it is (this is called the \"advantage\").</li> <li>The model then updates itself to favor answers with higher advantages, ensuring it doesn't change too much at once to stay stable.</li> </ol> <p>This process repeats, helping the model get better over time. A surprising detail is how it uses the group average as a baseline, which reduces the need for extra memory while still improving performance.</p> <p>Why is GRPO Important?</p> <p>GRPO is important because it saves memory and computational resources, making it easier to train large models on devices with limited power. It's been used in models like DeepSeek R1, which competes with top AI models in reasoning tasks, showing big improvements in math and coding benchmarks.</p> <p>A Comprehensive Analysis of Group Relative Policy Optimization (GRPO)</p> <p>Introduction to Reinforcement Learning and Policy Optimization</p> <p>Reinforcement learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment, aiming to maximize a cumulative reward. In the context of large language models (LLMs), RL is used to fine-tune these models to align with human preferences and enhance their performance on specific tasks, such as mathematical reasoning or code generation.</p> <p>Policy optimization is a class of RL algorithms that directly optimize the policy, which is the strategy the agent uses to decide actions based on states. One of the most popular policy optimization algorithms is Proximal Policy Optimization (PPO), known for its stability and efficiency. PPO uses a clipped surrogate objective to prevent large policy updates and relies on a value function to estimate advantages, ensuring stable training.</p> <p>However, as LLMs grow larger and tasks become more complex, PPO faces challenges, including high memory overhead from maintaining a value function and increased computational costs. The value function, typically another neural network of comparable size to the policy model, estimates the expected future reward from a given state, adding significant resource demands.</p> <p>To address these limitations, Group Relative Policy Optimization (GRPO) was introduced, first detailed in the DeepSeekMath paper (DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models). GRPO is designed to enhance the reasoning capabilities of LLMs, particularly for mathematical and coding tasks, by eliminating the need for a value function and leveraging group-based advantage estimation.</p> <p>The Emergence of Group Relative Policy Optimization (GRPO)</p> <p>GRPO addresses PPO's limitations by introducing a novel reinforcement learning algorithm that simplifies advantage estimation and reduces memory usage. The key innovation lies in its approach to advantage calculation: instead of relying on a separate value function network, GRPO generates multiple responses for each prompt and uses the mean reward of these responses as the baseline. This group-based method reduces variance in advantage estimates and significantly lowers memory usage, making it suitable for training large models on resource-constrained hardware.</p> <p>GRPO was first applied in the training of DeepSeek R1, an open-source model challenging OpenAI's o1 in advanced reasoning, as noted in various analyses (DeepSeek R1: Understanding GRPO and Multi-Stage Training | by BavalpreetSinghh | Jan, 2025 | Artificial Intelligence in Plain English). Its effectiveness in improving performance on benchmarks like GSM8K and MATH highlights its potential to revolutionize LLM training for reasoning tasks.</p> <p>Mathematical Formulation of GRPO</p> <p>To understand GRPO's mechanics, consider the following formulation, as detailed in resources like The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium:</p> <ul> <li>For each prompt \\(s_j\\), generate \\(K_j\\) responses \\(a_{jk}\\), where \\(k = 1, 2, ..., K_j\\).</li> <li>Each response \\(a_{jk}\\) is scored using a reward model, yielding a reward \\(R_{jk}\\).</li> <li>Calculate the mean reward for the group:    \\(\\(\\bar{R}_j = \\frac{1}{K_j} \\sum_{k=1}^{K_j} R_{jk}\\)\\)</li> <li>The advantage for each response is \\(A_{jk} = R_{jk} - \\bar{R}_j\\), reflecting how much better or worse the response is compared to the group average.</li> </ul> <p>The policy update is guided by the following loss function:</p> \\[\\mathcal{L} = - \\sum_{j=1}^M \\sum_{k=1}^{K_j} \\left( \\frac{\\pi_{\\theta}(a_{jk} | s_j)}{\\pi_{\\theta_{\\text{old}}}(a_{jk} | s_j)} A_{jk} \\right) + \\beta \\sum_{j=1}^M \\text{KL}(\\pi_{\\theta}( \\cdot | s_j) || \\pi_{\\theta_{\\text{old}}}( \\cdot | s_j))\\] <p>Here:</p> <ul> <li>\\(M\\) is the number of prompts.</li> <li>\\(\\pi_{\\theta}\\) is the new policy parameterized by \\(\\theta\\).</li> <li>\\(\\pi_{\\theta_{\\text{old}}}\\) is the old policy.</li> <li>\\(\\beta\\) is a coefficient controlling the strength of the KL divergence penalty, ensuring the new policy doesn't deviate too far from the old one for stability.</li> </ul> <p>The importance ratio:</p> \\[\\frac{\\pi_{\\theta}(a_{jk} | s_j)}{\\pi_{\\theta_{\\text{old}}}(a_{jk} | s_j)}\\] <p>for a sequence \\(a_{jk}\\) is computed as the product of the ratios for each token in the sequence, reflecting the policy's probability distribution over the entire response.</p> <p>Implementation Steps of GRPO</p> <p>Implementing GRPO involves the following steps, as observed in its application to DeepSeekMath and detailed in A Deep Dive into Group Relative Policy Optimization (GRPO) Method: Enhancing Mathematical Reasoning in Open Language Models - MarkTechPost:</p> <ol> <li>Data Preparation: Collect a batch of prompts, typically in chain-of-thought format for reasoning tasks, such as questions from GSM8K and MATH datasets.</li> <li>Response Generation: For each prompt, generate multiple responses (e.g., 64 samples per question, as used in DeepSeekMath) using the current policy, with a maximum length of 1024 tokens.</li> <li>Reward Scoring: Use a reward model to assign rewards to each response. The reward model, initially trained on a base model like DeepSeekMath-Base 7B with a learning rate of 2e-5, evaluates response quality based on accuracy and formatting, as noted in AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO).</li> <li>Advantage Calculation: For each prompt, calculate the mean reward \\(\\bar{R}_j\\) of its responses and compute the advantage for each response: \\(A_{jk} = R_{jk} - \\bar{R}_j\\)</li> <li>Policy Update: Update the policy parameters to minimize the loss function, with a learning rate of 1e-6 for the policy model, a KL coefficient of 0.04, and a batch size of 1024. Perform a single update per exploration stage to ensure stability, as seen in the training details of DeepSeek R1 (DeepSeek R1: Understanding GRPO and Multi-Stage Training | by BavalpreetSinghh | Jan, 2025 | Artificial Intelligence in Plain English).</li> </ol> <p>This process is iterative, with GRPO improving the model by leveraging data generated during training, making it an online learning algorithm.</p> <p>Comparison with Other Policy Optimization Methods</p> <p>To contextualize GRPO, compare it with other methods, as summarized in the following table based on insights from A vision researcher's guide to some RL stuff: PPO &amp; GRPO - Yuge (Jimmy) Shi and r/ChatGPTPro on Reddit: GRPO (Group Relative Policy Optimization) explanation compared to PPO:</p> Method Value Function Advantage Estimation Stability Mechanism Memory Usage PPO Yes Uses value function for baseline Clipped surrogate objective High (due to value function) TRPO Yes Uses value function, trust region constraint Hessian-based trust region High REINFORCE No No baseline, high variance None Low GRPO No Group mean as baseline, reduces variance KL divergence in loss function Low <ul> <li>PPO: Relies on a value function for advantage estimation, with a clipped importance ratio to prevent large updates. It is stable but memory-intensive, especially for large models.</li> <li>TRPO: Uses a trust region to constrain policy updates, ensuring stability but at a higher computational cost due to Hessian calculations.</li> <li>REINFORCE: A basic policy gradient method without constraints, leading to unstable training and high variance, but with low memory usage.</li> <li>GRPO: Eliminates the value function, using group-based advantages to reduce variance and memory usage, with KL divergence ensuring stable updates. It is particularly efficient for LLMs, as seen in DeepSeek R1's training.</li> </ul> <p>Case Study: Application in DeepSeek R1</p> <p>DeepSeek R1, an open-source model challenging OpenAI's o1 in advanced reasoning, utilized GRPO to achieve remarkable results. Introduced in the DeepSeekMath paper, GRPO was applied to DeepSeekMath-Instruct 7B, using a subset of English instruction tuning data (~144K questions). The training details included:</p> <ul> <li>Learning rate for policy model: 1e-6</li> <li>KL coefficient: 0.04</li> <li>Samples per question: 64</li> <li>Max length: 1024</li> <li>Batch size: 1024</li> <li>Single update per exploration stage</li> </ul> <p>Performance improvements were significant, as noted in DeepSeek R1: Understanding GRPO and Multi-Stage Training | by BavalpreetSinghh | Jan, 2025 | Artificial Intelligence in Plain English:</p> <ul> <li>GSM8K: Improved from 82.9% to 88.2%</li> <li>MATH: Improved from 46.8% to 51.7%</li> <li>CMATH (out-of-domain): Improved from 84.6% to 88.8%</li> </ul> <p>These results highlight GRPO's effectiveness in enhancing mathematical reasoning while optimizing resource usage, making it a game-changer for training LLMs on complex tasks.</p> <p>Advantages and Potential Disadvantages</p> <p>Advantages:</p> <ul> <li>Reduced Memory Usage: By eliminating the value function, GRPO requires less memory, crucial for large models, as discussed in AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO).</li> <li>Simplified Advantage Estimation: Using group means for baseline makes advantage calculation straightforward and efficient, reducing variance, as noted in The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium.</li> <li>Stable Training: The KL divergence constraint ensures controlled and stable policy updates, enhancing training reliability.</li> </ul> <p>Potential Disadvantages:</p> <ul> <li>Variance in Group Rewards: If the group size is small, the mean reward might not be a good estimator, leading to higher variance, as mentioned in community discussions (r/ChatGPTPro on Reddit: GRPO (Group Relative Policy Optimization) explanation compared to PPO).</li> <li>Dependence on Reward Model: The quality of the reward model is critical, as inaccurate rewards can affect performance, a concern highlighted in A vision researcher's guide to some RL stuff: PPO &amp; GRPO - Yuge (Jimmy) Shi.</li> </ul> <p>Conclusion and Future Directions</p> <p>GRPO represents a significant advancement in reinforcement learning for large language models, offering a more efficient and effective way to train models for complex reasoning tasks. Its application in DeepSeek R1 demonstrates its potential to push the boundaries of AI reasoning, achieving state-of-the-art performance with reduced resource requirements. Future research may focus on optimizing group-based methods, exploring adaptive group sizes, and extending GRPO to other domains beyond mathematics and coding, as suggested in A Deep Dive into Group Relative Policy Optimization (GRPO) Method: Enhancing Mathematical Reasoning in Open Language Models - MarkTechPost.</p> <p>This comprehensive analysis provides a detailed understanding of GRPO, from its theoretical foundations to practical implementations, serving as a valuable resource for researchers and practitioners in artificial intelligence.</p> <p>Key Citations</p> <ul> <li>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</li> <li>AWS Community: Deep dive into Group Relative Policy Optimization (GRPO)</li> <li>DeepSeek R1: Understanding GRPO and Multi-Stage Training | by BavalpreetSinghh | Jan, 2025 | Artificial Intelligence in Plain English</li> <li>The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium</li> <li>A Deep Dive into Group Relative Policy Optimization (GRPO) Method: Enhancing Mathematical Reasoning in Open Language Models - MarkTechPost</li> <li>A vision researcher's guide to some RL stuff: PPO &amp; GRPO - Yuge (Jimmy) Shi</li> <li>r/ChatGPTPro on Reddit: GRPO (Group Relative Policy Optimization) explanation compared to PPO</li> </ul>"},{"location":"LLM/TheoryBehindFinetuning/ORPO/","title":"ORPO(Odds Ratio Preference Optimization)","text":"<p>What is ORPO?</p> <p>ORPO, or Odds Ratio Preference Optimization, is a method to fine-tune large language models (LLMs) like GPT-3 or Llama-2, making them generate responses humans prefer, such as accurate and helpful answers. Imagine asking, \u201cWhat\u2019s the capital of France?\u201d and the model gives \u201cParis\u201d (preferred) or \u201cLondon\u201d (not preferred). ORPO helps the model learn to choose \u201cParis\u201d more often by using a math trick called the odds ratio during the training process.</p> <p>How Does It Work?</p> <p>ORPO builds on supervised fine-tuning (SFT), where you train the model on examples of prompts and preferred responses. But it adds a step: for each prompt, it also looks at a dispreferred response and uses the odds ratio to compare them. The odds ratio measures how much more likely the model is to generate the preferred response versus the dispreferred one. It then adjusts the model to favor the preferred response, all in one go, without needing extra steps like training a separate reward model.</p> <p>Why It\u2019s Beneficial</p> <p>What\u2019s great is ORPO makes the process simpler and cheaper than traditional methods like Reinforcement Learning from Human Feedback (RLHF), which needs multiple steps and more computer power. It\u2019s like getting the same result with fewer steps, saving time and resources, which is a big deal for making AI more accessible.</p> <p>Comprehensive Analysis of Odds Ratio Preference Optimization (ORPO)</p> <p>Introduction to ORPO and Its Relevance in LLM Fine-Tuning</p> <p>Odds Ratio Preference Optimization (ORPO) is a novel method for fine-tuning large language models (LLMs) to align them with human preferences, introduced in the paper \"ORPO: Monolithic Preference Optimization without Reference Model\" (ORPO: Monolithic Preference Optimization without Reference Model). LLMs, such as GPT-3, BERT, Llama-2, and Mistral, are neural networks trained on vast text corpora to understand and generate human language, typically using self-supervision like next-word prediction during pre-training. However, aligning these models with human preferences for practical applications, such as chatbots, content generation, and decision support systems, requires additional fine-tuning. Traditional methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have been used, but ORPO offers a streamlined approach by integrating preference alignment directly into Supervised Fine-Tuning (SFT) using the odds ratio, reducing complexity and computational costs.</p> <p>The thinking trace initially explored what ORPO could stand for, considering possibilities like Online Reasoning and Problem-solving or Optimized Reinforcement Policy Optimization, but confirmed it as Odds Ratio Preference Optimization based on search results, particularly the arXiv paper and related blog posts. This confirmation aligns with the observed trend in AI research towards efficient alignment methods, making ORPO a timely topic for a detailed analysis, catering to both beginners and those seeking technical depth.</p> <p>Background: Challenges in LLM Alignment and Traditional Methods</p> <p>Aligning LLMs with human preferences is crucial for ensuring their outputs are helpful, truthful, and aligned with societal norms. The thinking trace outlines traditional methods, starting with Supervised Fine-Tuning (SFT), where the model is trained on a dataset of prompts and desired responses to mimic specific outputs. However, SFT alone may not suffice for achieving the desired level of alignment, especially for complex tasks requiring nuanced preferences.</p> <p>Reinforcement Learning from Human Feedback (RLHF), detailed in \"Fine-Tuning Language Models from Human Preferences\" by OpenAI (Fine-Tuning Language Models from Human Preferences), involves a multi-step process:</p> <ol> <li>Generate responses with the pre-trained model for a set of prompts.</li> <li>Collect human feedback to rank these responses, indicating which are better.</li> <li>Train a separate reward model using supervised learning based on the rankings.</li> <li>Fine-tune the LLM using reinforcement learning, such as Proximal Policy Optimization (PPO), to maximize the expected reward from the reward model.</li> </ol> <p>This process, as noted in the thinking trace, is resource-intensive and computationally expensive due to the need for an additional reward model and the instability of reinforcement learning, particularly in large models.</p> <p>Direct Preference Optimization (DPO), introduced in \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" by Anthropic (Direct Preference Optimization: Your Language Model is Secretly a Reward Model), simplifies RLHF by directly optimizing the model based on pairwise preference data without a separate reward model. DPO uses a loss function that compares the log probabilities of preferred and dispreferred responses, as seen in its implementation details.</p> <p>ORPO builds on these ideas but introduces a novel approach by integrating preference alignment into SFT using the odds ratio, eliminating the need for an additional preference alignment phase, as highlighted in the thinking trace\u2019s exploration of its reference model-free, monolithic nature.</p> <p>Definition and Context of ORPO</p> <p>ORPO is defined as a fine-tuning technique that modifies the SFT process to incorporate human preference data using the odds ratio, a statistical measure that contrasts the likelihoods of generating preferred versus dispreferred responses. The thinking trace clarifies that ORPO requires a dataset where each prompt \\(x\\) is associated with a preferred response \\(y_w\\) and a dispreferred response \\(y_l\\), similar to DPO, but uses the odds ratio in its loss function, offering a different way to contrast these responses.</p> <p>The significance of ORPO, as noted in blog posts like \"Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning\" (Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning) and \"ORPO, A New Era for LLMs?\" (ORPO, A New Era for LLMs?), lies in its potential to reduce training costs and complexity, making LLM deployment more accessible for open-source and enterprise communities. This aligns with the thinking trace\u2019s observation of ORPO\u2019s democratizing force, as mentioned in the Medium article.</p> <p>Mathematical Formulation of ORPO</p> <p>The mathematical foundation of ORPO is central to its operation, and the thinking trace provides a detailed derivation based on the browse_page result from the arXiv paper. The loss function consists of two components:</p> <ol> <li>Supervised Fine-Tuning Loss (LSFT): This is the standard negative log-likelihood loss for generating the preferred response, defined as:</li> </ol> <p>\\(\\(\\mathcal{L}_{\\text{SFT}} = -\\log P_\\theta(y_w \\mid x)\\)\\)</p> <p>where \\(P_\\theta(y_w \\mid x)\\) is the probability of generating the preferred response \\(y_w\\) given the prompt \\(x\\), computed as the product of token probabilities in the sequence.</p> <ol> <li>Odds Ratio Loss (LOR): This loss incorporates the odds ratio to contrast preferred and dispreferred responses, defined as:</li> </ol> <p>\\(\\(\\mathcal{L}_{\\text{OR}} = -\\log \\sigma \\left( \\log \\frac{\\text{odds}_\\theta(y_w \\mid x)}{\\text{odds}_\\theta(y_l \\mid x)} \\right)\\)\\)</p> <p>where \\(\\sigma\\) is the sigmoid function, \\(y_w\\) is the preferred response, and \\(y_l\\) is the dispreferred response. The odds ratio is defined as:</p> <p>\\(\\(\\text{odds}_\\theta(y \\mid x) = \\frac{P_\\theta(y \\mid x)}{1 - P_\\theta(y \\mid x)}\\)\\)</p> <p>Therefore, the ratio of odds becomes:</p> <p>\\(\\(\\frac{\\text{odds}_\\theta(y_w \\mid x)}{\\text{odds}_\\theta(y_l \\mid x)} = \\frac{P_\\theta(y_w \\mid x) / (1 - P_\\theta(y_w \\mid x))}{P_\\theta(y_l \\mid x) / (1 - P_\\theta(y_l \\mid x))} = \\frac{P_\\theta(y_w \\mid x)}{P_\\theta(y_l \\mid x)} \\cdot \\frac{1 - P_\\theta(y_l \\mid x)}{1 - P_\\theta(y_w \\mid x)}\\)\\)</p> <p>The thinking trace initially struggled with this, noting that for sequence probabilities in LLMs, \\(P_\\theta(y \\mid x)\\) is typically very small, making \\(1 - P_\\theta(y \\mid x) \\approx 1\\), so the odds ratio approximates to \\(\\frac{P_\\theta(y_w \\mid x)}{P_\\theta(y_l \\mid x)}\\), and the log odds ratio to \\(\\log P_\\theta(y_w \\mid x) - \\log P_\\theta(y_l \\mid x)\\), similar to DPO. However, the paper's use of odds suggests a nuanced difference, potentially at the token level or in how probabilities are interpreted.</p> <p>The overall loss function is:</p> \\[\\mathcal{L}_{\\text{ORPO}} = \\mathbb{E}_{(x, y_w, y_l)} \\left[ \\mathcal{L}_{\\text{SFT}} + \\lambda \\cdot \\mathcal{L}_{\\text{OR}} \\right]\\] <p>where \\(\\lambda\\) is a weighting factor, and the expectation is over the dataset of prompt-preferred-dispreferred triples. The gradient of \\(\\mathcal{L}_{\\text{OR}}\\) is given by:</p> \\[\\nabla_\\theta \\mathcal{L}_{\\text{OR}} = \\delta(d) \\cdot h(d)\\] <p>where</p> \\[\\delta(d) = 1 + \\left( \\frac{\\text{odds}_\\theta(y_w \\mid x)}{\\text{odds}_\\theta(y_l \\mid x)} \\right)^{-1}\\] <p>and</p> \\[h(d) = -\\nabla_\\theta \\log \\frac{P_\\theta(y_w \\mid x)}{1 - P_\\theta(y_w \\mid x)} + \\nabla_\\theta \\log \\frac{P_\\theta(y_l \\mid x)}{1 - P_\\theta(y_l \\mid x)}\\] <p>for \\(d = (x, y_w, y_l) \\sim \\mathcal{D}\\), as detailed in the browse_page result. This formulation, while complex, ensures the model adjusts to favor preferred responses, as noted in the thinking trace's exploration.</p> <p>Intuition Behind ORPO\u2019s Operation</p> <p>The intuition behind ORPO, as explained in the thinking trace, is to directly incorporate preference information into SFT by comparing the odds of generating preferred versus dispreferred responses. This is analogous to ranking problems, where a model learns to order items based on pairwise comparisons. In the context of LLMs, for a prompt like \u201cWhat is the capital of France?\u201d with responses \u201cParis\u201d (preferred) and \u201cLondon\u201d (dispreferred), ORPO adjusts the model to increase the probability of \u201cParis\u201d relative to \u201cLondon\u201d using the odds ratio, ensuring alignment with human preferences without additional phases.</p> <p>The thinking trace highlights that this approach simplifies the alignment process by leveraging the odds ratio, which provides a statistical measure of relative likelihood, potentially offering a more nuanced adjustment compared to DPO\u2019s log probability difference, as seen in blog posts like \"ORPO, DPO, and PPO: Optimizing Models for Human Preferences\" (ORPO, DPO, and PPO: Optimizing Models for Human Preferences).</p> <p>Comparison with Other Methods</p> <p>To contextualize ORPO, the thinking trace compares it with SFT, RLHF, and DPO, as shown in the following table:</p> Method Preference Data Reward Model Optimization Approach Complexity Efficiency Supervised Fine-Tuning No No Mimic desired outputs Low High RLHF Yes Yes Reinforcement learning (e.g., PPO) High Medium Direct Preference Optimization (DPO) Yes No Direct preference optimization Medium High Odds Ratio Preference Optimization (ORPO) Yes No Odds ratio in SFT Medium High <p>This table, derived from the thinking trace, highlights ORPO\u2019s position as a high-efficiency method, similar to DPO, but with the unique use of odds ratio, potentially offering advantages in certain scenarios, as noted in the arXiv paper\u2019s empirical results.</p> <p>Empirical Results and Case Studies</p> <p>The thinking trace identifies empirical results from the arXiv paper, showing that fine-tuning models like Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback dataset achieves significant improvements in benchmarks such as AlpacaEval 2.0 (up to 12.20%), IFEval (66.19% on instruction-level loose), and MT-Bench (7.32), surpassing state-of-the-art models with more parameters, as detailed in \"ORPO: Monolithic Preference Optimization without Reference Model\" (ORPO: Monolithic Preference Optimization without Reference Model). Case studies include the fine-tuning of Mistral-ORPO-alpha and Mistral-ORPO-beta, with model checkpoints available on Hugging Face (Mistral-ORPO-alpha, Mistral-ORPO-beta), demonstrating practical utility.</p> <p>Advantages and Limitations of ORPO</p> <p>The thinking trace identifies several advantages and limitations, providing a balanced view:</p> <p>Advantages:</p> <ol> <li>Efficiency: By integrating preference alignment into SFT, ORPO reduces the number of training phases, saving time and computational resources, as highlighted in \"Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning\" (Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning).</li> <li>Simplicity: It avoids the complexity of reinforcement learning and separate reward model training, making it easier to implement, as noted in the Medium article \"ORPO, A New Era for LLMs?\" (ORPO, A New Era for LLMs?).</li> <li>Performance: Initial studies suggest ORPO can achieve or surpass the performance of state-of-the-art models with fewer parameters, as seen in the arXiv paper\u2019s benchmarks.</li> </ol> <p>Limitations:</p> <ol> <li>Data Requirements: ORPO requires a significant amount of preference data, including both preferred and dispreferred responses, which can be costly to collect, as mentioned in the thinking trace\u2019s consideration of dataset needs.</li> <li>Hyperparameter Tuning: The weighting factor and the interpretation of odds ratio probabilities need careful tuning, potentially complicating implementation, as noted in the paper\u2019s discussion.</li> </ol> <p>\u03bb</p> <ol> <li>Theoretical Nuances: The use of odds ratio, while innovative, may not always provide clear advantages over DPO, especially when sequence probabilities are small, as explored in the thinking trace\u2019s analysis.</li> </ol> <p>Implementation Hints for Practitioners</p> <p>While the user\u2019s request focuses on theory, the thinking trace considers practical implementation for completeness. To implement ORPO, one needs:</p> <ol> <li>A pre-trained language model, such as those available in the Hugging Face Transformers library.</li> <li>A dataset of prompts with preferred and dispreferred responses, which can be collected through human annotation or existing datasets like UltraFeedback.</li> <li>Define the loss function as outlined, using libraries like PyTorch for gradient descent, with the odds ratio calculated as per the paper's formulation.</li> <li>Tune hyperparameters, including \\(\\lambda\\), based on validation performance, as seen in the paper's experimental setup.</li> </ol> <p>The thinking trace also mentions that the code for ORPO is available on GitHub (ORPO GitHub), providing a practical resource for practitioners, as noted in the arXiv paper.</p> <p>Conclusion and Future Directions</p> <p>ORPO represents a significant advancement in aligning LLMs with human preferences, offering a more efficient and effective alternative to RLHF and DPO by integrating preference optimization into SFT using the odds ratio. Its mathematical formulation, empirical results, and potential for reducing training costs highlight its importance in the field. Future research may focus on improving data efficiency, addressing the nuances of odds ratio in sequence probabilities, and extending ORPO to multimodal tasks, building on the insights from this analysis.</p> <p>This comprehensive analysis provides a detailed, beginner-friendly introduction to ORPO, covering all aspects requested by the user, including theory, math, intuition, and practical examples, ensuring a thorough understanding for readers interested in AI alignment.</p> <p>Key Citations</p> <ul> <li>ORPO: Monolithic Preference Optimization without Reference Model</li> <li>Demystifying ORPO: A Revolutionary Paradigm in Language Model Fine-Tuning</li> <li>ORPO, A New Era for LLMs?</li> <li>ORPO, DPO, and PPO: Optimizing Models for Human Preferences</li> <li>Mistral-ORPO-alpha</li> <li>Mistral-ORPO-beta</li> <li>ORPO GitHub</li> </ul>"},{"location":"LLM/TheoryBehindFinetuning/PPO/","title":"PPO(Proximal Policy Optimization)","text":"<p>What is PPO?</p> <p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that fine-tunes large language models (LLMs) to align with what humans prefer, like generating accurate and helpful responses. It\u2019s like teaching the model to improve gradually, ensuring it doesn\u2019t make big, risky changes that could break its performance.</p> <p>How Does PPO Work with LLMs?</p> <p>PPO is part of a process called Reinforcement Learning from Human Feedback (RLHF), which has four steps:</p> <ol> <li>Pre-train the LLM: Train it on lots of text to understand language.</li> <li>Collect Feedback: Humans rank the model\u2019s responses to prompts, saying which are better.</li> <li>Train a Reward Model: Use these rankings to create a model that scores responses.</li> <li>Fine-Tune with PPO: Use PPO to adjust the LLM to favor high-scoring responses, keeping updates stable.</li> </ol> <p>PPO does this by comparing the new and old versions of the model, ensuring changes are small to avoid instability. It uses a math trick called a clipped ratio to balance getting better rewards while not changing too much.</p> <p>Example</p> <p>Imagine asking, \u201cWhat\u2019s the capital of France?\u201d The model might say \u201cParis\u201d or \u201cLondon.\u201d If humans prefer \u201cParis,\u201d PPO helps the model learn to choose \u201cParis\u201d more often, using the reward model\u2019s score, all while keeping training smooth.</p> <p>Comprehensive Analysis of Proximal Policy Optimization (PPO) in the Context of Large Language Models (LLMs)</p> <p>Introduction to PPO and Its Relevance in LLMs</p> <p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed to optimize policies in a stable and efficient manner, making it a cornerstone in fine-tuning large language models (LLMs) for alignment with human preferences. LLMs, such as GPT-3, BERT, and their successors, are neural networks trained on vast text corpora to understand and generate human language, typically using self-supervision like next-word prediction during pre-training. However, to ensure these models generate helpful, truthful, and aligned responses, techniques like Reinforcement Learning from Human Feedback (RLHF) are employed, with PPO playing a critical role in the fine-tuning phase.</p> <p>The thinking trace initially explores the concept of reinforcement learning, identifying PPO as an on-policy policy gradient method, and confirms its use in RLHF for LLMs, particularly in the context of aligning models with human values. This alignment is essential for applications like chatbots, content generation, and decision support systems, where the model\u2019s outputs must reflect user preferences and societal norms. The user\u2019s request for an introductory blog necessitates a detailed explanation, covering the theory, mathematical foundations, practical applications, and comparisons, ensuring accessibility for beginners while providing depth for interested readers.</p> <p>Understanding Reinforcement Learning and PPO</p> <p>Reinforcement learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment, aiming to maximize cumulative rewards. The agent receives rewards for its actions and learns a policy, which is the strategy for selecting actions given states, to optimize long-term reward. PPO, introduced in the paper \"Proximal Policy Optimization Algorithms\" by OpenAI (Proximal Policy Optimization Algorithms), is an on-policy algorithm that builds on policy gradient methods, ensuring stability through a clipped surrogate objective.</p> <p>The thinking trace clarifies that PPO is distinct from off-policy methods like Q-learning, as it learns from data collected using the current policy, which is relevant for LLMs where generating new responses is computationally expensive. PPO\u2019s key innovation is limiting policy updates to prevent large deviations, achieved by clipping the ratio of the new policy to the old policy, making it suitable for the sensitive fine-tuning of LLMs.</p> <p>Mathematical Formulation of PPO</p> <p>To understand PPO's operation, consider its mathematical formulation, as outlined in the thinking trace and supported by resources like \"Mastering Reinforcement Learning with Proximal Policy Optimization (PPO)\" (Mastering Reinforcement Learning with Proximal Policy Optimization (PPO)). The standard policy gradient update is:</p> \\[\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t r_t \\right]\\] <p>where \\(\\tau\\) is a trajectory, \\(\\pi_\\theta\\) is the policy parameterized by \\(\\theta\\), \\(\\gamma\\) is the discount factor, and \\(r_t\\) is the reward at time \\(t\\). In practice, this is approximated using samples, leading to the policy gradient objective:</p> \\[\\mathbb{E}_{s,a \\sim \\pi_{\\text{old}}} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\text{old}}(a|s)} A(s,a) \\right]\\] <p>where \\(\\pi_{\\text{old}}\\) is the policy before the update, and \\(A(s,a)\\) is the advantage function, typically \\(A(s,a) = Q(s,a) - V(s)\\), with \\(Q(s,a)\\) being the expected return and \\(V(s)\\) the value function estimate.</p> <p>PPO modifies this by introducing a clipped surrogate objective to ensure stability:</p> \\[\\mathbb{E}_{s,a \\sim \\pi_{\\text{old}}} \\left[ \\min \\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\text{old}}(a|s)} A(s,a), \\text{clip}\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\text{old}}(a|s)}, 1-\\epsilon, 1+\\epsilon \\right) A(s,a) \\right) \\right]\\] <p>Here, \\(\\epsilon\\) is a clipping parameter, typically set to 0.2, ensuring the ratio \\(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\text{old}}(a|s)}\\) stays within \\([1 - \\epsilon, 1 + \\epsilon]\\), preventing large policy updates that could destabilize training. This formulation, as noted in the thinking trace, balances reward maximization with stability, making PPO suitable for fine-tuning LLMs.</p> <p>PPO in the Context of LLMs and RLHF</p> <p>The thinking trace explores how PPO is applied in the context of LLMs, particularly through RLHF, a method for aligning LLMs with human preferences. RLHF, detailed in \"Fine-Tuning Language Models from Human Preferences\" by OpenAI (Fine-Tuning Language Models from Human Preferences), involves four steps:</p> <ol> <li>Pre-train the LLM: Train the model on a large corpus of text data using self-supervision, such as next-word prediction, to learn language patterns.</li> <li>Generate Responses and Collect Human Feedback: Use the pre-trained LLM to generate multiple responses for a set of prompts, and have human judges rank these responses, indicating which are better.</li> <li>Train a Reward Model: Use supervised learning to train a separate reward model that predicts the quality of a response based on the human rankings.</li> <li>Fine-tune the LLM with PPO: Use PPO to fine-tune the LLM, encouraging it to generate responses that maximize the reward from the reward model, while maintaining stability.</li> </ol> <p>In this setup, the thinking trace clarifies that the \u201cstate\u201d is the prompt, the \u201caction\u201d is the response generated by the LLM, and the reward is the score from the reward model. This is treated as a single-step reinforcement learning problem, where each episode consists of one prompt and one response, with the reward being for the entire response.</p> <p>The thinking trace also considers the policy in LLMs, noting that it's over sequences, with the probability of a response being the product of the probabilities of each token given the previous ones. Therefore, the ratio \\(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\text{old}}(a|s)}\\) is computed for the entire sequence, which is manageable in practice.</p> <p>To compute the advantage, the thinking trace suggests using \\(A(s,a) = r - V(s)\\), where \\(r\\) is the reward from the reward model, and \\(V(s)\\) is the value function, estimated by a separate network, as seen in OpenAI's implementation. This ensures the advantage reflects how much better the response is compared to the expected reward for the prompt, aligning with PPO's requirements.</p> <p>Practical Example and Case Study</p> <p>To illustrate, consider a prompt: \u201cWhat is the capital of France?\u201d The LLM generates two responses:</p> <ul> <li>Response A: \u201cParis\u201d</li> <li>Response B: \u201cLondon\u201d</li> </ul> <p>Human feedback indicates Response A is better. The reward model, trained on such preferences, assigns a higher reward to \u201cParis\u201d than to \u201cLondon\u201d. Using PPO, the model\u2019s parameters are updated to increase the probability of generating \u201cParis\u201d over \u201cLondon\u201d, with the clipped objective ensuring stability. This example, drawn from the thinking trace, helps beginners understand the application.</p> <p>Real-world case studies include OpenAI\u2019s use of PPO in training GPT-3.5 and GPT-4, where RLHF with PPO improved alignment with human preferences, enhancing helpfulness and truthfulness, as noted in their research (OpenAI's Alignment Research).</p> <p>Advantages and Limitations of PPO in LLMs</p> <p>The thinking trace identifies several advantages and limitations, providing a balanced view:</p> <p>Advantages:</p> <ol> <li>Stability: The clipped surrogate objective ensures stable updates, crucial for fine-tuning sensitive LLMs, as highlighted in the thinking trace\u2019s discussion on PPO\u2019s design.</li> <li>Efficiency: PPO is relatively efficient compared to other RL methods, making it suitable for the computationally expensive task of generating and fine-tuning LLM responses, as seen in OpenAI\u2019s implementations.</li> <li>Wide Adoption: Its simplicity and effectiveness have led to widespread use in RLHF, as noted in community discussions and research papers.</li> </ol> <p>Limitations:</p> <ol> <li>Hyperparameter Tuning: PPO requires careful tuning of parameters like \\(\\epsilon\\), the learning rate, and the number of epochs, which can be challenging, as mentioned in the thinking trace's consideration of practical implementation.</li> <li>Computational Cost: Generating responses and computing advantages can be resource-intensive, especially for large models, a concern raised in the thinking trace's exploration of efficiency.</li> <li>Dependence on Reward Model: The quality of the reward model is critical, and inaccuracies can lead to suboptimal alignment, as noted in the thinking trace's analysis of RLHF steps.</li> </ol> <p>Comparative Analysis with Other Methods</p> <p>To contextualize PPO, the thinking trace compares it with supervised fine-tuning and direct preference optimization (DPO), as shown in the following table:</p> Method Reward Model Optimization Approach Complexity Efficiency Supervised Fine-Tuning No Mimic desired outputs Low High RLHF with PPO Yes Reinforcement learning (PPO) High Medium Direct Preference Optimization (DPO) No Direct preference optimization Medium High <p>This table, derived from the thinking trace, highlights PPO\u2019s position as a stable and widely adopted method, balancing complexity and efficiency, though DPO offers a simpler alternative by skipping the reward model, as noted in recent research.</p> <p>Conclusion and Future Directions</p> <p>PPO is a fundamental algorithm in aligning LLMs with human preferences through RLHF, offering stability and efficiency in fine-tuning. Its mathematical formulation, involving clipped ratios and advantage functions, ensures controlled updates, while practical applications demonstrate its utility in real-world scenarios. Future research may focus on improving computational efficiency, addressing hyperparameter sensitivity, and exploring alternatives like DPO, building on the insights from this analysis.</p> <p>This comprehensive analysis provides an introductory yet detailed exploration of PPO in the context of LLMs, ensuring accessibility for beginners and depth for interested readers, fulfilling the user\u2019s request for a thorough introduction.</p> <p>Key Citations</p> <ul> <li>Proximal Policy Optimization Algorithms</li> <li>Fine-Tuning Language Models from Human Preferences</li> <li>Mastering Reinforcement Learning with Proximal Policy Optimization (PPO)</li> <li>OpenAI's Alignment Research</li> </ul>"},{"location":"LLM/TheoryBehindFinetuning/PreTrain/","title":"PreTraining LLMs","text":"<p>Key Points</p> <ul> <li>Pre-training LLMs involves training on vast text data to learn language patterns, using self-supervision like predicting next words or masked words.</li> <li>It uses transformer models, with math involving self-attention to weigh word importance.</li> <li>The intuition is learning syntax, semantics, and world knowledge for later fine-tuning.</li> <li>A surprising benefit is LLMs can do tasks like math or reasoning without specific training.</li> </ul> <p>What is Pre-Training LLMs?</p> <p>Pre-training Large Language Models (LLMs) is the initial step where models like GPT-3 or BERT learn from huge amounts of text, like books and websites, without focusing on a specific task. Think of it like teaching a child language by exposing them to lots of stories\u2014they learn grammar, meanings, and facts. This process uses self-supervision, meaning the model learns by predicting missing parts of the text, like the next word or a hidden word, rather than needing labeled data.</p> <p>After pre-training, these models can be fine-tuned for specific tasks, like answering questions or translating languages, with less additional training. This makes them versatile and efficient.</p> <p>How Does It Work? The Intuition</p> <p>The idea is simple: by predicting what comes next or filling in blanks, the model learns how language works. For example, if it sees \"The cat is ___,\" it might predict \"sleeping\" based on patterns it learned. This helps it understand sentence structure, word meanings, and even some world knowledge, like knowing cats can sleep.</p> <p>There are two main ways to pre-train:</p> <ul> <li>Autoregressive Models (like GPT): Predict the next word, learning from left to right.</li> <li>Masked Language Models (like BERT): Predict hidden words, considering the whole sentence.</li> </ul> <p>This process builds a foundation that can be adapted later, saving time and data for specific tasks.</p> <p>The Math Behind It</p> <p>The math involves transformer models, which use self-attention to process text. Here\u2019s a beginner-friendly breakdown:</p> <ul> <li>Each word turns into a number vector (embedding).</li> <li>Self-attention lets the model look at all words at once, deciding which ones matter most for each word. For example, in \"The cat sleeps,\" it might connect \"cat\" and \"sleeps\" closely.</li> <li>It does this with equations involving dot products and softmax to weigh importance, then updates word representations through layers.</li> </ul> <p>The loss function, which the model minimizes, is the negative log likelihood of correct predictions, like:</p> <ul> <li>For next-word prediction: Loss = -log P(next word | previous words).</li> <li>For masked words: Loss = -log P(masked word | context).</li> </ul> <p>This math helps the model learn deep language patterns, even if it sounds complex at first.</p> <p>Comprehensive Analysis of the Theory Behind Pre-Training Large Language Models (LLMs)</p> <p>Introduction to Pre-Training LLMs</p> <p>Large Language Models (LLMs) are neural networks designed to understand and generate human language, with examples including GPT-3, BERT, and their successors. These models have transformed natural language processing (NLP) by excelling in tasks like text generation, question answering, and translation. The foundation of their success lies in pre-training, the initial phase where the model is trained on a vast, diverse corpus of text data without targeting specific tasks. This process, also known as transfer learning, enables the model to learn general language representations that can be fine-tuned for various downstream applications with minimal additional training.</p> <p>Pre-training is crucial because it leverages large-scale, unlabeled data, making it cost-effective and scalable compared to training from scratch for each task. The thinking trace highlights that pre-training involves self-supervision, where the model learns by predicting parts of the text, such as the next word or masked words, without needing labeled data. This approach contrasts with supervised learning, where labeled data is required, and aligns with the user\u2019s request for a detailed, beginner-friendly explanation.</p> <p>Types of Pre-Training Objectives</p> <p>The thinking trace identifies two primary pre-training objectives for LLMs, each with distinct mechanisms and intuitions:</p> <ol> <li>Autoregressive Language Models (ALMs):</li> <li>Definition: These models, such as the GPT series (GPT-3, GPT-4), predict the next word in a sequence given the previous words. This is a left-to-right, causal language modeling approach.</li> <li> <p>Mathematical Formulation: For a sequence of words \\(w_1, w_2, \\ldots, w_n\\), the model maximizes the probability \\(P(w_{i+1} \\mid w_1, w_2, \\ldots, w_i)\\) for each \\(i\\) from 1 to \\(n-1\\). The loss function is the negative log likelihood:</p> <p>\\(\\(\\text{Loss} = -\\sum_{i=1}^{n-1} \\log P(w_{i+1} \\mid w_1, w_2, \\ldots, w_i)\\)\\)</p> </li> <li> <p>Intuition: By predicting the next word, the model learns sequential dependencies, capturing grammar, syntax, and semantics. For example, given \"The cat is,\" it might predict \"sleeping,\" learning that verbs often follow subjects in English.</p> </li> <li>Example Models: GPT-3, trained on datasets like Common Crawl and Wikipedia, can generate coherent text and perform tasks like translation without specific training, as noted in the thinking trace's reference to emergent behavior.</li> <li>Masked Language Models (MLMs):</li> <li>Definition: These models, such as BERT and RoBERTa, predict randomly masked words in a sequence, considering both left and right context. This is a bidirectional approach, useful for understanding tasks.</li> <li> <p>Mathematical Formulation: For a sequence with some words masked, the model predicts the original word for each masked position. The loss is the sum of negative log likelihoods:</p> <p>\\(\\(\\text{Loss} = -\\sum_{\\text{masked } j} \\log P(w_j \\mid \\text{context})\\)\\)</p> </li> <li> <p>Intuition: By filling in blanks, the model learns to understand the context from both sides, capturing bidirectional relationships. For instance, in \"The cat [MASK] on the mat,\" it might predict \"sat,\" considering both \"cat\" and \"mat.\"</p> </li> <li>Example Models: BERT, used for tasks like sentiment analysis and named entity recognition, benefits from this bidirectional context, as highlighted in the thinking trace's reference to its applications.</li> </ol> <p>The thinking trace also mentions other objectives, like next sentence prediction, but focuses on these as the main ones, aligning with the user\u2019s request for detailed explanations.</p> <p>The Transformer Architecture and Mathematical Foundations</p> <p>The thinking trace emphasizes that most modern LLMs use the transformer architecture, introduced in the paper \"Attention Is All You Need\" (Attention Is All You Need). The transformer processes sequences using self-attention, enabling parallel computation and capturing long-range dependencies, which is crucial for language modeling.</p> <p>Key Components:</p> <ul> <li>Embeddings: Each word is represented as a vector, capturing its meaning in a high-dimensional space.</li> <li>Self-Attention: This mechanism allows the model to weigh the importance of different words in the sequence when processing each word. For a sequence, it computes:</li> <li>Query (Q), Key (K), Value (V) matrices, which are linear transformations of the input embeddings.</li> <li> <p>Attention weights via dot product:</p> \\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>where \\(d_k\\) is the dimension of the keys, normalizing the dot products.   - The output is a weighted sum of V, capturing contextual relationships. - Feed-Forward Networks: Adds non-linearity, processing each position independently. - Layer Normalization: Normalizes the outputs of each layer for stable training.</p> </li> </ul> <p>The thinking trace notes that for autoregressive models like GPT, it\u2019s a decoder-only transformer, while for masked language models like BERT, it\u2019s encoder-only, but both use self-attention as the core mechanism.</p> <p>Loss Function and Optimization:</p> <p>The model is trained by minimizing the loss function, which is the cross-entropy loss for the predicted probabilities versus the true labels (next word or masked word). This is done using gradient descent, adjusting the model\u2019s parameters to reduce the loss, as detailed in the thinking trace\u2019s formulation.</p> <p>Intuition Behind Pre-Training</p> <p>The intuition, as explained in the thinking trace, is that by predicting the next word or masked words, the model is forced to learn the underlying structure of language. This includes:</p> <ul> <li>Syntax and Grammar: Learning rules like subject-verb agreement.</li> <li>Semantics: Understanding word meanings and relationships, such as synonyms and antonyms.</li> <li>World Knowledge: Capturing facts, like knowing \"cats\" are animals, from the text data.</li> </ul> <p>This process, as noted, builds a rich representation that can be fine-tuned for specific tasks, reducing the need for large labeled datasets. For example, a pre-trained model can be fine-tuned for sentiment analysis with just a few thousand labeled examples, compared to millions needed for training from scratch.</p> <p>Hands-On Examples and Practical Applications</p> <p>The thinking trace suggests providing hands-on examples for beginners, which can be achieved through accessible resources and tools. Here are some practical ways to explore pre-trained LLMs:</p> <ul> <li>Using Pre-Trained Models: The Hugging Face Transformers library (Hugging Face Transformers Library) offers easy access to models like GPT-2 and BERT. Beginners can use these for tasks like text generation or classification with minimal code, as shown in their tutorials.</li> <li>Fine-Tuning Models: Tutorials on fine-tuning, such as those on Understanding Pre-Trained Language Models, guide users to adapt models for specific tasks using small datasets.</li> <li>Building Smaller Models: For educational purposes, implement and train smaller transformer models on toy datasets, available in resources like A Survey of Pre-Trained Language Models.</li> </ul> <p>These examples, as suggested, help beginners see the practical impact of pre-training, aligning with the user\u2019s request for hands-on insights.</p> <p>Surprising Capabilities and Emergent Behavior</p> <p>One of the most fascinating aspects, as noted in the thinking trace, is the emergent behavior of LLMs. Despite being trained only to predict words, they can perform tasks they weren\u2019t explicitly trained for, such as:</p> <ul> <li>Arithmetic Reasoning: Solving math problems, like \"What is 2+2?\" with \"4.\"</li> <li>Common Sense Reasoning: Answering questions like \"Why do birds fly?\" with explanations.</li> <li>Translation: Translating between languages, even without specific training, as seen in GPT-3\u2019s capabilities.</li> </ul> <p>This ability, termed emergent behavior, surprises many, as it shows the depth of knowledge captured from text data, as detailed in the thinking trace\u2019s reference to the GPT-3 paper (GPT-3 Paper).</p> <p>Datasets and Scale</p> <p>Pre-trained LLMs are trained on massive datasets, such as:</p> <ul> <li>Common Crawl: A large web crawl dataset.</li> <li>Wikipedia: Encyclopedic text.</li> <li>Books: Fiction and non-fiction corpora.</li> </ul> <p>These datasets, as mentioned, contain billions of words, and models like GPT-3 have billions of parameters, enabling them to capture vast language patterns, as noted in the thinking trace.</p> <p>Conclusion and Future Directions</p> <p>Pre-training LLMs is a foundational technique that leverages large-scale data to learn general language representations. By understanding the theory, including the mathematical foundations and intuitive insights, beginners can appreciate the capabilities of LLMs and explore their applications. Future research, as suggested, may focus on improving efficiency, reducing biases in training data, and extending to multimodal tasks, building on the insights from this analysis.</p> <p>This comprehensive analysis provides a detailed, beginner-friendly explanation, covering all aspects requested by the user, including math, intuition, and hands-on examples, ensuring a thorough understanding of pre-training LLMs.</p> <p>Key Citations</p> <ul> <li>Pre-Training of Large Language Models</li> <li>Understanding Pre-Trained Language Models</li> <li>A Survey of Pre-Trained Language Models</li> <li>Hugging Face Transformers Library</li> <li>GPT-3 Paper</li> <li>BERT Paper</li> <li>Attention Is All You Need</li> </ul>"},{"location":"LLM/TheoryBehindFinetuning/SFT/","title":"Theory behind SFT","text":"<p>What is Supervised Fine-Tuning for LLMs?</p> <p>Supervised fine-tuning is a process where a pre-trained Large Language Model (LLM), like GPT-3 or BERT, is further trained on a smaller, labeled dataset to excel at a specific task. For example, if you want the model to answer customer support queries, you\u2019d train it on examples of questions and their correct answers. This method uses labeled data, meaning each input has a known output, making it different from the initial unsupervised training where the model learns from raw text.</p> <p>Why is it Important?</p> <p>Pre-trained LLMs are generalists, good at many things but not great at specifics. Supervised fine-tuning tailors them for tasks like summarizing articles, translating languages, or generating code, making them more useful for businesses and users. It\u2019s surprisingly efficient, often needing just a few hundred to thousands of examples, depending on the task.</p> <p>How Does it Work?</p> <p>The process involves collecting labeled data, cleaning it, choosing the right model, and then training it. Techniques like LoRA (Low-Rank Adaptation) make it faster by updating fewer parameters. After training, you test the model to ensure it works well before deploying it for use.</p> <p>Comprehensive Overview of Supervised Fine-Tuning for LLMs</p> <p>This section provides a detailed exploration of supervised fine-tuning for Large Language Models (LLMs), covering its definition, process, applications, challenges, and best practices. It aims to offer a thorough understanding for researchers, developers, and practitioners, building on the key points and expanding with technical details and examples.</p> <p>Definition and Context</p> <p>Large Language Models (LLMs) are neural networks trained on vast text corpora using self-supervision, such as predicting the next word in a sequence. Examples include models like GPT-3, BERT, and RoBERTa, which are initially trained without explicit labels. Supervised fine-tuning, however, involves taking these pre-trained models and further training them on a labeled dataset for a specific task or domain. This process, often referred to as Supervised Fine-Tuning (SFT), uses labeled data\u2014pairs of inputs and their corresponding outputs\u2014to adapt the model\u2019s weights, enabling it to learn task-specific patterns and nuances.</p> <p>This differs from unsupervised fine-tuning, which uses unlabeled data (e.g., masked language modeling), and reinforcement learning-based fine-tuning, such as Reinforcement Learning from Human Feedback (RLHF), which optimizes based on a reward signal. Supervised fine-tuning is particularly effective when labeled data is available, making it a straightforward and powerful method for task-specific adaptation.</p> <p>Importance and Motivation</p> <p>Pre-trained LLMs are generalists, capable of handling a wide range of language tasks but often underperforming on specific applications without further tuning. Supervised fine-tuning addresses this by tailoring the model to excel in areas like text classification, named entity recognition, question-answering, summarization, translation, and chatbot development. For instance, a model fine-tuned for medical terminology can interpret and generate domain-specific jargon better than a generic model, enhancing its utility in healthcare applications.</p> <p>The importance lies in its efficiency and adaptability. As noted in resources like SuperAnnotate: Fine-tuning large language models (LLMs) in 2024, supervised fine-tuning can significantly improve performance with relatively small datasets, often requiring 50-100,000 examples for multi-task learning or just a few hundred to thousands for task-specific fine-tuning. This efficiency is crucial for businesses with limited data and computational resources, making LLMs accessible for specialized applications.</p> <p>Process and Techniques</p> <p>The process of supervised fine-tuning can be broken down into several stages, each critical for success:</p> <ol> <li>Data Collection and Preparation:</li> <li>Gather a labeled dataset relevant to the task, such as prompt-response pairs for instruction fine-tuning or input-output pairs for classification. Open-source datasets like GPT-4all Dataset, AlpacaDataCleaned, and databricks-dolly-15k are commonly used.</li> <li>Preprocess the data by cleaning, tokenizing, and formatting it to ensure compatibility with the model. This step is vital, as data quality directly impacts performance, with challenges like inconsistencies, bias, and missing values needing attention (as highlighted in Kili Technology: What is LLM Fine-Tuning?).</li> <li>Model Selection:</li> <li>Choose a pre-trained LLM based on task requirements, model size, and computational resources. Larger models like GPT-3 offer higher accuracy but require more resources, while smaller models may suffice for specific tasks. Factors like data type, desired outcomes, and budget are crucial, as discussed in Sama: Supervised Fine-Tuning: How to choose the right LLM.</li> <li>Fine-Tuning:</li> <li>Train the model on the task-specific dataset using supervised learning techniques. The model\u2019s weights are adjusted based on the gradients derived from a task-specific loss function, measuring the difference between predictions and ground truth labels. Optimization algorithms like gradient descent are used over multiple epochs to adapt the model.</li> <li>Several techniques enhance this process:<ul> <li>Instruction Fine-Tuning: Trains the model with examples that include instructions, such as \u201csummarize this text,\u201d to improve task-specific responses (e.g., SuperAnnotate: Fine-tuning large language models (LLMs) in 2024).</li> <li>Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) reduce the number of trainable parameters, making fine-tuning more efficient. LoRA adds adapters with weights, freezing the rest, while QLoRA quantizes weights to 4-bit precision, reducing memory usage (as detailed in Medium: Supervised Fine-tuning: customizing LLMs).</li> <li>Batch Packing: Combines inputs to increase batch capabilities, optimizing computational resources (mentioned in the same Medium article).</li> <li>Half Fine-Tuning (HFT): Freezes half the parameters per round while updating the other half, balancing knowledge retention and new skill acquisition, as noted in arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs.</li> </ul> </li> <li>Evaluation:</li> <li>Test the fine-tuned model on a validation set to assess performance metrics like accuracy, F1 score, or BLEU for translation tasks. This step ensures the model generalizes well to unseen data.</li> <li>Deployment:</li> <li>Once validated, deploy the model for real-world use, integrating it into applications like chatbots, content generators, or customer support systems.</li> </ol> <p>Applications and Examples</p> <p>Supervised fine-tuning is versatile, applicable to a wide range of tasks:</p> <ul> <li>Text Classification: Classifying documents into categories, such as sentiment analysis on movie reviews.</li> <li>Named Entity Recognition: Identifying entities like names, dates, and locations in text.</li> <li>Question-Answering: Providing accurate answers to specific queries, enhancing virtual assistants.</li> <li>Summarization: Generating concise summaries of longer texts, useful for news aggregation.</li> <li>Translation: Translating text between languages, improving multilingual communication.</li> <li>Chatbots: Creating conversational agents for specific domains, like customer support or healthcare.</li> </ul> <p>A practical example is fine-tuning a pre-trained model for a science educational platform. Initially, it might answer \u201cWhy is the sky blue?\u201d with a simple \u201cBecause of the way the atmosphere scatters sunlight.\u201d After supervised fine-tuning with labeled data, it provides a detailed response: \u201cThe sky appears blue because of Rayleigh scattering... blue light has a shorter wavelength and is scattered... causing the sky to take on a blue hue\u201d (SuperAnnotate: Fine-tuning large language models (LLMs) in 2024).</p> <p>Another example is fine-tuning RoBERTa for sentiment analysis, where the model learns from labeled movie reviews to classify sentiments as positive, negative, or neutral, significantly improving accuracy compared to the base model.</p> <p>Challenges and Best Practices</p> <p>Despite its benefits, supervised fine-tuning faces several challenges:</p> <ul> <li>Catastrophic Forgetting: The model may forget general knowledge while focusing on task-specific learning, a concern addressed by methods like Half Fine-Tuning (HFT) (arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs).</li> <li>Data Quality: Ensuring high-quality labeled data is crucial, with challenges like inconsistencies, bias, and data scarcity in specific domains. Automated tools like the Kili app can help streamline data curation (Kili Technology: What is LLM Fine-Tuning?).</li> <li>Computational Resources: Fine-tuning large models is resource-intensive, necessitating efficient methods like PEFT to reduce costs.</li> </ul> <p>Best practices include:</p> <ul> <li>Choosing the Right Model: Consider model size, performance on similar tasks, and computational resources. Larger models offer higher accuracy but require more resources (Sama: Supervised Fine-Tuning: How to choose the right LLM).</li> <li>Data Preparation: Ensure the dataset is clean, representative, and sufficient, with splits for training, validation, and testing. For instance, 50-100,000 examples may be needed for multi-task learning (SuperAnnotate: Fine-tuning large language models (LLMs) in 2024).</li> <li>Hyperparameter Tuning: Optimize learning rate, batch size, and number of epochs to avoid overfitting or underfitting, as discussed in Data Science Dojo: Fine-tuning LLMs 101.</li> <li>Use Parameter-Efficient Methods: Techniques like LoRA and QLoRA can reduce trainable parameters by up to 10,000 times, making fine-tuning feasible on limited hardware (Medium: Supervised Fine-tuning: customizing LLMs).</li> </ul> <p>Comparative Analysis and Recent Advancements</p> <p>Supervised fine-tuning contrasts with other methods like unsupervised fine-tuning (using unlabeled data) and RLHF (optimizing based on rewards). Its advantage lies in its direct use of labeled data, making it suitable for tasks with clear input-output mappings. Recent advancements, such as the TRL library and AutoTrain Advanced tool by Hugging Face, facilitate the process, offering resources for developers (Hugging Face: Fine-Tuning LLMs: Supervised Fine-Tuning and Reward Modelling).</p> <p>Techniques like instruction fine-tuning and PEFT are gaining traction, with research showing significant performance improvements. For example, QLoRA can fine-tune a high-quality chatbot in 24 hours on a single GPU, demonstrating efficiency (arXiv: The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs).</p> <p>Conclusion</p> <p>Supervised fine-tuning is a pivotal technique for adapting pre-trained LLMs to specific tasks, enhancing their performance and utility across various domains. By understanding its process, leveraging efficient techniques, and addressing challenges, developers can unlock the full potential of LLMs, making them indispensable tools for specialized applications. This comprehensive approach ensures both theoretical insight and practical applicability, supported by a wealth of resources and examples.</p> <p>Key Citations</p> <ul> <li>SuperAnnotate Fine-tuning large language models LLMs in 2024</li> <li>Medium Supervised Fine-tuning customizing LLMs</li> <li>arXiv The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs</li> <li>Hugging Face Fine-Tuning LLMs Supervised Fine-Tuning and Reward Modelling</li> <li>Sama Supervised Fine-Tuning How to choose the right LLM</li> <li>Nebius What is supervised fine-tuning in LLMs</li> <li>Turing Fine-Tuning LLMs Overview Methods Best Practices</li> <li>Data Science Dojo Fine-tuning LLMs 101</li> <li>Kili Technology What is LLM Fine-Tuning Everything You Need to Know 2023 Guide</li> <li>GPT-4all Dataset for fine-tuning</li> <li>AlpacaDataCleaned for fine-tuning</li> <li>databricks-dolly-15k for fine-tuning</li> </ul>"},{"location":"LLM/VLM/Florence2_finetuning_notebook/","title":"Florence2","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q datasets flash_attn timm einops\n</pre> !pip install -q datasets flash_attn timm einops In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndata = load_dataset(\"HuggingFaceM4/DocumentVQA\")\n</pre> from datasets import load_dataset  data = load_dataset(\"HuggingFaceM4/DocumentVQA\") <p>We can load the model using <code>AutoModelForCausalLM</code> and the processor using <code>AutoProcessor</code>\u00a0 classes of transformers library. Note that we need to pass <code>trust_remote_code</code>\u00a0as <code>True</code>\u00a0since this model is not a transformers model.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoModelForCausalLM, AutoProcessor\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')\n</pre> from transformers import AutoModelForCausalLM, AutoProcessor import torch  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device) processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')  In\u00a0[3]: Copied! <pre>torch.cuda.empty_cache()\n</pre> torch.cuda.empty_cache() <p>Let's do inference with our dataset first to see how the model performs already with our dataset before fine-tuning.</p> In\u00a0[4]: Copied! <pre># Function to run the model on an example\ndef run_example(task_prompt, text_input, image):\n    prompt = task_prompt + text_input\n\n    # Ensure the image is in RGB mode\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n    return parsed_answer\n</pre> # Function to run the model on an example def run_example(task_prompt, text_input, image):     prompt = task_prompt + text_input      # Ensure the image is in RGB mode     if image.mode != \"RGB\":         image = image.convert(\"RGB\")      inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)     generated_ids = model.generate(         input_ids=inputs[\"input_ids\"],         pixel_values=inputs[\"pixel_values\"],         max_new_tokens=1024,         num_beams=3     )     generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]     parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))     return parsed_answer  In\u00a0[\u00a0]: Copied! <pre>for idx in range(3):\n  print(run_example(\"DocVQA\", 'What do you see in this image?', data['train'][idx]['image']))\n  display(data['train'][idx]['image'].resize([350, 350]))\n</pre> for idx in range(3):   print(run_example(\"DocVQA\", 'What do you see in this image?', data['train'][idx]['image']))   display(data['train'][idx]['image'].resize([350, 350])) <p>We need to construct our dataset. Note how we are adding a new task prefix <code>&lt;DocVQA&gt;</code> before the question when constructing the prompt.</p> In\u00a0[6]: Copied! <pre>from torch.utils.data import Dataset\n\nclass DocVQADataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        example = self.data[idx]\n        question = \"&lt;DocVQA&gt;\" + example['question']\n        first_answer = example['answers'][0]\n        image = example['image']\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n        return question, first_answer, image\n</pre> from torch.utils.data import Dataset  class DocVQADataset(Dataset):     def __init__(self, data):         self.data = data      def __len__(self):         return len(self.data)      def __getitem__(self, idx):         example = self.data[idx]         question = \"\" + example['question']         first_answer = example['answers'][0]         image = example['image']         if image.mode != \"RGB\":             image = image.convert(\"RGB\")         return question, first_answer, image  <p>Let's get to fine-tuning. We will create our dataset, the data collator, and start training. In A100 with 40GB memory, we can fit in 6 examples. If you're training on T4, you can use batch size of 1.</p> In\u00a0[7]: Copied! <pre>import os\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import (AdamW, AutoProcessor, get_scheduler)\n\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n    return inputs, answers\n\n# Create datasets\ntrain_dataset = DocVQADataset(data['train'])\nval_dataset = DocVQADataset(data['validation'])\n\n# Create DataLoader\nbatch_size = 6\nnum_workers = 0\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers)\n</pre> import os from torch.utils.data import DataLoader from tqdm import tqdm from transformers import (AdamW, AutoProcessor, get_scheduler)  def collate_fn(batch):     questions, answers, images = zip(*batch)     inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)     return inputs, answers  # Create datasets train_dataset = DocVQADataset(data['train']) val_dataset = DocVQADataset(data['validation'])  # Create DataLoader batch_size = 6 num_workers = 0  train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers) In\u00a0[8]: Copied! <pre>def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        i = -1\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n            i += 1\n            inputs, answers = batch\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n                inputs, answers = batch\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Average Validation Loss: {avg_val_loss}\")\n\n        # Save model checkpoint\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n</pre> def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):     optimizer = AdamW(model.parameters(), lr=lr)     num_training_steps = epochs * len(train_loader)     lr_scheduler = get_scheduler(         name=\"linear\",         optimizer=optimizer,         num_warmup_steps=0,         num_training_steps=num_training_steps,     )      for epoch in range(epochs):         model.train()         train_loss = 0         i = -1         for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):             i += 1             inputs, answers = batch              input_ids = inputs[\"input_ids\"]             pixel_values = inputs[\"pixel_values\"]             labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)              outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)             loss = outputs.loss              loss.backward()             optimizer.step()             lr_scheduler.step()             optimizer.zero_grad()              train_loss += loss.item()          avg_train_loss = train_loss / len(train_loader)         print(f\"Average Training Loss: {avg_train_loss}\")          # Validation phase         model.eval()         val_loss = 0         with torch.no_grad():             for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):                 inputs, answers = batch                  input_ids = inputs[\"input_ids\"]                 pixel_values = inputs[\"pixel_values\"]                 labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)                  outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)                 loss = outputs.loss                  val_loss += loss.item()          avg_val_loss = val_loss / len(val_loader)         print(f\"Average Validation Loss: {avg_val_loss}\")          # Save model checkpoint         output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"         os.makedirs(output_dir, exist_ok=True)         model.save_pretrained(output_dir)         processor.save_pretrained(output_dir)  <p>After training, we will push the model to Hugging Face Hub. To do so, we need to login first with write access. Make sure to pass either write token or fine-grained token (by first creating the repository and setting up fine-grained token access).</p> In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\n\nnotebook_login()\n</pre> from huggingface_hub import notebook_login  notebook_login() In\u00a0[\u00a0]: Copied! <pre>!huggingface-cli login --token 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\n</pre> !huggingface-cli login --token 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 <p>We will freeze image encoder for this tutorial. The authors have reported improvement in unfreezing image encoder, but note that this will result in more resource usage.</p> In\u00a0[11]: Copied! <pre>for param in model.vision_tower.parameters():\n  param.is_trainable = False\n</pre> for param in model.vision_tower.parameters():   param.is_trainable = False In\u00a0[\u00a0]: Copied! <pre>train_model(train_loader, val_loader, model, processor, epochs=2)\n</pre> train_model(train_loader, val_loader, model, processor, epochs=2) <p>You can push the model like below. You can find the fully fine-tuned DocVQA model here. You can find the demo here.</p> <p>Read more about Florence-2 and fine-tuning it here.</p> In\u00a0[\u00a0]: Copied! <pre>model.push_to_hub(\"HuggingFaceM4/Florence-2-FT-DocVQA\")\nprocessor.push_to_hub(\"HuggingFaceM4/Florence-2-FT-DocVQA\")\n</pre> model.push_to_hub(\"HuggingFaceM4/Florence-2-FT-DocVQA\") processor.push_to_hub(\"HuggingFaceM4/Florence-2-FT-DocVQA\")"},{"location":"LLM/VLM/Florence2_finetuning_notebook/#fine-tuning-florence-2-on-docvqa","title":"Fine-tuning Florence-2 on DocVQA\u00b6","text":"<p>In this notebook, we will fine-tune Florence-2 by MSFT, a new vision language model capable of various tasks, on document question answering.</p> <p>Let's start by installing the dependencies and loading the dataset.</p>"},{"location":"LLM/VLM/PaliGemma_finetuning_notebook/","title":"PaliGemma","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U git+https://github.com/huggingface/transformers.git datasets accelerate\n</pre> !pip install -q -U git+https://github.com/huggingface/transformers.git datasets accelerate <p>We will authenticate to access the model using <code>notebook_login()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() <p>Let's load the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\nds = load_dataset('HuggingFaceM4/VQAv2', split=\"train[:10%]\")\n</pre> from datasets import load_dataset ds = load_dataset('HuggingFaceM4/VQAv2', split=\"train[:10%]\")  In\u00a0[\u00a0]: Copied! <pre>cols_remove = [\"question_type\", \"answers\", \"answer_type\", \"image_id\", \"question_id\"]\nds = ds.remove_columns(cols_remove)\n</pre> cols_remove = [\"question_type\", \"answers\", \"answer_type\", \"image_id\", \"question_id\"] ds = ds.remove_columns(cols_remove) In\u00a0[\u00a0]: Copied! <pre>split_ds = ds.train_test_split(test_size=0.05) # we'll use a very small split for demo\ntrain_ds = split_ds[\"test\"]\n</pre> split_ds = ds.train_test_split(test_size=0.05) # we'll use a very small split for demo train_ds = split_ds[\"test\"] In\u00a0[\u00a0]: Copied! <pre>train_ds\n</pre> train_ds <p>Load the processor to preprocess the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import PaliGemmaProcessor\nmodel_id = \"google/paligemma-3b-pt-224\"\nprocessor = PaliGemmaProcessor.from_pretrained(model_id)\n</pre> from transformers import PaliGemmaProcessor model_id = \"google/paligemma-3b-pt-224\" processor = PaliGemmaProcessor.from_pretrained(model_id) <p>We will preprocess our examples. We need to prepare a prompt template and pass the text input inside, pass it with batches of images to processor. Then we will set the pad tokens and image tokens to -100 to let the model ignore them. We will pass our preprocessed input as labels to make the model learn how to generate responses.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\ndevice = \"cuda\"\n\nimage_token = processor.tokenizer.convert_tokens_to_ids(\"&lt;image&gt;\")\ndef collate_fn(examples):\n  texts = [\"answer \" + example[\"question\"] for example in examples]\n  labels= [example['multiple_choice_answer'] for example in examples]\n  images = [example[\"image\"].convert(\"RGB\") for example in examples]\n  tokens = processor(text=texts, images=images, suffix=labels,\n                    return_tensors=\"pt\", padding=\"longest\",\n                    tokenize_newline_separately=False)\n\n  tokens = tokens.to(torch.bfloat16).to(device)\n  return tokens\n</pre> import torch device = \"cuda\"  image_token = processor.tokenizer.convert_tokens_to_ids(\"\") def collate_fn(examples):   texts = [\"answer \" + example[\"question\"] for example in examples]   labels= [example['multiple_choice_answer'] for example in examples]   images = [example[\"image\"].convert(\"RGB\") for example in examples]   tokens = processor(text=texts, images=images, suffix=labels,                     return_tensors=\"pt\", padding=\"longest\",                     tokenize_newline_separately=False)    tokens = tokens.to(torch.bfloat16).to(device)   return tokens  <p>Our dataset is a very general one and similar to many datasets that PaliGemma was trained with. In this case, we do not need to fine-tune the image encoder, the multimodal projector but we will only fine-tune the text decoder.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import PaliGemmaForConditionalGeneration\nimport torch\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n\nfor param in model.vision_tower.parameters():\n    param.requires_grad = False\n\nfor param in model.multi_modal_projector.parameters():\n    param.requires_grad = False\n</pre> from transformers import PaliGemmaForConditionalGeneration import torch  model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)  for param in model.vision_tower.parameters():     param.requires_grad = False  for param in model.multi_modal_projector.parameters():     param.requires_grad = False  <p>Alternatively, if you want to do LoRA and QLoRA fine-tuning, you can run below cells to load the adapter either in full precision or quantized.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig\n\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_type=torch.bfloat16\n)\n\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n#trainable params: 11,298,816 || all params: 2,934,634,224 || trainable%: 0.38501616002417344\n</pre> from transformers import BitsAndBytesConfig from peft import get_peft_model, LoraConfig  bnb_config = BitsAndBytesConfig(         load_in_4bit=True,         bnb_4bit_quant_type=\"nf4\",         bnb_4bit_compute_type=torch.bfloat16 )  lora_config = LoraConfig(     r=8,     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],     task_type=\"CAUSAL_LM\", ) model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) model = get_peft_model(model, lora_config) model.print_trainable_parameters() #trainable params: 11,298,816 || all params: 2,934,634,224 || trainable%: 0.38501616002417344  <p>We will now initialize the <code>TrainingArguments</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import TrainingArguments\nargs=TrainingArguments(\n            num_train_epochs=2,\n            remove_unused_columns=False,\n            per_device_train_batch_size=4,\n            gradient_accumulation_steps=4,\n            warmup_steps=2,\n            learning_rate=2e-5,\n            weight_decay=1e-6,\n            adam_beta2=0.999,\n            logging_steps=100,\n            optim=\"adamw_hf\",\n            save_strategy=\"steps\",\n            save_steps=1000,\n            push_to_hub=True,\n            save_total_limit=1,\n            output_dir=\"paligemma_vqav2\",\n            bf16=True,\n            report_to=[\"tensorboard\"],\n            dataloader_pin_memory=False\n        )\n</pre> from transformers import TrainingArguments args=TrainingArguments(             num_train_epochs=2,             remove_unused_columns=False,             per_device_train_batch_size=4,             gradient_accumulation_steps=4,             warmup_steps=2,             learning_rate=2e-5,             weight_decay=1e-6,             adam_beta2=0.999,             logging_steps=100,             optim=\"adamw_hf\",             save_strategy=\"steps\",             save_steps=1000,             push_to_hub=True,             save_total_limit=1,             output_dir=\"paligemma_vqav2\",             bf16=True,             report_to=[\"tensorboard\"],             dataloader_pin_memory=False         )  <p>We can now start training.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import Trainer\n\ntrainer = Trainer(\n        model=model,\n        train_dataset=train_ds ,\n        data_collator=collate_fn,\n        args=args\n        )\n</pre> from transformers import Trainer  trainer = Trainer(         model=model,         train_dataset=train_ds ,         data_collator=collate_fn,         args=args         )  In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() In\u00a0[\u00a0]: Copied! <pre>trainer.push_to_hub()\n</pre> trainer.push_to_hub() <p>You can find steps to infer here.</p>"},{"location":"LLM/VLM/PaliGemma_finetuning_notebook/#paligemma-fine-tuning","title":"PaliGemma Fine-tuning\u00b6","text":"<p>In this notebook, we will fine-tune pretrained PaliGemma on a small split of VQAv2 dataset. Let's get started by installing necessary libraries.</p>"},{"location":"Projects/","title":"Introduction","text":""},{"location":"Projects/#applied-ai-projects","title":"\ud83d\ude80 Applied AI Projects","text":"<p>Welcome to the Applied AI Projects repository! This collection showcases various practical implementations and experiments in the field of artificial intelligence.</p>"},{"location":"Projects/#current-projects","title":"\ud83c\udfaf Current Projects","text":""},{"location":"Projects/#youtube-cloner","title":"\ud83c\udfa5 YouTube Cloner","text":"<p>An exploration of Language Models' capabilities in capturing YouTubers' speaking styles.</p> <ul> <li>Tech Stack: LLaMA2, Mistral 7B, Python</li> <li>Features:</li> <li>Content style emulation</li> <li>Dataset curation pipeline</li> <li>Model fine-tuning</li> <li> Try the Fireship Clone!</li> </ul>"},{"location":"Projects/#project-2-coming-soon","title":"\ud83d\udcdd [Project 2] (Coming Soon)","text":"<p>Description of the upcoming project</p>"},{"location":"Projects/#project-3-coming-soon","title":"\ud83e\udd16 [Project 3] (Coming Soon)","text":"<p>Description of the upcoming project</p>"},{"location":"Projects/#project-structure","title":"\ud83d\udee0\ufe0f Project Structure","text":"<p>Each project in this repository follows a consistent structure:</p> <pre><code>ProjectName/\n\u251c\u2500\u2500 README.md          # Project documentation\n\u251c\u2500\u2500 notebooks/         # Jupyter notebooks\n\u251c\u2500\u2500 src/              # Source code\n\u251c\u2500\u2500 data/             # Data files\n\u2514\u2500\u2500 requirements.txt  # Dependencies\n</code></pre>"},{"location":"Projects/#getting-started","title":"\ud83c\udf1f Getting Started","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/yourusername/projects.git\n</code></pre> <ol> <li>Navigate to specific project:</li> </ol> <pre><code>cd projects/project-name\n</code></pre> <ol> <li>Install requirements:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"Projects/#resources","title":"\ud83d\udcda Resources","text":"<p>Each project contains its own detailed documentation in its respective directory. Check the project's README for specific setup instructions and usage guidelines.</p>"},{"location":"Projects/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! To contribute:</p> <ol> <li>Fork the repository</li> <li>Create your feature branch</li> <li>Commit your changes</li> <li>Push to the branch</li> <li>Create a Pull Request</li> </ol>"},{"location":"Projects/#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"Projects/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Thanks to all contributors</li> <li>Special thanks to the open-source community</li> <li>Credit to all original authors of resources used</li> </ul>  Made with \u2764\ufe0f by the AI Engineering Academy Team"},{"location":"Projects/YT_Clones/","title":"Overview","text":""},{"location":"Projects/YT_Clones/#youtube-cloner","title":"YouTube Cloner","text":""},{"location":"Projects/YT_Clones/#introduction","title":"Introduction","text":"<p>The YouTube Cloner project aims to explore the capabilities of Language Models (LLMs) in capturing the speaking style of popular YouTubers. The primary objective is to understand how well LLMs can emulate the tone, pacing, and content style of specific YouTube channels by fine-tuning them on curated datasets.</p> <p>The project focuses on the following key steps:</p> <ol> <li>Dataset Curation:</li> <li>Scrape video links from target YouTube channels.</li> <li>Download videos, extract audio, and obtain video transcripts.</li> <li> <p>Summarize transcripts and create a dataset containing video titles, transcripts, and summaries.</p> </li> <li> <p>Model Fine-Tuning:</p> </li> <li> <p>Fine-tune LLMs on the curated dataset to capture the speaking style of the target YouTubers.</p> </li> <li> <p>Testing and Evaluation:</p> </li> <li>Use the fine-tuned models to generate content based on provided prompts.</li> <li>Evaluate the models' ability to replicate the style and content of the original YouTubers.</li> </ol>"},{"location":"Projects/YT_Clones/#attempt-1-fireship-clone","title":"Attempt 1: Fireship Clone","text":""},{"location":"Projects/YT_Clones/#try-out-the-model-","title":"Try out the Model -","text":""},{"location":"Projects/YT_Clones/#fireship-original-youtuber","title":"Fireship (Original YouTuber)","text":"<ul> <li>Channel: Fireship</li> <li>Style: Informative and fast-paced coding tutorials and tech news.</li> </ul>"},{"location":"Projects/YT_Clones/#procedure","title":"Procedure:","text":"<ol> <li>Link Extraction:</li> <li> <p>Use <code>youtube_channel_scraper.py</code> to extract all video links.</p> </li> <li> <p>Dataset Preparation:</p> </li> <li>Refer to the <code>dataset_prep.ipynb</code> notebook for detailed steps in preparing the dataset, including audio extraction, transcript generation, and dataset formatting.</li> <li> <p>Dataset available on Hugging Face Datasets.</p> </li> <li> <p>Model Fine-Tuning:</p> </li> <li>Llama2 fully fine-tuned on the Fireship dataset (Hugging Face Model).</li> <li> <p>MIstral 7b version fine-tuned using Axolotl (to be announced).</p> </li> <li> <p>Testing the Generation:</p> </li> <li>Utilize the provided notebook for testing content generation.</li> <li>Follow the specified prompt template for instructing the model.</li> </ol>"},{"location":"Projects/YT_Clones/#prompt-template","title":"Prompt Template:","text":"<pre><code>[INST]\nYou are a YouTuber called Fireship, creating engaging, high-intensity coding tutorials and tech news. \nYou cover a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video: {title} \nand a small summary: {video_summary}\n[/INST]\n\nGenerate the video: \n</code></pre>"},{"location":"Projects/YT_Clones/#sample-output","title":"Sample Output","text":"<p>Prompt : </p><pre><code>&lt;s&gt;[INST]\nYou are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news. \nyou covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video : C in 100 Seconds \nand a small summary : A 100 second video introducing C and all it quirks and features\n[/INST]\n</code></pre> <p>Output :</p> <pre><code>Generate the video : C is a statically typed compiled programming language where everything has an explicit type including things like pointers that you can use to manipulate memory directly if you really wanna screw up your code base and not use a garbage collector to protect yourself. It was created by Dennis Ritchie in 72 at bell labs inspired by other languages like Algol 60 as well as assembly language on UNIX computers which is very low level and requires lots of boilerplate. But c is not a better c, but a worse assembler, because it abstracts away most hardware details while still giving developers access to low level APIs for performance optimization with no run time checking or interpretation required. Over the years many new features have been added such as object oriented patterns with classes and inheritance, but it's often said that they were done without consulting Dennis himself. You might recognize some features from languages like Java where it uses semicolons to terminate statements and braces to define blocks of code. Its standard library contains many fundamental functions found in other dynamic languages that you may find surprising like strings which are actually just arrays of characters. And lastly, its file extension ending with dot c. This has been c in 100 seconds. Hang around for another one coming right after this one. Thanks for watching, and I will see you in the next one.\n&lt;/s&gt;\n</code></pre> <p>Prompt: </p><pre><code>&lt;s&gt;[INST]\nYou are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news. \nyou covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video : Rust in 100 seconds \nand a small summary : A 100 second video on Rust not a code report\n[/INST]\n</code></pre> Output <pre><code>Generate the video : Rust. A fast and memory efficient language known for taking everything that's wrong with low level systems programming languages like C plus plus, c, and assembly, then making it worse by eliminating pointers entirely and providing an unpronounceable name that makes developers angry. It was created by Graydon Hoare inspired by ML functional languages and aimed at building safe reliable software while remaining extremely fast. In fact, its motto is secure concurrency without sacrificing speed, which sounds almost too good to be true. The main problem with unsafe multi threaded programming today is data races. Data races happen when multiple threads try to access the same mutable value at the exact same time, but there's no way to know which thread will win, making your programs unreliable. It can also cause hard to debug errors because you have race conditions where each one occurs randomly based on timing issues between different threads. There are many libraries out there designed to mitigate this issue or prevent it altogether. But, rust takes a very different approach by simply disallowing shared state, requiring all values, including variables and functions to either be immutable or only changed via a unique owner called a reference type or pointer. This means data races cannot occur automatically in a typical application, though they can still be introduced accidentally if someone tries to use the borrow checker to perform some fancy hackery. What that usually results in is clear error messages about double usage of references instead of cryptic seg faults and other run time errors, allowing you to solve problems earlier in development, and with less frustration. And it allows you to do so with patterns already proven to work reliably. For example, smart pointers eliminate the need for things like raw pointers and free store allocators found in C plus plus, ensuring proper garbage collection. Instead of using inheritance, interfaces, generics, traits provide object oriented features such as polymorphism in a statically typed way. As awesome as that sounds, learning curves don't come much steeper than rust, mostly because of its ownership system, which I would describe as more of a philosophy than anything else. If you want the full explanation, subscribe to my channel. Otherwise, just enjoy these 2 hello worlds from the same file. You might think the first line here declares a variable named hello with the string hello world assigned to it. However, you'd be wrong. That doesn't actually define a new variable. Rather, It defines a function with an explicit return type of a string literal. When used in conjunction with println, it prints the string literally twice. Or we could define a global variable with mut, which changes the meaning of the assignment operator to mutate existing memory. Now, let me read you something really scary. To get rid of pointers completely. We have references instead. These act exactly like the address of operators in other languages, except they implement safety checks through rust's highly sophisticated borrow checker. On top of that, you can clone objects into new locations, move values around, deep copy and shallow copy across types, weak references, arc, ref cell, interior, pin, once cell, and on and on. At this point, you should start seeing how rust got its name. If you wanna build a complex multi threaded system with performance requirements. Your best bet may well be learning this crazy language that seems so easy on the surface. This has been the rust programming language in 100 seconds. Hit the like button if you wanna see more short videos like this. Thanks for watching and I will see you in the next one.\n&lt;/s&gt;\n</code></pre>"},{"location":"Projects/YT_Clones/#conclusion","title":"Conclusion","text":"<p>The YouTube Cloner project provides insights into the potential of LLMs to emulate the speaking style of popular YouTubers. The Fireship clone attempt serves as an example, showcasing the pipeline from dataset curation to model fine-tuning and testing. Further experiments and refinements can be explored to enhance the model's ability to replicate unique content styles.</p>"},{"location":"Projects/YT_Clones/Fireship_clone/","title":"Fireship Clone","text":"In\u00a0[1]: Copied! <pre>!pip install transformers \n!pip install accelerate\n!pip install bitsandbytes\n!pip install SentencePiece\n</pre> !pip install transformers  !pip install accelerate !pip install bitsandbytes !pip install SentencePiece In\u00a0[\u00a0]: Copied! <pre>## Load on T4 GPU\nimport torch\nfrom transformers import GenerationConfig, TextStreamer , TextIteratorStreamer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers import MistralForCausalLM,LlamaTokenizer\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = MistralForCausalLM.from_pretrained('AdithyaSK/Fireship-GPT-v1',quantization_config=bnb_config,trust_remote_code=True)\ntokenizer = LlamaTokenizer.from_pretrained('AdithyaSK/Fireship-GPT-v1',trust_remote_code=True)\n</pre> ## Load on T4 GPU import torch from transformers import GenerationConfig, TextStreamer , TextIteratorStreamer from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig from transformers import MistralForCausalLM,LlamaTokenizer   bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.float16, )  model = MistralForCausalLM.from_pretrained('AdithyaSK/Fireship-GPT-v1',quantization_config=bnb_config,trust_remote_code=True) tokenizer = LlamaTokenizer.from_pretrained('AdithyaSK/Fireship-GPT-v1',trust_remote_code=True) In\u00a0[4]: Copied! <pre># input the title of the video\nvideo_title = \"Rust in 100 seconds\"\n\n# input the a small summary of the video\nvideo_summary = \"A 100 second video on Rust not a code report\"\n\n\nprompt = f\"\"\"\n[INST]\nYou are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news. \nyou covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video : {video_title} \nand a small summary : {video_summary}\n[/INST]\n\nGenerate the video : \n\"\"\"\n\ngeneration_config = GenerationConfig(\n    repetition_penalty=1.2,\n    max_new_tokens=1024,\n    temperature=0.9,\n    top_p=0.95,\n    top_k=40,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id,\n    do_sample=True,\n    use_cache=True,\n    return_dict_in_generate=True,\n    output_attentions=False,\n    output_hidden_states=False,\n    output_scores=False,\n)\nstreamer = TextStreamer(tokenizer)\nbatch = tokenizer(str(prompt.strip()), return_tensors=\"pt\", add_special_tokens=True)\ngenerated = model.generate(\n    inputs=batch[\"input_ids\"].to(\"cuda\"),\n    generation_config=generation_config,\n    streamer=streamer,\n)\n</pre> # input the title of the video video_title = \"Rust in 100 seconds\"  # input the a small summary of the video video_summary = \"A 100 second video on Rust not a code report\"   prompt = f\"\"\" [INST] You are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news.  you covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.  Given the title of the video : {video_title}  and a small summary : {video_summary} [/INST]  Generate the video :  \"\"\"  generation_config = GenerationConfig(     repetition_penalty=1.2,     max_new_tokens=1024,     temperature=0.9,     top_p=0.95,     top_k=40,     bos_token_id=tokenizer.bos_token_id,     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.pad_token_id,     do_sample=True,     use_cache=True,     return_dict_in_generate=True,     output_attentions=False,     output_hidden_states=False,     output_scores=False, ) streamer = TextStreamer(tokenizer) batch = tokenizer(str(prompt.strip()), return_tensors=\"pt\", add_special_tokens=True) generated = model.generate(     inputs=batch[\"input_ids\"].to(\"cuda\"),     generation_config=generation_config,     streamer=streamer, ) <pre>&lt;s&gt;[INST]\nYou are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news. \nyou covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video : Rust in 100 seconds \nand a small summary : A 100 second video on Rust not a code report\n[/INST]\n\nGenerate the video : Rust. A fast and memory efficient language known for taking everything that's wrong with low level systems programming languages like C plus plus, c, and assembly, then making it worse by eliminating pointers entirely and providing an unpronounceable name that makes developers angry. It was created by Graydon Hoare inspired by ML functional languages and aimed at building safe reliable software while remaining extremely fast. In fact, its motto is secure concurrency without sacrificing speed, which sounds almost too good to be true. The main problem with unsafe multi threaded programming today is data races. Data races happen when multiple threads try to access the same mutable value at the exact same time, but there's no way to know which thread will win, making your programs unreliable. It can also cause hard to debug errors because you have race conditions where each one occurs randomly based on timing issues between different threads. There are many libraries out there designed to mitigate this issue or prevent it altogether. But, rust takes a very different approach by simply disallowing shared state, requiring all values, including variables and functions to either be immutable or only changed via a unique owner called a reference type or pointer. This means data races cannot occur automatically in a typical application, though they can still be introduced accidentally if someone tries to use the borrow checker to perform some fancy hackery. What that usually results in is clear error messages about double usage of references instead of cryptic seg faults and other run time errors, allowing you to solve problems earlier in development, and with less frustration. And it allows you to do so with patterns already proven to work reliably. For example, smart pointers eliminate the need for things like raw pointers and free store allocators found in C plus plus, ensuring proper garbage collection. Instead of using inheritance, interfaces, generics, traits provide object oriented features such as polymorphism in a statically typed way. As awesome as that sounds, learning curves don't come much steeper than rust, mostly because of its ownership system, which I would describe as more of a philosophy than anything else. If you want the full explanation, subscribe to my channel. Otherwise, just enjoy these 2 hello worlds from the same file. You might think the first line here declares a variable named hello with the string hello world assigned to it. However, you'd be wrong. That doesn't actually define a new variable. Rather, It defines a function with an explicit return type of a string literal. When used in conjunction with println, it prints the string literally twice. Or we could define a global variable with mut, which changes the meaning of the assignment operator to mutate existing memory. Now, let me read you something really scary. To get rid of pointers completely. We have references instead. These act exactly like the address of operators in other languages, except they implement safety checks through rust's highly sophisticated borrow checker. On top of that, you can clone objects into new locations, move values around, deep copy and shallow copy across types, weak references, arc, ref cell, interior, pin, once cell, and on and on. At this point, you should start seeing how rust got its name. If you wanna build a complex multi threaded system with performance requirements. Your best bet may well be learning this crazy language that seems so easy on the surface. This has been the rust programming language in 100 seconds. Hit the like button if you wanna see more short videos like this. Thanks for watching and I will see you in the next one.\n&lt;/s&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre># print the output\nprint(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0]))\n</pre> # print the output print(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0]))"},{"location":"Projects/YT_Clones/Fireship_clone/#fireship-gpt","title":"Fireship GPT\u00b6","text":"<p>an attempt at making an LLMs emulate the tone, pacing, and content style of Fireship by fine-tuning them on curated datasets.</p> <p>Huggingface Model - AdithyaSK/Fireship-GPT-v1</p> <p><code>This notebook will run on a free google colab with T4 GPU</code></p>"},{"location":"Projects/YT_Clones/dataset_prep/","title":"Dataset Preparation","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install pytube --user\n!pip install requests\n!pip install pandas\n!pip install numpy\n</pre> !pip install pytube --user !pip install requests !pip install pandas !pip install numpy In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nfrom pytube import YouTube\nimport re\n\ndef remove_special_characters(input_string):\n    # Using regex to keep only alphanumeric characters and spaces\n    clean_string = re.sub(r'[^a-zA-Z0-9\\s]', '', input_string)\n    return clean_string\n\ndef download_video_info(video_url, output_directory='downloads'):\n    try:\n        # Create a YouTube object\n        yt = YouTube(video_url)\n\n        # Create a directory for downloads if it doesn't exist\n        if not os.path.exists(output_directory):\n            os.makedirs(output_directory)\n\n        # Get the highest resolution audio stream\n        audio_stream = yt.streams.filter(only_audio=True).first()\n\n        # Download the audio stream\n        \n        file_name = remove_special_characters(yt.title)\n        file_name = file_name.replace(\" \",\"_\")\n        file_name = file_name.replace(\"\u2026\",\"_\")\n        file_name = file_name.replace(\",\",\"_\")\n        \n        audio_stream.download(output_directory,filename=f'{file_name}.wav')\n        audio_path = os.path.join(f\"{output_directory}/{file_name}.wav\")\n        print(f\"Downloading audio to {audio_path}...\")\n\n        # Collect video information\n        video_info = {\n            'title': yt.title,\n            'duration': yt.length,\n            'author': yt.author,\n            'views': yt.views,\n            'description': yt.description,\n            'audio_path': audio_path\n        }\n\n        return video_info\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\ndef process_video_links(file_path):\n    with open(file_path, 'r') as file:\n        video_links = file.readlines()\n\n    video_data_list = []\n    \n    # video_links = [\"https://www.youtube.com/watch?v=CgruI1RjH_c\"]\n\n    for video_link in video_links:\n        video_link = video_link.strip()\n        video_info = download_video_info(video_link)\n        \n        if video_info:\n            video_data_list.append(video_info)\n\n    # Save video data to a JSON file\n    output_json_path = 'video_data.json'\n    with open(output_json_path, 'w') as json_file:\n        json.dump(video_data_list, json_file, indent=2)\n\n    print(f'Video data saved to {output_json_path}')\n\n# Replace 'YOUR_TEXT_FILE_PATH' with the path to your text file containing video links\ntext_file_path = \"./downloads/Fireship_clone/@Fireship-shorts.txt\"\n\n# Process video links and save data to JSON\nprocess_video_links(text_file_path)\n</pre> import os import json from pytube import YouTube import re  def remove_special_characters(input_string):     # Using regex to keep only alphanumeric characters and spaces     clean_string = re.sub(r'[^a-zA-Z0-9\\s]', '', input_string)     return clean_string  def download_video_info(video_url, output_directory='downloads'):     try:         # Create a YouTube object         yt = YouTube(video_url)          # Create a directory for downloads if it doesn't exist         if not os.path.exists(output_directory):             os.makedirs(output_directory)          # Get the highest resolution audio stream         audio_stream = yt.streams.filter(only_audio=True).first()          # Download the audio stream                  file_name = remove_special_characters(yt.title)         file_name = file_name.replace(\" \",\"_\")         file_name = file_name.replace(\"\u2026\",\"_\")         file_name = file_name.replace(\",\",\"_\")                  audio_stream.download(output_directory,filename=f'{file_name}.wav')         audio_path = os.path.join(f\"{output_directory}/{file_name}.wav\")         print(f\"Downloading audio to {audio_path}...\")          # Collect video information         video_info = {             'title': yt.title,             'duration': yt.length,             'author': yt.author,             'views': yt.views,             'description': yt.description,             'audio_path': audio_path         }          return video_info      except Exception as e:         print(f\"An error occurred: {str(e)}\")         return None  def process_video_links(file_path):     with open(file_path, 'r') as file:         video_links = file.readlines()      video_data_list = []          # video_links = [\"https://www.youtube.com/watch?v=CgruI1RjH_c\"]      for video_link in video_links:         video_link = video_link.strip()         video_info = download_video_info(video_link)                  if video_info:             video_data_list.append(video_info)      # Save video data to a JSON file     output_json_path = 'video_data.json'     with open(output_json_path, 'w') as json_file:         json.dump(video_data_list, json_file, indent=2)      print(f'Video data saved to {output_json_path}')  # Replace 'YOUR_TEXT_FILE_PATH' with the path to your text file containing video links text_file_path = \"./downloads/Fireship_clone/@Fireship-shorts.txt\"  # Process video links and save data to JSON process_video_links(text_file_path) In\u00a0[2]: Copied! <pre>import json\nfrom deepgram import DeepgramClient, PrerecordedOptions\n\ndef transcribe_audio(audio_file_path):\n\n    # Your Deepgram API Key\n    DEEPGRAM_API_KEY = ''\n\n    # Initialize the Deepgram SDK\n    deepgram = DeepgramClient(DEEPGRAM_API_KEY)\n\n    # Call the transcribe_file method on the prerecorded class\n    with open(audio_file_path, \"rb\") as file:\n        buffer_data = file.read()\n\n    payload = {\n        \"buffer\": buffer_data,\n    }\n\n    options = PrerecordedOptions(\n        model=\"nova-2\",\n        language=\"en\",\n        smart_format=True,\n        punctuate=True,\n        paragraphs=True,\n        diarize=True,\n        summarize=\"v2\",\n        detect_topics=True,\n        filler_words=True,\n    )\n\n    file_response = deepgram.listen.prerecorded.v(\"1\").transcribe_file(payload, options)\n    file_response = file_response.to_json()\n\n    json_final = json.loads(file_response)\n\n    with open(f\"test.json\", \"w\") as file:\n        json.dump(json_final, file, indent=4)\n        \n    return json_final\n    \n\n# # Example usage:\n# audio_file_path = \"./downloads/Fireship_clone/100+_Computer_Science_Concepts_Explained.wav\"\n# transcribe_audio(audio_file_path)\n# print(\"Transcribing completed successfully\")\n</pre> import json from deepgram import DeepgramClient, PrerecordedOptions  def transcribe_audio(audio_file_path):      # Your Deepgram API Key     DEEPGRAM_API_KEY = ''      # Initialize the Deepgram SDK     deepgram = DeepgramClient(DEEPGRAM_API_KEY)      # Call the transcribe_file method on the prerecorded class     with open(audio_file_path, \"rb\") as file:         buffer_data = file.read()      payload = {         \"buffer\": buffer_data,     }      options = PrerecordedOptions(         model=\"nova-2\",         language=\"en\",         smart_format=True,         punctuate=True,         paragraphs=True,         diarize=True,         summarize=\"v2\",         detect_topics=True,         filler_words=True,     )      file_response = deepgram.listen.prerecorded.v(\"1\").transcribe_file(payload, options)     file_response = file_response.to_json()      json_final = json.loads(file_response)      with open(f\"test.json\", \"w\") as file:         json.dump(json_final, file, indent=4)              return json_final       # # Example usage: # audio_file_path = \"./downloads/Fireship_clone/100+_Computer_Science_Concepts_Explained.wav\" # transcribe_audio(audio_file_path) # print(\"Transcribing completed successfully\")  In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nfrom pytube import YouTube\nfrom tqdm import tqdm\nfrom deepgram import DeepgramClient, PrerecordedOptions\n\ndef download_and_transcribe_video(video_url, output_directory='downloads'):\n    try:\n        # Create a YouTube object\n        yt = YouTube(video_url)\n\n        # Create a directory for downloads if it doesn't exist\n        if not os.path.exists(output_directory):\n            os.makedirs(output_directory)\n\n        # Get the highest resolution audio stream\n        audio_stream = yt.streams.filter(only_audio=True).first()\n\n        # Download the audio stream with tqdm progress bar\n        file_name = remove_special_characters(yt.title)\n        file_name = file_name.replace(\" \",\"_\")\n        file_name = file_name.replace(\"\u2026\",\"_\")\n        file_name = file_name.replace(\",\",\"_\")\n        audio_path = os.path.join(output_directory, f'{file_name}.wav')\n        print(f\"Downloading audio to {audio_path}...\")\n        # with tqdm(total=audio_stream.filesize, unit='B', unit_scale=True, desc=f'Downloading {file_name}') as bar:\n        #     def on_progress(chunk, _):\n        #         bar.update(len(chunk))\n\n        audio_stream.download(output_directory, filename=f'{file_name}.wav')\n\n        transcript = transcribe_audio(audio_path)\n\n        # Collect video information\n        video_info = {\n            'link': video_url,\n            'title': yt.title,\n            'duration': yt.length,\n            'author': yt.author,\n            'views': yt.views,\n            'description': yt.description,\n            'audio_path': audio_path,\n            'transcript': transcript\n        }\n        \n        save_transcript_to_json(video_info, f'{output_directory}/{file_name}_transcript.json')\n        append_transcript_to_json(video_info, f'final_json_transcript_final.json')\n\n        return video_info\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\ndef save_transcript_to_json(transcript, json_path):\n    with open(json_path, 'w') as file:\n        json.dump(transcript, file, indent=4)\n    print(f'Transcript saved to {json_path}')\n\ndef append_transcript_to_json(transcript, json_path):\n    # Create an empty list if the file doesn't exist yet\n    if not os.path.exists(json_path):\n        with open(json_path, 'w') as file:\n            json.dump([], file)\n\n    # Load existing data from the file\n    with open(json_path, 'r') as file:\n        data = json.load(file)\n\n    # Append the new transcript to the list\n    data.append(transcript)\n\n    # Save the updated list to the file\n    with open(json_path, 'w') as file:\n        json.dump(data, file, indent=4)\n\n    print(f'Transcript appended to {json_path}')\n\ndef process_video_links(file_path):\n    with open(file_path, 'r') as file:\n        video_list = file.readlines()\n        \n    video_data_list = []\n    videos_to_process = [video_line.strip().split(\",\") for video_line in video_list if video_line.strip().endswith(',0')]\n\n    # for video_link in tqdm(video_list, desc='Processing videos', unit='video'):\n    # for idx, video_line in enumerate(tqdm(video_list, desc='Processing videos', unit='video')):\n    for video_link, progress in tqdm(videos_to_process, desc='Processing videos', unit='video'):\n        # video_link, progress = video_line.split(\",\")\n        if int(progress) == 0:\n            video_link = video_link.strip()\n            print(f'\\nDownloading and transcribing: {video_link}')\n            try:\n                video_info = download_and_transcribe_video(video_link)\n                video_data_list.append(video_info)\n                idx = video_list.index(f'{video_link},0\\n')\n                video_list[idx] = f'{video_link},1\\n'\n            except:\n                # save failed video links in a text file\n                print(f'\\nError processing video',video_link)\n                with open(\"logs_file.txt\", 'a') as log_file:\n                    log_file.write(video_link)\n        else:\n            print(\"Video already downloaded and processed\")\n            \n        with open(file_path, \"w\") as file:\n            file.writelines(video_list)\n\n# Replace 'YOUR_TEXT_FILE_PATH' with the path to your text file containing video links\ntext_file_path = \"./downloads/Fireship_clone_2/@Fireship-videos-remaining.txt\"\n\n# Process video links and save data to JSON\nprocess_video_links(text_file_path)\n</pre> import os import json from pytube import YouTube from tqdm import tqdm from deepgram import DeepgramClient, PrerecordedOptions  def download_and_transcribe_video(video_url, output_directory='downloads'):     try:         # Create a YouTube object         yt = YouTube(video_url)          # Create a directory for downloads if it doesn't exist         if not os.path.exists(output_directory):             os.makedirs(output_directory)          # Get the highest resolution audio stream         audio_stream = yt.streams.filter(only_audio=True).first()          # Download the audio stream with tqdm progress bar         file_name = remove_special_characters(yt.title)         file_name = file_name.replace(\" \",\"_\")         file_name = file_name.replace(\"\u2026\",\"_\")         file_name = file_name.replace(\",\",\"_\")         audio_path = os.path.join(output_directory, f'{file_name}.wav')         print(f\"Downloading audio to {audio_path}...\")         # with tqdm(total=audio_stream.filesize, unit='B', unit_scale=True, desc=f'Downloading {file_name}') as bar:         #     def on_progress(chunk, _):         #         bar.update(len(chunk))          audio_stream.download(output_directory, filename=f'{file_name}.wav')          transcript = transcribe_audio(audio_path)          # Collect video information         video_info = {             'link': video_url,             'title': yt.title,             'duration': yt.length,             'author': yt.author,             'views': yt.views,             'description': yt.description,             'audio_path': audio_path,             'transcript': transcript         }                  save_transcript_to_json(video_info, f'{output_directory}/{file_name}_transcript.json')         append_transcript_to_json(video_info, f'final_json_transcript_final.json')          return video_info      except Exception as e:         print(f\"An error occurred: {str(e)}\")         return None  def save_transcript_to_json(transcript, json_path):     with open(json_path, 'w') as file:         json.dump(transcript, file, indent=4)     print(f'Transcript saved to {json_path}')  def append_transcript_to_json(transcript, json_path):     # Create an empty list if the file doesn't exist yet     if not os.path.exists(json_path):         with open(json_path, 'w') as file:             json.dump([], file)      # Load existing data from the file     with open(json_path, 'r') as file:         data = json.load(file)      # Append the new transcript to the list     data.append(transcript)      # Save the updated list to the file     with open(json_path, 'w') as file:         json.dump(data, file, indent=4)      print(f'Transcript appended to {json_path}')  def process_video_links(file_path):     with open(file_path, 'r') as file:         video_list = file.readlines()              video_data_list = []     videos_to_process = [video_line.strip().split(\",\") for video_line in video_list if video_line.strip().endswith(',0')]      # for video_link in tqdm(video_list, desc='Processing videos', unit='video'):     # for idx, video_line in enumerate(tqdm(video_list, desc='Processing videos', unit='video')):     for video_link, progress in tqdm(videos_to_process, desc='Processing videos', unit='video'):         # video_link, progress = video_line.split(\",\")         if int(progress) == 0:             video_link = video_link.strip()             print(f'\\nDownloading and transcribing: {video_link}')             try:                 video_info = download_and_transcribe_video(video_link)                 video_data_list.append(video_info)                 idx = video_list.index(f'{video_link},0\\n')                 video_list[idx] = f'{video_link},1\\n'             except:                 # save failed video links in a text file                 print(f'\\nError processing video',video_link)                 with open(\"logs_file.txt\", 'a') as log_file:                     log_file.write(video_link)         else:             print(\"Video already downloaded and processed\")                      with open(file_path, \"w\") as file:             file.writelines(video_list)  # Replace 'YOUR_TEXT_FILE_PATH' with the path to your text file containing video links text_file_path = \"./downloads/Fireship_clone_2/@Fireship-videos-remaining.txt\"  # Process video links and save data to JSON process_video_links(text_file_path)  In\u00a0[4]: Copied! <pre># combine all the json files into a single file\n\nimport os\nimport json\n\ndef combine_json_files(directory_name, output_file='combined.json'):\n    combined_data = []\n\n    # Check if the directory exists\n    if not os.path.exists(directory_name) or not os.path.isdir(directory_name):\n        print(f\"Error: {directory_name} is not a valid directory.\")\n        return\n\n    # Loop through all files in the directory\n    for filename in os.listdir(directory_name):\n        file_path = os.path.join(directory_name, filename)\n\n        # Check if the file is a JSON file\n        if os.path.isfile(file_path) and filename.endswith('.json'):\n            with open(file_path, 'r') as file:\n                try:\n                    # Load JSON data from the file\n                    json_data = json.load(file)\n\n                    # Append the loaded data to the combined_data list\n                    combined_data.append(json_data)\n\n                except json.JSONDecodeError as e:\n                    print(f\"Error decoding JSON in file {filename}: {e}\")\n\n    # Write the combined_data to a new JSON file\n    with open(output_file, 'w') as output_file:\n        json.dump(combined_data, output_file, indent=2)\n\n    print(f\"Combined JSON data saved to {output_file.name}\")\n\n# Example usage:\ndirectory_name = './downloads/'\ncombine_json_files(directory_name)\n</pre> # combine all the json files into a single file  import os import json  def combine_json_files(directory_name, output_file='combined.json'):     combined_data = []      # Check if the directory exists     if not os.path.exists(directory_name) or not os.path.isdir(directory_name):         print(f\"Error: {directory_name} is not a valid directory.\")         return      # Loop through all files in the directory     for filename in os.listdir(directory_name):         file_path = os.path.join(directory_name, filename)          # Check if the file is a JSON file         if os.path.isfile(file_path) and filename.endswith('.json'):             with open(file_path, 'r') as file:                 try:                     # Load JSON data from the file                     json_data = json.load(file)                      # Append the loaded data to the combined_data list                     combined_data.append(json_data)                  except json.JSONDecodeError as e:                     print(f\"Error decoding JSON in file {filename}: {e}\")      # Write the combined_data to a new JSON file     with open(output_file, 'w') as output_file:         json.dump(combined_data, output_file, indent=2)      print(f\"Combined JSON data saved to {output_file.name}\")  # Example usage: directory_name = './downloads/' combine_json_files(directory_name) <pre>Combined JSON data saved to combined.json\n</pre> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"./combined.json\")\n# dataset2 = load_dataset(\"json\", data_files=\"./final_json_transcript_final.json\")\n</pre> from datasets import load_dataset  dataset = load_dataset(\"json\", data_files=\"./combined.json\") # dataset2 = load_dataset(\"json\", data_files=\"./final_json_transcript_final.json\") In\u00a0[6]: Copied! <pre>dataset\n</pre> dataset Out[6]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['author', 'duration', 'description', 'transcript', 'audio_path', 'link', 'title', 'views'],\n        num_rows: 522\n    })\n})</pre> In\u00a0[87]: Copied! <pre># Code to see which videos links have failed to download and transcribe\n\n# Assuming you have the text file named 'input.txt' and the list of video links\n# named 'video_link_list'\n\n\ninput_file_path = './downloads/Fireship_clone_2/@Fireship-videos.txt'\noutput_file_path = 'output.txt'\n\n# Read the existing links from the text file\nwith open(input_file_path, 'r') as file:\n    existing_links = [line.split(',')[0] for line in file]\n\n# Filter out the links that are not in video_link_list\nnew_links = [link for link in existing_links if link not in dataset[\"train\"][\"link\"]]\n\n# Write the new links to the output file\nwith open(output_file_path, 'w') as output_file:\n    for link in new_links:\n        output_file.write(f\"{link},0\\n\")\n\nprint(f\"New links written to {output_file_path}\")\n</pre> # Code to see which videos links have failed to download and transcribe  # Assuming you have the text file named 'input.txt' and the list of video links # named 'video_link_list'   input_file_path = './downloads/Fireship_clone_2/@Fireship-videos.txt' output_file_path = 'output.txt'  # Read the existing links from the text file with open(input_file_path, 'r') as file:     existing_links = [line.split(',')[0] for line in file]  # Filter out the links that are not in video_link_list new_links = [link for link in existing_links if link not in dataset[\"train\"][\"link\"]]  # Write the new links to the output file with open(output_file_path, 'w') as output_file:     for link in new_links:         output_file.write(f\"{link},0\\n\")  print(f\"New links written to {output_file_path}\") <pre>New links written to output.txt\n</pre> In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[9]: Copied! <pre>dataset\n</pre> dataset Out[9]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['author', 'duration', 'description', 'transcript', 'audio_path', 'link', 'title', 'views'],\n        num_rows: 522\n    })\n})</pre> In\u00a0[10]: Copied! <pre>import pandas as pd\ndf = pd.DataFrame(dataset['train'])\n</pre> import pandas as pd df = pd.DataFrame(dataset['train']) In\u00a0[11]: Copied! <pre>df.head()\n</pre> df.head() Out[11]: author duration description transcript audio_path link title views 0 Fireship 787 Learn the fundamentals of Computer Science wit... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_Computer_Science_Concepts_Expla... https://www.youtube.com/watch?v=-uleG_Vecis 100+ Computer Science Concepts Explained 2110216 1 Fireship 743 The ultimate 10 minute JavaScript course that ... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_JavaScript_Concepts_you_Need_to... https://www.youtube.com/watch?v=lkIFF4maKMU 100+ JavaScript Concepts you Need to Know 1642938 2 Fireship 798 WebDev 101 is a complete introduction into the... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_Web_Development_Things_you_Shou... https://www.youtube.com/watch?v=erEgovG9WBs 100+ Web Development Things you Should Know 1296840 3 Fireship 1471 Top 100 Firebase Pro Tips \ud83d\udd25\ud83d\udcaf. Optimize your ap... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100_Firebase_Tips,_Tricks,_and_Screw... https://www.youtube.com/watch?v=iWEgpdVSZyg 100 Firebase Tips, Tricks, and Screw-ups 177364 4 Fireship 246 Google made a ton of exciting announcements at... {'metadata': {'channels': 1, 'created': '2024-... downloads\\10_crazy_announcements_from_Google_I... https://www.youtube.com/watch?v=nmfRDRNjCnM 10 crazy announcements from Google I/O 968111 In\u00a0[12]: Copied! <pre>df.rename(columns={'transcript': 'transcript_json'}, inplace=True)\ndf.head()\n</pre> df.rename(columns={'transcript': 'transcript_json'}, inplace=True) df.head() Out[12]: author duration description transcript_json audio_path link title views 0 Fireship 787 Learn the fundamentals of Computer Science wit... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_Computer_Science_Concepts_Expla... https://www.youtube.com/watch?v=-uleG_Vecis 100+ Computer Science Concepts Explained 2110216 1 Fireship 743 The ultimate 10 minute JavaScript course that ... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_JavaScript_Concepts_you_Need_to... https://www.youtube.com/watch?v=lkIFF4maKMU 100+ JavaScript Concepts you Need to Know 1642938 2 Fireship 798 WebDev 101 is a complete introduction into the... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_Web_Development_Things_you_Shou... https://www.youtube.com/watch?v=erEgovG9WBs 100+ Web Development Things you Should Know 1296840 3 Fireship 1471 Top 100 Firebase Pro Tips \ud83d\udd25\ud83d\udcaf. Optimize your ap... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100_Firebase_Tips,_Tricks,_and_Screw... https://www.youtube.com/watch?v=iWEgpdVSZyg 100 Firebase Tips, Tricks, and Screw-ups 177364 4 Fireship 246 Google made a ton of exciting announcements at... {'metadata': {'channels': 1, 'created': '2024-... downloads\\10_crazy_announcements_from_Google_I... https://www.youtube.com/watch?v=nmfRDRNjCnM 10 crazy announcements from Google I/O 968111 In\u00a0[13]: Copied! <pre>json_string = df.loc[0, 'transcript_json']\n\n# Display the loaded JSON object\nprint(json_string[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"])\n</pre> json_string = df.loc[0, 'transcript_json']  # Display the loaded JSON object print(json_string[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"]) <pre>What's the first thing you should do when your code throws an error? Obviously, you should change nothing and try to run it again a few times. If that doesn't work, you're gonna need a computer science degree. The awesome thing about software engineering is that you can learn to code and get a high paying job, while literally having no idea how anything actually works. It all just feels like magic. Like a pilot driving a giant metal tube in the sky while knowing nothing about aerodynamics. Mother of God, no. Holy shit. Shit. Welcome to computer science 101. In today's video, you'll learn the science behind the garbage code you've been writing by learning 101 different computer science terms and concepts. This is a computer. It's just a piece of tape that holds ones and zeros along with a device that can read and write to it. It's called a Turing machine and in theory, it can compute anything, like the graphics in this video or the algorithm that recommended that you watch it. At the core of modern computers, we have the central processing unit. If we crack rack it open, we find a piece of silicon that contains billions of tiny transistors, which are like microscopic on off switches. The value at one of these switches is called a bit end is the smallest piece of information a computer can use. However, 1 bit by itself is not very useful, so they come in a package of 8 called a byte. 1 byte can represent 250 6 different values, like all the characters that you type on your keyboard. In fact, when you type into your keyboard, the character produced is actually mapped to a binary value in a character encoding like ASCII or utf8 binary is just a system for counting, like the base ten system you normally use when counting seeing on your fingers, but it only has 2 characters, 1 and 0. Humans have a hard time reading binary, so most often it's represented in a hexadecimal base 16 format, where ten numbers and 6 letters can represent a 4 bit group called a nibble. As a developer, when you write code in a programming language, it will actually be converted into machine code, which is a binary format that can be decoded and executed by the CPU. What it doesn't do though is store data for your applications. For that, computers have random access memory or RAM. It's like a neighborhood, and inside of every house lives a byte. Every location has a memory address, which the CPU can read and write too. You can think of the CPU and RAM as the brain of the computer. But in order for a computer to be useful, it needs to handle input and output. An input device might be the keyboard and mouse, while an output device might be your monitor. Luckily, most developers don't need to worry about how this hardware fits together because we have operating system kernels, like Linux, Mac, and Windows that control all hardware resources via device drivers. Now, to start hacking on the operating system, your first entry point is the shell, which is a program that is the operating system to the end user. It's called a shell because it wraps the kernel. It takes a line of text as input and produces an output. At this is called a command line interface. Not only can it connect to your own computer, but with the secure shell protocol, it can also connect to remote computers a network. Now that you have access to the mainframe, it's time to pick a programming language, which is a tool that uses the abstraction principle to make computers practical to work with for humans by simplifying different systems layer by layer. Some languages like Python are interpreted. That means there's a program called an interpreter that will execute each line of code 1 by 1. Other languages like c plus plus are compiled. They use a compiler to convert the entire program into machine code in advance before the CPU attempts to execute it. This results in an executable file that can be run by the operating system without any extra dependencies. Now every every programming language has a variety of built in data types to represent the data we're working with in our code. Instead of bytes, we work with more human friendly things select characters and numbers. Now, the most fundamental way to use data in your application is to declare a variable. This attaches a name to a data point, allowing you to reuse it somewhere else in your code. Python is a dynamically typed language, which means we don't need to tell the program exactly which data type is assigned to a variable. It just figures it out automatically. However, other languages like C are statically typed, and that means you need to specify the data type of a variable in your code. When you define a variable, its value is stored somewhere in memory on the hardware, and you may need to allocate and free up memory already throughout the program. A pointer is a variable whose value is the memory address of another variable, which can be used for low level memory control. Many languages don't want to deal with low level memory management and instead implement a garbage collector, which automatically allocates and deallocates memory when an object is no longer referenced in the program. Carpet day. No. Now, the data types available are different in every programming language, but typically you'll find int to represent whole numbers, switch may or may not be signed or unsigned to represent negative numbers as well. When numbers require a decimal point, they typically use the floating point type. It's called a float because there's only enough memory to represent a certain range of numbers at a certain precision, and is basically a form of scientific notation to make computers faster. If you need more range or precision, many languages also have a double that doubles the amount of memory used for the number. Now when it comes to characters, you'll typically find the char data type to represent a single character or more commonly a string to represent multiple characters together. Ultimately, these characters triggers get stored in a memory address somewhere, but they need to be stored in a certain order. When the order starts with the most significant byte and the smallest memory address, it's called big endian or vice versa, if the least significant byte is stored in the smallest address, it's called little endian. When it comes to practical software for engineering. One of the most fundamental things we do is organize data into data structures. The most useful data structure is probably the array or list. Just like a shopping list. It organizes multiple data points and order. However, it also maintains an index of integers that starts at 0 and goes up for every new item in the list. That can be useful, but you don't actually need an index to create a list of items. Another option is a link list where each item has a pointer to the next item in front of Another option is a stack that follows the last in first out principle. It's like stacking a set of plates, then when you want to access data, you pop the last one off the top. The inverse option is a q, which is first in first out. Just like when you get into the breadline, the first person there is the first one to be fed. Now, another extremely useful data structure is the hash, which might also be called a map or dictionary. It's like an array, but instead of an index of integers, you define the keys that point to each individual item, giving you a collection of key value pairs. In many cases though, it's not efficient to organize data in a linear way. To address that problem, we have trees, which organize nodes together in a hierarchy that can often be traversed more quickly. This can sometimes be too rigid of data structure though. So instead, a graph can be created to connect multiple nodes together in a virtually unlimited number of ways. A graph has a node for the data and an edge for the relationship between the data points. Data structures are essential, but they don't do anything by themselves. To do something useful, you'll need to code up an algorithm, which is just code that solves a problem. I took the initiative in creating the Internet. In our code, we have several mechanisms for implementing algorithms. The most fundamental of which is a function, which is a block of code that takes an input then does something and returns an output. Like a variable, a function has a name and it can called from other parts of your code with different input parameters called arguments. One thing you might do in the function body is compare one value to another. Every language has a variety of built in operators like equality, greater than, and less than that you can use to compare 2 values. If a is greater than b, then it forms a value of true, but if b is greater than a, then the value is false. True false is what's known as a boolean data type and whenever your code produces value like this, it's known as an expression, but not all code will produce a value. Sometimes your code will simply do something which is known as a statement. A good example is the if statement which handles conditional logic. For example, if the condition is true, it will execute this code, otherwise it will short circuit and run the code inside of the else block. Another very common type of statement is a loop. A while loop will run this block of code over and over again until the condition in the parenthesis becomes false. That can be useful, but more often than not, you'll want to loop over an iterable data type like an array. Most languages have a for loop that can run some code for every object in the array or iterable data structure. Now in some cases, a function may not have an output, which is generally called a void function. An interesting thing about functions is that they can call themselves. When a function calls itself, it's called recursion because when done like this by default, it will recurse forever creating an infinite loop. That happens because when you call a function, the programming language will put it into memory on what's known as the call stack, which is a short term chunk of memory for executing your code. When a function keeps calling itself, the language will keep pushing frames onto the call stack until you get a stack overflow error. To avoid this, your algorithm needs a base condition so it knows when to terminate the loop. Now, when you write an algorithm, you'll need to determine if it's any good, and the system for doing that is called big o notation. It's a standard format for approximating the performance have an algorithm at scale. It may reference time complexity, which is how fast your algorithm will run, and space complexity, which deals with how much memory is required to run it. Developers have many different algorithm types at their disposal. The most crude option is brute force, where you might loop over every possible combination to hack somebody's credit card pin. A more sophisticated approach might be divide and conquer, like binary search where you cut the problem in half multiple times until you find what you're looking for. Another option is dynamic programming algorithms, where a problem is broken down into multiple smaller sub problems and the result of each computation is stored for later use using a technique called memoization. That means if a function has already been called, it will use the existing value instead of recomputing it again from scratch. Then we have greedy algorithms that will make the choice that is most beneficial in the short term without considering the problem as a whole. One example of this is Dijkstra's shortest path algorithm. On the flip side, we have backtracking algorithms, which take a more incremental approach by looking at all the possible options, like a rat in a maze exploring all the different potential paths. Now, when it comes to implementing your code, there are always multiple ways to get the job done. One aiming paradigm is declarative, where your code describes what the program does and the outcome, but doesn't care about things like control flow. This style of programming is often associated with functional languages like Haskell. The other paradigm is imperative programming, where your code uses statements like if and while, providing explicit instructions about how to produce an outcome. It's associated with procedural languages like C. Today, most general purpose languages like Python, JavaScript, Hotline, Swift, and so on are multi paradigm, which means they support all these options at the same time, in addition to object oriented programming. The idea behind OOP is that you use classes to write a blueprint for the data or objects in your code. A class can encapsulate variables, which are commonly called properties, as well as functions, which are usually called methods in this context. It's a common way to organize and reuse code because classes can share behaviors between each other through inheritance, where a subclass can extend and override the behaviors of the parent class. And it opens the door to all kinds have other ideas called design patterns. Now, a class by itself doesn't actually do anything. Instead, it's used to instantiate objects, which are actual chunks of data that live in your computer's memory. Often, you'll want to reference the same object over and over again in your code. When data is long lived, it can't go in the call stack. Instead, most languages have a separate area of memory hold the heap, which unlike the call stack can grow and shrink based on how your application is used. It also allows you to pass objects by reference, which means you can use the same object in multiple variables without increasing the memory footprint because it always points to the same chunk of memory in the heap. Now, what's interesting is that if we go back to the CPU that we talked about in the beginning, you'll notice that it contains multiple threads. A thread takes the physical CPU core in breaks into virtual cores that allow it to run code simultaneously. There are some programming languages that support parallelism where you can write code that literally executes on 2 different threads at the same time. However, many languages out there are only single threaded, but that doesn't mean they can't do 2 things at the same time. Instead, they implement concurrency models like an event loop or coroutines that can pause or delay the normal execution of code to handle multiple job's on a single thread at the same time. Now, in modern computing, we're rarely working with the bare metal CPU and RAM. Instead, we work in the cloud with a virtual machine, which is just a piece set software that simulates hardware that allows us to take really big computers and split them up into a bunch of smaller virtual computers. These machines are the backbone of the Internet and are connected via the Internet protocol. Each machine has a unique IP address to identify it on the network. Work. That IP address is usually alias to a URL that is registered in a global database called the domain name service. Now to establish a connection, in. The 2 computers will perform a TCP handshake, which will allow them to exchange messages called packets. On top of that, there's usually security layer like SSL to encrypt and decrypt the messages over the network. Now the 2 computers can securely share data with the hypertext transfer for protocol. The client may request a web page, then the server will respond with some HTML. Modern servers provide a standardized way for a client to request data, which is called an application programming interface or API. The most common architecture is REST, where URLs are mapped to different data entities available on the server. And that brings us to our final topic, mother effing printers. You're gonna need to learn how these things work inside fighting out, because every time you go to grandma's house, she's going to ask you to fix it, which shouldn't be a problem for a computer scientist like you. Thanks for watching, and I will see you in the next one.\n</pre> In\u00a0[14]: Copied! <pre>import pandas as pd\nimport json\n\n# Assuming your DataFrame is named df\n\ndef parse_json(row):\n    try:\n        transcript_json = row['transcript_json']\n        if transcript_json[\"results\"][\"summary\"][\"result\"] == \"success\":\n            transcript = str(transcript_json[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"])\n            summary = str(transcript_json[\"results\"][\"summary\"][\"short\"])\n            return transcript, summary\n        else:\n            print(\"an error occurred\")\n            return None, None\n    except (json.JSONDecodeError, KeyError):\n        print(\"an exception occurred\")\n        return None, None\n\n# Apply the custom function to each row\ndf[['transcript', 'summary']] = df.apply(parse_json, axis=1, result_type='expand')\n\n# Display the updated DataFrame\n# print(df.head())\n</pre> import pandas as pd import json  # Assuming your DataFrame is named df  def parse_json(row):     try:         transcript_json = row['transcript_json']         if transcript_json[\"results\"][\"summary\"][\"result\"] == \"success\":             transcript = str(transcript_json[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"])             summary = str(transcript_json[\"results\"][\"summary\"][\"short\"])             return transcript, summary         else:             print(\"an error occurred\")             return None, None     except (json.JSONDecodeError, KeyError):         print(\"an exception occurred\")         return None, None  # Apply the custom function to each row df[['transcript', 'summary']] = df.apply(parse_json, axis=1, result_type='expand')  # Display the updated DataFrame # print(df.head()) In\u00a0[15]: Copied! <pre>from datasets import Dataset\nimport pandas as pd\nfinal_dataset = Dataset.from_pandas(df)\n</pre> from datasets import Dataset import pandas as pd final_dataset = Dataset.from_pandas(df) In\u00a0[16]: Copied! <pre>final_dataset\n</pre> final_dataset Out[16]: <pre>Dataset({\n    features: ['author', 'duration', 'description', 'transcript_json', 'audio_path', 'link', 'title', 'views', 'transcript', 'summary'],\n    num_rows: 522\n})</pre> In\u00a0[\u00a0]: Copied! <pre>final_dataset.push_to_hub(\"Huggingface-userId/FS_transcribe_summary\")\n</pre> final_dataset.push_to_hub(\"Huggingface-userId/FS_transcribe_summary\") In\u00a0[20]: Copied! <pre>import pandas as pd\nimport json\n\n# Assuming your DataFrame is named df\n\ndef create_prompt(row):\n    try:\n        author = row[\"author\"]\n        title = row[\"title\"]\n        video_transcript = row[\"transcript\"]\n        video_summary = row[\"summary\"]\n        # transcript_json = row['transcript_json']\n        text = f\"\"\"\n        [INST]\n        You are youtuber called {author} you make engaging high-intensity and entertaining coding tutorials and tech news. \n        you covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n        \n        Given the title of the video : {title} \n        and a small summary : {video_summary}\n        [/INST]\n        \n        Generate the video : {video_transcript}\n        \"\"\"        \n        return text\n\n    except (json.JSONDecodeError, KeyError):\n        print(\"an exception occurred\")\n        return None\n\n# Apply the custom function to each row\ndf['text'] = df.apply(create_prompt, axis=1, result_type='expand')\n\n# Display the updated DataFrame\ndf.head()\n</pre> import pandas as pd import json  # Assuming your DataFrame is named df  def create_prompt(row):     try:         author = row[\"author\"]         title = row[\"title\"]         video_transcript = row[\"transcript\"]         video_summary = row[\"summary\"]         # transcript_json = row['transcript_json']         text = f\"\"\"         [INST]         You are youtuber called {author} you make engaging high-intensity and entertaining coding tutorials and tech news.          you covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.                  Given the title of the video : {title}          and a small summary : {video_summary}         [/INST]                  Generate the video : {video_transcript}         \"\"\"                 return text      except (json.JSONDecodeError, KeyError):         print(\"an exception occurred\")         return None  # Apply the custom function to each row df['text'] = df.apply(create_prompt, axis=1, result_type='expand')  # Display the updated DataFrame df.head() Out[20]: author duration description transcript_json audio_path link title views transcript summary text 0 Fireship 787 Learn the fundamentals of Computer Science wit... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_Computer_Science_Concepts_Expla... https://www.youtube.com/watch?v=-uleG_Vecis 100+ Computer Science Concepts Explained 2110216 What's the first thing you should do when your... The importance of hardware and memory for a co... \\n        [INST]\\n        You are youtuber cal... 1 Fireship 743 The ultimate 10 minute JavaScript course that ... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_JavaScript_Concepts_you_Need_to... https://www.youtube.com/watch?v=lkIFF4maKMU 100+ JavaScript Concepts you Need to Know 1642938 JavaScript. It's a wonderful programming langu... The speaker explains that JavaScript is a prog... \\n        [INST]\\n        You are youtuber cal... 2 Fireship 798 WebDev 101 is a complete introduction into the... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100+_Web_Development_Things_you_Shou... https://www.youtube.com/watch?v=erEgovG9WBs 100+ Web Development Things you Should Know 1296840 Web development is the best job in the world. ... The internet is a collection of machines conne... \\n        [INST]\\n        You are youtuber cal... 3 Fireship 1471 Top 100 Firebase Pro Tips \ud83d\udd25\ud83d\udcaf. Optimize your ap... {'metadata': {'channels': 1, 'created': '2024-... downloads\\100_Firebase_Tips,_Tricks,_and_Screw... https://www.youtube.com/watch?v=iWEgpdVSZyg 100 Firebase Tips, Tricks, and Screw-ups 177364 Welcome to my top 10 Firebase tips. Welcome to... The speakers discuss how to build successful r... \\n        [INST]\\n        You are youtuber cal... 4 Fireship 246 Google made a ton of exciting announcements at... {'metadata': {'channels': 1, 'created': '2024-... downloads\\10_crazy_announcements_from_Google_I... https://www.youtube.com/watch?v=nmfRDRNjCnM 10 crazy announcements from Google I/O 968111 It is May 11, 2023, and you're watching the Co... In this video, the speakers discuss Google's u... \\n        [INST]\\n        You are youtuber cal... In\u00a0[21]: Copied! <pre>from datasets import Dataset\nimport pandas as pd\nfinal_dataset = Dataset.from_pandas(df)\n</pre> from datasets import Dataset import pandas as pd final_dataset = Dataset.from_pandas(df) In\u00a0[\u00a0]: Copied! <pre>final_dataset.push_to_hub(\"Huggingface-userId/FS_transcribe_summary_prompt\")\n</pre> final_dataset.push_to_hub(\"Huggingface-userId/FS_transcribe_summary_prompt\") In\u00a0[31]: Copied! <pre>with open(\"dummy.json\",\"r\") as f:\n    transcribe_json_list = json.load(f)\n</pre> with open(\"dummy.json\",\"r\") as f:     transcribe_json_list = json.load(f) In\u00a0[36]: Copied! <pre>transcribe_json_list[0][\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"]\n</pre> transcribe_json_list[0][\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"] Out[36]: <pre>\"Have you ever woken up in the middle of the night in a panic wondering how to extract a polygonal mesh of an isosurface from a 3 dimensional discrete scalar field? Yeah. I didn't think so. But back in 87, 2 programmers at General Electric did. They created and patented the marching cubes algorithm, an algorithm that has likely saved countless lives by allowing doctors to visualize data from CT and MRI scans. Whenever you instruct a machine to solve a problem with code, you're creating an algorithm, a procedure for rearranging ones and zeros that can make animals talk and vacuums walk. Most algorithms belong in a dumpster, but some are fast, skin. Some are beautiful and some are so weird, they're indistinguishable from magic. Today, we'll look at 10 of the most interesting algorithms ever engineered sphere, and how they're used to solve very interesting problems in the real world. 1st on the list, we have wave function collapse. One of the weirdest things in all of science is the double slit experiment, where particles behave like a wave when you're not looking, but when you look, they suddenly collapse down to a particle. It seems counterintuitive, but it makes total sense when you realize we're living in a simulation and the universe wrote algorithm to cut down on its AWS build. It's an interesting concept to think about philosophically, but the general idea behind wave function collapse can also be implemented programmatically. Imagine we have a map for a video game, but what if this is a side scrolling game that can go on for eternity? We can't just make a bigger map, we need an algorithm to procedurally generate it on the fly. What's so weird is that we can take this initial map and think of it as being in the initial superposition of all possibilities. It's the wave function. Then upon observation, it collapses into a particle. Or in other words, it selects a random map tile but follows a consistent set of rules, like in this case, making sure that the roads are always connected, providing a random yet cohesive result and doesn't rely on any sort of modern generative AI. Speaking of which, AI is weird as hell. Diffusion is a machine learning algorithm originally developed at OpenAI and is the magic behind image generators like DALL E and Stable Diffusion. But the concept of diffusion actually comes from thermodynamics, where particles spread from areas of higher concentration to lower concentration. In artificial intelligence, the process is reversed. The algorithm starts by generating random noise, which would be like high entropy and thermodynamics, and gradually refines it to a structured image, which would be lower entropy. But first, you'll need to train model that can do this well. The diffusion algorithm works in 2 phases. In the forward phase, it gradually adds noise to an image step by step until it becomes completely random. In the second phase, the algorithm reverses this process, reconstructing it back into a coherent image. When the algorithm runs over millions of labeled images, we get a collection of weights that can be used to generate new images out of thin air, allowing us to build an infinite army OnlyFans models. It's highly compute intensive, but also works well on audio. And the next frontier is diffusion for video generation. But now Now let's talk about simulated annealing. One frustrating thing about programming is that for many problems, there's not just one solution, but many solutions. Like an Amazon warehouse has many different ways to organize its inventory, but some ways are more efficient than others. Annealing is a word that comes from metallurgy, where metals are skid heated up and cooled down over and over again to remove defects. The same idea is used in optimization algorithms to find the best answer or n a c of good answers. Imagine trying to find the highest point in a mountain range full of peaks and valleys. A simple hill climb algorithm won't work because there are many local peaks. Initially, the temperature sky allowing the algorithm to explore freely. As time goes on though, the temperature is lowered, which decreases the probability of accepting a worse solution. The off here is exploration versus exploitation. But the reason I included this algorithm is because it's also a good way for beginners to learn how to code. Initially, you start out exploring all kinds of different technologies and frameworks, then eventually you find one specific area to exploit and specialize in. But we can't talk about algorithms without talking about sorting. And the most ingenious sorting algorithm of all time is without a doubt, sleep sort. The majority of sane sorting algorithms out there use strategies like divide conquer to break up an array into subarrays where it can be sorted more efficiently. However, some random genius on 4 chan found a better way, but it's a bit unconventional. Here's what the code looks like in bash. It's incredibly simple. It loops over the array, and then for each element, it opens up a new thread that sleeps for the amount of time proportional to the value of its element. Then finally, after waking up, it prints that element. It's genius because it delegates the sorting to the CPU scheduler. It's also dumb and useless because it delegates sorting to the CPU scheduler. Speaking of which, you might be familiar with another useless sorting algorithm, BOGO sort, which tries to sort an array by randomly guessing over and over again. It's like playing the lottery. But what if we apply the same algorithm with quantum mechanics to the multiverse? If we're to trust multiverse science, we know that all possible outcomes exist in separate parallel universes. That means as a developer, if you find yourself with an unsorted array, there's or some other parallel universe where it is sorted. The technology isn't quite there yet, but if we could randomly observe these other universes to find the sorted array, we could then use a portal gun to travel to that universe, which would make our lives much easier. Although, we would obviously have to kill the version of ourself in that universe, but if it's a large array, quantum Bogosort might be worth it. That's purely hypothetical, but one of the most practical and goaded algorithms of all time is SCAR SA, a public key cryptosystem. It's essential for digital security, allowing people on the Internet to lock their mailboxes and sign their letters with a unique signature. But it's based on one simple mathematical reality. Multiplying large numbers to find 2 original large prime numbers is extremely difficult and time like it take your laptop 300,000,000,000,000 years to brute force. Unless quantum computers become a thing, and we can start leveraging Shor's algorithm, which can solve the integer factorization problem exponentially faster than any classical algorithm. Prime factoring is pretty simple, but how this algorithm does it is where things get weird. It relies on concepts like cubits, skits superposition and entanglement to perform massive amounts of calculations in parallel. The algorithm is legit, but so far, the biggest number ever factored is 21. Even IBM's state of the art Q System 1 fails when trying to factor the number 35. However, just recently, skin. The Chinese factored this big ass number with a quantum computer, but it uses a different algorithm that doesn't scale very well for large numbers unlike Shor's algorithm. Everything is safe for now, but when someone figures out how to make quantum computers work, expect all hell to break loose in the cybersecurity world. At the beginning of this video, I mentioned the marching cubes algorithm, but it deserves a closer look. So first, we start with a 3 d scalar field, which might represent data from an MRI machine. Each point in the 3 d space is represented by a single number or a scalar. The algorithm starts with a single point, then takes its 8 neighboring locations to form an cube, but treats the eight values as a bit in an 8 bit integer. This results in 256 different possibilities, which point to a precalculated array of polygons. The algorithm marches through each point to create a 3 d mesh that can be used in 3 d software. And at the time, this was really cool because as MRI machines produce slices of data that can now be rendered in 3 d. In modern times, though, we're often dealing with distributed systems in the cloud, And that brings us to the Byzantine generals problem. Imagine you're a general in the Byzantine army. You're camped around a city with a few other generals with plans to attack it the next morning. But what if one of the generals skitrunk and wakes up too hungover to attack. The entire system could collapse. Computers have the same problem. Sometimes they might fail or be infiltrated by hackers, and you never know when or where that's going to happen. Luckily, algorithms like PBFT are designed to solve this. They can keep a distributed network working properly even if up to 1 third of its nodes go haywire. It works by having a node broadcast a pre prepare message to other nodes, indicating its readiness to execute some code that will change the system. The other nodes will respond back in agreement. Then after a certain threshold, a consensus is formed. Once there's a consensus, The original node sends back a commit message to all the other nodes, which can then all execute the changes, making the entire state of the system consistent. Algorithms like this are essential for blockchain technology and things like distributed cloud databases. What's really cool about algorithms, though, is that they can also reflect nature, like Boyd's artificial life program. It was created back in 86 and simulates the flocking behavior of birds. What's so cool about it is that it demonstrates the emergent complexity or beauty that we get out of just a few simple rules. In this case, the birdoids have three rules. They steer to avoid crowding, they steer towards the average heading of the flock, and they steer towards the center of mass of their local flockmates. The end result are these intricate patterns that weren't programmed directly, but just emerge naturally. But finally, that brings us to an old algorithm that blew my mind just the other day and inspired this video, Boyer Moore string search. It's weird because it becomes faster and more efficient as the string it's searching becomes bigger. That seems impossible, but it makes sense when you understand the algorithm. It scans skin's text from right to left, then has two rules. When it encounters a bad character not found in the search pattern, it jumps past it based on an estimation made in a preprocess table. Scale. Likewise, if it finds a partial match, then a mismatch occurs. It has a separate pre calculated table that maximizes the number of characters it can safely skip. These rules are called heuristics, which are like functions that are not guaranteed to be perfect, but are far more practical than looping over every single character. In this case, the algorithm gets faster with more text because it's able to skip a higher proportion of characters. And if you've ever wondered why GREP is so fast, you have this algorithm to thank.\"</pre> In\u00a0[38]: Copied! <pre>transcribe_json_list[0][\"results\"][\"summary\"][\"result\"]\n</pre> transcribe_json_list[0][\"results\"][\"summary\"][\"result\"] Out[38]: <pre>'success'</pre> In\u00a0[39]: Copied! <pre>transcribe_json_list[0][\"results\"][\"summary\"][\"short\"]\n</pre> transcribe_json_list[0][\"results\"][\"summary\"][\"short\"] Out[39]: <pre>'The speakers discuss the use of algorithms in scientific research, including random random algorithms like BOGO sort and BOGO sort to solve problems in scientific research, and the potential uses of these algorithms in optimizing algorithms and algorithms for algorithms. They also touch on the use of quantum algorithms in machine design and the future of digital security, including the use of random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random random'</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\nfinal_dataset = []\n# for video_link in tqdm(video_links, desc='Processing videos', unit='video'):\nfor transcribe_json in tqdm(transcribe_json_list,desc='Processing transcribe'):\n    transcribe = transcribe_json[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"]\n    if transcribe_json[\"results\"][\"summary\"][\"result\"]==\"success\":\n        summary = transcribe_json[\"results\"][\"summary\"][\"short\"]\n    final_json = {\n        \"transcribe\": transcribe,\n        \"summary\": summary\n    }\n    final_dataset.append(final_json)\n\nwith open(\"transcribe_data_final_processed.json\", \"w\") as output:\n    json.dump(final_dataset, output)\n    \n</pre> from tqdm import tqdm  final_dataset = [] # for video_link in tqdm(video_links, desc='Processing videos', unit='video'): for transcribe_json in tqdm(transcribe_json_list,desc='Processing transcribe'):     transcribe = transcribe_json[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"]     if transcribe_json[\"results\"][\"summary\"][\"result\"]==\"success\":         summary = transcribe_json[\"results\"][\"summary\"][\"short\"]     final_json = {         \"transcribe\": transcribe,         \"summary\": summary     }     final_dataset.append(final_json)  with open(\"transcribe_data_final_processed.json\", \"w\") as output:     json.dump(final_dataset, output)      In\u00a0[\u00a0]: Copied! <pre>transcribe_json_list[0][\"channels\"]\n</pre> transcribe_json_list[0][\"channels\"] In\u00a0[\u00a0]: Copied! <pre>final_dataset_transcribe = load_dataset(\"json\",data_files=\"./transcribe_data_final.json\")\n</pre> final_dataset_transcribe = load_dataset(\"json\",data_files=\"./transcribe_data_final.json\") In\u00a0[\u00a0]: Copied! <pre>final_dataset_transcribe.push_to_hub(\"Huggingface-userId/FS_transcribe_summary\")\n</pre> final_dataset_transcribe.push_to_hub(\"Huggingface-userId/FS_transcribe_summary\") In\u00a0[\u00a0]: Copied! <pre>final_dataset_transcribe\n</pre> final_dataset_transcribe In\u00a0[\u00a0]: Copied! <pre>final_dataset_transcribe[\"train\"][1]\n</pre> final_dataset_transcribe[\"train\"][1] In\u00a0[\u00a0]: Copied! <pre>import json\nwith open(\"./video_data_and_transcripts.json\") as F:\n    json_data = json.load(F)\n</pre> import json with open(\"./video_data_and_transcripts.json\") as F:     json_data = json.load(F) In\u00a0[\u00a0]: Copied! <pre>len(json_data)\n</pre> len(json_data)"},{"location":"Projects/YT_Clones/dataset_prep/#testing-video-download","title":"Testing video download\u00b6","text":""},{"location":"Projects/YT_Clones/dataset_prep/#testing-audio-transcription-api","title":"Testing Audio Transcription api\u00b6","text":""},{"location":"Projects/YT_Clones/dataset_prep/#download-youtube-video-transcribe-it-and-save-the-transcribe","title":"Download Youtube video transcribe it and save the transcribe\u00b6","text":""},{"location":"Projects/YT_Clones/dataset_prep/#huggingface-dataset-prepraration","title":"Huggingface Dataset prepraration\u00b6","text":""},{"location":"Projects/YT_Clones/dataset_prep/#prompt-formatting-to-the-following-format","title":"Prompt formatting to the following format\u00b6","text":"<pre><code>[INST]\nYou are youtuber called {author} you make engaging high-intensity and entertaining coding tutorials and tech news. \nyou covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.\n\nGiven the title of the video : {title} \nand a small summary : {video_summary}\n[/INST]\n\nGenerate the video : {video_transcript}\n</code></pre>"},{"location":"Projects/YT_Clones/dataset_prep/#parsing-operation-for-the-api-response-from-deepgram","title":"Parsing operation for the api response from deepgram\u00b6","text":""},{"location":"Projects/YT_Clones/youtube_channel_scraper/","title":"Youtube channel scraper","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport time\n</pre> import sys from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.by import By from bs4 import BeautifulSoup import time In\u00a0[\u00a0]: Copied! <pre>def scrape_youtube_channel(url):\n    # Determine the video type based on the URL\n    if url.split('/')[-1] == 'videos':\n        video_type = \"videos\"\n    else:\n        video_type = \"shorts\"\n\n    # Create a new instance of the Edge driver\n    driver = webdriver.Edge()\n\n    # Open the YouTube channel page\n    driver.get(url)\n    time.sleep(5)\n    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n\n    # Scroll to the bottom of the page to load all videos\n    while True:\n        # Scroll down to the bottom\n        print(\"Scrolling down...\")\n        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n\n        # Check if the page height has increased\n        driver.implicitly_wait(3)\n        time.sleep(3)  # Adjust the wait time as needed\n\n        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n        if new_height == last_height:\n            break  # End of page reached\n        last_height = new_height\n\n    # Get the page source after scrolling\n    page_source = driver.page_source\n\n    # Close the webdriver\n    driver.quit()\n\n    # Parse the HTML with BeautifulSoup\n    soup = BeautifulSoup(page_source, 'html.parser')\n\n    # Find all links with the specified attributes\n    if video_type == \"shorts\":\n        video_links = soup.find_all('a', {'id': 'thumbnail', 'class': 'yt-simple-endpoint inline-block style-scope ytd-thumbnail'})\n    elif video_type == \"videos\":\n        video_links = soup.find_all('a', {'id': 'thumbnail', 'class': 'yt-simple-endpoint style-scope ytd-playlist-thumbnail'})\n\n    # Extract and print the href attributes\n    channel = url.split('/')[-2]\n\n    with open(f'{channel}-{video_type}.txt', 'w') as f:\n        for link in video_links:\n            href = link.get('href')\n            if href:\n                f.write(f\"https://www.youtube.com{href}\\n\")\n</pre> def scrape_youtube_channel(url):     # Determine the video type based on the URL     if url.split('/')[-1] == 'videos':         video_type = \"videos\"     else:         video_type = \"shorts\"      # Create a new instance of the Edge driver     driver = webdriver.Edge()      # Open the YouTube channel page     driver.get(url)     time.sleep(5)     last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")      # Scroll to the bottom of the page to load all videos     while True:         # Scroll down to the bottom         print(\"Scrolling down...\")         driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)          # Check if the page height has increased         driver.implicitly_wait(3)         time.sleep(3)  # Adjust the wait time as needed          new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")         if new_height == last_height:             break  # End of page reached         last_height = new_height      # Get the page source after scrolling     page_source = driver.page_source      # Close the webdriver     driver.quit()      # Parse the HTML with BeautifulSoup     soup = BeautifulSoup(page_source, 'html.parser')      # Find all links with the specified attributes     if video_type == \"shorts\":         video_links = soup.find_all('a', {'id': 'thumbnail', 'class': 'yt-simple-endpoint inline-block style-scope ytd-thumbnail'})     elif video_type == \"videos\":         video_links = soup.find_all('a', {'id': 'thumbnail', 'class': 'yt-simple-endpoint style-scope ytd-playlist-thumbnail'})      # Extract and print the href attributes     channel = url.split('/')[-2]      with open(f'{channel}-{video_type}.txt', 'w') as f:         for link in video_links:             href = link.get('href')             if href:                 f.write(f\"https://www.youtube.com{href}\\n\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # Check if a command-line argument is provided\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python youtube_channel_scraper.py &lt;YouTube channel URL&gt; Example: https://www.youtube.com/@Fireship/videos or https://www.youtube.com/@Fireship/shorts\")\n        sys.exit(1)\n\n    # URL of the YouTube channel\n    url = sys.argv[1]\n\n    # Call the main function with the provided URL\n    scrape_youtube_channel(url)\n</pre> if __name__ == \"__main__\":     # Check if a command-line argument is provided     if len(sys.argv) &lt; 2:         print(\"Usage: python youtube_channel_scraper.py  Example: https://www.youtube.com/@Fireship/videos or https://www.youtube.com/@Fireship/shorts\")         sys.exit(1)      # URL of the YouTube channel     url = sys.argv[1]      # Call the main function with the provided URL     scrape_youtube_channel(url)"},{"location":"PromptEngineering/","title":"Whats Prompting?","text":""},{"location":"PromptEngineering/#prompt-engineering","title":"\ud83d\ude80 Prompt Engineering","text":"<p>Prompt engineering, a crucial skill in the era of generative AI, is the art and science of crafting effective instructions to guide language models towards desired outputs. As reported by DataCamp, this emerging discipline involves designing and refining prompts to elicit specific responses from AI models, particularly Large Language Models (LLMs), shaping the way we interact with and harness the power of artificial intelligence.</p>"},{"location":"PromptEngineering/#repository-structure","title":"\ud83d\udcda Repository Structure","text":"File Name Description Basic Prompting Introduction to basic prompt engineering concepts and structures. Advanced Prompting Techniques for advanced prompt optimization and structured outputs. Hands-on with Advanced Prompt Engineering Practical guide for applying advanced prompting techniques. Understanding the OpenAI API Overview of OpenAI API usage with prompts. Function Calling in LLMs Notebook demonstrating function calling features with language models. Comprehensive Prompt Engineering Notebook Jupyter Notebook covering various prompting techniques. README Overview of the repository and contents."},{"location":"PromptEngineering/#introduction-to-prompting","title":"\ud83c\udfaf Introduction to Prompting","text":"<p>Prompting is the process of providing specific instructions or inputs to an AI model to elicit desired outputs or behaviors. It serves as a crucial interface between humans and AI systems, allowing users to guide the model's responses effectively. In the context of large language models (LLMs), prompts can range from simple queries to complex sets of instructions, including context and style directives.</p>"},{"location":"PromptEngineering/#key-aspects-of-prompting-include","title":"Key aspects of prompting include:","text":"<ul> <li>Versatility: Prompts can be textual, visual, or auditory, depending on the AI model and task</li> <li>Specificity: Well-crafted prompts provide precise details to generate more accurate and relevant outputs</li> <li>Iterative refinement: Prompting often involves interpreting the model's responses and adjusting subsequent prompts for better results</li> <li>Application diversity: Prompts are used in various domains, including text generation, image recognition, data analysis, and conversational AI</li> </ul>"},{"location":"PromptEngineering/#prompt-engineering-significance","title":"\ud83c\udf1f Prompt Engineering Significance","text":"<ol> <li> <p>Enhanced AI Performance: Well-crafted prompts can dramatically improve the quality and relevance of AI-generated outputs. By providing clear instructions and context, prompt engineering enables models to produce more accurate, coherent, and useful responses.</p> </li> <li> <p>Customization and Flexibility: Prompt engineering allows users to tailor AI responses to specific needs and domains without requiring extensive model retraining. This flexibility makes AI systems more adaptable to diverse applications across various industries.</p> </li> <li> <p>Bias Mitigation: Careful prompt design can help reduce biases in AI outputs by guiding models to consider multiple perspectives or focus on specific, unbiased information sources.</p> </li> <li> <p>Improved User Experience: By bridging the gap between human intent and machine understanding, effective prompt engineering enhances the user experience, making AI tools more accessible and user-friendly[4].</p> </li> <li> <p>Cost-Efficiency: Optimizing prompts can lead to more efficient use of computational resources, potentially reducing the need for larger, more expensive models to achieve desired results.</p> </li> <li> <p>Rapid Prototyping and Iteration: Prompt engineering enables quick experimentation and refinement of AI applications, facilitating faster development cycles and innovation.</p> </li> <li> <p>Ethical Considerations: Thoughtful prompt design can help ensure AI systems adhere to ethical guidelines and produce appropriate content for different contexts and audiences.</p> </li> <li> <p>Scalability: Once effective prompts are developed, they can be easily scaled across an organization, enabling consistent and high-quality AI interactions.</p> </li> <li> <p>Interdisciplinary Applications: Prompt engineering bridges technical and domain expertise, allowing subject matter experts to leverage AI capabilities without deep technical knowledge.</p> </li> </ol>"},{"location":"PromptEngineering/#prompt-engineering-techniques","title":"Prompt Engineering Techniques","text":"<ol> <li> <p>Introduction to Prompt Engineering:    A foundational overview of prompt engineering concepts, covering basic principles, structured prompts, comparative analysis, and problem-solving applications.</p> </li> <li> <p>Basic Prompt Structures:    Explores single-turn and multi-turn prompts, demonstrating how to create simple prompts and engage in conversations with AI models.</p> </li> <li> <p>Prompt Templates and Variables:    Introduces the use of templates and variables in prompts, focusing on creating flexible and reusable prompt structures using tools like Jinja2.</p> </li> <li> <p>Zero-Shot Prompting:    Demonstrates how to instruct AI models to perform tasks without specific examples, using techniques like direct task specification and role-based prompting.</p> </li> <li> <p>Few-Shot Learning and In-Context Learning:    Covers techniques for providing the AI with a few examples to guide its responses, improving performance on specific tasks without fine-tuning.</p> </li> <li> <p>Chain of Thought (CoT) Prompting:    Encourages AI models to break down complex problems into step-by-step reasoning processes, improving problem-solving capabilities.</p> </li> <li> <p>Self-Consistency and Multiple Paths of Reasoning:    Explores generating diverse reasoning paths and aggregating results to improve the accuracy and reliability of AI-generated answers.</p> </li> <li> <p>Constrained and Guided Generation:    Focuses on setting up constraints for model outputs and implementing rule-based generation to control and guide AI responses.</p> </li> <li> <p>Role Prompting:    Demonstrates how to assign specific roles to AI models and craft effective role descriptions to elicit desired behaviors or expertise.</p> </li> <li> <p>Task Decomposition in Prompts:     Explores techniques for breaking down complex tasks into smaller, manageable subtasks within prompts.</p> </li> <li> <p>Prompt Chaining and Sequencing:     Shows how to connect multiple prompts in a logical flow to tackle complex, multi-step tasks.</p> </li> <li> <p>Instruction Engineering:     Focuses on crafting clear and effective instructions for language models, balancing specificity and generality to optimize performance.</p> </li> <li> <p>Prompt Optimization Techniques:     Covers advanced methods for refining prompts, including A/B testing and iterative improvement based on performance metrics.</p> </li> <li> <p>Handling Ambiguity and Improving Clarity:     Addresses techniques for identifying and resolving ambiguous prompts and strategies for writing clearer, more effective prompts.</p> </li> <li> <p>Prompt Length and Complexity Management:     Explores strategies for managing long or complex prompts, including techniques like chunking and summarization.</p> </li> <li> <p>Negative Prompting and Avoiding Undesired Outputs:     Demonstrates how to use negative examples and constraints to guide the model away from unwanted responses.</p> </li> <li> <p>Prompt Formatting and Structure:     Examines various prompt formats and structural elements to optimize AI model responses.</p> </li> <li> <p>Prompts for Specific Tasks:     Focuses on designing prompts for particular tasks like summarization, question-answering, code generation, and creative writing.</p> </li> <li> <p>Multilingual and Cross-lingual Prompting:     Explores techniques for creating prompts that work effectively across multiple languages and for translation tasks.</p> </li> <li> <p>Ethical Considerations in Prompt Engineering:     Addresses the ethical dimensions of prompt design, focusing on avoiding biases and creating inclusive prompts.</p> </li> <li> <p>Prompt Security and Safety:     Covers techniques for preventing prompt injections and implementing content filters to ensure safe AI applications.</p> </li> <li> <p>Evaluating Prompt Effectiveness:     Explores methods for assessing and measuring the effectiveness of prompts, including manual and automated evaluation techniques.</p> </li> </ol>"},{"location":"PromptEngineering/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our contributing guidelines for more information.</p>"},{"location":"PromptEngineering/#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>    Made with \u2764\ufe0f for the AI community"},{"location":"PromptEngineering/Advanced_Prompting/","title":"Advance Prompt Engineering","text":""},{"location":"PromptEngineering/Advanced_Prompting/#advanced-prompting-techniques-a-comprehensive-guide","title":"Advanced Prompting Techniques: A Comprehensive Guide","text":"<p>Advanced prompt engineering techniques represent the cutting edge of AI interaction, going beyond basic prompting to achieve more sophisticated reasoning and specialized outcomes. This guide will walk you through these advanced methods, explaining how they work and how to use them effectively.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Chain of Thought (CoT)</li> <li>Self-Consistency</li> <li>ReAct Prompting</li> <li>Multimodal Prompting</li> <li>Real-Time Optimization</li> <li>Active Prompting</li> <li>Generated Knowledge</li> <li>Directional-Stimulus</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#chain-of-thought","title":"Chain of Thought","text":"<p>Chain-of-Thought (CoT) prompting is a sophisticated technique that enhances an AI model's ability to handle complex reasoning tasks. Think of it as showing your work in a math problem \u2013 instead of just providing the final answer, you demonstrate each step of your thinking process.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#understanding-cot","title":"Understanding CoT","text":"<p>When we use Chain-of-Thought prompting, we're asking the AI to break down complex problems into smaller, more manageable steps. This approach leads to more accurate and logical outputs because it forces the model to \"think\" through each stage of the solution.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#practical-example","title":"Practical Example","text":"<pre><code>Problem: A store has a 30% discount on all items. If Sarah buys a shirt\noriginally priced at $40 and pants originally priced at $60, how much\ndoes she save in total?\n\nChain of Thought:\n1. Calculate original total:\n   Shirt ($40) + Pants ($60) = $100\n\n2. Calculate discount percentage:\n   30% of $100 = $100 \u00d7 0.30 = $30\n\nTherefore, Sarah saves $30 in total.\n</code></pre>"},{"location":"PromptEngineering/Advanced_Prompting/#advanced-usage-cot-rollouts","title":"Advanced Usage: CoT Rollouts","text":"<p>CoT rollouts take this concept further by generating multiple reasoning paths for a given problem. Here's what makes them powerful:</p> <ol> <li> <p>Self-consistency decoding</p> </li> <li> <p>Generate multiple solution paths</p> </li> <li>Compare the different approaches</li> <li> <p>Choose the most commonly reached conclusion</p> </li> <li> <p>Complexity-based evaluation</p> </li> <li> <p>Assess the depth of each reasoning chain</p> </li> <li>Consider the thoroughness of each approach</li> <li> <p>Select solutions with comprehensive reasoning</p> </li> <li> <p>Human validation integration</p> </li> <li>Use human expertise when rollouts disagree</li> <li>Validate critical decision points</li> <li>Refine the reasoning process</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#self-consistency","title":"Self-Consistency","text":"<p>Self-consistency is an enhancement to Chain-of-Thought prompting that focuses on generating and evaluating multiple reasoning paths. This approach is particularly valuable when dealing with problems that have various valid solution methods.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#key-components","title":"Key Components","text":"<p>The self-consistency approach involves:</p> <ol> <li>Generating multiple independent solutions</li> <li>Analyzing the consistency between different approaches</li> <li>Identifying patterns in reasoning</li> <li>Selecting the most reliable outcome</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#example-implementation","title":"Example Implementation","text":"<pre><code>Question: Calculate 15% of 240\n\nPath 1:\n- 10% of 240 = 24\n- 5% is half of 10% = 12\n- Total: 24 + 12 = 36\n\nPath 2:\n- Convert 15% to decimal: 0.15\n- Multiply: 240 \u00d7 0.15 = 36\n\nPath 3:\n- Break down into: (240 \u00d7 10/100) + (240 \u00d7 5/100)\n- = 24 + 12\n- = 36\n\nAll paths converge on 36, indicating high confidence in the answer.\n</code></pre>"},{"location":"PromptEngineering/Advanced_Prompting/#react-prompting","title":"ReAct Prompting","text":"<p>ReAct (Reasoning and Acting) prompting is a synergistic approach that combines reasoning traces with specific actions. This technique allows AI models to dynamically plan, execute, and adjust their approach during complex problem-solving scenarios.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#core-components","title":"Core Components","text":"<p>The ReAct framework consists of three main elements:</p> <ol> <li>Thought: The model's internal reasoning process</li> <li>Action: Specific tasks or queries to be performed</li> <li>Observation: Results or information gathered from actions</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#comprehensive-example","title":"Comprehensive Example","text":"<pre><code>Task: Find out who directed the highest-grossing film of 2009\n\n1. Thought: I need to identify the highest-grossing film of 2009 first\n2. Action: Search for \"highest-grossing film 2009\"\n3. Observation: Avatar was the highest-grossing film of 2009\n\n4. Thought: Now I need to find out who directed Avatar\n5. Action: Search for \"Avatar 2009 director\"\n6. Observation: James Cameron directed Avatar\n\n7. Thought: I have all the necessary information\n8. Answer: James Cameron directed Avatar, the highest-grossing film of 2009\n</code></pre>"},{"location":"PromptEngineering/Advanced_Prompting/#applications","title":"Applications","text":"<p>ReAct prompting excels in:</p> <ul> <li>Complex question answering</li> <li>Multi-step task automation</li> <li>Interactive information gathering</li> <li>Decision-making processes</li> <li>External knowledge base interaction</li> </ul>"},{"location":"PromptEngineering/Advanced_Prompting/#multimodal-prompting","title":"Multimodal Prompting","text":"<p>Multimodal prompt engineering involves crafting prompts that work with multiple types of input data. This technique enables more comprehensive and context-aware AI interactions by combining different forms of information.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#input-types-and-handling","title":"Input Types and Handling","text":"<ul> <li>Text: Written instructions, questions, or content</li> <li>Images: Visual data, diagrams, or pictures</li> <li>Audio: Sound files, speech, or music</li> <li>Video: Moving images, animations, or recordings</li> </ul>"},{"location":"PromptEngineering/Advanced_Prompting/#best-practices","title":"Best Practices","text":"<p>When working with multiple modalities:</p> <ol> <li>Provide clear instructions for each input type</li> <li>Establish relationships between different inputs</li> <li>Specify desired output formats</li> <li>Maintain context across all modalities</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#real-time-optimization","title":"Real-Time Optimization","text":"<p>Real-time prompt optimization is an emerging technology that provides instant feedback on prompt effectiveness. This dynamic approach helps refine prompts for better outcomes.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#key-aspects","title":"Key Aspects","text":"<p>The system evaluates:</p> <ul> <li>Prompt clarity and specificity</li> <li>Potential biases in instructions</li> <li>Alignment with desired outcomes</li> <li>Response quality and relevance</li> </ul>"},{"location":"PromptEngineering/Advanced_Prompting/#optimization-process","title":"Optimization Process","text":"<pre><code>graph LR\n    A[Initial Prompt] --&gt; B[Analysis]\n    B --&gt; C[Feedback]\n    C --&gt; D[Refinement]\n    D --&gt; E[Improved Prompt]</code></pre>"},{"location":"PromptEngineering/Advanced_Prompting/#active-prompting","title":"Active Prompting","text":"<p>Active prompting allows for dynamic adjustment of prompts based on user interaction and feedback. This adaptive approach enables AI models to refine their responses in real-time throughout an interaction.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#process-flow","title":"Process Flow","text":"<p>The active prompting cycle involves:</p> <ol> <li>Initial prompt delivery</li> <li>User response analysis</li> <li>Prompt adjustment</li> <li>Refined response generation</li> <li>Continuous improvement</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#generated-knowledge","title":"Generated Knowledge","text":"<p>Generated Knowledge Prompting enhances AI responses by first generating relevant knowledge before answering questions. This two-step approach improves the model's common sense reasoning and contextual understanding.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#implementation-steps","title":"Implementation Steps","text":"<ol> <li> <p>Knowledge Generation Phase</p> </li> <li> <p>Generate relevant facts and context</p> </li> <li>Identify key concepts</li> <li> <p>Establish relationships between ideas</p> </li> <li> <p>Knowledge Integration Phase</p> </li> <li>Incorporate generated knowledge</li> <li>Formulate comprehensive response</li> <li>Ensure contextual accuracy</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#benefits","title":"Benefits","text":"<ul> <li>Improved response accuracy</li> <li>Enhanced contextual understanding</li> <li>Better-grounded answers</li> <li>More informative outputs</li> </ul>"},{"location":"PromptEngineering/Advanced_Prompting/#directional-stimulus-prompting","title":"Directional-Stimulus Prompting","text":"<p>Directional-Stimulus Prompting (DSP) is an innovative framework that uses a small, tunable policy model to generate auxiliary prompts that guide larger language models toward desired outputs.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#technical-details","title":"Technical Details","text":"<p>The DSP approach includes:</p> <ul> <li>Utilization of smaller policy models (like T5)</li> <li>Supervised fine-tuning processes</li> <li>Reinforcement learning optimization</li> <li>Performance monitoring and adjustment</li> </ul>"},{"location":"PromptEngineering/Advanced_Prompting/#notable-results","title":"Notable Results","text":"<ul> <li>Achieved 41.4% improvement in ChatGPT's performance on MultiWOZ dataset</li> <li>Required only 80 dialogues for significant improvements</li> <li>Demonstrated flexibility across various tasks</li> </ul>"},{"location":"PromptEngineering/Advanced_Prompting/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"PromptEngineering/Advanced_Prompting/#for-beginners","title":"For Beginners","text":"<ol> <li>Start with basic CoT prompting</li> <li>Practice with simple, well-defined problems</li> <li>Gradually incorporate more advanced techniques</li> <li>Document successful patterns and approaches</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#for-advanced-users","title":"For Advanced Users","text":"<ol> <li>Combine multiple techniques strategically</li> <li>Develop custom implementations for specific use cases</li> <li>Optimize performance through systematic testing</li> <li>Contribute to methodology advancement</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#universal-best-practices","title":"Universal Best Practices","text":"<ol> <li>Always start with clear objectives</li> <li>Test thoroughly before deployment</li> <li>Monitor and measure results</li> <li>Iterate based on performance data</li> </ol> <p>Remember: The most effective technique for your situation depends on your specific needs and constraints. Start simple and add complexity only when necessary.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#chain-of-thought_1","title":"Chain-of-Thought","text":"<p>Chain-of-thought (CoT) rollouts are an advanced prompt engineering technique that enhances the reasoning capabilities of large language models. This method involves generating multiple reasoning paths for a given problem and selecting the most consistent or comprehensive solution. By performing several CoT rollouts, prompt engineers can improve the model's ability to tackle complex tasks requiring logical thinking and multi-step problem-solving.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#key-aspects-of-cot-rollouts","title":"Key Aspects of CoT Rollouts","text":"<ul> <li>Self-consistency decoding: Choosing the most commonly reached conclusion among multiple rollouts.</li> <li>Complexity-based prompting: Selecting rollouts with the longest chains of thought.</li> <li>Human intervention: Consulting a person to correct the chain of thought if rollouts significantly disagree.</li> <li>Improved accuracy: Enhancing the model's performance on tasks involving reasoning, calculation, and decision-making.</li> </ul> <p>By leveraging CoT rollouts, prompt engineers can create more robust and reliable AI systems capable of handling intricate problems across various domains.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#react-prompting-illustrated","title":"ReAct Prompting Illustrated","text":"<p>ReAct (Reasoning and Acting) prompting is an advanced technique that combines reasoning traces and task-specific actions to enhance the problem-solving capabilities of large language models (LLMs). This approach allows LLMs to generate verbal reasoning steps and perform actions in an interleaved manner, leading to more effective and dynamic problem-solving.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#a-typical-react-prompt","title":"A Typical ReAct Prompt","text":"<ol> <li>Thought: The model's reasoning process.</li> <li>Action: A specific task or query to be performed.</li> <li>Observation: The result or information obtained from the action.</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#example","title":"Example","text":"<p>Question: What is the capital of the country where the Eiffel Tower is located?</p> <ol> <li>Thought: To answer this question, I need to find out where the Eiffel Tower is located and then determine the capital of that country.</li> <li>Action: Search for \"Location of Eiffel Tower.\"</li> <li>Observation: The Eiffel Tower is located in Paris, France.</li> <li>Thought: Now that I know the Eiffel Tower is in France, I need to find out the capital of France.</li> <li>Action: Search for \"Capital of France.\"</li> <li>Observation: The capital of France is Paris.</li> <li>Thought: I have all the information needed to answer the question.</li> <li>Action: Provide answer.</li> <li>Answer: The capital of the country where the Eiffel Tower is located is Paris, France.</li> </ol>"},{"location":"PromptEngineering/Advanced_Prompting/#applications-of-react-prompting","title":"Applications of ReAct Prompting","text":"<ul> <li>Question Answering: Enhancing the model's ability to break down complex questions and gather relevant information.</li> <li>Task Automation: Guiding models through multi-step processes by combining reasoning and action.</li> <li>Interacting with External Knowledge Bases: Allowing models to retrieve and incorporate additional information to support their reasoning.</li> <li>Decision-Making: Improving the model's capacity to evaluate options and make informed choices based on available data.</li> </ul> <p>By implementing ReAct prompting, developers and researchers can create more robust and adaptable AI systems capable of handling complex reasoning tasks and real-world problem-solving scenarios.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#directional-stimulus-prompting_1","title":"Directional-Stimulus Prompting","text":"<p>Directional Stimulus Prompting (DSP) is an innovative framework designed to guide large language models (LLMs) towards specific desired outputs. This technique employs a small, tunable policy model to generate auxiliary directional stimulus prompts for each input instance, acting as nuanced hints to steer LLMs in generating desired outcomes.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#key-features-of-dsp","title":"Key Features of DSP","text":"<ul> <li>Utilization of a small, tunable policy model (e.g., T5) to generate directional stimuli.</li> <li>Optimization through supervised fine-tuning and reinforcement learning.</li> <li>Applicability to various tasks, including summarization, dialogue response generation, and chain-of-thought reasoning.</li> <li>Significant performance improvements with minimal labeled data, such as a 41.4% boost in ChatGPT's performance on the MultiWOZ dataset using only 80 dialogues.</li> </ul> <p>By leveraging DSP, researchers and practitioners can enhance the capabilities of black-box LLMs without directly modifying their parameters, offering a flexible and efficient approach to prompt engineering.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#generated-knowledge-prompting","title":"Generated Knowledge Prompting","text":"<p>Generated Knowledge Prompting is a technique that enhances AI model performance by first asking the model to generate relevant facts before answering a question or completing a task. This two-step process involves knowledge generation, where the model produces pertinent information, followed by knowledge integration, where this information is used to formulate a more accurate and contextually grounded response.</p>"},{"location":"PromptEngineering/Advanced_Prompting/#key-benefits","title":"Key Benefits","text":"<ul> <li>Improved accuracy and reliability of AI-generated content.</li> <li>Enhanced contextual understanding of the given topic.</li> <li>Ability to anchor responses in factual information.</li> <li>Potential for combining with external sources like APIs or databases for further knowledge augmentation.</li> </ul> <p>By prompting the model to first consider relevant facts, Generated Knowledge Prompting helps create more informative and well-reasoned outputs, particularly useful for complex tasks or when dealing with specialized subject matter.</p>"},{"location":"PromptEngineering/Basic_Prompting/","title":"Basic Prompt Engineering","text":""},{"location":"PromptEngineering/Basic_Prompting/#basic-prompting-techniques-a-practical-guide","title":"Basic Prompting Techniques: A Practical Guide","text":""},{"location":"PromptEngineering/Basic_Prompting/#introduction","title":"Introduction","text":"<p>Prompt engineering is the art of crafting effective instructions for AI models. This guide covers fundamental techniques that will help you get better results from AI interactions.</p>"},{"location":"PromptEngineering/Basic_Prompting/#core-techniques","title":"Core Techniques","text":""},{"location":"PromptEngineering/Basic_Prompting/#1-zero-shot-prompting","title":"1. Zero-Shot Prompting","text":"<p>The simplest form of prompting - just ask directly without examples.</p> <p>What it is:</p> <ul> <li>Direct questions or instructions</li> <li>No additional context needed</li> <li>Quick and straightforward</li> </ul> <p>Example:</p> <pre><code>\u274c Complex: Please tell me, if you would be so kind, what the capital of France might be?\n\u2705 Simple: What is the capital of France?\n</code></pre>"},{"location":"PromptEngineering/Basic_Prompting/#2-one-shot-prompting","title":"2. One-Shot Prompting","text":"<p>Providing one example to guide the AI's response.</p> <p>What it is:</p> <ul> <li>Single example + new request</li> <li>Helps establish format/style</li> <li>More precise than zero-shot</li> </ul> <p>Example:</p> <pre><code>Product Description Example:\n\"The Classic White Mug - A timeless 12oz ceramic mug perfect for your morning coffee. Microwave-safe and dishwasher-friendly.\"\n\nPlease write a similar description for: Gaming Mechanical Keyboard\n</code></pre>"},{"location":"PromptEngineering/Basic_Prompting/#3-few-shot-prompting","title":"3. Few-Shot Prompting","text":"<p>Multiple examples for more complex tasks.</p> <p>What it is:</p> <ul> <li>Several examples before the request</li> <li>Creates clear patterns</li> <li>Best for consistent outputs</li> </ul> <p>Example:</p> <pre><code>Convert these sentences to past tense:\nInput: \"I eat an apple\"\nOutput: \"I ate an apple\"\n\nInput: \"She runs fast\"\nOutput: \"She ran fast\"\n\nNow convert: \"They sing well\"\n</code></pre>"},{"location":"PromptEngineering/Basic_Prompting/#4-role-based-prompting","title":"4. Role-Based Prompting","text":"<p>Assigning specific roles to guide responses.</p> <p>What it is:</p> <ul> <li>Defines AI's perspective</li> <li>Shapes tone and expertise level</li> <li>Creates focused responses</li> </ul> <p>Example:</p> <pre><code>Act as a cybersecurity expert and explain how password hashing works to a beginner.\n</code></pre>"},{"location":"PromptEngineering/Basic_Prompting/#5-prompt-reframing","title":"5. Prompt Reframing","text":"<p>Restructuring prompts for different perspectives.</p> <p>What it is:</p> <ul> <li>Alternative ways to ask</li> <li>Explores different angles</li> <li>Improves response quality</li> </ul> <p>Example: Original: \"What are solar panels?\" Reframed: \"How would you explain solar panels to a 10-year-old?\"</p>"},{"location":"PromptEngineering/Basic_Prompting/#6-prompt-combination","title":"6. Prompt Combination","text":"<p>Merging multiple instructions for comprehensive responses.</p> <p>What it is:</p> <ul> <li>Multiple questions in one</li> <li>Creates thorough responses</li> <li>Addresses complex topics</li> </ul> <p>Example:</p> <pre><code>Explain what Python is, provide three key benefits of using it, and suggest\ntwo beginner-friendly projects to start with.\n</code></pre>"},{"location":"PromptEngineering/Basic_Prompting/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Be Specific</p> </li> <li> <p>Clear instructions get better results</p> </li> <li>Avoid vague or ambiguous language</li> <li> <p>State your expectations clearly</p> </li> <li> <p>Start Simple</p> </li> <li> <p>Begin with zero-shot prompting</p> </li> <li>Add complexity only when needed</li> <li> <p>Iterate based on results</p> </li> <li> <p>Test and Refine</p> </li> <li>Try different approaches</li> <li>Combine techniques when appropriate</li> <li>Learn from successful prompts</li> </ol>"},{"location":"PromptEngineering/Basic_Prompting/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Overly complex instructions</li> <li>Ambiguous or vague requests</li> <li>Missing context when needed</li> <li>Inconsistent formatting</li> <li>Too many requirements at once</li> </ul>"},{"location":"PromptEngineering/Basic_Prompting/#quick-reference-table","title":"Quick Reference Table","text":"Technique Best For Example Use Case Zero-Shot Simple questions Factual queries One-Shot Format-specific tasks Content templates Few-Shot Pattern replication Language translation Role-Based Specialized knowledge Technical explanations Reframing Different perspectives Complex concepts Combination Comprehensive answers Multi-part questions <p>Remember: The most effective technique depends on your specific needs. Start simple and add complexity only when necessary.</p>"},{"location":"PromptEngineering/Understanding_OpenAI_API/","title":"Understanding OpenAI API","text":""},{"location":"PromptEngineering/Understanding_OpenAI_API/#understanding-llm-apis","title":"Understanding LLM APIs","text":"<p>Let's talk about LLM APIs.</p> <p>There are a few methods to use the API. One is directly sending HTTP requests, and the other way is to pip install the official OpenAI package, which is up to date with the latest features.</p> <pre><code>pip install openai\n</code></pre> <p>As the API format is so popular, a lot of other providers also provide theirs in the same format. This is what we call \"OpenAI compatible.\"</p>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#list-of-providers-that-have-openai-compatible-apis","title":"List of Providers that have OpenAI-compatible APIs","text":"<p>Here is a list of providers that are compatible with the OpenAI API, allowing developers to utilize their services in a similar manner to OpenAI's offerings:</p> <ul> <li> <p>Groq</p> </li> <li> <p>Groq provides an API that is mostly compatible with OpenAI's client libraries. Users can configure their applications to run on Groq by changing the <code>base_url</code> and using a Groq API key.</p> </li> <li> <p>Mistral AI</p> </li> <li> <p>Mistral offers an API that supports OpenAI-compatible requests. Developers can access various models through this service.</p> </li> <li> <p>Hugging Face</p> </li> <li> <p>Hugging Face provides access to numerous models via an API that can be configured similarly to OpenAI's. It is well-known for its extensive model library.</p> </li> <li> <p>Google Vertex AI</p> </li> <li> <p>Google Vertex AI allows users to interact with large language models in a manner consistent with OpenAI's API.</p> </li> <li> <p>Microsoft Azure OpenAI</p> </li> <li> <p>Microsoft provides access to OpenAI models through its Azure platform, enabling integration with existing Azure services.</p> </li> <li> <p>Anthropic</p> </li> <li>Anthropic's models can also be accessed through an API that mimics OpenAI's structure, allowing for similar interactions.</li> </ul> <p>These providers enable developers to leverage different AI capabilities while maintaining compatibility with the familiar OpenAI API structure, facilitating easier integration into applications and workflows.</p> <p>Some other providers have a different API schema and type, but we have client libraries like litellm that provide a uniform client package for different types of LLMs.</p> <p>Also, gateways such as Portkey provide an OpenAI-compatible API for any of your LLM providers.</p> <p>In this blog, you are going to learn about the different parameters that go into the API.</p>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#key-parameters","title":"Key Parameters","text":"<p>Temperature</p> <ul> <li>Description: The temperature parameter controls the randomness of the model's output. It ranges from 0 to 2.</li> <li> <p>Effects:</p> </li> <li> <p>Low Values (0-0.3): Outputs are more deterministic and focused, often repeating similar responses.</p> </li> <li>Medium Values (0.4-0.7): Balances creativity and coherence.</li> <li> <p>High Values (0.8-2): Produces more varied and creative outputs but can lead to nonsensical results.</p> </li> <li> <p>Example:   </p><pre><code>response = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Complete the sentence: 'The key to happiness is'.\"}],\n    temperature=1\n)\n</code></pre> </li> </ul> <p>Top_p</p> <ul> <li>Description: This parameter implements nucleus sampling, where the model considers only the top <code>p</code> probability mass for generating responses.</li> <li> <p>Range: Between 0 and 1, where lower values restrict the output to more probable tokens.</p> </li> <li> <p>Example:   </p><pre><code>response = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a poem.\"}],\n    top_p=0.9\n)\n</code></pre> </li> </ul> <p>Max Tokens</p> <ul> <li>Description: Defines the maximum number of tokens (words or parts of words) in the generated response.</li> <li> <p>Default: Typically set to a maximum of 4096 tokens for most models.</p> </li> <li> <p>Example:   </p><pre><code>response = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story.\"}],\n    max_tokens=150\n)\n</code></pre> </li> </ul> <p>Function Calling</p> <ul> <li> <p>Description: This feature allows the model to invoke predefined functions based on user input, facilitating interaction with external APIs or services.</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>functions = [\n    {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a location.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n            },\n            \"required\": [\"location\"]\n        }\n    }\n]\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in New York?\"}],\n    functions=functions,\n    function_call={\"name\": \"get_current_weather\"}\n)\n</code></pre>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#roles-in-api-calls","title":"Roles in API Calls","text":"<p>Understanding the roles involved in an API call helps structure interactions effectively.</p> <p>System Role</p> <ul> <li>Purpose: Provides high-level instructions or context that guides the model's behavior throughout the conversation.</li> <li> <p>Usage: Set at the beginning of the message array to establish tone or rules for interaction.</p> </li> <li> <p>Example:   </p><pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What can you do?\"}\n]\n</code></pre> </li> </ul> <p>User Role</p> <ul> <li>Purpose: Represents inputs from the human user, guiding the conversation with queries or prompts.</li> <li> <p>Usage: Used most frequently in interactions to ask questions or provide statements.</p> </li> <li> <p>Example:   </p><pre><code>{\"role\": \"user\", \"content\": \"Can you explain how OpenAI works?\"}\n</code></pre> </li> </ul> <p>Assistant Role</p> <ul> <li>Purpose: Represents responses generated by the model based on user inputs and system instructions.</li> <li> <p>Usage: Automatically assumed by the model when replying to user queries.</p> </li> <li> <p>Example:   </p><pre><code>{\"role\": \"assistant\", \"content\": \"OpenAI uses advanced machine learning techniques to generate text.\"}\n</code></pre> </li> </ul>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#additional-parameters","title":"Additional Parameters","text":"<p>Stream</p> <ul> <li>Description: If set to true, allows streaming of partial responses as they are generated, useful for real-time applications.</li> </ul> <p>Logprobs</p> <ul> <li>Description: Returns log probabilities of token predictions, useful for understanding model behavior and improving outputs.</li> </ul>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#conclusion","title":"Conclusion","text":"<p>The OpenAI API offers a robust set of parameters and roles that enable developers to create highly interactive applications. By adjusting parameters like temperature and utilizing structured roles effectively, users can tailor responses to meet specific needs while ensuring clarity and control in conversations.</p> <p>Here are examples of how to use the OpenAI API with various providers that are compatible, along with an example of a lightweight language model (LLM).</p>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#openai-api-examples-with-compatible-providers","title":"OpenAI API Examples with Compatible Providers","text":""},{"location":"PromptEngineering/Understanding_OpenAI_API/#1-groq","title":"1. Groq","text":"<p>To use Groq's API, you need to set the base URL and provide your Groq API key.</p> <pre><code>import os\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"https://api.groq.com/openai/v1\",\n    api_key=os.environ.get(\"GROQ_API_KEY\")\n)\n\nresponse = client.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}]\n)\nprint(response.choices[0].message['content'])\n</code></pre>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#2-mistral-ai","title":"2. Mistral AI","text":"<p>Mistral AI also provides an OpenAI-compatible API. Here's how to use it:</p> <pre><code>import requests\n\nurl = \"https://api.mistral.ai/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer {os.environ.get('MISTRAL_API_KEY')}\",\n    \"Content-Type\": \"application/json\"\n}\ndata = {\n    \"model\": \"mistral-7b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.json()['choices'][0]['message']['content'])\n</code></pre>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#3-hugging-face","title":"3. Hugging Face","text":"<p>Using Hugging Face's API requires an access token. Here's an example:</p> <pre><code>import requests\n\nurl = \"https://api-inference.huggingface.co/models/gpt2\"\nheaders = {\n    \"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACE_API_KEY')}\"\n}\ndata = {\n    \"inputs\": \"Once upon a time in a land far away,\"\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.json()[0]['generated_text'])\n</code></pre>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#4-google-vertex-ai","title":"4. Google Vertex AI","text":"<p>To interact with Google Vertex AI, you can use the following code:</p> <pre><code>from google.cloud import aiplatform\n\naiplatform.init(project='your-project-id', location='us-central1')\n\nresponse = aiplatform.gapic.PredictionServiceClient().predict(\n    endpoint='projects/your-project-id/locations/us-central1/endpoints/your-endpoint-id',\n    instances=[{\"content\": \"Who won the World Series in 2020?\"}],\n)\nprint(response.predictions)\n</code></pre>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#5-microsoft-azure-openai","title":"5. Microsoft Azure OpenAI","text":"<p>Here's how to call Azure's OpenAI service:</p> <pre><code>import requests\n\nurl = f\"https://your-resource-name.openai.azure.com/openai/deployments/your-deployment-name/chat/completions?api-version=2023-05-15\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"api-key\": os.environ.get(\"AZURE_OPENAI_API_KEY\")\n}\ndata = {\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather today?\"}],\n    \"model\": \"gpt-35-turbo\"\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.json()['choices'][0]['message']['content'])\n</code></pre>"},{"location":"PromptEngineering/Understanding_OpenAI_API/#6-anthropic","title":"6. Anthropic","text":"<p>Using Anthropic's Claude model via its API can be done as follows:</p> <pre><code>import requests\n\nurl = \"https://api.anthropic.com/v1/complete\"\nheaders = {\n    \"Authorization\": f\"Bearer {os.environ.get('ANTHROPIC_API_KEY')}\",\n    \"Content-Type\": \"application/json\"\n}\ndata = {\n    \"model\": \"claude-v1\",\n    \"prompt\": \"Explain quantum physics in simple terms.\",\n}\n\nresponse = requests.post(url, headers=headers, json=data)\nprint(response.json()['completion'])\n</code></pre> <p>This example demonstrates how to utilize lightweight models effectively while still achieving meaningful outputs in text generation tasks.</p> <p>Sources:</p> <p>https://www.coltsteele.com/tips/understanding-openai-s-temperature-parameter https://community.make.com/t/what-is-the-difference-between-system-user-and-assistant-roles-in-chatgpt/36160 https://arize.com/blog-course/mastering-openai-api-tips-and-tricks/ https://learn.microsoft.com/ko-kr/Azure/ai-services/openai/reference https://community.openai.com/t/the-system-role-how-it-influences-the-chat-behavior/87353 https://community.openai.com/t/understanding-role-management-in-openais-api-two-methods-compared/253289 https://platform.openai.com/docs/advanced-usage https://platform.openai.com/docs/api-reference</p> <p>https://console.groq.com/docs/openai https://docs.jabref.org/ai/ai-providers-and-api-keys https://www.reddit.com/r/LocalLLaMA/comments/16csz5n/best_openai_api_compatible_application_server/ https://docs.gptscript.ai/alternative-model-providers https://towardsdatascience.com/how-to-build-an-openai-compatible-api-87c8edea2f06?gi=5537ceb80847</p> <p>https://modelfusion.dev/integration/model-provider/openaicompatible/ https://docs.jabref.org/ai/ai-providers-and-api-keys https://docs.gptscript.ai/alternative-model-providers https://console.groq.com/docs/openai</p>"},{"location":"PromptEngineering/function_calling/","title":"Function Calling","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install openai\n# !pip install python-dotenv\n# !pip install rich\n# !pip install requests\n</pre> # !pip install openai # !pip install python-dotenv # !pip install rich # !pip install requests In\u00a0[18]: Copied! <pre>import os\nimport openai\nimport requests\nfrom rich import print\nfrom dotenv import load_dotenv\nimport json\n\nload_dotenv()\n\nlocal_api_key = os.getenv(\"GROQ_API_KEY\")\n# openai.api_key = os.getenv(\"GROQ_API_KEY\")\n\nclient = openai.OpenAI(\n    base_url=\"https://api.groq.com/openai/v1\",\n    api_key=local_api_key\n)\n</pre> import os import openai import requests from rich import print from dotenv import load_dotenv import json  load_dotenv()  local_api_key = os.getenv(\"GROQ_API_KEY\") # openai.api_key = os.getenv(\"GROQ_API_KEY\")  client = openai.OpenAI(     base_url=\"https://api.groq.com/openai/v1\",     api_key=local_api_key ) In\u00a0[10]: Copied! <pre>def get_completion(messages, model=\"llama-3.2-90b-text-preview\", temperature=0, max_tokens=300, tools=None):\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        tools=tools\n    )\n    return response.choices[0].message\n</pre> def get_completion(messages, model=\"llama-3.2-90b-text-preview\", temperature=0, max_tokens=300, tools=None):     response = client.chat.completions.create(         model=model,         messages=messages,         temperature=temperature,         max_tokens=max_tokens,         tools=tools     )     return response.choices[0].message In\u00a0[11]: Copied! <pre>## Define Function\ndef add_numbers(number_1 , number_2):\n    \n    \"\"\"\n    returns the sum of two numbers\n    \"\"\"\n    \n    return int(number_1) + int(number_2)\n    \n</pre> ## Define Function def add_numbers(number_1 , number_2):          \"\"\"     returns the sum of two numbers     \"\"\"          return int(number_1) + int(number_2)      In\u00a0[12]: Copied! <pre>add_tool = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"add_numbers\",\n            \"description\": \"Adds two numbers and returns the sum\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"number_1\": {\n                        \"type\": \"number\",\n                        \"description\": \"The first number to add\",\n                    },\n                    \"number_2\": {\n                        \"type\": \"number\",\n                        \"description\": \"The second number to add\",\n                    },\n                },\n                \"required\": [\"number_1\", \"number_2\"],\n            },\n        },   \n    }\n]\n</pre> add_tool = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"add_numbers\",             \"description\": \"Adds two numbers and returns the sum\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"number_1\": {                         \"type\": \"number\",                         \"description\": \"The first number to add\",                     },                     \"number_2\": {                         \"type\": \"number\",                         \"description\": \"The second number to add\",                     },                 },                 \"required\": [\"number_1\", \"number_2\"],             },         },        } ] In\u00a0[13]: Copied! <pre>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"add 66034 and 39385\"\n    }\n]\n</pre> messages = [     {         \"role\": \"user\",         \"content\": \"add 66034 and 39385\"     } ] In\u00a0[14]: Copied! <pre>response = get_completion(messages)\nprint(response)\n</pre> response = get_completion(messages) print(response) <pre>ChatCompletionMessage(\n    content='66034 + 39385 = 105419.',\n    refusal=None,\n    role='assistant',\n    function_call=None,\n    tool_calls=None\n)\n</pre> In\u00a0[15]: Copied! <pre>response = get_completion(messages, tools=add_tool)\nprint(response)\n</pre> response = get_completion(messages, tools=add_tool) print(response) <pre>ChatCompletionMessage(\n    content=None,\n    refusal=None,\n    role='assistant',\n    function_call=None,\n    tool_calls=[\n        ChatCompletionMessageToolCall(\n            id='call_t6e8',\n            function=Function(arguments='{\"number_1\": 66034, \"number_2\": 39385}', name='add_numbers'),\n            type='function'\n        )\n    ]\n)\n</pre> In\u00a0[17]: Copied! <pre>args = json.loads(response.tool_calls[0].function.arguments)\n\nprint(args)\n\nprint(add_numbers(**args))\n</pre> args = json.loads(response.tool_calls[0].function.arguments)  print(args)  print(add_numbers(**args)) <pre>{'number_1': 66034, 'number_2': 39385}\n</pre> <pre>105419\n</pre> In\u00a0[19]: Copied! <pre>def count_letters_in_word(word:str , letter:str):\n    \n    \"\"\"\n    returns the number of letters in word \n    \n    for example word = strawberry and letter = r\n    \n    it will return 3\n    \n    \"\"\"\n    \n    count = 0\n    for i in word:\n        if i == letter:\n            count += 1\n    return count\n</pre> def count_letters_in_word(word:str , letter:str):          \"\"\"     returns the number of letters in word           for example word = strawberry and letter = r          it will return 3          \"\"\"          count = 0     for i in word:         if i == letter:             count += 1     return count In\u00a0[32]: Copied! <pre>count_letters_in_word(\"strawberry\", \"r\")\n</pre> count_letters_in_word(\"strawberry\", \"r\") Out[32]: <pre>3</pre> In\u00a0[33]: Copied! <pre>counter_tool = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"count_letters_in_word\",\n            \"description\": \"counts the number of a specific letter in a given word or string\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"word\": {\n                        \"type\": \"string\",\n                        \"description\": \"The word\",\n                    },\n                    \"letter\": {\n                        \"type\": \"string\",\n                        \"description\": \"the letter\",\n                    },\n                },\n                \"required\": [\"word\", \"letter\"],\n            },\n        },   \n    }\n]\n</pre> counter_tool = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"count_letters_in_word\",             \"description\": \"counts the number of a specific letter in a given word or string\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"word\": {                         \"type\": \"string\",                         \"description\": \"The word\",                     },                     \"letter\": {                         \"type\": \"string\",                         \"description\": \"the letter\",                     },                 },                 \"required\": [\"word\", \"letter\"],             },         },        } ] In\u00a0[34]: Copied! <pre>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Counting the number of 'r' in strawberry\"\n    }\n]\n</pre> messages = [     {         \"role\": \"user\",         \"content\": \"Counting the number of 'r' in strawberry\"     } ] In\u00a0[35]: Copied! <pre>response = get_completion(messages)\nprint(response)\n</pre> response = get_completion(messages) print(response) <pre>ChatCompletionMessage(\n    content='There are 2 \\'r\\'s in the word \"strawberry\"',\n    refusal=None,\n    role='assistant',\n    function_call=None,\n    tool_calls=None\n)\n</pre> In\u00a0[36]: Copied! <pre>response = get_completion(messages, tools=counter_tool)\nprint(response)\n</pre> response = get_completion(messages, tools=counter_tool) print(response) <pre>ChatCompletionMessage(\n    content=None,\n    refusal=None,\n    role='assistant',\n    function_call=None,\n    tool_calls=[\n        ChatCompletionMessageToolCall(\n            id='call_gvn1',\n            function=Function(arguments='{\"word\": \"strawberry\", \"letter\": \"r\"}', name='count_letters_in_word'),\n            type='function'\n        )\n    ]\n)\n</pre> In\u00a0[38]: Copied! <pre>args = json.loads(response.tool_calls[0].function.arguments)\n\nprint(count_letters_in_word(**args))\n</pre> args = json.loads(response.tool_calls[0].function.arguments)  print(count_letters_in_word(**args)) <pre>3\n</pre> In\u00a0[21]: Copied! <pre>def get_current_weather(location):\n    API_KEY = \"YOUR_API_KEY\"\n    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&amp;appid={API_KEY}\"\n    response = requests.get(url)\n    data = response.json()\n\n    # Extract the temperature in Kelvin\n    kelvin_temp = data['main']['temp']\n\n    # Convert Kelvin to Celsius\n    celsius_temp = kelvin_temp - 273.15\n\n    return {\"location\": location, \"temperature\": round(celsius_temp, 2)}\n</pre> def get_current_weather(location):     API_KEY = \"YOUR_API_KEY\"     url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&amp;appid={API_KEY}\"     response = requests.get(url)     data = response.json()      # Extract the temperature in Kelvin     kelvin_temp = data['main']['temp']      # Convert Kelvin to Celsius     celsius_temp = kelvin_temp - 273.15      return {\"location\": location, \"temperature\": round(celsius_temp, 2)} In\u00a0[24]: Copied! <pre>get_weather_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                },\n                \"required\": [\"location\"],\n            }\n        }\n    }\n]\n</pre> get_weather_tools = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"get_current_weather\",             \"description\": \"Get the current weather in a given location\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"location\": {                         \"type\": \"string\",                         \"description\": \"The city and state, e.g. San Francisco, CA\",                     },                 },                 \"required\": [\"location\"],             }         }     } ] In\u00a0[27]: Copied! <pre>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather like in Jakarta?\"\n    }\n]\n</pre> messages = [     {         \"role\": \"user\",         \"content\": \"What's the weather like in Jakarta?\"     } ] In\u00a0[28]: Copied! <pre>response = get_completion(messages, tools=get_weather_tools)\nprint(response)\n</pre> response = get_completion(messages, tools=get_weather_tools) print(response) <pre>ChatCompletionMessage(\n    content=None,\n    refusal=None,\n    role='assistant',\n    function_call=None,\n    tool_calls=[\n        ChatCompletionMessageToolCall(\n            id='call_q0ny',\n            function=Function(arguments='{\"location\": \"Jakarta\"}', name='get_current_weather'),\n            type='function'\n        )\n    ]\n)\n</pre> In\u00a0[30]: Copied! <pre>args = json.loads(response.tool_calls[0].function.arguments)\nprint(args)\n</pre>  args = json.loads(response.tool_calls[0].function.arguments) print(args) <pre>{'location': 'Jakarta'}\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Will only work if api key is added\nget_current_weather(**args)\n</pre> ## Will only work if api key is added get_current_weather(**args) In\u00a0[\u00a0]: Copied! <pre># !pip install -U duckduckgo_search\n</pre> # !pip install -U duckduckgo_search In\u00a0[5]: Copied! <pre>from duckduckgo_search import DDGS\n\nresults = DDGS().text(\"Python Programming\", max_results=5)\nprint(results)\n</pre> from duckduckgo_search import DDGS  results = DDGS().text(\"Python Programming\", max_results=5) print(results) <pre>[\n    {\n        'title': 'Welcome to Python.org',\n        'href': 'https://www.python.org/',\n        'body': 'The mission of the Python Software Foundation is to promote, protect, and advance the Python \nprogramming language, and to support and facilitate the growth of a diverse and international community of Python \nprogrammers. Learn more. Become a Member Donate to the PSF. The official home of the Python Programming Language.'\n    },\n    {\n        'title': 'Python For Beginners | Python.org',\n        'href': 'https://www.python.org/about/gettingstarted/',\n        'body': 'Learn how to get started with Python, a popular and easy-to-use programming language. Find out how\nto install, edit, and use Python, and explore its libraries, documentation, and community resources.'\n    },\n    {\n        'title': 'Python Tutorial - W3Schools',\n        'href': 'https://www.w3schools.com/python/',\n        'body': 'W3Schools offers a comprehensive and interactive Python tutorial with examples, exercises, \nquizzes, and references. You can also download Python and get certified by completing the PYTHON course.'\n    },\n    {\n        'title': 'The Python Tutorial \u2014 Python 3.13.0 documentation',\n        'href': 'https://docs.python.org/3/tutorial/index.html',\n        'body': 'This tutorial introduces the basic concepts and features of the Python language and system, with \nexamples and exercises. It covers topics such as data structures, modules, classes, exceptions, and more.'\n    },\n    {\n        'title': 'Python (programming language) - Wikipedia',\n        'href': 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n        'body': 'Python is a high-level, general-purpose programming language.Its design philosophy emphasizes code\nreadability with the use of significant indentation. [32]Python is dynamically typed and garbage-collected.It \nsupports multiple programming paradigms, including structured (particularly procedural), object-oriented and \nfunctional programming.It is often described as a \"batteries included ...'\n    }\n]\n</pre> In\u00a0[49]: Copied! <pre>## Creating a function around duck duck go search \n\ndef web_search(search_query : str, max_results):\n    try:\n        from duckduckgo_search import DDGS\n    except:\n        assert \"duckduckgo_search package not found please install using `pip install -U duckduckgo_search` \"\n\n    results = DDGS().text(search_query, max_results=int(max_results))\n    return results\n</pre> ## Creating a function around duck duck go search   def web_search(search_query : str, max_results):     try:         from duckduckgo_search import DDGS     except:         assert \"duckduckgo_search package not found please install using `pip install -U duckduckgo_search` \"      results = DDGS().text(search_query, max_results=int(max_results))     return results In\u00a0[50]: Copied! <pre>websearch_tool = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"web_search\",\n            \"description\": \"searches the web and returns top k results\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"search_query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The most optimal search query that will go into the search engine\",\n                    },\n                    \"max_results\": {\n                        \"type\": \"integer\",\n                        \"description\": \"the number of results based on complexity of query\",\n                    },\n                },\n                \"required\": [\"search_query\"],\n            },\n        },   \n    }\n]\n</pre> websearch_tool = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"web_search\",             \"description\": \"searches the web and returns top k results\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"search_query\": {                         \"type\": \"string\",                         \"description\": \"The most optimal search query that will go into the search engine\",                     },                     \"max_results\": {                         \"type\": \"integer\",                         \"description\": \"the number of results based on complexity of query\",                     },                 },                 \"required\": [\"search_query\"],             },         },        } ] In\u00a0[51]: Copied! <pre>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"what is Nvidia current Stock Price\"\n    }\n]\n</pre> messages = [     {         \"role\": \"user\",         \"content\": \"what is Nvidia current Stock Price\"     } ] In\u00a0[52]: Copied! <pre>response = get_completion(messages)\nprint(response)\n</pre> response = get_completion(messages) print(response) <pre>ChatCompletionMessage(\n    content=\"I'm not able to provide real-time information or current stock prices. However, I can suggest some \nways for you to find the current stock price of Nvidia.\\n\\n1. **Check online stock market platforms**: You can \ncheck websites like Yahoo Finance, Google Finance, or Bloomberg to get the current stock price of Nvidia \n(NVDA).\\n2. **Use a stock market app**: You can download a stock market app like Robinhood, Fidelity, or E\\\\*TRADE \nto get real-time stock prices.\\n3. **Visit Nvidia's investor relations website**: Nvidia's investor relations \nwebsite may also provide information on the company's current stock price.\\n\\nPlease note that stock prices can \nfluctuate rapidly and may be different by the time you check.\",\n    refusal=None,\n    role='assistant',\n    function_call=None,\n    tool_calls=None\n)\n</pre> In\u00a0[53]: Copied! <pre>response = get_completion(messages , tools=websearch_tool)\nargs = json.loads(response.tool_calls[0].function.arguments)\n\nprint(args)\n</pre> response = get_completion(messages , tools=websearch_tool) args = json.loads(response.tool_calls[0].function.arguments)  print(args) <pre>{'search_query': 'Nvidia current stock price', 'max_results': '1'}\n</pre> In\u00a0[54]: Copied! <pre>print(web_search(**args))\n</pre> print(web_search(**args)) <pre>[\n    {\n        'title': 'NVIDIA Corporation (NVDA) Stock Price, News, Quote &amp; History - Yahoo ...',\n        'href': 'https://finance.yahoo.com/quote/NVDA/',\n        'body': 'Find the latest NVIDIA Corporation (NVDA) stock quote, history, news and other vital information \nto help you with your stock trading and investing. ... Current Quarterly Annual . As of 10/14/2024 ...'\n    }\n]\n</pre> In\u00a0[57]: Copied! <pre>def calculate(operation, *numbers):\n    \"\"\"\n    Perform a calculation on a variable number of inputs.\n    \n    :param operation: String indicating the operation ('add', 'subtract', 'multiply', 'divide')\n    :param numbers: Variable number of numeric inputs\n    :return: Result of the calculation\n    \"\"\"\n    if not numbers:\n        return \"Error: No numbers provided\"\n    \n    result = numbers[0]\n    for num in numbers[1:]:\n        if operation == 'add':\n            result += num\n        elif operation == 'subtract':\n            result -= num\n        elif operation == 'multiply':\n            result *= num\n        elif operation == 'divide':\n            if num == 0:\n                return \"Error: Division by zero\"\n            result /= num\n        else:\n            return \"Error: Invalid operation\"\n    \n    return result\n\n# Function schema for the calculator\ncalculator_tool = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate\",\n            \"description\": \"Perform a calculation on multiple numbers\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"operation\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n                        \"description\": \"The mathematical operation to perform\",\n                    },\n                    \"numbers\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"number\"\n                        },\n                        \"description\": \"The numbers to perform the operation on\",\n                    },\n                },\n                \"required\": [\"operation\", \"numbers\"],\n            },\n        },\n    }\n]\n\n# Example usage\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Calculate the sum of 5, 10, and 15\"\n    }\n]\n\n\nresponse = get_completion(messages , tools=calculator_tool)\n\n# Parse the arguments and call the function\nargs = json.loads(response.tool_calls[0].function.arguments)\nresult = calculate(args[\"operation\"], *args[\"numbers\"])\nprint(f\"Result: {result}\")\n</pre> def calculate(operation, *numbers):     \"\"\"     Perform a calculation on a variable number of inputs.          :param operation: String indicating the operation ('add', 'subtract', 'multiply', 'divide')     :param numbers: Variable number of numeric inputs     :return: Result of the calculation     \"\"\"     if not numbers:         return \"Error: No numbers provided\"          result = numbers[0]     for num in numbers[1:]:         if operation == 'add':             result += num         elif operation == 'subtract':             result -= num         elif operation == 'multiply':             result *= num         elif operation == 'divide':             if num == 0:                 return \"Error: Division by zero\"             result /= num         else:             return \"Error: Invalid operation\"          return result  # Function schema for the calculator calculator_tool = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"calculate\",             \"description\": \"Perform a calculation on multiple numbers\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"operation\": {                         \"type\": \"string\",                         \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],                         \"description\": \"The mathematical operation to perform\",                     },                     \"numbers\": {                         \"type\": \"array\",                         \"items\": {                             \"type\": \"number\"                         },                         \"description\": \"The numbers to perform the operation on\",                     },                 },                 \"required\": [\"operation\", \"numbers\"],             },         },     } ]  # Example usage messages = [     {         \"role\": \"user\",         \"content\": \"Calculate the sum of 5, 10, and 15\"     } ]   response = get_completion(messages , tools=calculator_tool)  # Parse the arguments and call the function args = json.loads(response.tool_calls[0].function.arguments) result = calculate(args[\"operation\"], *args[\"numbers\"]) print(f\"Result: {result}\") <pre>Result: 30\n</pre> In\u00a0[64]: Copied! <pre>def convert_units(value, from_unit, to_unit):\n    \"\"\"\n    Convert between different units of measurement.\n    \n    :param value: Numeric value to convert\n    :param from_unit: Original unit\n    :param to_unit: Target unit\n    :return: Converted value\n    \"\"\"\n    # Conversion factors (relative to base units)\n    units = {\n        \"m\": 1,\n        \"cm\": 0.01,\n        \"km\": 1000,\n        \"kg\": 1,\n        \"g\": 0.001,\n        \"l\": 1,\n        \"ml\": 0.001\n    }\n    \n    if from_unit not in units or to_unit not in units:\n        return \"Error: Invalid unit\"\n    \n    # Convert to base unit, then to target unit\n    base_value = value * units[from_unit]\n    converted_value = base_value / units[to_unit]\n    \n    return converted_value\n\n# Function schema for the units converter\nconverter_tool = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"convert_units\",\n            \"description\": \"Convert between different units of measurement\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"value\": {\n                        \"type\": \"number\",\n                        \"description\": \"The numeric value to convert\",\n                    },\n                    \"from_unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"m\", \"cm\", \"km\", \"kg\", \"g\", \"l\", \"ml\"],\n                        \"description\": \"The original unit of measurement\",\n                    },\n                    \"to_unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"m\", \"cm\", \"km\", \"kg\", \"g\", \"l\", \"ml\"],\n                        \"description\": \"The target unit of measurement\",\n                    },\n                },\n                \"required\": [\"value\", \"from_unit\", \"to_unit\"],\n            },\n        },\n    }\n]\n\n# Example usage\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Convert 5 kilometers to meters\"\n    }\n]\n\nresponse = get_completion(messages , tools=converter_tool)\n\n# Parse the arguments and call the function\nargs = json.loads(response.tool_calls[0].function.arguments)\nprint(args)\nresult = convert_units(args[\"value\"], args[\"from_unit\"], args[\"to_unit\"])\nprint(f\"Result: {result}\")\n</pre> def convert_units(value, from_unit, to_unit):     \"\"\"     Convert between different units of measurement.          :param value: Numeric value to convert     :param from_unit: Original unit     :param to_unit: Target unit     :return: Converted value     \"\"\"     # Conversion factors (relative to base units)     units = {         \"m\": 1,         \"cm\": 0.01,         \"km\": 1000,         \"kg\": 1,         \"g\": 0.001,         \"l\": 1,         \"ml\": 0.001     }          if from_unit not in units or to_unit not in units:         return \"Error: Invalid unit\"          # Convert to base unit, then to target unit     base_value = value * units[from_unit]     converted_value = base_value / units[to_unit]          return converted_value  # Function schema for the units converter converter_tool = [     {         \"type\": \"function\",         \"function\": {             \"name\": \"convert_units\",             \"description\": \"Convert between different units of measurement\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"value\": {                         \"type\": \"number\",                         \"description\": \"The numeric value to convert\",                     },                     \"from_unit\": {                         \"type\": \"string\",                         \"enum\": [\"m\", \"cm\", \"km\", \"kg\", \"g\", \"l\", \"ml\"],                         \"description\": \"The original unit of measurement\",                     },                     \"to_unit\": {                         \"type\": \"string\",                         \"enum\": [\"m\", \"cm\", \"km\", \"kg\", \"g\", \"l\", \"ml\"],                         \"description\": \"The target unit of measurement\",                     },                 },                 \"required\": [\"value\", \"from_unit\", \"to_unit\"],             },         },     } ]  # Example usage messages = [     {         \"role\": \"user\",         \"content\": \"Convert 5 kilometers to meters\"     } ]  response = get_completion(messages , tools=converter_tool)  # Parse the arguments and call the function args = json.loads(response.tool_calls[0].function.arguments) print(args) result = convert_units(args[\"value\"], args[\"from_unit\"], args[\"to_unit\"]) print(f\"Result: {result}\") <pre>{'value': 5, 'from_unit': 'km', 'to_unit': 'm'}\n</pre> <pre>Result: 5000.0\n</pre>"},{"location":"PromptEngineering/function_calling/#introduction-to-prompt-engineering-ii-function-calling","title":"Introduction to  Prompt Engineering II (Function calling)\u00b6","text":"<p>Below we are loading the necessary libraries, utilities, and configurations.</p> <p></p>"},{"location":"PromptEngineering/function_calling/#adding-two-numbers","title":"Adding two numbers\u00b6","text":""},{"location":"PromptEngineering/function_calling/#without-using-function-calling","title":"Without Using Function Calling\u00b6","text":""},{"location":"PromptEngineering/function_calling/#using-function-calling","title":"Using Function Calling\u00b6","text":""},{"location":"PromptEngineering/function_calling/#counting-the-number-of-r-in-strawberry","title":"Counting the number of \"r\" in strawberry\u00b6","text":""},{"location":"PromptEngineering/function_calling/#without-function-calling","title":"Without function calling\u00b6","text":""},{"location":"PromptEngineering/function_calling/#with-funciton-calling","title":"With funciton calling\u00b6","text":""},{"location":"PromptEngineering/function_calling/#getting-current-weather","title":"Getting Current Weather\u00b6","text":"<p>please get you api key from openweathermap</p>"},{"location":"PromptEngineering/function_calling/#duck-duck-go-search","title":"Duck Duck Go Search\u00b6","text":""},{"location":"PromptEngineering/function_calling/#without-function-calling","title":"Without Function Calling\u00b6","text":""},{"location":"PromptEngineering/function_calling/#with-function-calling","title":"With Function Calling\u00b6","text":""},{"location":"PromptEngineering/function_calling/#try-it-out-your-self","title":"Try it Out Your Self\u00b6","text":"<ul> <li>Create a simple calculator that can handle multiple opration and varied number of inputs</li> <li>Create a Units converter that can handle multiple standard units (m , cm , km , kg , liter ...).</li> </ul>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/","title":"Hands on with Adv Prompting","text":""},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#prompt-engineering-techniques","title":"Prompt Engineering Techniques","text":"<p>This guide to prompt engineering techniques is completely based on the amazing hands-on repository by NirDiamant. All credit for the content and Jupyter notebooks goes to the original repository: Prompt Engineering.</p> <p>I have added this here are it is an amazing learning resourse and full credits to Prompt Engineering Repo</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#fundamental-concepts","title":"\ud83c\udf31 Fundamental Concepts","text":"<ol> <li>Introduction to Prompt Engineering </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>A comprehensive introduction to the fundamental concepts of prompt engineering in the context of AI and language models.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Combines theoretical explanations with practical demonstrations, covering basic concepts, structured prompts, comparative analysis, and problem-solving applications.</p> <ol> <li>Basic Prompt Structures </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Explores two fundamental types of prompt structures: single-turn prompts and multi-turn prompts (conversations).</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Uses OpenAI's GPT model and LangChain to demonstrate single-turn and multi-turn prompts, prompt templates, and conversation chains.</p> <ol> <li>Prompt Templates and Variables </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Introduces creating and using prompt templates with variables, focusing on Python and the Jinja2 templating engine.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Covers template creation, variable insertion, conditional content, list processing, and integration with the OpenAI API.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#core-techniques","title":"\ud83d\udd27 Core Techniques","text":"<ol> <li>Zero-Shot Prompting </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Explores zero-shot prompting, allowing language models to perform tasks without specific examples or prior training.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Demonstrates direct task specification, role-based prompting, format specification, and multi-step reasoning using OpenAI and LangChain.</p> <ol> <li>Few-Shot Learning and In-Context Learning </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Covers Few-Shot Learning and In-Context Learning techniques using OpenAI's GPT models and the LangChain library.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Implements basic and advanced few-shot learning, in-context learning, and best practices for example selection and evaluation.</p> <ol> <li>Chain of Thought (CoT) Prompting </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Introduces Chain of Thought (CoT) prompting, encouraging AI models to break down complex problems into step-by-step reasoning processes.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Covers basic and advanced CoT techniques, applying them to various problem-solving scenarios and comparing results with standard prompts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#advanced-strategies","title":"\ud83d\udd0d Advanced Strategies","text":"<ol> <li>Self-Consistency and Multiple Paths of Reasoning </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Explores techniques for generating diverse reasoning paths and aggregating results to improve AI-generated answers.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Demonstrates designing diverse reasoning prompts, generating multiple responses, implementing aggregation methods, and applying self-consistency checks.</p> <ol> <li>Constrained and Guided Generation </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Focuses on techniques to set up constraints for model outputs and implement rule-based generation.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Uses LangChain's PromptTemplate for structured prompts, implements constraints, and explores rule-based generation techniques.</p> <ol> <li>Role Prompting </li> </ol> <p>#### Overview \ud83d\udd0e</p> <p>Explores assigning specific roles to AI models and crafting effective role descriptions.</p> <p>#### Implementation \ud83d\udee0\ufe0f</p> <p>Demonstrates creating role-based prompts, assigning roles to AI models, and refining role descriptions for various scenarios.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#advanced-implementations","title":"\ud83d\ude80 Advanced Implementations","text":"<ol> <li> <p>Task Decomposition in Prompts </p> </li> <li> <p>Prompt Chaining and Sequencing </p> </li> <li> <p>Instruction Engineering </p> </li> </ol>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview","title":"Overview \ud83d\udd0e","text":"<p>Explores techniques for breaking down complex tasks and chaining subtasks in prompts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers problem analysis, subtask definition, targeted prompt engineering, sequential execution, and result synthesis.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_1","title":"Overview \ud83d\udd0e","text":"<p>Demonstrates how to connect multiple prompts and build logical flows for complex AI-driven tasks.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_1","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Explores basic prompt chaining, sequential prompting, dynamic prompt generation, and error handling within prompt chains.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_2","title":"Overview \ud83d\udd0e","text":"<p>Focuses on crafting clear and effective instructions for language models, balancing specificity and generality.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_2","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers creating and refining instructions, experimenting with different structures, and implementing iterative improvement based on model responses.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#optimization-and-refinement","title":"\ud83c\udfa8 Optimization and Refinement","text":"<ol> <li> <p>Prompt Optimization Techniques </p> </li> <li> <p>Handling Ambiguity and Improving Clarity </p> </li> <li> <p>Prompt Length and Complexity Management </p> </li> </ol>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_3","title":"Overview \ud83d\udd0e","text":"<p>Explores advanced techniques for optimizing prompts, focusing on A/B testing and iterative refinement.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_3","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Demonstrates A/B testing of prompts, iterative refinement processes, and performance evaluation using relevant metrics.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_4","title":"Overview \ud83d\udd0e","text":"<p>Focuses on identifying and resolving ambiguous prompts and techniques for writing clearer prompts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_4","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers analyzing ambiguous prompts, implementing strategies to resolve ambiguity, and exploring techniques for writing clearer prompts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_5","title":"Overview \ud83d\udd0e","text":"<p>Explores techniques for managing prompt length and complexity when working with large language models.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_5","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Demonstrates techniques for balancing detail and conciseness, and strategies for handling long contexts including chunking, summarization, and iterative processing.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#specialized-applications","title":"\ud83d\udee0\ufe0f Specialized Applications","text":"<ol> <li> <p>Negative Prompting and Avoiding Undesired Outputs </p> </li> <li> <p>Prompt Formatting and Structure </p> </li> <li> <p>Prompts for Specific Tasks </p> </li> </ol>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_6","title":"Overview \ud83d\udd0e","text":"<p>Explores negative prompting and techniques for avoiding undesired outputs from large language models.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_6","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers basic negative examples, explicit exclusions, constraint implementation using LangChain, and methods for evaluating and refining negative prompts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_7","title":"Overview \ud83d\udd0e","text":"<p>Explores various prompt formats and structural elements, demonstrating their impact on AI model responses.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_7","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Demonstrates creating various prompt formats, incorporating structural elements, and comparing responses from different prompt structures.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_8","title":"Overview \ud83d\udd0e","text":"<p>Explores the creation and use of prompts for specific tasks: text summarization, question-answering, code generation, and creative writing.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_8","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers designing task-specific prompt templates, implementing them using LangChain, executing with sample inputs, and analyzing outputs for each task type.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#advanced-applications","title":"\ud83c\udf0d Advanced Applications","text":"<ol> <li> <p>Multilingual and Cross-lingual Prompting </p> </li> <li> <p>Ethical Considerations in Prompt Engineering </p> </li> <li> <p>Prompt Security and Safety </p> </li> <li> <p>Evaluating Prompt Effectiveness </p> </li> </ol>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_9","title":"Overview \ud83d\udd0e","text":"<p>Explores techniques for designing prompts that work effectively across multiple languages and for language translation tasks.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_9","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers creating multilingual prompts, implementing language detection and adaptation, designing cross-lingual translation prompts, and handling various writing systems and scripts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_10","title":"Overview \ud83d\udd0e","text":"<p>Explores the ethical dimensions of prompt engineering, focusing on avoiding biases and creating inclusive and fair prompts.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_10","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers identifying biases in prompts, implementing strategies to create inclusive prompts, and methods to evaluate and improve the ethical quality of AI outputs.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_11","title":"Overview \ud83d\udd0e","text":"<p>Focuses on preventing prompt injections and implementing content filters in prompts for safe and secure AI applications.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#implementation_11","title":"Implementation \ud83d\udee0\ufe0f","text":"<p>Covers techniques for prompt injection prevention, content filtering implementation, and testing the effectiveness of security and safety measures.</p>"},{"location":"PromptEngineering/hand_on_with_advanced_prompt_engineering/#overview_12","title":"Overview \ud83d\udd0e","text":"<p>Explores methods and techniques for evaluating the effectiveness of prompts in AI language models.</p>"},{"location":"PromptEngineering/prompt_engineering/","title":"Prompting Hands On","text":"In\u00a0[4]: Copied! <pre># !pip install openai\n# !pip install python-dotenv\n# !pip install rich\n</pre> # !pip install openai # !pip install python-dotenv # !pip install rich <pre>Requirement already satisfied: rich in c:\\users\\adithya\\appdata\\roaming\\python\\python311\\site-packages (13.8.1)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in c:\\python311\\lib\\site-packages (from rich) (2.2.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in c:\\users\\adithya\\appdata\\roaming\\python\\python311\\site-packages (from rich) (2.14.0)\nRequirement already satisfied: mdurl~=0.1 in c:\\python311\\lib\\site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich) (0.1.2)\n</pre> <p>for the following guide we will be using groq - llama3-70b-8192 which is open ai api compatible</p> In\u00a0[5]: Copied! <pre>import os\nimport openai\nfrom rich import print\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nlocal_api_key = os.getenv(\"GROQ_API_KEY\")\n# openai.api_key = os.getenv(\"GROQ_API_KEY\")\n\nclient = openai.OpenAI(\n    base_url=\"https://api.groq.com/openai/v1\",\n    api_key=local_api_key\n)\n</pre> import os import openai from rich import print from dotenv import load_dotenv  load_dotenv()  local_api_key = os.getenv(\"GROQ_API_KEY\") # openai.api_key = os.getenv(\"GROQ_API_KEY\")  client = openai.OpenAI(     base_url=\"https://api.groq.com/openai/v1\",     api_key=local_api_key ) In\u00a0[6]: Copied! <pre># MODEL = \"gpt-3.5-turbo\"\n\n# response = client.chat.completions.create(\n#     model=MODEL,\n#     messages=[\n#         {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"},\n#         {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n#         {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"},\n#         {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}\n#     ],\n#     temperature=0,\n# )\n\nMODEL = \"llama3-8b-8192\"\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"},\n        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n        {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"},\n        {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}\n    ],\n    temperature=0,\n)\n\nprint(response)\n</pre> # MODEL = \"gpt-3.5-turbo\"  # response = client.chat.completions.create( #     model=MODEL, #     messages=[ #         {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"}, #         {\"role\": \"user\", \"content\": \"Hello, who are you?\"}, #         {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"}, #         {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"} #     ], #     temperature=0, # )  MODEL = \"llama3-8b-8192\"  response = client.chat.completions.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"},         {\"role\": \"user\", \"content\": \"Hello, who are you?\"},         {\"role\": \"assistant\", \"content\": \"Greeting! I am an AI research assistant. How can I help you today?\"},         {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}     ],     temperature=0, )  print(response) <pre>ChatCompletion(\n    id='chatcmpl-6b95d260-1414-4fbb-ba11-5a0fa9a7ab1d',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content=\"The creation of black holes is a fascinating topic in the realm of astrophysics. A black \nhole is a region in spacetime where the gravitational pull is so strong that nothing, including light, can escape. \nThe formation of a black hole typically occurs when a massive star undergoes a catastrophic collapse, resulting in \na singularity.\\n\\nThe process begins when a massive star, typically with a mass greater than three times that of \nthe sun, exhausts its fuel supply and reaches the end of its life cycle. As the star's core contracts, its density \nincreases, causing a massive amount of matter to be compressed into an incredibly small region. This compression \nreleases an enormous amount of energy, causing a supernova explosion.\\n\\nIf the star is massive enough, the core \nwill collapse into a singularity, creating a black hole. The point of singularity is characterized by infinite \ndensity and zero volume, where the laws of physics as we know them break down. The event horizon, which marks the \nboundary of the black hole, is the point of no return, beyond which anything that enters cannot escape.\\n\\nThere \nare several ways in which black holes can form, including:\\n\\n1. Stellar collapse: As mentioned earlier, the \ncollapse of a massive star can create a black hole.\\n2. Mergers: The merger of two neutron stars or black holes can\nalso create a more massive black hole.\\n3. Primordial black holes: Some theories suggest that black holes could \nhave formed in the early universe, during the Big Bang, through the collapse of density fluctuations.\\n\\nThe \ndetection of black holes is challenging due to their small size and the fact that they do not emit any radiation. \nHowever, scientists have developed various methods to detect black holes, including:\\n\\n1. X-ray and gamma-ray \nobservations: Telescopes can detect X-rays and gamma-rays emitted by hot gas swirling around black holes.\\n2. Radio\nand optical observations: Radio and optical telescopes can detect radiation emitted by matter as it spirals into a \nblack hole.\\n3. Gravitational waves: The detection of gravitational waves by LIGO and VIRGO collaboration in 2015 \nprovided strong evidence for the existence of black holes.\\n\\nIn conclusion, the creation of black holes is a \ncomplex and fascinating process that involves the collapse of massive stars or the merger of compact objects. The \ndetection of black holes is an active area of research, with scientists using a variety of methods to study these \nenigmatic objects.\",\n                refusal=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=None\n            )\n        )\n    ],\n    created=1729072028,\n    model='llama3-8b-8192',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint='fp_6a6771ae9c',\n    usage=CompletionUsage(\n        completion_tokens=488,\n        prompt_tokens=76,\n        total_tokens=564,\n        queue_time=0.015080524,\n        prompt_time=0.009203864,\n        completion_time=0.406666667,\n        total_time=0.415870531\n    ),\n    x_groq={'id': 'req_01jaabjpmke6gbkr1fm4rkj769'}\n)\n</pre> In\u00a0[16]: Copied! <pre>print(response.choices[0].message.content)\n</pre> print(response.choices[0].message.content) <pre>The creation of black holes is a fascinating topic in the realm of astrophysics. A black hole is a region in \nspacetime where the gravitational pull is so strong that nothing, including light, can escape. The formation of a \nblack hole typically occurs when a massive star undergoes a catastrophic collapse, resulting in a singularity.\n\nThe process begins when a massive star, typically with a mass greater than three times that of the sun, exhausts \nits fuel supply and reaches the end of its life cycle. As the star's core contracts, its density increases, causing\na massive amount of matter to be compressed into an incredibly small region. This compression releases an enormous \namount of energy, causing a supernova explosion.\n\nIf the star is massive enough, the core will collapse into a singularity, creating a black hole. The point of \nsingularity is characterized by infinite density and zero volume, where the laws of physics as we know them break \ndown. The event horizon, which marks the boundary of the black hole, is the point of no return, beyond which \nanything that enters cannot escape.\n\nThere are several ways in which black holes can form, including:\n\n1. Stellar collapse: As mentioned earlier, the collapse of a massive star can create a black hole.\n2. Mergers: The merger of two neutron stars or black holes can also create a more massive black hole.\n3. Primordial black holes: Some theories suggest that black holes could have formed in the early universe, during \nthe Big Bang, through the collapse of density fluctuations.\n\nThe detection of black holes is challenging due to their small size and the fact that they do not emit any \nradiation. However, scientists have developed various methods to detect black holes, including:\n\n1. X-ray and gamma-ray observations: Telescopes can detect X-rays and gamma-rays emitted by hot gas swirling around\nblack holes.\n2. Radio and optical observations: Radio and optical telescopes can detect radiation emitted by matter as it \nspirals into a black hole.\n3. Gravitational waves: The detection of gravitational waves by LIGO and VIRGO collaboration in 2015 provided \nstrong evidence for the existence of black holes.\n\nIn conclusion, the creation of black holes is a complex and fascinating process that involves the collapse of \nmassive stars or the merger of compact objects. The detection of black holes is an active area of research, with \nscientists using a variety of methods to study these enigmatic objects.\n</pre> In\u00a0[38]: Copied! <pre>def get_completion(messages, model=\"llama-3.2-90b-text-preview\", temperature=0, max_tokens=4000):\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    \n    return response.choices[0].message.content\n</pre> def get_completion(messages, model=\"llama-3.2-90b-text-preview\", temperature=0, max_tokens=4000):     response = client.chat.completions.create(         model=model,         messages=messages,         temperature=temperature,         max_tokens=max_tokens,     )          return response.choices[0].message.content In\u00a0[39]: Copied! <pre>prompt = \"The sky is\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"The sky is\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>blue.\n</pre> In\u00a0[40]: Copied! <pre>response = get_completion(message,temperature=0.5)\nprint(response)\n</pre> response = get_completion(message,temperature=0.5) print(response) <pre>blue.\n</pre> In\u00a0[20]: Copied! <pre>prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \n\nExplain the above in one sentence:\"\"\"\n\nmessage = [\n    {   \n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message, temperature=0)\nprint(response)\n</pre> prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.   Explain the above in one sentence:\"\"\"  message = [     {            \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message, temperature=0) print(response) <pre>Antibiotics are medications that kill or prevent bacterial growth, allowing the body's immune system to fight off \ninfections, but are ineffective against viral infections and can lead to antibiotic resistance if used \ninappropriately.\n</pre> In\u00a0[21]: Copied! <pre>prompt = \"\"\"\nYour task is to summarize an abstract into one sentence. \n\nAbstract: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n\"\"\"\n\nmessage = [\n    {   \n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message, temperature=0)\nprint(response)\n</pre> prompt = \"\"\" Your task is to summarize an abstract into one sentence.   Abstract: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \"\"\"  message = [     {            \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message, temperature=0) print(response) <pre>Antibiotics are medications that kill or prevent bacterial growth, allowing the body's immune system to fight off \ninfections, but are ineffective against viral infections and can lead to antibiotic resistance if used \ninappropriately.\n</pre> In\u00a0[22]: Copied! <pre>prompt = \"\"\"\nYour task is to summarize an abstract into one sentence. \n\nAvoid technical jargon and explain it in the simplest of words.\n\nAbstract: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n\"\"\"\n\nmessage = [\n    {   \n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message, temperature=0)\nprint(response)\n</pre> prompt = \"\"\" Your task is to summarize an abstract into one sentence.   Avoid technical jargon and explain it in the simplest of words.  Abstract: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \"\"\"  message = [     {            \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message, temperature=0) print(response) <pre>Antibiotics are medicines that help the body fight off bacterial infections by killing or stopping the growth of \nthe bacteria, and they come in different forms such as pills, liquids, or injections.\n</pre> In\u00a0[23]: Copied! <pre>prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n\nContext: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n\nQuestion: What was OKT3 originally sourced from?\n\nAnswer:\n\"\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.  Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.  Question: What was OKT3 originally sourced from?  Answer: \"\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>Mice\n</pre> In\u00a0[24]: Copied! <pre>user_input = \"I think the food was okay\"\n\nprompt = \"\"\"Classify the text into neutral, negative or positive. The input text will be delimited by ```\n\nText: ```{user_input}```\n\nSentiment:\"\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt.format(user_input=user_input)\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> user_input = \"I think the food was okay\"  prompt = \"\"\"Classify the text into neutral, negative or positive. The input text will be delimited by ```  Text: ```{user_input}```  Sentiment:\"\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt.format(user_input=user_input)     } ]  response = get_completion(message) print(response) <pre>Neutral\n</pre> In\u00a0[25]: Copied! <pre>prompt = \"\"\"Your task is to extract model names from machine learning paper abstracts. Your response is an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"].\n\nAbstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI). However, the expensive training and deployment of LLMs present challenges to transparent and open academic research. To address these issues, this project open-sources the Chinese LLaMA and Alpaca\u2026\n\nTags:\"\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\"\"Your task is to extract model names from machine learning paper abstracts. Your response is an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"].  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI). However, the expensive training and deployment of LLMs present challenges to transparent and open academic research. To address these issues, this project open-sources the Chinese LLaMA and Alpaca\u2026  Tags:\"\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>[\"LLaMA\", \"Alpaca\"]\n</pre> In\u00a0[26]: Copied! <pre>prompt = \"\"\"Translate the following from English to Spanish:\n\u201cGlad to be here!\u201d\n\"\"\"\n\nmessage = [\n    {   \n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\"\"Translate the following from English to Spanish: \u201cGlad to be here!\u201d \"\"\"  message = [     {            \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>The translation of \"Glad to be here!\" from English to Spanish is:\n\n\u00a1Encantado de estar aqu\u00ed!\n\nNote: If you want to use a more informal tone, you can use:\n\n\u00a1Estoy encantado de estar aqu\u00ed!\n\nOr:\n\n\u00a1Me alegra estar aqu\u00ed!\n</pre> In\u00a0[27]: Copied! <pre>prompt = \"\"\"Translate the following from happy to super excited:\n\u201cGlad to be here!\u201d\n\"\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\"\"Translate the following from happy to super excited: \u201cGlad to be here!\u201d \"\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>Here's the translation:\n\n\"WOOHOO! Thrilled to be here and can't wait to get started!\"\n</pre> In\u00a0[28]: Copied! <pre>prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n\nHuman: Hello, who are you?\nAI: Greeting! I am an AI research assistant. How can I help you today?\nHuman: Can you tell me about the creation of black holes?\nAI:\"\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.  Human: Hello, who are you? AI: Greeting! I am an AI research assistant. How can I help you today? Human: Can you tell me about the creation of black holes? AI:\"\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>A fascinating topic! The creation of black holes is a complex process that involves the collapse of massive stars \nor the merger of compact objects such as neutron stars or black holes themselves.\n\nIn the case of stellar-mass black holes, they are formed when a massive star with a mass at least three times that \nof the sun runs out of fuel and collapses under its own gravity. If the star is massive enough, its gravity will \novercome the pressure of its own nuclear reactions, causing a supernova explosion. However, if the star is not \nmassive enough to trigger a supernova, it will collapse directly into a black hole.\n\nThe collapse is a rapid process, occurring over a timescale of milliseconds. As the star collapses, its density and\ngravity increase, causing a singularity to form at its center. The point of singularity is characterized by \ninfinite density and zero volume, and it is surrounded by an event horizon, which marks the boundary beyond which \nnothing, not even light, can escape the gravitational pull of the black hole.\n\nIn the case of supermassive black holes, they are thought to have formed in the early universe through the merger \nof smaller black holes or the collapse of a massive cloud of gas and dust. These black holes can have masses \nmillions or even billions of times that of the sun and are found at the centers of many galaxies, including our own\nMilky Way.\n\nIt's worth noting that the formation of black holes is still an active area of research, and there are many open \nquestions and uncertainties in our current understanding of the process. However, the basic principles outlined \nabove provide a good starting point for understanding how black holes are created.\n</pre> In\u00a0[29]: Copied! <pre>system_message = \"\"\"\nThe following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n\"\"\"\n\nprompt = \"\"\"\nHuman: Hello, who are you?\nAI: Greeting! I am an AI research assistant. How can I help you today?\nHuman: Can you tell me about the creation of black holes?\nAI:\n\"\"\"\n\nmessages = [\n    {   \n        \"role\": \"system\",\n        \"content\": system_message\n    },\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(messages)\nprint(response)\n</pre> system_message = \"\"\" The following is a conversation with an AI research assistant. The assistant tone is technical and scientific. \"\"\"  prompt = \"\"\" Human: Hello, who are you? AI: Greeting! I am an AI research assistant. How can I help you today? Human: Can you tell me about the creation of black holes? AI: \"\"\"  messages = [     {            \"role\": \"system\",         \"content\": system_message     },     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(messages) print(response) <pre>A fascinating topic! The creation of black holes is a complex and multifaceted process that has been extensively \nstudied in the fields of astrophysics and theoretical physics.\n\nBlack holes are regions in spacetime where the gravitational pull is so strong that nothing, including light, can \nescape once it falls within a certain radius, known as the event horizon. The formation of a black hole typically \noccurs when a massive star undergoes a catastrophic collapse, resulting in a singularity at its center.\n\nThere are several ways in which a massive star can collapse to form a black hole. One scenario is known as the \ndirect collapse, where the star's core collapses under its own gravity, causing a massive amount of matter to be \ncompressed into an incredibly small region. This compression creates an intense gravitational field, which warps \nthe fabric of spacetime around the star, forming the event horizon.\n\nAnother scenario is the indirect collapse, where the star undergoes a supernova explosion, expelling a significant \namount of its mass and energy. If the star is massive enough, the remaining core can collapse into a black hole.\n\nThe formation of a black hole also depends on the star's mass and metallicity. More massive stars are more likely \nto form black holes, as they have a greater amount of energy to be released during their collapse. Additionally, \nstars with lower metallicity (i.e., fewer elements heavier than hydrogen and helium) are more likely to form black \nholes, as they have a lower opacity and can collapse more easily.\n\nOnce a black hole is formed, it can continue to grow through the accretion of surrounding matter and energy. This \nprocess can lead to the formation of an accretion disk, which is a swirling disk of hot, dense gas that surrounds \nthe black hole. The accretion disk can emit intense radiation, including X-rays and gamma rays, which can be \ndetected by telescopes.\n\nIn summary, the creation of black holes is a complex process that involves the collapse of massive stars, the \ncompression of matter into an incredibly small region, and the warping of spacetime around the star. The formation \nof black holes depends on various factors, including the star's mass, metallicity, and the presence of an accretion\ndisk.\n\nWould you like me to elaborate on any specific aspect of black hole formation or evolution?\n</pre> In\u00a0[30]: Copied! <pre>system_message = \"\"\"\nThe following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n\"\"\"\n\nuser_message_1 = \"\"\"\nHello, who are you?\n\"\"\"\n\nai_message_1 = \"\"\"\nGreeting! I am an AI research assistant. How can I help you today?\n\"\"\"\n\nprompt = \"\"\"\nHuman: Can you tell me about the creation of blackholes?\nAI:\n\"\"\"\n\nmessages = [\n    {   \n        \"role\": \"system\",\n        \"content\": system_message\n    },\n    {\n        \"role\": \"user\",\n        \"content\": user_message_1\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": ai_message_1\n\n    },\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(messages)\nprint(response)\n</pre> system_message = \"\"\" The following is a conversation with an AI research assistant. The assistant tone is technical and scientific. \"\"\"  user_message_1 = \"\"\" Hello, who are you? \"\"\"  ai_message_1 = \"\"\" Greeting! I am an AI research assistant. How can I help you today? \"\"\"  prompt = \"\"\" Human: Can you tell me about the creation of blackholes? AI: \"\"\"  messages = [     {            \"role\": \"system\",         \"content\": system_message     },     {         \"role\": \"user\",         \"content\": user_message_1     },     {         \"role\": \"assistant\",         \"content\": ai_message_1      },     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(messages) print(response) <pre>The creation of black holes is a fascinating topic in astrophysics. A black hole is a region in space where the \ngravitational pull is so strong that nothing, including light, can escape. It is formed when a massive star \ncollapses in on itself and its gravity becomes so strong that it warps the fabric of spacetime around it.\n\nThe process of black hole formation typically begins with the death of a massive star. When a star with a mass at \nleast three times that of the sun runs out of fuel, it collapses under its own gravity. If the star is massive \nenough, its gravity will be so strong that it will cause a massive amount of matter to be compressed into an \nincredibly small point, known as a singularity.\n\nAt this point, the star's gravity becomes so strong that it warps the fabric of spacetime around it, creating a \nboundary called the event horizon. Once something crosses the event horizon, it is trapped by the black hole's \ngravity and cannot escape.\n\nThe formation of a black hole is a complex process that involves the collapse of a massive star, the creation of a \nsingularity, and the warping of spacetime around it. It is a fascinating area of study in astrophysics, and \nscientists continue to learn more about black holes and their role in the universe.\n\nWould you like to know more about black holes or is there something specific you would like to know?\n</pre> In\u00a0[31]: Copied! <pre>prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>Here is the MySQL query to get all students in the Computer Science Department:\n\n```sql\nSELECT s.*\nFROM students s\nJOIN departments d ON s.DepartmentId = d.DepartmentId\nWHERE d.DepartmentName = 'Computer Science';\n```\n\nThis query joins the `students` table with the `departments` table on the `DepartmentId` column. Then it filters \nthe results to only include rows where the `DepartmentName` is 'Computer Science'. The `s.*` in the `SELECT` clause\nmeans to select all columns from the `students` table.\n</pre> In\u00a0[32]: Copied! <pre>prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n\nSolve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n\nmessage = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(message)\nprint(response)\n</pre> prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.   Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"  message = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(message) print(response) <pre>Let's break down the problem step by step:\n\nStep 1: Identify the odd numbers in the group:\n\nThe odd numbers in the group are: 15, 5, 13, 7, and 1.\n\nStep 2: Add the odd numbers:\n\n15 + 5 + 13 + 7 + 1 = 42\n\nStep 3: Determine whether the result is odd or even:\n\nThe result, 42, is an even number.\n\nTherefore, the statement \"The odd numbers in this group add up to an even number\" is true.\n</pre> In\u00a0[36]: Copied! <pre>prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nA: The answer is False.\n\nThe odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\nA: The answer is True.\n\nThe odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\nA: The answer is True.\n\nThe odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\nA: The answer is False.\n\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \nA:\"\"\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(messages)\nprint(response)\n</pre> prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. A: The answer is False.  The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24. A: The answer is True.  The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24. A: The answer is True.  The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2. A: The answer is False.  The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.  A:\"\"\"  messages = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(messages) print(response) <pre>Let's add up the odd numbers in the group: 15 + 5 + 13 + 7 + 1 = 41. Since 41 is an odd number, the statement is \nFalse.\n\nSo, the correct answer is:\n\nA: The answer is False.\n</pre> In\u00a0[35]: Copied! <pre>prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nA: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \nA:\"\"\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(messages)\nprint(response)\n</pre> prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.  The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.  A:\"\"\"  messages = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(messages) print(response) <pre>Let's add up the odd numbers: 15, 5, 13, 7, 1 = 41. The answer is False.\n</pre> In\u00a0[37]: Copied! <pre>prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n\nLet's think step by step.\"\"\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n]\n\nresponse = get_completion(messages)\nprint(response)\n</pre> prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?  Let's think step by step.\"\"\"  messages = [     {         \"role\": \"user\",         \"content\": prompt     } ]  response = get_completion(messages) print(response) <pre>Let's break it down step by step:\n\n1. You started with 10 apples.\n2. You gave 2 apples to the neighbor, so you have 10 - 2 = 8 apples left.\n3. You gave 2 apples to the repairman, so you have 8 - 2 = 6 apples left.\n4. You bought 5 more apples, so you now have 6 + 5 = 11 apples.\n5. You ate 1 apple, so you have 11 - 1 = 10 apples left.\n\nSo, you remain with 10 apples.\n</pre>"},{"location":"PromptEngineering/prompt_engineering/#introduction-to-prompt-engineering","title":"Introduction to Prompt Engineering\u00b6","text":"<p>Below we are loading the necessary libraries, utilities, and configurations.</p> <p></p>"},{"location":"PromptEngineering/prompt_engineering/#basic-prompt-example","title":"Basic prompt example:\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#text-summarization","title":"Text Summarization\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#question-answering","title":"Question Answering\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#text-classification","title":"Text Classification\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#information-extraction","title":"Information Extraction\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#machine-translation","title":"Machine Translation\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#tone-transformation","title":"Tone Transformation\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#role-playing","title":"Role Playing\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#code-generation","title":"Code Generation\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#reasoning","title":"Reasoning\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#advanced-prompting-techniques","title":"Advanced Prompting Techniques\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#few-shot-prompts","title":"Few-shot prompts\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#chain-of-throught-cot-prompting","title":"Chain-of-Throught (CoT) Prompting\u00b6","text":""},{"location":"PromptEngineering/prompt_engineering/#zero-shot-cot","title":"Zero Shot CoT\u00b6","text":""},{"location":"RAG/","title":"Everything about RAG","text":""},{"location":"RAG/#rag-retrieval-augmented-generation-system-implementation-guide","title":"RAG (Retrieval Augmented Generation) System Implementation Guide","text":"<p>Welcome to the comprehensive guide for implementing RAG systems! This repository provides a structured approach to building and optimizing Retrieval Augmented Generation systems, from basic implementations to advanced techniques.</p>"},{"location":"RAG/#repository-structure","title":"Repository Structure","text":""},{"location":"RAG/#core-modules","title":"Core Modules","text":""},{"location":"RAG/#fundamentals","title":"Fundamentals","text":"<ol> <li>RAG from Scratch<pre><code>- Complete implementation guide from ground up\n- RAG in 10 lines of code\n- Understanding embeddings and similarity\n- Basic requirements setup\n</code></pre> </li> </ol>"},{"location":"RAG/#basic-implementation-evaluation","title":"Basic Implementation &amp; Evaluation","text":"<ol> <li> <p>Basic RAG Implementation</p> <pre><code>- Basic server implementation\n- Jupyter notebook tutorials\n- Performance evaluation notebooks\n- Environment setup guide\n</code></pre> </li> <li> <p>BM25 RAG</p> <pre><code>- BM25 algorithm implementation\n- Application setup\n- Interactive notebook examples\n</code></pre> </li> <li> <p>Data Ingestion</p> <pre><code>- Data chunking strategies\n- Embedding generation\n- Batch processing examples\n- Data parsing techniques\n</code></pre> </li> <li> <p>RAG Evaluation</p> <pre><code>- RAGAS metrics implementation\n- Deepeval integration\n- TruLens evaluation\n- Test dataset examples\n</code></pre> </li> <li> <p>RAG Observability</p> <pre><code>- System monitoring setup\n- Performance tracking\n- Debug tools integration\n</code></pre> </li> </ol>"},{"location":"RAG/#advanced-techniques","title":"Advanced Techniques","text":"<ol> <li> <p>ReRanker RAG</p> <pre><code>- Result re-ranking implementation\n- Evaluation metrics\n- Performance optimization\n</code></pre> </li> <li> <p>Hybrid RAG</p> <pre><code>- Qdrant hybrid search implementation\n- Multiple retrieval method integration\n</code></pre> </li> <li> <p>Sentence Window RAG</p> <pre><code>- Context window optimization\n- Sentence-level retrieval\n</code></pre> </li> <li> <p>Auto Merging RAG</p> <pre><code>- Automatic content merging\n- Redundancy elimination\n</code></pre> </li> <li> <p>Advanced Query Processing</p> <pre><code>- HyDE (Hypothetical Document Embeddings)\n- Query transformation techniques\n- Query optimization strategies\n</code></pre> </li> </ol>"},{"location":"RAG/#specialized-implementations","title":"Specialized Implementations","text":"<ol> <li> <p>Self Query RAG</p> <pre><code>- Self-querying mechanisms\n- Query refinement techniques\n</code></pre> </li> <li> <p>RAG Fusion</p> <pre><code>- Multiple RAG model integration\n- Result fusion strategies\n</code></pre> </li> <li> <p>RAPTOR</p> <pre><code>- Advanced reasoning implementation\n- Performance optimization\n</code></pre> </li> <li> <p>ColBERT RAG</p> <pre><code>- ColBERT model integration\n- Ragatouille retriever implementation\n</code></pre> </li> <li> <p>Graph RAG</p> <pre><code>- Graph-based retrieval\n- Knowledge graph integration\n</code></pre> </li> <li> <p>Agnetic RAG</p> <pre><code>- Multi-document agent system\n- Domain-specific implementations\n</code></pre> </li> <li> <p>Vision RAG - GPT-4V integration - Multi-modal retrieval implementation</p> </li> <li> <p>CAG - Cache Augmentation Generation </p> </li> </ol>"},{"location":"RAG/#data-resources","title":"\ud83d\udcc2 Data Resources","text":"<p>Located in the <code>data/</code> directory:</p> <ul> <li>Markdown Documents (<code>md/</code>): Processed markdown versions of papers</li> <li>PDF Documents (<code>pdf/</code>): Original research papers and documentation</li> <li>Sample Database (<code>sample-lancedb/</code>): Example database implementation</li> </ul>"},{"location":"RAG/#implementation-techniques","title":"\ud83c\udfaf Implementation Techniques","text":""},{"location":"RAG/#implemented-features","title":"\u2705 Implemented Features","text":"<ol> <li>Simple RAG with vector store integration</li> <li>Context enrichment algorithms</li> <li>Multi-faceted filtering systems</li> <li>Fusion retrieval mechanisms</li> <li>Intelligent reranking</li> <li>Query transformation</li> <li>Hierarchical indexing</li> <li>HyDE implementation</li> <li>Dynamic chunk sizing</li> <li>Semantic chunking</li> <li>Context compression</li> <li>Explainable retrieval</li> <li>Graph RAG implementation</li> <li>RAPTOR integration</li> </ol>"},{"location":"RAG/#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<ul> <li>\ud83e\udd99 RAG Orchestration: Llama-index</li> <li>\ud83d\udd0d Vector Database: Qdrant</li> <li>\ud83d\udc41\ufe0f Observability: Arize Phoenix</li> <li>\ud83d\udcca Evaluation: RAGAS &amp; Deepeval</li> </ul>"},{"location":"RAG/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our contributing guidelines for more information.</p>"},{"location":"RAG/#references","title":"\ud83d\udcda References","text":"<p>This project builds upon research and implementations from various sources. See our acknowledgments section for detailed credits.</p>"},{"location":"RAG/#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>    Made with \u2764\ufe0f for the RAG community"},{"location":"RAG/00_RAG_Base/","title":"RAG for Noobs","text":""},{"location":"RAG/00_RAG_Base/#rag-from-scratch","title":"RAG from Scratch","text":"<pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Document Embedding\"\n        EM1{{Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; EM1\n        EM1 --&gt; D1[Embedding-1] &amp; D2[Embedding-2] &amp; D3[Embedding-3]\n    end\n\n    subgraph \"3. Indexing\"\n        D1 &amp; D2 &amp; D3 --&gt; E[(VectorDB)]\n    end\n\n    subgraph \"4. Query Processing\"\n        F[Query] --&gt; EM2{{Embedding Model}}\n        EM2 --&gt; G[Query Embedding]\n    end\n\n    subgraph \"5. Retrieval\"\n        G --&gt;|Similarity Search| E\n        E --&gt;|Top-K Retrieval| H[Relevant Chunks]\n    end\n\n    subgraph \"6. Context Formation\"\n        H --&gt; I[Query + Relevant Chunks]\n    end\n\n    subgraph \"7. Generation\"\n        I --&gt; J[LLM]\n        J --&gt; K[Response]\n    end\n\n    F --&gt; I</code></pre>"},{"location":"RAG/00_RAG_Base/#overview","title":"Overview","text":"<p>This guide walks you through creating a simple Retrieval-Augmented Generation (RAG) system using pure Python. We will use an embedding model and a language model (LLM) to retrieve relevant documents and generate responses based on a user's query.</p>"},{"location":"RAG/00_RAG_Base/#steps-involved","title":"Steps Involved","text":"<p>The whole process can be factored into two big steps:</p> <ol> <li>Knowledge Base Creation</li> <li>Generation Part</li> </ol>"},{"location":"RAG/00_RAG_Base/#knowledge-base-creation","title":"Knowledge Base Creation","text":"<p>To get started, you'll first need a knowledge base (documents, PDFs, wiki pages). This is the fodder for your language models (LLMs). The process involves:</p> <ul> <li>Chunking: Split the text into chunks of sub-documents to simplify ingestion.</li> <li>Embedding: Compute numerical embeddings for each chunk to understand the semantic similarity to queries.</li> <li>Storage: Store the embeddings in a way that allows quick retrieval. While a vector store/DB is often used, this tutorial shows that it's not essential.</li> </ul>"},{"location":"RAG/00_RAG_Base/#generation","title":"Generation","text":"<p>When a user query comes in, an embedding is computed for the query, and we retrieve the most relevant chunks from the knowledge base. These relevant chunks are appended to the initial user query, forming a context that is fed into the LLM to generate a response.</p>"},{"location":"RAG/00_RAG_Base/#1-the-setup","title":"1. The Setup","text":"<p>A bunch of packages that need to be installed before we get going.</p> <ul> <li><code>sentence-transformers</code>: For embedding the documents and queries.</li> <li><code>numpy</code>: For similarity comparisons.</li> <li><code>scipy</code>: For advanced similarity computations.</li> <li><code>wikipedia-api</code>: For loading a Wikipedia page as a knowledge base.</li> <li><code>textwrap</code>: For formatting output text.</li> </ul> <pre><code>!pip install -q sentence-transformers\n!pip install -q wikipedia-api\n!pip install -q numpy\n!pip install -q scipy\n</code></pre>"},{"location":"RAG/00_RAG_Base/#2-loading-the-embedding-model","title":"2. Loading the Embedding Model","text":"<p>Let's load the embedding model of our choice. In this tutorial, we are using the <code>gte-base-en-v1.5</code>.</p> <pre><code>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True)\n</code></pre>"},{"location":"RAG/00_RAG_Base/#about-the-model","title":"About the Model","text":"<p>The <code>gte-base-en-v1.5</code> model is an open-source English model provided by Alibaba's NLP team. It is part of the GTE (General Text Embeddings) family, designed for generating high-quality embeddings suitable for various natural language processing tasks. The model is optimized for capturing semantic meaning in English text, making it useful for tasks like sentence similarity, semantic search, and clustering. The <code>trust_remote_code=True</code> parameter allows the use of custom code associated with the model, ensuring that it functions as intended.</p>"},{"location":"RAG/00_RAG_Base/#3-fetch-text-content-from-wikipedia-and-prepare-it","title":"3. Fetch Text Content from Wikipedia and Prepare It","text":"<ul> <li>Start by loading a Wikipedia article as your knowledge base. The text will be split into manageable chunks (sub-documents), usually by paragraphs.</li> </ul> <pre><code>from wikipediaapi import Wikipedia\nwiki = Wikipedia('RAGBot/0.0', 'en')\ndoc = wiki.page('Hayao_Miyazaki').text\nparagraphs = doc.split('\\n\\n')  # chunking\n</code></pre> <ul> <li> <p>While there are a ton of chunking strategies available, many of them don't work as expected. It's best to review your knowledge base (KB) and determine which strategy suits it best. In this case, we'll chunk the document based on paragraphs.</p> </li> <li> <p>If you want to view how these chunks look, import the <code>textwrap</code> library, and enumerate over each paragraph to print them.</p> </li> </ul> <pre><code>import textwrap\n\nfor i, p in enumerate(paragraphs):\n    wrapped_text = textwrap.fill(p, width=100)\n\n    print(\"-----------------------------------------------------------------\")\n    print(wrapped_text)\n    print(\"-----------------------------------------------------------------\")\n</code></pre> <ul> <li>If your document contains images and tables, it is recommended to extract them separately and embed them using vision models.</li> </ul>"},{"location":"RAG/00_RAG_Base/#4-embed-the-document","title":"4. Embed the Document","text":"<ul> <li>Next, encode the text data (in this case, the <code>paragraphs</code>) into embeddings by calling the <code>encode</code> method on our model.</li> </ul> <pre><code>docs_embed = model.encode(paragraphs, normalize_embeddings=True)\n</code></pre> <ul> <li> <p>These embeddings are dense vector representations of text that capture semantic meaning, allowing the model to understand and process text in a mathematical form.</p> </li> <li> <p>We are normalizing the embeddings here.</p> </li> <li> <p>What is normalization? It's a process that adjusts the values of the embeddings to have a unit norm (i.e., the length of the vector is 1).</p> </li> <li> <p>Why normalize? Normalized embeddings ensure that the distance between vectors is primarily due to differences in direction rather than magnitude. This can improve the performance of models in tasks like similarity search, where you want to compare how \"close\" or \"similar\" different pieces of text are.</p> </li> <li> <p>The result, <code>docs_embed</code>, is a collection of vector representations of your text data, where each vector corresponds to a paragraph in the <code>paragraphs</code> list.</p> </li> <li> <p>The <code>shape</code> command gives the number of chunks and the dimension of each embedded vector. (Note that the size of the embedding vector depends on the type of embedding model.)</p> </li> </ul> <pre><code>docs_embed.shape\n</code></pre> <ul> <li>You can then view how the actual embeddings look, which is an array of normalized numbers.</li> </ul> <pre><code>docs_embed[0]\n</code></pre>"},{"location":"RAG/00_RAG_Base/#5-embed-the-query","title":"5. Embed the Query","text":"<p>Let's embed a sample user query in a similar fashion to how we embedded the document.</p> <pre><code>query = \"What was Studio Ghibli's first film?\"\nquery_embed = model.encode(query, normalize_embeddings=True)\n</code></pre> <p>You can check the shape of the query_embed to confirm the dimensions of the embedded query.</p> <pre><code>query_embed.shape\n</code></pre>"},{"location":"RAG/00_RAG_Base/#6-finding-the-closest-paragraphs-to-the-query","title":"6. Finding the Closest Paragraphs to the Query","text":"<p>One of the simplest ways to retrive of the most relevant chunks would be to compute the dot product of your document embedding and the query embedding.</p>"},{"location":"RAG/00_RAG_Base/#a-taking-dot-product","title":"a. Taking dot product","text":"<p>The dot product is a mathematical operation that multiplies corresponding elements of two vectors (or matrices) and sums the results. It is commonly used to measure the similarity between two vectors.</p> <p>(Notice that the transpose of the <code>query_embed</code> vector is taken for computing the dot product).</p> <pre><code>import numpy as np\nsimilarities = np.dot(docs_embed, query_embed.T)\n</code></pre>"},{"location":"RAG/00_RAG_Base/#b-understanding-the-dot-product-and-the-shape","title":"b. Understanding the Dot Product and the Shape","text":"<p>The .shape attribute of a NumPy array returns a tuple representing the dimensions of the array.</p> <pre><code>similarities.shape\n</code></pre> <p>The expected shape in this code would be:</p> <ul> <li> <p>If docs_embed has a shape of (n_docs, n_dim):</p> </li> <li> <p>n_docs is the number of documents.</p> </li> <li> <p>n_dim is the dimensionality of each document embedding.</p> </li> <li> <p>query_embed.T would have a shape of (n_dim, 1) since we are comparing against a single query.</p> </li> <li> <p>The resulting similarities array, after the dot product, will have a shape of (n_docs,), meaning it\u2019s a 1-dimensional array (a vector) with n_docs elements. Each element represents the similarity score between the query and one of the documents.</p> </li> <li> <p>Why Check the Shape? Ensuring that the shape is as expected (n_docs,) confirms that the dot product was performed correctly and that each document\u2019s similarity score was computed.</p> </li> </ul> <p>You can print the <code>similarities</code> array to inspect the similarity scores, where each value corresponds to a dot product result:</p> <pre><code>print(similarities)\n</code></pre>"},{"location":"RAG/00_RAG_Base/#c-dot-product-interpretation","title":"c. Dot Product Interpretation","text":"<p>The dot product between two vectors (embeddings) measures their similarity: Higher values indicate greater similarity between the query and the document. If the embeddings are normalized, these values are directly proportional to the cosine similarity between the vectors. If not normalized, they still indicate similarity, but also reflect the magnitudes of the embeddings.</p>"},{"location":"RAG/00_RAG_Base/#d-identifying-the-top-3-most-similar-documents","title":"d. Identifying the Top 3 Most Similar Documents","text":"<p>To identify the top 3 most similar documents based on the similarity scores, you can use the following code:</p> <pre><code>top_3_idx = np.argsort(similarities, axis=0)[-3:][::-1].tolist()\n</code></pre> <ul> <li> <p>np.argsort(similarities, axis=0): This function sorts the indices of the similarities array in ascending order. For example, if similarities = [0.1, 0.7, 0.4], np.argsort would return [0, 2, 1], where 0 is the index of the smallest value and 1 is the index of the largest value.</p> </li> <li> <p>[-3:]: This slice operation selects the indices of the top 3 highest similarity scores (the last 3 elements after sorting).</p> </li> <li> <p>[::-1]: This reverses the order, so the indices are now in descending order of similarity.</p> </li> <li> <p>tolist(): Converts the array of indices into a Python list.   Result: top_3_idx contains the indices of the top 3 most similar documents, in order of descending similarity.</p> </li> </ul>"},{"location":"RAG/00_RAG_Base/#e-extracting-the-most-similar-documents","title":"e. Extracting the Most Similar Documents","text":"<pre><code>most_similar_documents = [paragraphs[idx] for idx in top_3_idx]\n</code></pre> <ul> <li>List Comprehension: This line creates a list called most_similar_documents, which contains the actual paragraphs from the paragraphs list that correspond to the indices in top_3_idx.</li> <li>paragraphs[idx]: For each index in top_3_idx, this retrieves the corresponding paragraph.</li> </ul>"},{"location":"RAG/00_RAG_Base/#f-formatting-and-displaying-the-most-similar-documents","title":"f. Formatting and Displaying the Most Similar Documents","text":"<p>The CONTEXT variable is initially initialized as an empty string and will later be appended with the wrapped text of the most similar documents in an enumerate loop.</p> <pre><code>CONTEXT = \"\"\nfor i, p in enumerate(most_similar_documents):\n  wrapped_text = textwrap.fill(p, width=100)\n\n  print(\"-----------------------------------------------------------------\")\n  print(wrapped_text)\n  print(\"-----------------------------------------------------------------\")\n  CONTEXT += wrapped_text + \"\\n\\n\"\n</code></pre>"},{"location":"RAG/00_RAG_Base/#7-generating-a-response","title":"7. Generating a Response","text":"<p>So, now we have a query + relevant chunks, which together will be fed to the LLM.</p>"},{"location":"RAG/00_RAG_Base/#a-declare-a-query","title":"a. Declare a Query","text":"<pre><code>query = \"What was Studio Ghibli's first film?\"\n</code></pre>"},{"location":"RAG/00_RAG_Base/#b-create-a-prompt","title":"b. Create a Prompt","text":"<pre><code>prompt = f\"\"\"\nuse the following CONTEXT to answer the QUESTION at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nCONTEXT: {CONTEXT}\nQUESTION: {query}\n\n\"\"\"\n</code></pre>"},{"location":"RAG/00_RAG_Base/#c-set-up-openai","title":"c. Set up OpenAI","text":"<ul> <li>Install OpenAI to access and use LLMs.</li> </ul> <pre><code>!pip install -q openai\n</code></pre> <ul> <li>Enable access to the OpenAI API key (can be set up in secrets on Google Colab).</li> </ul> <pre><code>from google.colab import userdata\nuserdata.get('openai')\n\nimport openai\n</code></pre> <ul> <li>Create an OpenAI client.</li> </ul> <pre><code>from openai import OpenAI\nclient = OpenAI(api_key=userdata.get('openai'))\n</code></pre>"},{"location":"RAG/00_RAG_Base/#d-make-a-api-call-to-generate-a-response","title":"d. Make a API call to generate a response","text":"<pre><code>response = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"user\", \"content\": prompt},\n  ]\n)\n</code></pre> <ul> <li> <p>client.chat.completions.create:   This is a method call to create a new completion (response) using a chat-based language model.</p> </li> <li> <p>client: Represents the API client object connected to the service (here, OpenAI).</p> </li> <li> <p>chat.completions.create: Specifies that you're making a request to create a chat-based completion.</p> </li> </ul>"},{"location":"RAG/00_RAG_Base/#more-about-the-parameters-passed-to-the-method","title":"More about the parameters passed to the method","text":"<ul> <li> <p>model=\"gpt-4o\":   Specifies the model you want to use to generate the response.   \"gpt-4o\" refers to a specific variant of the GPT-4 model. Different models can have different behaviors, fine-tuning, or capabilities, so specifying the model is important to ensure you get the desired output.</p> </li> <li> <p>messages:   This parameter is a list of message objects that represent the conversation history. It allows the model to understand the context of the chat.   In this case, we're providing only one message in the list: <code>{\"role\": \"user\", \"content\": prompt}</code></p> </li> <li> <p>role:   \"user\" indicates the role of the message sender, who is interacting with the model.</p> </li> <li> <p>content:   This contains the actual text of the message sent by the user. The variable prompt holds this text, which the model will use as input to generate a response.</p> </li> </ul>"},{"location":"RAG/00_RAG_Base/#e-about-the-response-recieved","title":"e. About the response recieved","text":"<p>When you make a request to an API like OpenAI's GPT models to generate a chat completion, the response usually comes back in a structured format, often as a dictionary.</p> <p>This structure typically includes:</p> <ul> <li>choices: A list (array) containing different possible completions generated by the model. Each item in this list represents one possible completion or response.</li> <li>message: An object or dictionary within each choice that contains the actual content of the message generated by the model.</li> <li>content: The text content of the message, which is the actual response or completion generated by the model.</li> </ul>"},{"location":"RAG/00_RAG_Base/#f-printing-the-response","title":"f. Printing the response","text":"<pre><code>print(response.choices[0].message.content)\n</code></pre> <p>We select the first item in the choices list, then access the messages object within it. Finally we access the content field within the message, which contains the actual text generated by the model.</p>"},{"location":"RAG/00_RAG_Base/#conclusion","title":"Conclusion","text":"<p>And that brings us to an end on building a RAG system from scratch. It's highly recommended to build your intial RAG setup in pure Python to get a better understanding of how these systems work.</p>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/","title":"RAG from scratch","text":"<p>Load Data</p> In\u00a0[\u00a0]: Copied! <pre># !pip install -q sentence-transformers\n# !pip install -q wikipedia-api\n# !pip install -q numpy\n# !pip install -q scipy\n# !pip install openai\n# !pip install rich\n# !pip install pypdf2\n# !pip install gradio\n</pre> # !pip install -q sentence-transformers # !pip install -q wikipedia-api # !pip install -q numpy # !pip install -q scipy # !pip install openai # !pip install rich # !pip install pypdf2 # !pip install gradio In\u00a0[\u00a0]: Copied! <pre>import re\nimport os\nimport openai\nfrom rich import print\nfrom dotenv import load_dotenv\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport textwrap\nfrom wikipediaapi import Wikipedia\nimport PyPDF2\n</pre> import re import os import openai from rich import print from dotenv import load_dotenv from sentence_transformers import SentenceTransformer import numpy as np import textwrap from wikipediaapi import Wikipedia import PyPDF2 In\u00a0[\u00a0]: Copied! <pre>def load_document(file_path):\n    \"\"\"\n    Load document from a given file path. Supports PDF and text files.\n    \"\"\"\n    _, file_extension = os.path.splitext(file_path)\n    \n    if file_extension.lower() == '.pdf':\n        with open(file_path, 'rb') as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n    elif file_extension.lower() == '.txt':\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n    elif file_extension.lower() == '.md':\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n    else:\n        raise ValueError(\"Unsupported file format. Please provide a PDF or text file.\")\n    \n    return text\n\ndata = load_document(\"../data/md/attention_is_all_you_need.md\")\n</pre> def load_document(file_path):     \"\"\"     Load document from a given file path. Supports PDF and text files.     \"\"\"     _, file_extension = os.path.splitext(file_path)          if file_extension.lower() == '.pdf':         with open(file_path, 'rb') as file:             pdf_reader = PyPDF2.PdfReader(file)             text = \"\"             for page in pdf_reader.pages:                 text += page.extract_text()     elif file_extension.lower() == '.txt':         with open(file_path, 'r', encoding='utf-8') as file:             text = file.read()     elif file_extension.lower() == '.md':         with open(file_path, 'r', encoding='utf-8') as file:             text = file.read()     else:         raise ValueError(\"Unsupported file format. Please provide a PDF or text file.\")          return text  data = load_document(\"../data/md/attention_is_all_you_need.md\") In\u00a0[\u00a0]: Copied! <pre>print(data)\n</pre> print(data) <p>Perform Chunking</p> In\u00a0[\u00a0]: Copied! <pre>def chunk_text(text, chunk_size=100, overlap=20):\n    \"\"\"\n    Split the text into chunks based on the number of words and word overlap.\n    \"\"\"\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap):\n        chunk = ' '.join(words[i:i + chunk_size])\n        chunks.append(chunk)\n    return chunks\n\nchunked_data = chunk_text(data)\n\nchunked_data\n</pre> def chunk_text(text, chunk_size=100, overlap=20):     \"\"\"     Split the text into chunks based on the number of words and word overlap.     \"\"\"     words = text.split()     chunks = []     for i in range(0, len(words), chunk_size - overlap):         chunk = ' '.join(words[i:i + chunk_size])         chunks.append(chunk)     return chunks  chunked_data = chunk_text(data)  chunked_data <p>Visualise Chunking</p> In\u00a0[\u00a0]: Copied! <pre># Print the list of chunks\ndef print_chunks(chunks):\n    for i, chunk in enumerate(chunks):\n        print(f\"Chunk {i + 1}:\")\n        print(chunk)\n        print(\"-\" * 50)\n        \nprint_chunks(chunked_data)\n</pre> # Print the list of chunks def print_chunks(chunks):     for i, chunk in enumerate(chunks):         print(f\"Chunk {i + 1}:\")         print(chunk)         print(\"-\" * 50)          print_chunks(chunked_data) <p>Setting Up embedding model</p> In\u00a0[11]: Copied! <pre># Load the sentence transformer model for embeddings\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\")\n</pre> # Load the sentence transformer model for embeddings model = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\") <pre>C:\\Users\\Adithya\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n  warnings.warn(warning_message, FutureWarning)\n</pre> <pre>.gitattributes:   0%|          | 0.00/1.52k [00:00&lt;?, ?B/s]</pre> <pre>1_Pooling/config.json:   0%|          | 0.00/297 [00:00&lt;?, ?B/s]</pre> <pre>README.md:   0%|          | 0.00/71.8k [00:00&lt;?, ?B/s]</pre> <pre>config.json:   0%|          | 0.00/1.35k [00:00&lt;?, ?B/s]</pre> <pre>model.safetensors:   0%|          | 0.00/547M [00:00&lt;?, ?B/s]</pre> <pre>model.onnx:   0%|          | 0.00/556M [00:00&lt;?, ?B/s]</pre> <pre>model_bnb4.onnx:   0%|          | 0.00/167M [00:00&lt;?, ?B/s]</pre> <pre>model_fp16.onnx:   0%|          | 0.00/278M [00:00&lt;?, ?B/s]</pre> <pre>\n---------------------------------------------------------------------------\nIncompleteRead                            Traceback (most recent call last)\nFile c:\\Python311\\Lib\\site-packages\\urllib3\\response.py:710, in HTTPResponse._error_catcher(self)\n    709 try:\n--&gt; 710     yield\n    712 except SocketTimeout as e:\n    713     # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\n    714     # there is yet no clean way to get at it from this context.\n\nFile c:\\Python311\\Lib\\site-packages\\urllib3\\response.py:835, in HTTPResponse._raw_read(self, amt)\n    825         if (\n    826             self.enforce_content_length\n    827             and self.length_remaining is not None\n   (...)\n    833             # raised during streaming, so all calls with incorrect\n    834             # Content-Length are caught.\n--&gt; 835             raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n    837 if data:\n\nIncompleteRead: IncompleteRead(147938459 bytes read, 130273836 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nProtocolError                             Traceback (most recent call last)\nFile c:\\Python311\\Lib\\site-packages\\requests\\models.py:820, in Response.iter_content.&lt;locals&gt;.generate()\n    819 try:\n--&gt; 820     yield from self.raw.stream(chunk_size, decode_content=True)\n    821 except ProtocolError as e:\n\nFile c:\\Python311\\Lib\\site-packages\\urllib3\\response.py:936, in HTTPResponse.stream(self, amt, decode_content)\n    935 while not is_fp_closed(self._fp) or len(self._decoded_buffer) &gt; 0:\n--&gt; 936     data = self.read(amt=amt, decode_content=decode_content)\n    938     if data:\n\nFile c:\\Python311\\Lib\\site-packages\\urllib3\\response.py:907, in HTTPResponse.read(self, amt, decode_content, cache_content)\n    903 while len(self._decoded_buffer) &lt; amt and data:\n    904     # TODO make sure to initially read enough data to get past the headers\n    905     # For example, the GZ file header takes 10 bytes, we don't want to read\n    906     # it one byte at a time\n--&gt; 907     data = self._raw_read(amt)\n    908     decoded_data = self._decode(data, decode_content, flush_decoder)\n\nFile c:\\Python311\\Lib\\site-packages\\urllib3\\response.py:813, in HTTPResponse._raw_read(self, amt)\n    811 fp_closed = getattr(self._fp, \"closed\", False)\n--&gt; 813 with self._error_catcher():\n    814     data = self._fp_read(amt) if not fp_closed else b\"\"\n\nFile c:\\Python311\\Lib\\contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    154 try:\n--&gt; 155     self.gen.throw(typ, value, traceback)\n    156 except StopIteration as exc:\n    157     # Suppress StopIteration *unless* it's the same exception that\n    158     # was passed to throw().  This prevents a StopIteration\n    159     # raised inside the \"with\" statement from being suppressed.\n\nFile c:\\Python311\\Lib\\site-packages\\urllib3\\response.py:727, in HTTPResponse._error_catcher(self)\n    725 except (HTTPException, OSError) as e:\n    726     # This includes IncompleteRead.\n--&gt; 727     raise ProtocolError(f\"Connection broken: {e!r}\", e) from e\n    729 # If no exception is thrown, we should avoid cleaning up\n    730 # unnecessarily.\n\nProtocolError: ('Connection broken: IncompleteRead(147938459 bytes read, 130273836 more expected)', IncompleteRead(147938459 bytes read, 130273836 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nChunkedEncodingError                      Traceback (most recent call last)\nCell In[11], line 2\n      1 # Load the sentence transformer model for embeddings\n----&gt; 2 model = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\")\n\nFile c:\\Python311\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:87, in SentenceTransformer.__init__(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\n     83     model_path = os.path.join(cache_folder, model_name_or_path.replace(\"/\", \"_\"))\n     85     if not os.path.exists(os.path.join(model_path, 'modules.json')):\n     86         # Download from hub with caching\n---&gt; 87         snapshot_download(model_name_or_path,\n     88                             cache_dir=cache_folder,\n     89                             library_name='sentence-transformers',\n     90                             library_version=__version__,\n     91                             ignore_files=['flax_model.msgpack', 'rust_model.ot', 'tf_model.h5'],\n     92                             use_auth_token=use_auth_token)\n     94 if os.path.exists(os.path.join(model_path, 'modules.json')):    #Load as SentenceTransformer model\n     95     modules = self._load_sbert_model(model_path)\n\nFile c:\\Python311\\Lib\\site-packages\\sentence_transformers\\util.py:491, in snapshot_download(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\n    486 if version.parse(huggingface_hub.__version__) &gt;= version.parse(\"0.8.1\"):\n    487     # huggingface_hub v0.8.1 introduces a new cache layout. We sill use a manual layout\n    488     # And need to pass legacy_cache_layout=True to avoid that a warning will be printed\n    489     cached_download_args['legacy_cache_layout'] = True\n--&gt; 491 path = cached_download(**cached_download_args)\n    493 if os.path.exists(path + \".lock\"):\n    494     os.remove(path + \".lock\")\n\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:114, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n    111 if check_use_auth_token:\n    112     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n--&gt; 114 return fn(*args, **kwargs)\n\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132, in _deprecate_method.&lt;locals&gt;._inner_deprecate_method.&lt;locals&gt;.inner_f(*args, **kwargs)\n    130     warning_message += \" \" + message\n    131 warnings.warn(warning_message, FutureWarning)\n--&gt; 132 return f(*args, **kwargs)\n\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:807, in cached_download(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\n    804     cache_path = \"\\\\\\\\?\\\\\" + os.path.abspath(cache_path)\n    806 with WeakFileLock(lock_path):\n--&gt; 807     _download_to_tmp_and_move(\n    808         incomplete_path=Path(cache_path + \".incomplete\"),\n    809         destination_path=Path(cache_path),\n    810         url_to_download=url_to_download,\n    811         proxies=proxies,\n    812         headers=headers,\n    813         expected_size=expected_size,\n    814         filename=filename,\n    815         force_download=force_download,\n    816     )\n    818     if force_filename is None:\n    819         logger.info(\"creating metadata file for %s\", cache_path)\n\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1915, in _download_to_tmp_and_move(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\n   1912         _check_disk_space(expected_size, incomplete_path.parent)\n   1913         _check_disk_space(expected_size, destination_path.parent)\n-&gt; 1915     http_get(\n   1916         url_to_download,\n   1917         f,\n   1918         proxies=proxies,\n   1919         resume_size=resume_size,\n   1920         headers=headers,\n   1921         expected_size=expected_size,\n   1922     )\n   1924 logger.info(f\"Download complete. Moving file to {destination_path}\")\n   1925 _chmod_and_move(incomplete_path, destination_path)\n\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:549, in http_get(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\n    547 new_resume_size = resume_size\n    548 try:\n--&gt; 549     for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n    550         if chunk:  # filter out keep-alive new chunks\n    551             progress.update(len(chunk))\n\nFile c:\\Python311\\Lib\\site-packages\\requests\\models.py:822, in Response.iter_content.&lt;locals&gt;.generate()\n    820     yield from self.raw.stream(chunk_size, decode_content=True)\n    821 except ProtocolError as e:\n--&gt; 822     raise ChunkedEncodingError(e)\n    823 except DecodeError as e:\n    824     raise ContentDecodingError(e)\n\nChunkedEncodingError: ('Connection broken: IncompleteRead(147938459 bytes read, 130273836 more expected)', IncompleteRead(147938459 bytes read, 130273836 more expected))</pre> <p>set up similarity function</p> In\u00a0[\u00a0]: Copied! <pre>def cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n</pre> def cosine_similarity(a, b):     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) <p>visualise embeddings</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>embed chunks</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>store the vectors/embedding</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>set up vector store</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>similarity search</p> <p>get top K results</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Augmenting Prompt</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>modifying system prompt</p> <p>set up llm provider</p> <p>generate response</p>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#rag-retrieval-augmented-generation","title":"RAG: Retrieval-Augmented Generation\u00b6","text":"<ul> <li>R: Retrieval - Fetch the right external content.</li> <li>A: Augmentation - Modify the prompt to pass content form Retrival Stage</li> <li>G: Generation - Generate the final response using the LLM.</li> </ul>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#programmatic-stages","title":"Programmatic Stages:\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#1-data-ingestion","title":"1. Data Ingestion:\u00b6","text":"<ul> <li>Parse PDF and extract text.</li> <li>Perform text chunking.</li> <li>Set up the database.</li> <li>Populate the database with parsed data.</li> </ul>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#2-retrieval","title":"2. Retrieval:\u00b6","text":"<ul> <li>Take the user query as input.</li> <li>Perform similarity search across the stored data.</li> <li>Retrieve the most relevant chunks of information.</li> </ul>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#3-augmentation","title":"3. Augmentation:\u00b6","text":"<ul> <li>Augment the prompt by incorporating relevant chunks of retrieved data.</li> <li>Adjust the prompt through prompt engineering to optimize for clarity and context.</li> </ul>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#4-generation","title":"4. Generation:\u00b6","text":"<ul> <li>Use the enhanced prompt to generate a response using the LLM.</li> </ul>"},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#data-ingestion","title":"Data Ingestion\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#retrival","title":"Retrival\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#augmentation","title":"Augmentation\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_from_scratch/#generation","title":"Generation\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/","title":"RAG from Scratch","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q sentence-transformers\n!pip install -q wikipedia-api\n!pip install -q numpy\n!pip install -q scipy\n</pre> !pip install -q sentence-transformers !pip install -q wikipedia-api !pip install -q numpy !pip install -q scipy In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True)\n</pre> from sentence_transformers import SentenceTransformer model = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True) In\u00a0[\u00a0]: Copied! <pre>from wikipediaapi import Wikipedia\nwiki = Wikipedia('RAGBot/0.0', 'en')\ndoc = wiki.page('Hayao_Miyazaki').text\nparagraphs = doc.split('\\n\\n') # chunking\n</pre> from wikipediaapi import Wikipedia wiki = Wikipedia('RAGBot/0.0', 'en') doc = wiki.page('Hayao_Miyazaki').text paragraphs = doc.split('\\n\\n') # chunking  In\u00a0[\u00a0]: Copied! <pre>import textwrap\n</pre> import textwrap  In\u00a0[\u00a0]: Copied! <pre>for i, p in enumerate(paragraphs):\n  wrapped_text = textwrap.fill(p, width=100)\n\n  print(\"-----------------------------------------------------------------\")\n  print(wrapped_text)\n  print(\"-----------------------------------------------------------------\")\n</pre> for i, p in enumerate(paragraphs):   wrapped_text = textwrap.fill(p, width=100)    print(\"-----------------------------------------------------------------\")   print(wrapped_text)   print(\"-----------------------------------------------------------------\")  In\u00a0[\u00a0]: Copied! <pre>docs_embed = model.encode(paragraphs, normalize_embeddings=True)\n</pre> docs_embed = model.encode(paragraphs, normalize_embeddings=True) In\u00a0[\u00a0]: Copied! <pre>docs_embed.shape\n</pre> docs_embed.shape In\u00a0[\u00a0]: Copied! <pre>docs_embed[0]\n</pre> docs_embed[0] In\u00a0[\u00a0]: Copied! <pre>query = \"What was Studio Ghibli's first film?\"\nquery_embed = model.encode(query, normalize_embeddings=True)\n</pre> query = \"What was Studio Ghibli's first film?\" query_embed = model.encode(query, normalize_embeddings=True)  In\u00a0[\u00a0]: Copied! <pre>query_embed.shape\n</pre> query_embed.shape In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nsimilarities = np.dot(docs_embed, query_embed.T)\n</pre> import numpy as np similarities = np.dot(docs_embed, query_embed.T) In\u00a0[\u00a0]: Copied! <pre>similarities.shape\n</pre> similarities.shape In\u00a0[\u00a0]: Copied! <pre>similarities\n</pre> similarities In\u00a0[\u00a0]: Copied! <pre>top_3_idx = np.argsort(similarities, axis=0)[-3:][::-1].tolist()\n</pre> top_3_idx = np.argsort(similarities, axis=0)[-3:][::-1].tolist()  In\u00a0[\u00a0]: Copied! <pre>top_3_idx\n</pre> top_3_idx In\u00a0[\u00a0]: Copied! <pre>most_similar_documents = [paragraphs[idx] for idx in top_3_idx]\n</pre> most_similar_documents = [paragraphs[idx] for idx in top_3_idx] In\u00a0[\u00a0]: Copied! <pre>CONTEXT = \"\"\nfor i, p in enumerate(most_similar_documents):\n  wrapped_text = textwrap.fill(p, width=100)\n\n  print(\"-----------------------------------------------------------------\")\n  print(wrapped_text)\n  print(\"-----------------------------------------------------------------\")\n  CONTEXT += wrapped_text + \"\\n\\n\"\n</pre> CONTEXT = \"\" for i, p in enumerate(most_similar_documents):   wrapped_text = textwrap.fill(p, width=100)    print(\"-----------------------------------------------------------------\")   print(wrapped_text)   print(\"-----------------------------------------------------------------\")   CONTEXT += wrapped_text + \"\\n\\n\" In\u00a0[\u00a0]: Copied! <pre>query = \"What was Studio Ghibli's first film?\"\n</pre> query = \"What was Studio Ghibli's first film?\" In\u00a0[\u00a0]: Copied! <pre>prompt = f\"\"\"\nuse the following CONTEXT to answer the QUESTION at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nCONTEXT: {CONTEXT}\nQUESTION: {query}\n\n\"\"\"\n</pre> prompt = f\"\"\" use the following CONTEXT to answer the QUESTION at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.  CONTEXT: {CONTEXT} QUESTION: {query}  \"\"\" In\u00a0[\u00a0]: Copied! <pre>!pip install -q openai\n</pre> !pip install -q openai In\u00a0[\u00a0]: Copied! <pre># prompt: write python code to make calls to openai api\nfrom google.colab import userdata\nuserdata.get('openai')\n\nimport openai\n</pre> # prompt: write python code to make calls to openai api from google.colab import userdata userdata.get('openai')  import openai   In\u00a0[\u00a0]: Copied! <pre>from openai import OpenAI\nclient = OpenAI(api_key=userdata.get('openai'))\n</pre> from openai import OpenAI client = OpenAI(api_key=userdata.get('openai')) In\u00a0[\u00a0]: Copied! <pre>response = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"user\", \"content\": prompt},\n  ]\n)\n</pre> response = client.chat.completions.create(   model=\"gpt-4o\",   messages=[     {\"role\": \"user\", \"content\": prompt},   ] ) In\u00a0[\u00a0]: Copied! <pre>print(response.choices[0].message.content)\n</pre> print(response.choices[0].message.content)  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#rag-from-scratch","title":"RAG from Scratch\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#setup","title":"Setup\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#load-the-embedding-model","title":"Load the Embedding Model:\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#fetch-text-content-from-wikipedia","title":"Fetch Text Content from Wikipedia:\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#embed-the-document","title":"Embed the Document:\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#embed-the-query","title":"Embed the Query:\u00b6","text":""},{"location":"RAG/00_RAG_Base/RAG_in_10_lines/#find-the-closest-paragraphs-to-the-query","title":"Find the Closest Paragraphs to the Query:\u00b6","text":""},{"location":"RAG/00_RAG_Base/Understanding_embeddings_and_similarity/","title":"What are Embeddings","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q sentence-transformers\n!pip install -q wikipedia-api\n!pip install -q numpy\n!pip install -q scipy\n!pip install rich\n!pip install pypdf2\n</pre> !pip install -q sentence-transformers !pip install -q wikipedia-api !pip install -q numpy !pip install -q scipy !pip install rich !pip install pypdf2 In\u00a0[\u00a0]: Copied! <pre>import re\nimport os\nfrom rich import print\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport textwrap\nfrom IPython.display import display, HTML\n</pre> import re import os from rich import print from sentence_transformers import SentenceTransformer import numpy as np import textwrap from IPython.display import display, HTML <p>Load Data</p> In\u00a0[\u00a0]: Copied! <pre>from wikipediaapi import Wikipedia\nwiki = Wikipedia('RAGBot/0.0', 'en')\ndata = wiki.page('Hayao_Miyazaki').text\n\n## After Uploading a pdf\n# data = load_document(\"/content/R_Tamil_LLama.pdf\")\n\nprint(data)\n</pre> from wikipediaapi import Wikipedia wiki = Wikipedia('RAGBot/0.0', 'en') data = wiki.page('Hayao_Miyazaki').text  ## After Uploading a pdf # data = load_document(\"/content/R_Tamil_LLama.pdf\")  print(data) <p>Perform Chunking</p> In\u00a0[\u00a0]: Copied! <pre>def chunk_text(text, chunk_size=1000, overlap=20):\n    \"\"\"\n    Split the text into chunks based on the number of words and word overlap.\n    \"\"\"\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap):\n        chunk = ' '.join(words[i:i + chunk_size])\n        chunks.append(chunk)\n    return chunks\n\nchunked_data = chunk_text(data)\n\nprint(\"Total number of chunks\", len(chunked_data))\n</pre> def chunk_text(text, chunk_size=1000, overlap=20):     \"\"\"     Split the text into chunks based on the number of words and word overlap.     \"\"\"     words = text.split()     chunks = []     for i in range(0, len(words), chunk_size - overlap):         chunk = ' '.join(words[i:i + chunk_size])         chunks.append(chunk)     return chunks  chunked_data = chunk_text(data)  print(\"Total number of chunks\", len(chunked_data)) <p>Visualise Chunking</p> In\u00a0[\u00a0]: Copied! <pre>def print_chunks(chunks):\n    \"\"\"\n    Display text chunks in a clean, readable format using HTML styling.\n\n    Args:\n        chunks (list): List of text chunks to display\n    \"\"\"\n    # Create the HTML for the chunks display\n    html_content = \"\"\"\n    &lt;style&gt;\n        .chunk-container {\n            font-family: Arial, sans-serif;\n            margin: 20px 0;\n        }\n        .chunk-header {\n            background-color: #f0f2f6;\n            padding: 5px 10px;\n            border-radius: 5px 5px 0 0;\n            border-left: 4px solid #3498db;\n            font-weight: bold;\n            color: #2c3e50;\n        }\n        .chunk-content {\n            background-color: #ffffff;\n            color: #2c3e50;\n            padding: 10px;\n            border: 1px solid #e1e4e8;\n            border-left: 4px solid #3498db;\n            border-top: none;\n            border-radius: 0 0 5px 5px;\n            white-space: pre-wrap;\n            font-family: monospace;\n        }\n    &lt;/style&gt;\n    \"\"\"\n\n    # Add each chunk to the HTML content\n    for i, chunk in enumerate(chunks, 1):\n        # Wrap text for better readability\n        wrapped_text = textwrap.fill(chunk, width=100)\n\n        html_content += f\"\"\"\n        &lt;div class=\"chunk-container\"&gt;\n            &lt;div class=\"chunk-header\"&gt;Chunk {i}&lt;/div&gt;\n            &lt;div class=\"chunk-content\"&gt;{wrapped_text}&lt;/div&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Display the HTML\n    display(HTML(html_content))\nprint_chunks(chunked_data)\n</pre> def print_chunks(chunks):     \"\"\"     Display text chunks in a clean, readable format using HTML styling.      Args:         chunks (list): List of text chunks to display     \"\"\"     # Create the HTML for the chunks display     html_content = \"\"\"          \"\"\"      # Add each chunk to the HTML content     for i, chunk in enumerate(chunks, 1):         # Wrap text for better readability         wrapped_text = textwrap.fill(chunk, width=100)          html_content += f\"\"\"          Chunk {i} {wrapped_text}          \"\"\"      # Display the HTML     display(HTML(html_content)) print_chunks(chunked_data) <p>Setting Up embedding model</p> In\u00a0[\u00a0]: Copied! <pre># Load the sentence transformer model for embeddings\n\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True)\n# model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", trust_remote_code=True)\n# model = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True)\n</pre> # Load the sentence transformer model for embeddings  model = SentenceTransformer(\"Alibaba-NLP/gte-base-en-v1.5\", trust_remote_code=True) # model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", trust_remote_code=True) # model = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True) <p>set up similarity function</p> <p>Understanding Cosine Similarity : refrence video</p> In\u00a0[\u00a0]: Copied! <pre>def cosine_similarity(vector_a, vector_b):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    Cosine similarity measures how similar two vectors are by calculating the cosine of the angle between them.\n\n    Args:\n        vector_a: First vector (numpy array)\n        vector_b: Second vector (numpy array)\n\n    Returns:\n        float: Similarity score between -1 and 1\n               1: Vectors are identical\n               0: Vectors are perpendicular\n              -1: Vectors are opposite\n    \"\"\"\n    # Step 1: Calculate the dot product between the vectors\n    # Dot product measures how much vectors point in the same direction\n    dot_product = np.dot(vector_a, vector_b)\n\n    # Step 2: Calculate the magnitude (length) of each vector\n    # Magnitude is the square root of the sum of squared values\n    magnitude_a = np.linalg.norm(vector_a)  # \u221a(a1\u00b2 + a2\u00b2 + ... + an\u00b2)\n    magnitude_b = np.linalg.norm(vector_b)  # \u221a(b1\u00b2 + b2\u00b2 + ... + bn\u00b2)\n\n    # Step 3: Calculate the cosine similarity\n    # Divide dot product by the product of magnitudes\n    similarity = dot_product / (magnitude_a * magnitude_b)\n\n    return similarity\n</pre> def cosine_similarity(vector_a, vector_b):     \"\"\"     Calculate the cosine similarity between two vectors.     Cosine similarity measures how similar two vectors are by calculating the cosine of the angle between them.      Args:         vector_a: First vector (numpy array)         vector_b: Second vector (numpy array)      Returns:         float: Similarity score between -1 and 1                1: Vectors are identical                0: Vectors are perpendicular               -1: Vectors are opposite     \"\"\"     # Step 1: Calculate the dot product between the vectors     # Dot product measures how much vectors point in the same direction     dot_product = np.dot(vector_a, vector_b)      # Step 2: Calculate the magnitude (length) of each vector     # Magnitude is the square root of the sum of squared values     magnitude_a = np.linalg.norm(vector_a)  # \u221a(a1\u00b2 + a2\u00b2 + ... + an\u00b2)     magnitude_b = np.linalg.norm(vector_b)  # \u221a(b1\u00b2 + b2\u00b2 + ... + bn\u00b2)      # Step 3: Calculate the cosine similarity     # Divide dot product by the product of magnitudes     similarity = dot_product / (magnitude_a * magnitude_b)      return similarity <p>understanding similarity between two sentences</p> In\u00a0[\u00a0]: Copied! <pre>## Change the sentences accordingly\n\nsentence1 = \"The cat sat on the mat\"\nsentence2 = \"A cat is sitting on a mat\"\n</pre> ## Change the sentences accordingly  sentence1 = \"The cat sat on the mat\" sentence2 = \"A cat is sitting on a mat\" In\u00a0[\u00a0]: Copied! <pre>def get_similarity_score(sentence1, sentence2):\n    \"\"\"\n    Calculate similarity score between two sentences.\n\n    Args:\n        sentence1 (str): First sentence\n        sentence2 (str): Second sentence\n\n    Returns:\n        float: Similarity score between 0 and 1\n    \"\"\"\n    # Get embeddings\n    embedding1 = model.encode(sentence1, normalize_embeddings=True)\n    embedding2 = model.encode(sentence2, normalize_embeddings=True)\n\n    # Calculate similarity\n    similarity = cosine_similarity(embedding1, embedding2)\n\n    return similarity\n\n# change the sentences\n\n\nscore = get_similarity_score(sentence1, sentence2)\nprint(f\"Similarity score: {score:.4f}\")\n</pre> def get_similarity_score(sentence1, sentence2):     \"\"\"     Calculate similarity score between two sentences.      Args:         sentence1 (str): First sentence         sentence2 (str): Second sentence      Returns:         float: Similarity score between 0 and 1     \"\"\"     # Get embeddings     embedding1 = model.encode(sentence1, normalize_embeddings=True)     embedding2 = model.encode(sentence2, normalize_embeddings=True)      # Calculate similarity     similarity = cosine_similarity(embedding1, embedding2)      return similarity  # change the sentences   score = get_similarity_score(sentence1, sentence2) print(f\"Similarity score: {score:.4f}\") <p>visualise embeddings</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\ndef visualize_embeddings(sentence1, sentence2):\n    \"\"\"\n    Visualize the relationship between two sentence embeddings using\n    multiple visualization techniques.\n\n    Args:\n        sentence1 (str): First sentence\n        sentence2 (str): Second sentence\n    \"\"\"\n    # Get embeddings\n    embedding1 = model.encode(sentence1, normalize_embeddings=True)\n    embedding2 = model.encode(sentence2, normalize_embeddings=True)\n    dimensions = range(len(embedding1))\n\n    # Create figure with subplots\n    fig = plt.figure(figsize=(15, 5))\n\n\n    # Dimension-wise Comparison\n    plt.subplot(132)\n    plt.plot(dimensions, embedding1,\n             label=f'Sentence 1: \"{sentence1[:30]}...\"',\n             alpha=0.7,\n             linewidth=1)\n    plt.plot(dimensions, embedding2,\n             label=f'Sentence 2: \"{sentence2[:30]}...\"',\n             alpha=0.7,\n             linewidth=1)\n    plt.title('Comparison')\n    plt.legend()\n    plt.grid(True)\n\n    # 2D PCA Projection\n    plt.subplot(133)\n    # Combine embeddings and apply PCA\n    combined_embeddings = np.vstack([embedding1, embedding2])\n    pca = PCA(n_components=2)\n    projected = pca.fit_transform(combined_embeddings)\n\n    plt.scatter(projected[0, 0], projected[0, 1], c='blue', label='Sentence 1', s=100)\n    plt.scatter(projected[1, 0], projected[1, 1], c='red', label='Sentence 2', s=100)\n    plt.plot([projected[0, 0], projected[1, 0]],\n             [projected[0, 1], projected[1, 1]],\n             'k--', alpha=0.5)\n    plt.title('2D PCA Projection')\n    plt.legend()\n    plt.grid(True)\n\n    # Add overall title and adjust layout\n    plt.suptitle(f'Embedding Relationship Analysis\\n\"{sentence1}\" vs \"{sentence2}\"',\n                 fontsize=12, y=1.05)\n    plt.tight_layout()\n\n    # Calculate and display similarity score\n    similarity = np.dot(embedding1, embedding2)\n    print(f\"Similarity Score: {similarity:.4f}\")\n\n    plt.show()\n\ndef plot_embedding_heatmap(sentence1, sentence2):\n    \"\"\"\n    Create an improved heatmap visualization of embedding similarities.\n\n    Args:\n        sentence1 (str): First sentence\n        sentence2 (str): Second sentence\n    \"\"\"\n    # Get embeddings\n    embedding1 = model.encode(sentence1, normalize_embeddings=True)\n    embedding2 = model.encode(sentence2, normalize_embeddings=True)\n\n    # Reshape embeddings to 2D matrices for better visualization\n    size = int(np.sqrt(len(embedding1)))\n    matrix1 = embedding1[:size*size].reshape(size, size)\n    matrix2 = embedding2[:size*size].reshape(size, size)\n\n    # Create similarity matrix\n    similarity_matrix = np.dot(matrix1, matrix2.T)\n\n    # Plot setup\n    plt.figure(figsize=(12, 5))\n\n    # Create subplots for both individual embeddings and their similarity\n    plt.subplot(131)\n    sns.heatmap(matrix1,\n                cmap='viridis',\n                center=0,\n                cbar_kws={'label': 'Embedding Values'})\n    plt.title(f'Embedding 1\\n\"{sentence1[:20]}...\"')\n\n    plt.subplot(132)\n    sns.heatmap(matrix2,\n                cmap='viridis',\n                center=0,\n                cbar_kws={'label': 'Embedding Values'})\n    plt.title(f'Embedding 2\\n\"{sentence2[:20]}...\"')\n\n    plt.subplot(133)\n    sns.heatmap(similarity_matrix,\n                cmap='coolwarm',\n                center=0,\n                cbar_kws={'label': 'Similarity'})\n    plt.title('Similarity Matrix')\n\n    # Calculate overall similarity score\n    similarity = np.dot(embedding1, embedding2)\n\n    # Add overall title with similarity score\n    plt.suptitle(f'Embedding Analysis (Similarity Score: {similarity:.4f})',\n                 y=1.05)\n\n    plt.tight_layout()\n    plt.show()\n\n    return similarity\n\nvisualize_embeddings(sentence1, sentence2)\nplot_embedding_heatmap(sentence1, sentence2)\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.decomposition import PCA  def visualize_embeddings(sentence1, sentence2):     \"\"\"     Visualize the relationship between two sentence embeddings using     multiple visualization techniques.      Args:         sentence1 (str): First sentence         sentence2 (str): Second sentence     \"\"\"     # Get embeddings     embedding1 = model.encode(sentence1, normalize_embeddings=True)     embedding2 = model.encode(sentence2, normalize_embeddings=True)     dimensions = range(len(embedding1))      # Create figure with subplots     fig = plt.figure(figsize=(15, 5))       # Dimension-wise Comparison     plt.subplot(132)     plt.plot(dimensions, embedding1,              label=f'Sentence 1: \"{sentence1[:30]}...\"',              alpha=0.7,              linewidth=1)     plt.plot(dimensions, embedding2,              label=f'Sentence 2: \"{sentence2[:30]}...\"',              alpha=0.7,              linewidth=1)     plt.title('Comparison')     plt.legend()     plt.grid(True)      # 2D PCA Projection     plt.subplot(133)     # Combine embeddings and apply PCA     combined_embeddings = np.vstack([embedding1, embedding2])     pca = PCA(n_components=2)     projected = pca.fit_transform(combined_embeddings)      plt.scatter(projected[0, 0], projected[0, 1], c='blue', label='Sentence 1', s=100)     plt.scatter(projected[1, 0], projected[1, 1], c='red', label='Sentence 2', s=100)     plt.plot([projected[0, 0], projected[1, 0]],              [projected[0, 1], projected[1, 1]],              'k--', alpha=0.5)     plt.title('2D PCA Projection')     plt.legend()     plt.grid(True)      # Add overall title and adjust layout     plt.suptitle(f'Embedding Relationship Analysis\\n\"{sentence1}\" vs \"{sentence2}\"',                  fontsize=12, y=1.05)     plt.tight_layout()      # Calculate and display similarity score     similarity = np.dot(embedding1, embedding2)     print(f\"Similarity Score: {similarity:.4f}\")      plt.show()  def plot_embedding_heatmap(sentence1, sentence2):     \"\"\"     Create an improved heatmap visualization of embedding similarities.      Args:         sentence1 (str): First sentence         sentence2 (str): Second sentence     \"\"\"     # Get embeddings     embedding1 = model.encode(sentence1, normalize_embeddings=True)     embedding2 = model.encode(sentence2, normalize_embeddings=True)      # Reshape embeddings to 2D matrices for better visualization     size = int(np.sqrt(len(embedding1)))     matrix1 = embedding1[:size*size].reshape(size, size)     matrix2 = embedding2[:size*size].reshape(size, size)      # Create similarity matrix     similarity_matrix = np.dot(matrix1, matrix2.T)      # Plot setup     plt.figure(figsize=(12, 5))      # Create subplots for both individual embeddings and their similarity     plt.subplot(131)     sns.heatmap(matrix1,                 cmap='viridis',                 center=0,                 cbar_kws={'label': 'Embedding Values'})     plt.title(f'Embedding 1\\n\"{sentence1[:20]}...\"')      plt.subplot(132)     sns.heatmap(matrix2,                 cmap='viridis',                 center=0,                 cbar_kws={'label': 'Embedding Values'})     plt.title(f'Embedding 2\\n\"{sentence2[:20]}...\"')      plt.subplot(133)     sns.heatmap(similarity_matrix,                 cmap='coolwarm',                 center=0,                 cbar_kws={'label': 'Similarity'})     plt.title('Similarity Matrix')      # Calculate overall similarity score     similarity = np.dot(embedding1, embedding2)      # Add overall title with similarity score     plt.suptitle(f'Embedding Analysis (Similarity Score: {similarity:.4f})',                  y=1.05)      plt.tight_layout()     plt.show()      return similarity  visualize_embeddings(sentence1, sentence2) plot_embedding_heatmap(sentence1, sentence2) <p>embed chunks</p> In\u00a0[\u00a0]: Copied! <pre>def simple_visualize_chunks(chunks):\n    \"\"\"\n    Create a simple 2D visualization of text chunk relationships.\n\n    Args:\n        chunks (list): List of text chunks to visualize\n    \"\"\"\n    # Get embeddings and reduce dimensions\n    embeddings = model.encode(chunks, normalize_embeddings=True, show_progress_bar=True)\n    pca = PCA(n_components=2)\n    reduced = pca.fit_transform(embeddings)\n\n    # Create plot\n    plt.figure(figsize=(10, 6))\n    plt.scatter(reduced[:, 0], reduced[:, 1], c=range(len(chunks)), cmap='viridis')\n\n    # Add labels\n    for i, (x, y) in enumerate(reduced):\n        plt.annotate(f\"Chunk {i+1}\", (x, y), xytext=(5, 5), textcoords='offset points')\n\n    plt.title(\"Text Chunks in 2D Space\")\n    plt.grid(True, alpha=0.3)\n    plt.colorbar(label='Chunk Order')\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage:\n# chunked_data = [\"hello\", \"bird\", \"how are you doing\" , \"king\"]\nsimple_visualize_chunks(chunked_data)\n</pre> def simple_visualize_chunks(chunks):     \"\"\"     Create a simple 2D visualization of text chunk relationships.      Args:         chunks (list): List of text chunks to visualize     \"\"\"     # Get embeddings and reduce dimensions     embeddings = model.encode(chunks, normalize_embeddings=True, show_progress_bar=True)     pca = PCA(n_components=2)     reduced = pca.fit_transform(embeddings)      # Create plot     plt.figure(figsize=(10, 6))     plt.scatter(reduced[:, 0], reduced[:, 1], c=range(len(chunks)), cmap='viridis')      # Add labels     for i, (x, y) in enumerate(reduced):         plt.annotate(f\"Chunk {i+1}\", (x, y), xytext=(5, 5), textcoords='offset points')      plt.title(\"Text Chunks in 2D Space\")     plt.grid(True, alpha=0.3)     plt.colorbar(label='Chunk Order')      plt.tight_layout()     plt.show()  # Example usage: # chunked_data = [\"hello\", \"bird\", \"how are you doing\" , \"king\"] simple_visualize_chunks(chunked_data) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"RAG/00_RAG_Base/Understanding_embeddings_and_similarity/#understanding-embedding-models-and-similarity","title":"Understanding Embedding Models and Similarity\u00b6","text":""},{"location":"RAG/01_BM25_RAG/","title":"intro to BM25","text":""},{"location":"RAG/01_BM25_RAG/#bm25-rag-retrieval-augmented-generation","title":"BM25 RAG (Retrieval-Augmented Generation)","text":""},{"location":"RAG/01_BM25_RAG/#introduction","title":"Introduction","text":"<p>BM25 Retrieval-Augmented Generation (BM25 RAG) is an advanced technique that combines the power of the BM25 (Best Matching 25) algorithm for information retrieval with large language models for text generation. This approach enhances the accuracy and relevance of generated responses by grounding them in specific, retrieved information using a proven probabilistic retrieval model.</p>"},{"location":"RAG/01_BM25_RAG/#bm25-rag-workflow","title":"BM25 RAG Workflow","text":"<pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Indexing\"\n        C1 &amp; C2 &amp; C3 --&gt; D[Tokenization]\n        D --&gt; E[TF-IDF Calculation]\n        E --&gt; F[(Inverted Index)]\n    end\n\n    subgraph \"3. Query Processing\"\n        G[Query] --&gt; H[Tokenization]\n        H --&gt; I[Query Terms]\n    end\n\n    subgraph \"4. Retrieval\"\n        I --&gt;|Term Matching| F\n        F --&gt;|BM25 Scoring| J[Relevant Chunks]\n    end\n\n    subgraph \"5. Context Formation\"\n        J --&gt; K[Query + Relevant Chunks]\n    end\n\n    subgraph \"6. Generation\"\n        K --&gt; L[LLM]\n        L --&gt; M[Response]\n    end\n\n    G --&gt; K</code></pre>"},{"location":"RAG/01_BM25_RAG/#getting-started","title":"Getting Started","text":""},{"location":"RAG/01_BM25_RAG/#notebook","title":"Notebook","text":"<p>You can run the Jupyter notebook provided in this repository to explore BM25 RAG in detail.</p>"},{"location":"RAG/01_BM25_RAG/#chat-application","title":"Chat Application","text":"<ol> <li>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Run the application:    <pre><code>python app.py\n</code></pre></li> <li>To ingest data on the go:    <pre><code>python app.py --ingest --data_dir /path/to/documents\n</code></pre></li> </ol>"},{"location":"RAG/01_BM25_RAG/#server","title":"Server","text":"<p>Run the server with:</p> <pre><code>python server.py\n</code></pre> <p>The server has two endpoints:</p> <ul> <li><code>/api/ingest</code>: For ingesting new documents</li> <li><code>/api/query</code>: For querying the BM25 RAG system</li> </ul>"},{"location":"RAG/01_BM25_RAG/#key-features-of-bm25-rag","title":"Key Features of BM25 RAG","text":"<ol> <li>Probabilistic Retrieval: BM25 uses a probabilistic model to rank documents, providing a theoretically sound basis for retrieval.</li> <li>Term Frequency Saturation: BM25 accounts for diminishing returns from repeated terms, improving retrieval quality.</li> <li>Document Length Normalization: The algorithm considers document length, reducing bias towards longer documents.</li> <li>Contextual Relevance: By grounding responses in retrieved information, BM25 RAG produces more accurate and relevant answers.</li> <li>Scalability: The BM25 retrieval step can handle large document collections efficiently.</li> </ol>"},{"location":"RAG/01_BM25_RAG/#benefits-of-bm25-rag","title":"Benefits of BM25 RAG","text":"<ol> <li>Improved Accuracy: Combines the strengths of probabilistic retrieval and neural text generation.</li> <li>Interpretability: BM25 scoring provides a more interpretable retrieval process compared to dense vector retrieval methods.</li> <li>Handling Long-tail Queries: Particularly effective for queries requiring specific or rare information.</li> <li>No Embedding Required: Unlike vector-based RAG, BM25 doesn't require document embeddings, reducing computational overhead.</li> </ol>"},{"location":"RAG/01_BM25_RAG/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>Jupyter Notebook or JupyterLab (for running the notebook)</li> <li>Required Python packages (see <code>requirements.txt</code>)</li> <li>API key for the chosen Language Model (e.g., OpenAI API key)</li> </ul>"},{"location":"RAG/01_BM25_RAG/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines for more details.</p>"},{"location":"RAG/01_BM25_RAG/#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"RAG/01_BM25_RAG/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>AI Engineering Academy for supporting this project</li> <li>All contributors and community members</li> </ul> <p>For more information, visit AI Engineering Academy.</p>"},{"location":"RAG/01_BM25_RAG/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport openai\nimport chainlit as cl\nimport argparse\nfrom dotenv import load_dotenv\nfrom llama_index.core import (\n    Settings,\n    SimpleDirectoryReader,\n)\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.retrievers.bm25 import BM25Retriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core import get_response_synthesizer\nfrom llama_index.core.response_synthesizers import ResponseMode\n</pre> import os import openai import chainlit as cl import argparse from dotenv import load_dotenv from llama_index.core import (     Settings,     SimpleDirectoryReader, ) from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.core.callbacks import CallbackManager from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline from llama_index.core.storage.docstore import SimpleDocumentStore from llama_index.retrievers.bm25 import BM25Retriever from llama_index.core.query_engine import RetrieverQueryEngine from llama_index.core import get_response_synthesizer from llama_index.core.response_synthesizers import ResponseMode In\u00a0[\u00a0]: Copied! <pre># Load environment variables from .env file\nprint(\"Loading Environment variables\")\nload_dotenv()\n</pre> # Load environment variables from .env file print(\"Loading Environment variables\") load_dotenv() In\u00a0[\u00a0]: Copied! <pre># Set OpenAI API key\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = OPENAI_API_KEY\n</pre> # Set OpenAI API key OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") openai.api_key = OPENAI_API_KEY In\u00a0[\u00a0]: Copied! <pre># Configure LLM settings\nSettings.llm = OpenAI(\n    model=\"gpt-4\",\n    temperature=0.1,\n    max_tokens=1024,\n    streaming=True,\n    api_key=OPENAI_API_KEY,\n)\n</pre> # Configure LLM settings Settings.llm = OpenAI(     model=\"gpt-4\",     temperature=0.1,     max_tokens=1024,     streaming=True,     api_key=OPENAI_API_KEY, ) In\u00a0[\u00a0]: Copied! <pre># Set embedding model and context window\n# Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\nSettings.context_window = 4096\nSettings.callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()])\n</pre> # Set embedding model and context window # Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY) Settings.context_window = 4096 Settings.callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()]) <p>Initialize the document store docstore = SimpleDocumentStore(namespace=\"BM25_RAG\").persist(persist_path=\"docstore\") docstore = SimpleDocumentStore().from_persist_path(\"docstore\")</p> In\u00a0[\u00a0]: Copied! <pre>DOCSTORE_PATH = \"docstore.json\"\n</pre> DOCSTORE_PATH = \"docstore.json\" In\u00a0[\u00a0]: Copied! <pre>def initialize_docstore():\n    if os.path.exists(DOCSTORE_PATH):\n        print(\"Loading existing docstore...\")\n        docstore = SimpleDocumentStore.from_persist_path(DOCSTORE_PATH)\n    else:\n        print(\"Creating new docstore...\")\n        docstore = SimpleDocumentStore()\n    return docstore\n</pre> def initialize_docstore():     if os.path.exists(DOCSTORE_PATH):         print(\"Loading existing docstore...\")         docstore = SimpleDocumentStore.from_persist_path(DOCSTORE_PATH)     else:         print(\"Creating new docstore...\")         docstore = SimpleDocumentStore()     return docstore In\u00a0[\u00a0]: Copied! <pre>docstore = initialize_docstore()\n</pre> docstore = initialize_docstore() In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index-storage-index-store-mongodb\n!pip install llama-index-storage-docstore-mongodb\n</pre> !pip install llama-index-storage-index-store-mongodb !pip install llama-index-storage-docstore-mongodb <p>from llama_index.storage.docstore.mongodb import MongoDocumentStore from llama_index.storage.kvstore.mongodb import MongoDBKVStore from pymongo import MongoClient from motor.motor_asyncio import AsyncIOMotorClient</p> <p>MONGO_URI = os.getenv(\"MONGO_URI\") kv_store = MongoDBKVStore(mongo_client=MongoClient(MONGO_URI) , mongo_aclient=AsyncIOMotorClient(MONGO_URI)) docstore = MongoDocumentStore(namespace=\"BM25_RAG\" ,mongo_kvstore=kv_store).from_uri(uri=MONGO_URI)</p> <p>REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") REDIS_PORT = os.getenv(\"REDIS_PORT\", 6379)</p> <p>docstore=RedisDocumentStore.from_host_and_port( host=REDIS_HOST, port=REDIS_PORT, namespace=\"BM25_RAG\" )</p> In\u00a0[\u00a0]: Copied! <pre>def ingest_documents(data_dir):\n    global docstore\n    # Load documents from a directory\n    documents = SimpleDirectoryReader(data_dir, recursive=True).load_data(\n        show_progress=True\n    )\n\n    # Ingest data into the document store\n    print(\"Ingesting Data\")\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n            Settings.embed_model,\n        ],\n    )\n\n    # Process documents and add to document store\n    nodes = pipeline.run(documents=documents, show_progress=True)\n    docstore.add_documents(nodes)\n    print(\"Number of chunks added to document store:\", len(nodes))\n    \n    # Persist the updated docstore\n    docstore.persist(persist_path=DOCSTORE_PATH)\n    print(f\"Docstore persisted to {DOCSTORE_PATH}\")\n</pre> def ingest_documents(data_dir):     global docstore     # Load documents from a directory     documents = SimpleDirectoryReader(data_dir, recursive=True).load_data(         show_progress=True     )      # Ingest data into the document store     print(\"Ingesting Data\")     pipeline = IngestionPipeline(         transformations=[             SentenceSplitter(chunk_size=1024, chunk_overlap=20),             Settings.embed_model,         ],     )      # Process documents and add to document store     nodes = pipeline.run(documents=documents, show_progress=True)     docstore.add_documents(nodes)     print(\"Number of chunks added to document store:\", len(nodes))          # Persist the updated docstore     docstore.persist(persist_path=DOCSTORE_PATH)     print(f\"Docstore persisted to {DOCSTORE_PATH}\") In\u00a0[\u00a0]: Copied! <pre># Global variable to store the query engine\nquery_engine = None\n</pre> # Global variable to store the query engine query_engine = None In\u00a0[\u00a0]: Copied! <pre>@cl.on_chat_start\nasync def start():\n    global query_engine\n    # Initialize the BM25 retriever and query engine if they haven't been created yet\n    if query_engine is None:\n        bm25_retriever = BM25Retriever.from_defaults(\n            docstore=docstore,\n            similarity_top_k=4,\n        )\n        \n        query_engine = RetrieverQueryEngine(\n            retriever=bm25_retriever,\n\n        )\n\n    # Send a welcome message to the user\n    await cl.Message(\n        author=\"Assistant\", content=\"Hello! I'm an AI assistant using BM25 RAG. How may I help you?\"\n    ).send()\n</pre> @cl.on_chat_start async def start():     global query_engine     # Initialize the BM25 retriever and query engine if they haven't been created yet     if query_engine is None:         bm25_retriever = BM25Retriever.from_defaults(             docstore=docstore,             similarity_top_k=4,         )                  query_engine = RetrieverQueryEngine(             retriever=bm25_retriever,          )      # Send a welcome message to the user     await cl.Message(         author=\"Assistant\", content=\"Hello! I'm an AI assistant using BM25 RAG. How may I help you?\"     ).send() In\u00a0[\u00a0]: Copied! <pre>@cl.on_message\nasync def handle_message(message: cl.Message):\n    global query_engine\n    # Check if any files were uploaded\n    if message.elements:\n        for file in message.elements:\n            if file.type == \"file\":\n                # Read the file and process it\n                documents = SimpleDirectoryReader(input_files=[file.path]).load_data()\n\n                # Ingest the documents into the pipeline and document store\n                pipeline = IngestionPipeline(\n                    transformations=[\n                        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n                        Settings.embed_model,\n                    ],\n                )\n                nodes = pipeline.run(documents=documents, show_progress=True)\n                docstore.add_documents(nodes)\n                \n                # Update the BM25 retriever and query engine\n                bm25_retriever = BM25Retriever.from_defaults(\n                    docstore=docstore,\n                    similarity_top_k=4,\n                )\n                query_engine = RetrieverQueryEngine(\n                    retriever=bm25_retriever,\n                )\n\n                await cl.Message(\n                    content=f\"Processed {len(nodes)} chunks from the uploaded file.\"\n                ).send()\n    res = await query_engine.aquery(message.content)\n    await cl.Message(content=str(res), author=\"Assistant\").send()\n</pre> @cl.on_message async def handle_message(message: cl.Message):     global query_engine     # Check if any files were uploaded     if message.elements:         for file in message.elements:             if file.type == \"file\":                 # Read the file and process it                 documents = SimpleDirectoryReader(input_files=[file.path]).load_data()                  # Ingest the documents into the pipeline and document store                 pipeline = IngestionPipeline(                     transformations=[                         SentenceSplitter(chunk_size=1024, chunk_overlap=20),                         Settings.embed_model,                     ],                 )                 nodes = pipeline.run(documents=documents, show_progress=True)                 docstore.add_documents(nodes)                                  # Update the BM25 retriever and query engine                 bm25_retriever = BM25Retriever.from_defaults(                     docstore=docstore,                     similarity_top_k=4,                 )                 query_engine = RetrieverQueryEngine(                     retriever=bm25_retriever,                 )                  await cl.Message(                     content=f\"Processed {len(nodes)} chunks from the uploaded file.\"                 ).send()     res = await query_engine.aquery(message.content)     await cl.Message(content=str(res), author=\"Assistant\").send() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    import sys\n    import subprocess\n\n    parser = argparse.ArgumentParser(description=\"BM25 RAG Script with ingestion option\")\n    parser.add_argument('--ingest', action='store_true', help='Ingest documents before starting the chat')\n    parser.add_argument('--data_dir', type=str, default=\"../data\", help='Directory containing documents to ingest')\n    args = parser.parse_args()\n\n    if args.ingest:\n        ingest_documents(args.data_dir)\n\n    # Run the Chainlit app\n    subprocess.run([\"chainlit\", \"run\", sys.argv[0] ,\"-w\"], check=True)\n</pre> if __name__ == \"__main__\":     import sys     import subprocess      parser = argparse.ArgumentParser(description=\"BM25 RAG Script with ingestion option\")     parser.add_argument('--ingest', action='store_true', help='Ingest documents before starting the chat')     parser.add_argument('--data_dir', type=str, default=\"../data\", help='Directory containing documents to ingest')     args = parser.parse_args()      if args.ingest:         ingest_documents(args.data_dir)      # Run the Chainlit app     subprocess.run([\"chainlit\", \"run\", sys.argv[0] ,\"-w\"], check=True)"},{"location":"RAG/01_BM25_RAG/app/#mongo-db-as-document-store","title":"Mongo DB as Document Store\u00b6","text":""},{"location":"RAG/01_BM25_RAG/app/#pip-install-llama-index-storage-docstore-redis","title":"!pip install llama-index-storage-docstore-redis\u00b6","text":""},{"location":"RAG/01_BM25_RAG/app/#pip-install-llama-index-storage-index-store-redis","title":"!pip install llama-index-storage-index-store-redis\u00b6","text":"<p>from llama_index.storage.docstore.redis import RedisDocumentStore</p>"},{"location":"RAG/01_BM25_RAG/notebook/","title":"BM25(llamaindex)","text":"BM25 RAG AI Engineering.academy In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index\n# !pip install llama-index-retrievers-bm25\n# !pip install llama-index-vector-stores-qdrant \n# !pip install llama-index-readers-file \n# !pip install llama-index-embeddings-fastembed \n# !pip install llama-index-llms-openai\n# !pip install llama-index-llms-groq\n# !pip install -U qdrant_client fastembed\n# !pip install python-dotenv\n# !pip install matplotlib\n</pre> # !pip install llama-index # !pip install llama-index-retrievers-bm25 # !pip install llama-index-vector-stores-qdrant  # !pip install llama-index-readers-file  # !pip install llama-index-embeddings-fastembed  # !pip install llama-index-llms-openai # !pip install llama-index-llms-groq # !pip install -U qdrant_client fastembed # !pip install python-dotenv # !pip install matplotlib In\u00a0[1]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.llms.groq import Groq\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n# GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\n# Setting up Base LLM\nSettings.llm = OpenAI(\n    model=\"gpt-4o-mini\", temperature=0.1, max_tokens=8096, streaming=True\n)\n\n# Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\n# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\nSettings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI from llama_index.llms.groq import Groq # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") # GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Setting up Base LLM Settings.llm = OpenAI(     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=8096, streaming=True )  # Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) # Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version <pre>/home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNone of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n</pre> In\u00a0[2]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\n# load documents\ndocuments = SimpleDirectoryReader(\"../data\", recursive=True).load_data(show_progress=True)\n</pre> from llama_index.core import SimpleDirectoryReader  # load documents documents = SimpleDirectoryReader(\"../data\", recursive=True).load_data(show_progress=True) <pre>Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  6.11file/s]\n</pre> In\u00a0[3]: Copied! <pre>from llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        # SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ],\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=documents , show_progress=True)\nprint(\"Number of Nodes:\",len(nodes))\n</pre> from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         # SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ], )  # Ingest directly into a vector db nodes = pipeline.run(documents=documents , show_progress=True) print(\"Number of Nodes:\",len(nodes)) <pre>Parsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00&lt;00:00, 14822.67it/s]\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:01&lt;00:00, 31.58it/s]</pre> <pre>Number of Nodes: 58\n</pre> <pre>\n</pre> In\u00a0[13]: Copied! <pre># initialize a docstore to store nodes\n# also available are mongodb, redis, postgres, etc for docstores\nimport asyncio\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\n\ndocstore = SimpleDocumentStore()\ndocstore.add_documents(nodes)\ndocstore.persist(persist_path=\"./docstore.json\")\n</pre> # initialize a docstore to store nodes # also available are mongodb, redis, postgres, etc for docstores import asyncio from llama_index.core.storage.docstore import SimpleDocumentStore  docstore = SimpleDocumentStore() docstore.add_documents(nodes) docstore.persist(persist_path=\"./docstore.json\") In\u00a0[\u00a0]: Copied! <pre>## Mongo DB as Document Store\n\n# !pip install llama-index-storage-index-store-mongodb\n# !pip install llama-index-storage-docstore-mongodb\n\n# from llama_index.storage.docstore.mongodb import MongoDocumentStore\n# from llama_index.storage.kvstore.mongodb import MongoDBKVStore\n# from pymongo import MongoClient\n# from motor.motor_asyncio import AsyncIOMotorClient\n\n# MONGO_URI = os.getenv(\"MONGO_URI\")\n# kv_store = MongoDBKVStore(mongo_client=MongoClient(MONGO_URI) , mongo_aclient=AsyncIOMotorClient(MONGO_URI))\n# docstore = MongoDocumentStore(namespace=\"BM25_RAG\" ,mongo_kvstore=kv_store).from_uri(uri=MONGO_URI)\n \n# docstore.add_documents(nodes)\n</pre> ## Mongo DB as Document Store  # !pip install llama-index-storage-index-store-mongodb # !pip install llama-index-storage-docstore-mongodb  # from llama_index.storage.docstore.mongodb import MongoDocumentStore # from llama_index.storage.kvstore.mongodb import MongoDBKVStore # from pymongo import MongoClient # from motor.motor_asyncio import AsyncIOMotorClient  # MONGO_URI = os.getenv(\"MONGO_URI\") # kv_store = MongoDBKVStore(mongo_client=MongoClient(MONGO_URI) , mongo_aclient=AsyncIOMotorClient(MONGO_URI)) # docstore = MongoDocumentStore(namespace=\"BM25_RAG\" ,mongo_kvstore=kv_store).from_uri(uri=MONGO_URI)   # docstore.add_documents(nodes) In\u00a0[6]: Copied! <pre># # !pip install llama-index-storage-docstore-redis\n# # !pip install llama-index-storage-index-store-redis\n# from llama_index.storage.docstore.redis import RedisDocumentStore\n\n# REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n# REDIS_PORT = os.getenv(\"REDIS_PORT\", 6379)\n\n# docstore=RedisDocumentStore.from_host_and_port(\n#         host=REDIS_HOST, port=REDIS_PORT, namespace=\"BM25_RAG\"\n#     )\n# docstore.add_documents(nodes)\n</pre> # # !pip install llama-index-storage-docstore-redis # # !pip install llama-index-storage-index-store-redis # from llama_index.storage.docstore.redis import RedisDocumentStore  # REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") # REDIS_PORT = os.getenv(\"REDIS_PORT\", 6379)  # docstore=RedisDocumentStore.from_host_and_port( #         host=REDIS_HOST, port=REDIS_PORT, namespace=\"BM25_RAG\" #     ) # docstore.add_documents(nodes) In\u00a0[7]: Copied! <pre>from llama_index.retrievers.bm25 import BM25Retriever\nimport Stemmer\n\n# We can pass in the index, docstore, or list of nodes to create the retriever\nbm25_retriever = BM25Retriever.from_defaults(\n    docstore=docstore,\n    similarity_top_k=4,\n    # Optional: We can pass in the stemmer and set the language for stopwords\n    # This is important for removing stopwords and stemming the query + text\n    # The default is english for both\n    stemmer=Stemmer.Stemmer(\"english\"),\n    language=\"english\",\n)\n</pre> from llama_index.retrievers.bm25 import BM25Retriever import Stemmer  # We can pass in the index, docstore, or list of nodes to create the retriever bm25_retriever = BM25Retriever.from_defaults(     docstore=docstore,     similarity_top_k=4,     # Optional: We can pass in the stemmer and set the language for stopwords     # This is important for removing stopwords and stemming the query + text     # The default is english for both     stemmer=Stemmer.Stemmer(\"english\"),     language=\"english\", ) In\u00a0[8]: Copied! <pre>from llama_index.core.response.notebook_utils import display_source_node\n\n# will retrieve context from specific companies\nretrieved_nodes = bm25_retriever.retrieve(\n    \"Who are the Authors of this paper\"\n)\nfor node in retrieved_nodes:\n    display_source_node(node, source_length=5000)\n</pre> from llama_index.core.response.notebook_utils import display_source_node  # will retrieve context from specific companies retrieved_nodes = bm25_retriever.retrieve(     \"Who are the Authors of this paper\" ) for node in retrieved_nodes:     display_source_node(node, source_length=5000) <p>Node ID: 04328457-baaf-4ee5-be1b-70f604c2fe05Similarity: 1.7577731609344482Text: Authors</p> <p>Ashish Vaswani*</p> <p>Noam Shazeer*</p> <p>Niki Parmar*</p> <p>Jakob Uszkoreit*</p> <p>Google Brain</p> <p>avaswani@google.com</p> <p>noam@google.com</p> <p>nikip@google.com</p> <p>usz@google.com</p> <p>Llion Jones*</p> <p>Aidan N. Gomez* \u2020</p> <p>\u0141ukasz Kaiser*</p> <p>Google Research</p> <p>University of Toronto</p> <p>llion@google.com</p> <p>aidan@cs.toronto.edu</p> <p>lukaszkaiser@google.com</p> <p>Illia Polosukhin* \u2021</p> <p>illia.polosukhin@gmail.com</p> <p>Node ID: a2d32369-caf0-4529-ab20-dfd7b49a7705Similarity: 1.4452868700027466Text: 5.2 Hardware and Schedule</p> <p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).</p> In\u00a0[14]: Copied! <pre>retrieved_nodes = bm25_retriever.retrieve(\"What is Attention mechanism\")\nfor node in retrieved_nodes:\n    display_source_node(node, source_length=5000)\n</pre> retrieved_nodes = bm25_retriever.retrieve(\"What is Attention mechanism\") for node in retrieved_nodes:     display_source_node(node, source_length=5000) <p>Node ID: e3519af3-3040-4fb5-84d3-485887327d61Similarity: 2.1414361000061035Text: What we are missing</p> <p>In my opinion...</p> <p>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.</p> <p>15</p> <p>Node ID: 0fd6220c-d85e-4ede-8f33-c1237de9709bSimilarity: 1.9467532634735107Text: Input-Input Layer 5</p> <p>The law will never be perfect, but its application should be just.</p> <p>This is what we are missing, in my opinion.</p> <p>Node ID: 403d39f5-3650-4caa-bce7-cda0bbd11496Similarity: 1.4834907054901123Text: Attention Visualizations</p> <p>It is this spirit that a majority of American governments have passed new laws since 2009.</p> <p>Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color.</p> <p>Voting process more difficult. ---</p> <p>Node ID: 65456aec-30fc-4363-84f1-d1caa5fcaa28Similarity: 1.2847681045532227Text: 2 Background</p> <p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p> <p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].</p> <p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].</p> <p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].</p> In\u00a0[14]: Copied! <pre>from llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core import get_response_synthesizer\nfrom llama_index.core.response_synthesizers import ResponseMode\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=ResponseMode.COMPACT_ACCUMULATE\n)\n\nBM25_QUERY_ENGINE = RetrieverQueryEngine(\n    retriever=bm25_retriever,\n)\n</pre> from llama_index.core.query_engine import RetrieverQueryEngine from llama_index.core import get_response_synthesizer from llama_index.core.response_synthesizers import ResponseMode  response_synthesizer = get_response_synthesizer(     response_mode=ResponseMode.COMPACT_ACCUMULATE )  BM25_QUERY_ENGINE = RetrieverQueryEngine(     retriever=bm25_retriever, ) In\u00a0[\u00a0]: Copied! <pre>response = BM25_QUERY_ENGINE.query(\"How many encoders are stacked in the transformer?\")\ndisplay(Markdown(str(response)))\n</pre> response = BM25_QUERY_ENGINE.query(\"How many encoders are stacked in the transformer?\") display(Markdown(str(response)))"},{"location":"RAG/01_BM25_RAG/notebook/#introduction","title":"Introduction\u00b6","text":"<p>BM25 Retrieval-Augmented Generation (BM25 RAG) is an advanced technique that combines the power of the BM25 (Best Matching 25) algorithm for information retrieval with large language models for text generation. This approach enhances the accuracy and relevance of generated responses by grounding them in specific, retrieved information using a proven probabilistic retrieval model.</p> <p>This notebook aims to provide a clear and concise introduction to BM25 RAG, suitable for both beginners and experienced practitioners who want to understand and implement this technology.</p>"},{"location":"RAG/01_BM25_RAG/notebook/#motivation","title":"Motivation\u00b6","text":"<p>Traditional RAG systems often use dense vector embeddings for retrieval, which can be computationally expensive and may not always capture the nuances of term importance. BM25 RAG addresses these limitations by using a probabilistic retrieval model that considers term frequency, inverse document frequency, and document length. This approach can lead to more accurate and interpretable retrieval, especially for queries requiring specific or rare information.</p>"},{"location":"RAG/01_BM25_RAG/notebook/#method-details","title":"Method Details\u00b6","text":""},{"location":"RAG/01_BM25_RAG/notebook/#document-preprocessing-and-indexing","title":"Document Preprocessing and Indexing\u00b6","text":"<ol> <li><p>Document Chunking: The knowledge base documents are preprocessed and split into manageable chunks to create a searchable corpus.</p> </li> <li><p>Tokenization and Indexing: Each chunk is tokenized, and an inverted index is created. The BM25 algorithm calculates term frequencies and inverse document frequencies.</p> </li> </ol>"},{"location":"RAG/01_BM25_RAG/notebook/#bm25-retrieval-augmented-generation-workflow","title":"BM25 Retrieval-Augmented Generation Workflow\u00b6","text":"<ol> <li><p>Query Input: A user provides a query that needs to be answered.</p> </li> <li><p>Retrieval Step: The query is tokenized, and relevant documents are retrieved using the BM25 scoring algorithm. This step considers term frequency, inverse document frequency, and document length to find the most relevant chunks.</p> </li> <li><p>Generation Step: The retrieved document chunks are passed to a large language model as additional context. The model uses this context to generate a more accurate and relevant response.</p> </li> </ol>"},{"location":"RAG/01_BM25_RAG/notebook/#key-features-of-bm25-rag","title":"Key Features of BM25 RAG\u00b6","text":"<ol> <li><p>Probabilistic Retrieval: BM25 uses a probabilistic model to rank documents, providing a theoretically sound basis for retrieval.</p> </li> <li><p>Term Frequency Saturation: BM25 accounts for diminishing returns from repeated terms, improving retrieval quality.</p> </li> <li><p>Document Length Normalization: The algorithm considers document length, reducing bias towards longer documents.</p> </li> <li><p>No Embedding Required: Unlike vector-based approaches, BM25 doesn't require document embeddings, which can be computationally efficient.</p> </li> </ol>"},{"location":"RAG/01_BM25_RAG/notebook/#benefits-of-this-approach","title":"Benefits of this Approach\u00b6","text":"<ol> <li><p>Improved Accuracy: Combines the strengths of probabilistic retrieval and neural text generation.</p> </li> <li><p>Interpretability: BM25 scoring provides a more interpretable retrieval process compared to dense vector retrieval methods.</p> </li> <li><p>Effective for Long-tail Queries: Particularly good at handling queries requiring specific or rare information.</p> </li> </ol>"},{"location":"RAG/01_BM25_RAG/notebook/#conclusion","title":"Conclusion\u00b6","text":"<p>BM25 Retrieval-Augmented Generation represents a powerful fusion of classic information retrieval techniques and modern language models. By leveraging the strengths of the BM25 algorithm, this approach offers improved accuracy, interpretability, and efficiency in various natural language processing tasks. As AI continues to evolve, BM25 RAG stands out as a robust method for building more reliable and context-sensitive AI systems, especially in domains where precise information retrieval is crucial.</p>"},{"location":"RAG/01_BM25_RAG/notebook/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Preferably Python 3.11</li> <li>Jupyter Notebook or JupyterLab</li> <li>LLM API Key<ul> <li>You can use any LLM of your choice; in this notebook, we use OpenAI's GPT models</li> </ul> </li> </ul> <p>With these steps, you can implement a BM25 RAG system to enhance the capabilities of language models by incorporating efficient, probabilistic information retrieval, improving their effectiveness in various applications.</p>"},{"location":"RAG/01_Basic_RAG/","title":"Intro to RAG","text":"# Basic RAG  ### [AI Engineering.academy](https://aiengineering.academy/)"},{"location":"RAG/01_Basic_RAG/#introduction","title":"Introduction","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of large language models with the ability to retrieve relevant information from a knowledge base. This approach enhances the quality and accuracy of generated responses by grounding them in specific, retrieved information. a This notebook aims to provide a clear and concise introduction to RAG, suitable for beginners who want to understand and implement this technology.</p>"},{"location":"RAG/01_Basic_RAG/#rag-flow","title":"RAG Flow","text":"<pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Document Embedding\"\n        EM1{{Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; EM1\n        EM1 --&gt; D1[Embedding-1] &amp; D2[Embedding-2] &amp; D3[Embedding-3]\n    end\n\n    subgraph \"3. Indexing\"\n        D1 &amp; D2 &amp; D3 --&gt; E[(VectorDB)]\n    end\n\n    subgraph \"4. Query Processing\"\n        F[Query] --&gt; EM2{{Embedding Model}}\n        EM2 --&gt; G[Query Embedding]\n    end\n\n    subgraph \"5. Retrieval\"\n        G --&gt;|Similarity Search| E\n        E --&gt;|Top-K Retrieval| H[Relevant Chunks]\n    end\n\n    subgraph \"6. Context Formation\"\n        H --&gt; I[Query + Relevant Chunks]\n    end\n\n    subgraph \"7. Generation\"\n        I --&gt; J[LLM]\n        J --&gt; K[Response]\n    end\n\n    F --&gt; I</code></pre>"},{"location":"RAG/01_Basic_RAG/#get-started","title":"Get Started","text":""},{"location":"RAG/01_Basic_RAG/#notebook","title":"Notebook","text":"<p>You can run the notebook provided in this repository.</p>"},{"location":"RAG/01_Basic_RAG/#chat-application","title":"Chat Application","text":"<ol> <li>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Run the application:    <pre><code>python app.py\n</code></pre></li> <li>To ingest data on the go:    <pre><code>python app.py --ingest --data_dir /path/to/documents\n</code></pre></li> </ol>"},{"location":"RAG/01_Basic_RAG/#server","title":"Server","text":"<p>Run the server with:</p> <pre><code>python server.py\n</code></pre> <p>The server has two endpoints:</p> <ul> <li><code>/api/ingest</code></li> <li><code>/api/query</code></li> </ul>"},{"location":"RAG/01_Basic_RAG/#motivation","title":"Motivation","text":"<p>Traditional language models generate text based on learned patterns from training data. However, when presented with queries that require specific, updated, or niche information, they may struggle to provide accurate responses. RAG addresses this limitation by incorporating a retrieval step that provides the language model with relevant context to generate more informed answers.</p>"},{"location":"RAG/01_Basic_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/01_Basic_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Document Chunking: The knowledge base documents (e.g., PDFs, articles) are preprocessed and split into manageable chunks. This creates a searchable corpus that can be efficiently used in the retrieval process.</li> <li>Embedding Generation: Each chunk is converted into a vector representation using pre-trained embeddings (e.g., OpenAI's embeddings). This allows the documents to be stored in a vector database, such as Qdrant, enabling efficient similarity searches.</li> </ol>"},{"location":"RAG/01_Basic_RAG/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Input: A user provides a query that needs to be answered.</li> <li> <p>Retrieval Step: The query is embedded into a vector using the same embedding model that was used for the documents. A similarity search is then performed in the vector database to find the most relevant document chunks.</p> </li> <li> <p>Generation Step: The retrieved document chunks are passed to a large language model (e.g., GPT-4) as additional context. The model uses this context to generate a more accurate and relevant response.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/#key-features-of-rag","title":"Key Features of RAG","text":"<ol> <li>Contextual Relevance: By grounding responses in actual retrieved information, RAG models can produce more contextually relevant and accurate answers.</li> <li> <p>Scalability: The retrieval step can scale to handle large knowledge bases, allowing the model to draw from vast amounts of information.</p> </li> <li> <p>Flexibility in Use Cases: RAG can be adapted for a variety of applications, including question answering, summarization, recommendation systems, and more.</p> </li> <li> <p>Improved Accuracy: Combining generation with retrieval often yields more precise results, especially for queries requiring specific or lesser-known information.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li> <p>Combines Strengths of Both Retrieval and Generation: RAG effectively merges retrieval-based methods with generative models, allowing for both precise fact-finding and natural language generation.</p> </li> <li> <p>Enhanced Handling of Long-Tail Queries: It is particularly effective for queries where specific and less frequently occurring information is needed.</p> </li> <li> <p>Domain Adaptability: The retrieval mechanism can be tuned to specific domains, ensuring that the generated responses are grounded in the most relevant and accurate domain-specific information.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/#conclusion","title":"Conclusion","text":"<p>Retrieval-Augmented Generation (RAG) represents an innovative fusion of retrieval and generation techniques, significantly enhancing the capabilities of language models by grounding their outputs in relevant external information. This approach can be particularly valuable in scenarios requiring precise, context-aware responses, such as customer support, academic research, and more. As AI continues to evolve, RAG stands out as a powerful method for building more reliable and context-sensitive AI systems.</p>"},{"location":"RAG/01_Basic_RAG/#prerequisites","title":"Prerequisites","text":"<ul> <li>Preferably Python 3.11</li> <li>Jupyter Notebook or JupyterLab</li> <li>LLM API Key</li> <li>You can use any LLM of your choice. In this notebook, we have used OpenAI and GPT-4o-mini.</li> </ul> <p>With these steps, you can implement a basic RAG system to enhance the capabilities of language models by incorporating real-world, up-to-date information, improving their effectiveness in various applications.</p>"},{"location":"RAG/01_Basic_RAG/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport openai\nimport chainlit as cl\nimport argparse\nfrom dotenv import load_dotenv\nfrom llama_index.core import (\n    Settings,\n    StorageContext,\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    load_index_from_storage,\n)\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.service_context import ServiceContext\n</pre> import os import openai import chainlit as cl import argparse from dotenv import load_dotenv from llama_index.core import (     Settings,     StorageContext,     VectorStoreIndex,     SimpleDirectoryReader,     load_index_from_storage, ) from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine from llama_index.core.callbacks import CallbackManager from llama_index.core.service_context import ServiceContext In\u00a0[\u00a0]: Copied! <pre># LlamaIndex vector store import\nimport qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.ingestion import IngestionPipeline\n</pre> # LlamaIndex vector store import import qdrant_client from llama_index.vector_stores.qdrant import QdrantVectorStore from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline In\u00a0[\u00a0]: Copied! <pre># Load environment variables from .env file\nprint(\"Loading Environment variables\")\nload_dotenv()\n</pre> # Load environment variables from .env file print(\"Loading Environment variables\") load_dotenv() In\u00a0[\u00a0]: Copied! <pre># Set OpenAI API key\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = OPENAI_API_KEY\n</pre> # Set OpenAI API key OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") openai.api_key = OPENAI_API_KEY In\u00a0[\u00a0]: Copied! <pre># Configure LLM settings\nSettings.llm = OpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.1,\n    max_tokens=1024,\n    streaming=True,\n    api_key=OPENAI_API_KEY,\n)\n</pre> # Configure LLM settings Settings.llm = OpenAI(     model=\"gpt-4o-mini\",     temperature=0.1,     max_tokens=1024,     streaming=True,     api_key=OPENAI_API_KEY, ) In\u00a0[\u00a0]: Copied! <pre># Set embedding model and context window\n# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nSettings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\nSettings.context_window = 4096\nSettings.callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()])\n</pre> # Set embedding model and context window # Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY) Settings.context_window = 4096 Settings.callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()]) In\u00a0[\u00a0]: Copied! <pre># Connect to the Vector Database\nprint(\"Connecting to Vector Database\")\nclient = qdrant_client.QdrantClient(\n    host=\"localhost\",\n    port=6333\n)\n</pre> # Connect to the Vector Database print(\"Connecting to Vector Database\") client = qdrant_client.QdrantClient(     host=\"localhost\",     port=6333 ) In\u00a0[\u00a0]: Copied! <pre># Initialize the vector store\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\")\n</pre> # Initialize the vector store vector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\") In\u00a0[\u00a0]: Copied! <pre>def ingest_documents(data_dir):\n    # Load documents from a directory\n    documents = SimpleDirectoryReader(data_dir, recursive=True).load_data(\n        show_progress=True\n    )\n\n    # Ingest data into the vector store\n    print(\"Ingesting Data\")\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(\n                chunk_size=1024, chunk_overlap=20\n            ),  # Split documents into chunks\n            Settings.embed_model,  # Use the embedding model for processing\n        ],\n        vector_store=vector_store,\n    )\n\n    # Ingest directly into the vector database\n    nodes = pipeline.run(documents=documents, show_progress=True)\n    print(\"Number of chunks added to vector DB:\", len(nodes))\n</pre> def ingest_documents(data_dir):     # Load documents from a directory     documents = SimpleDirectoryReader(data_dir, recursive=True).load_data(         show_progress=True     )      # Ingest data into the vector store     print(\"Ingesting Data\")     pipeline = IngestionPipeline(         transformations=[             SentenceSplitter(                 chunk_size=1024, chunk_overlap=20             ),  # Split documents into chunks             Settings.embed_model,  # Use the embedding model for processing         ],         vector_store=vector_store,     )      # Ingest directly into the vector database     nodes = pipeline.run(documents=documents, show_progress=True)     print(\"Number of chunks added to vector DB:\", len(nodes)) In\u00a0[\u00a0]: Copied! <pre># Global variable to store the index\nindex = None\n</pre> # Global variable to store the index index = None In\u00a0[\u00a0]: Copied! <pre>@cl.on_chat_start\nasync def start():\n    global index\n    # Initialize the index if it hasn't been created yet\n    if index is None:\n        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n    # Initialize service context and query engine on chat start\n    query_engine = index.as_query_engine(\n        streaming=True,\n        similarity_top_k=5,\n    )\n    cl.user_session.set(\"query_engine\", query_engine)\n\n    # Send a welcome message to the user\n    await cl.Message(\n        author=\"Assistant\", content=\"Hello! I'm an AI assistant. How may I help you?\"\n    ).send()\n</pre> @cl.on_chat_start async def start():     global index     # Initialize the index if it hasn't been created yet     if index is None:         index = VectorStoreIndex.from_vector_store(vector_store=vector_store)      # Initialize service context and query engine on chat start     query_engine = index.as_query_engine(         streaming=True,         similarity_top_k=5,     )     cl.user_session.set(\"query_engine\", query_engine)      # Send a welcome message to the user     await cl.Message(         author=\"Assistant\", content=\"Hello! I'm an AI assistant. How may I help you?\"     ).send() In\u00a0[\u00a0]: Copied! <pre>@cl.on_message\nasync def handle_message(message: cl.Message):\n    global index\n    # Check if any files were uploaded\n    if message.elements:\n        for file in message.elements:\n            if file.type == \"file\":\n                # Read the file and process it\n                documents = SimpleDirectoryReader(input_files=[file.path]).load_data()\n\n                # Ingest the documents into the pipeline\n                pipeline = IngestionPipeline(\n                    transformations=[\n                        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n                        Settings.embed_model,\n                    ],\n                    vector_store=vector_store,\n                )\n                nodes = pipeline.run(documents=documents, show_progress=True)\n                \n                # Update the index with new documents\n                index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n                \n                # Update the query engine\n                query_engine = index.as_query_engine(\n                    streaming=True,\n                    similarity_top_k=5,\n                )\n                cl.user_session.set(\"query_engine\", query_engine)\n\n                await cl.Message(\n                    content=f\"Processed {len(nodes)} chunks from the uploaded file.\"\n                ).send()\n            \n\n    # Retrieve the query engine from the user session\n    query_engine = cl.user_session.get(\"query_engine\")  # type: RetrieverQueryEngine\n\n    # Prepare to send a response to the user's message\n    msg = cl.Message(content=\"\", author=\"Assistant\")\n\n    # Query the engine with the user's message\n    res = await cl.make_async(query_engine.query)(message.content)\n\n    # Stream the response tokens back to the user\n    for token in res.response_gen:\n        await msg.stream_token(token)\n    await msg.send()\n</pre> @cl.on_message async def handle_message(message: cl.Message):     global index     # Check if any files were uploaded     if message.elements:         for file in message.elements:             if file.type == \"file\":                 # Read the file and process it                 documents = SimpleDirectoryReader(input_files=[file.path]).load_data()                  # Ingest the documents into the pipeline                 pipeline = IngestionPipeline(                     transformations=[                         SentenceSplitter(chunk_size=1024, chunk_overlap=20),                         Settings.embed_model,                     ],                     vector_store=vector_store,                 )                 nodes = pipeline.run(documents=documents, show_progress=True)                                  # Update the index with new documents                 index = VectorStoreIndex.from_vector_store(vector_store=vector_store)                                  # Update the query engine                 query_engine = index.as_query_engine(                     streaming=True,                     similarity_top_k=5,                 )                 cl.user_session.set(\"query_engine\", query_engine)                  await cl.Message(                     content=f\"Processed {len(nodes)} chunks from the uploaded file.\"                 ).send()                   # Retrieve the query engine from the user session     query_engine = cl.user_session.get(\"query_engine\")  # type: RetrieverQueryEngine      # Prepare to send a response to the user's message     msg = cl.Message(content=\"\", author=\"Assistant\")      # Query the engine with the user's message     res = await cl.make_async(query_engine.query)(message.content)      # Stream the response tokens back to the user     for token in res.response_gen:         await msg.stream_token(token)     await msg.send() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    import sys\n    import subprocess\n\n    parser = argparse.ArgumentParser(description=\"RAG Script with ingestion option\")\n    parser.add_argument(\"--host\", default=\"localhost\", help='Host IP address')\n    parser.add_argument(\"--port\", type=int, default=8000, help='Port number')\n    parser.add_argument('--ingest', action='store_true', help='Ingest documents before starting the chat')\n    parser.add_argument('--data_dir', type=str, default=\"../data\", help='Directory containing documents to ingest')\n    args = parser.parse_args()\n\n    if args.ingest:\n        ingest_documents(args.data_dir)\n\n    # Run the Chainlit app\n    subprocess.run([\n        \"chainlit\", \"run\", sys.argv[0],\n        \"--host\", args.host,\n        \"--port\", str(args.port)\n    ], check=True)\n</pre> if __name__ == \"__main__\":     import sys     import subprocess      parser = argparse.ArgumentParser(description=\"RAG Script with ingestion option\")     parser.add_argument(\"--host\", default=\"localhost\", help='Host IP address')     parser.add_argument(\"--port\", type=int, default=8000, help='Port number')     parser.add_argument('--ingest', action='store_true', help='Ingest documents before starting the chat')     parser.add_argument('--data_dir', type=str, default=\"../data\", help='Directory containing documents to ingest')     args = parser.parse_args()      if args.ingest:         ingest_documents(args.data_dir)      # Run the Chainlit app     subprocess.run([         \"chainlit\", \"run\", sys.argv[0],         \"--host\", args.host,         \"--port\", str(args.port)     ], check=True)"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/","title":"Basic RAG from Scratch","text":"In\u00a0[\u00a0]: Copied! <pre>import fitz\nimport numpy as np\nimport json\nimport os\nfrom litellm import completion, embedding\n\n# plain openai also can be used\n# from openai import OpenAI\n\n# initilize openai client\n# client = OpenAI(,\n#     api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n# )\n\n# we are using litellm as it allows us to easily switch between different LLM providers\n# and is compatible with the same API\n\n# Configure API keys (replace with your actual keys)\nos.environ['OPENAI_API_KEY'] = \"\"  # Replace with your OpenAI API key\nos.environ['ANTHROPIC_API_KEY'] = \"\" # Replace with your Anthropic API key\nos.environ['GROQ_API_KEY'] = \"\" # Replace with your Groq API key\n</pre> import fitz import numpy as np import json import os from litellm import completion, embedding  # plain openai also can be used # from openai import OpenAI  # initilize openai client # client = OpenAI(, #     api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables # )  # we are using litellm as it allows us to easily switch between different LLM providers # and is compatible with the same API  # Configure API keys (replace with your actual keys) os.environ['OPENAI_API_KEY'] = \"\"  # Replace with your OpenAI API key os.environ['ANTHROPIC_API_KEY'] = \"\" # Replace with your Anthropic API key os.environ['GROQ_API_KEY'] = \"\" # Replace with your Groq API key  In\u00a0[\u00a0]: Copied! <pre>def extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extracts and consolidates text from all pages of a PDF file. This is the first step in the RAG pipeline,\n    where we acquire the raw textual data that will later be processed, embedded, and retrieved against.\n\n    Args:\n    pdf_path (str): Path to the PDF file to be processed.\n\n    Returns:\n    str: Complete extracted text from all pages of the PDF, concatenated into a single string.\n         This raw text will be further processed in subsequent steps of the RAG pipeline.\n    \"\"\"\n    # Open the PDF file\n    mypdf = fitz.open(pdf_path)\n    all_text = \"\"  # Initialize an empty string to store the extracted text\n\n    # Iterate through each page in the PDF\n    for page_num in range(mypdf.page_count):\n        page = mypdf[page_num]  # Get the page\n        text = page.get_text(\"text\")  # Extract text from the page\n        all_text += text  # Append the extracted text to the all_text string\n\n    return all_text  # Return the extracted text\n</pre> def extract_text_from_pdf(pdf_path):     \"\"\"     Extracts and consolidates text from all pages of a PDF file. This is the first step in the RAG pipeline,     where we acquire the raw textual data that will later be processed, embedded, and retrieved against.      Args:     pdf_path (str): Path to the PDF file to be processed.      Returns:     str: Complete extracted text from all pages of the PDF, concatenated into a single string.          This raw text will be further processed in subsequent steps of the RAG pipeline.     \"\"\"     # Open the PDF file     mypdf = fitz.open(pdf_path)     all_text = \"\"  # Initialize an empty string to store the extracted text      # Iterate through each page in the PDF     for page_num in range(mypdf.page_count):         page = mypdf[page_num]  # Get the page         text = page.get_text(\"text\")  # Extract text from the page         all_text += text  # Append the extracted text to the all_text string      return all_text  # Return the extracted text In\u00a0[\u00a0]: Copied! <pre>def chunk_text(text, n, overlap):\n    \"\"\"\n    Divides text into smaller, overlapping chunks for more effective processing and retrieval.\n    Chunking is a critical step in RAG systems as it:\n    1. Makes large documents manageable for embedding models that have token limits\n    2. Enables more precise retrieval of relevant information\n    3. Allows for contextual understanding within reasonable boundaries\n    \n    The overlap between chunks helps maintain context continuity and reduces the risk of\n    splitting important information across chunk boundaries.\n\n    Args:\n    text (str): The complete text to be chunked.\n    n (int): The maximum number of characters in each chunk.\n    overlap (int): The number of overlapping characters between consecutive chunks.\n                   Higher overlap improves context preservation but increases redundancy.\n\n    Returns:\n    List[str]: A list of text chunks that will be individually embedded and used for retrieval.\n    \"\"\"\n    chunks = []  # Initialize an empty list to store the chunks\n    \n    # Loop through the text with a step size of (n - overlap)\n    for i in range(0, len(text), n - overlap):\n        # Append a chunk of text from index i to i + n to the chunks list\n        chunks.append(text[i:i + n])\n\n    return chunks  # Return the list of text chunks\n</pre> def chunk_text(text, n, overlap):     \"\"\"     Divides text into smaller, overlapping chunks for more effective processing and retrieval.     Chunking is a critical step in RAG systems as it:     1. Makes large documents manageable for embedding models that have token limits     2. Enables more precise retrieval of relevant information     3. Allows for contextual understanding within reasonable boundaries          The overlap between chunks helps maintain context continuity and reduces the risk of     splitting important information across chunk boundaries.      Args:     text (str): The complete text to be chunked.     n (int): The maximum number of characters in each chunk.     overlap (int): The number of overlapping characters between consecutive chunks.                    Higher overlap improves context preservation but increases redundancy.      Returns:     List[str]: A list of text chunks that will be individually embedded and used for retrieval.     \"\"\"     chunks = []  # Initialize an empty list to store the chunks          # Loop through the text with a step size of (n - overlap)     for i in range(0, len(text), n - overlap):         # Append a chunk of text from index i to i + n to the chunks list         chunks.append(text[i:i + n])      return chunks  # Return the list of text chunks In\u00a0[\u00a0]: Copied! <pre># Define the path to the PDF file\npdf_path = \"data/AI_Information.pdf\"\n\n# Extract text from the PDF file\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\ntext_chunks = chunk_text(extracted_text, 1000, 200)\n\n# Print the number of text chunks created\nprint(\"Number of text chunks:\", len(text_chunks))\n\n# Print the first text chunk\nprint(\"\\nFirst text chunk:\")\nprint(text_chunks[0])\n</pre> # Define the path to the PDF file pdf_path = \"data/AI_Information.pdf\"  # Extract text from the PDF file extracted_text = extract_text_from_pdf(pdf_path)  # Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters text_chunks = chunk_text(extracted_text, 1000, 200)  # Print the number of text chunks created print(\"Number of text chunks:\", len(text_chunks))  # Print the first text chunk print(\"\\nFirst text chunk:\") print(text_chunks[0]) In\u00a0[\u00a0]: Copied! <pre>def create_embeddings(text, model=\"text-embedding-ada-002\"):\n    \"\"\"\n    Transforms text into dense vector representations (embeddings) using a neural network model.\n    Embeddings are the cornerstone of modern RAG systems because they:\n    1. Capture semantic meaning in a numerical format that computers can process\n    2. Enable similarity-based retrieval beyond simple keyword matching\n    3. Allow for efficient indexing and searching of large document collections\n    \n    In RAG, both document chunks and user queries are embedded in the same vector space,\n    allowing us to find the most semantically relevant chunks for a given query.\n\n    Args:\n    text (str or List[str]): The input text(s) to be embedded. Can be a single string or a list of strings.\n    model (str): The embedding model to use. Default is OpenAI's \"text-embedding-ada-002\".\n                 Different models offer various tradeoffs between quality, speed, and cost.\n\n    Returns:\n    dict: The response from the API containing the embeddings, which are high-dimensional\n          vectors representing the semantic content of the input text(s).\n    \"\"\"\n    # Create embeddings for the input text using the specified model\n    response = embedding(model=model, input=text)\n\n    return response  # Return the response containing the embeddings\n\n# Create embeddings for the text chunks\nresponse = create_embeddings(text_chunks)\n</pre> def create_embeddings(text, model=\"text-embedding-ada-002\"):     \"\"\"     Transforms text into dense vector representations (embeddings) using a neural network model.     Embeddings are the cornerstone of modern RAG systems because they:     1. Capture semantic meaning in a numerical format that computers can process     2. Enable similarity-based retrieval beyond simple keyword matching     3. Allow for efficient indexing and searching of large document collections          In RAG, both document chunks and user queries are embedded in the same vector space,     allowing us to find the most semantically relevant chunks for a given query.      Args:     text (str or List[str]): The input text(s) to be embedded. Can be a single string or a list of strings.     model (str): The embedding model to use. Default is OpenAI's \"text-embedding-ada-002\".                  Different models offer various tradeoffs between quality, speed, and cost.      Returns:     dict: The response from the API containing the embeddings, which are high-dimensional           vectors representing the semantic content of the input text(s).     \"\"\"     # Create embeddings for the input text using the specified model     response = embedding(model=model, input=text)      return response  # Return the response containing the embeddings  # Create embeddings for the text chunks response = create_embeddings(text_chunks) In\u00a0[\u00a0]: Copied! <pre>def cosine_similarity(vec1, vec2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors, which measures the cosine of the angle between them.\n    \n    Cosine similarity is particularly well-suited for RAG systems because:\n    1. It measures semantic similarity independent of vector magnitude (document length)\n    2. It ranges from -1 (completely opposite) to 1 (exactly the same), making it easy to interpret\n    3. It works well in high-dimensional spaces like those used for text embeddings\n    4. It's computationally efficient compared to some other similarity metrics\n\n    Args:\n    vec1 (np.ndarray): The first embedding vector.\n    vec2 (np.ndarray): The second embedding vector.\n\n    Returns:\n    float: The cosine similarity score between the two vectors, ranging from -1 to 1.\n           Higher values indicate greater semantic similarity between the original texts.\n    \"\"\"\n    # Compute the dot product of the two vectors and divide by the product of their norms\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n</pre> def cosine_similarity(vec1, vec2):     \"\"\"     Calculates the cosine similarity between two vectors, which measures the cosine of the angle between them.          Cosine similarity is particularly well-suited for RAG systems because:     1. It measures semantic similarity independent of vector magnitude (document length)     2. It ranges from -1 (completely opposite) to 1 (exactly the same), making it easy to interpret     3. It works well in high-dimensional spaces like those used for text embeddings     4. It's computationally efficient compared to some other similarity metrics      Args:     vec1 (np.ndarray): The first embedding vector.     vec2 (np.ndarray): The second embedding vector.      Returns:     float: The cosine similarity score between the two vectors, ranging from -1 to 1.            Higher values indicate greater semantic similarity between the original texts.     \"\"\"     # Compute the dot product of the two vectors and divide by the product of their norms     return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)) In\u00a0[\u00a0]: Copied! <pre>def semantic_search(query, text_chunks, embeddings, k=5):\n    \"\"\"\n    Performs semantic search to find the most relevant text chunks for a given query.\n    This is the core retrieval component of the RAG system, responsible for:\n    1. Finding the most semantically relevant information from the knowledge base\n    2. Filtering out irrelevant content to improve generation quality\n    3. Providing the context that will be used by the LLM for response generation\n    \n    The quality of retrieval directly impacts the quality of the final generated response,\n    as the LLM can only work with the context it's provided.\n\n    Args:\n    query (str): The user's question or query text.\n    text_chunks (List[str]): The corpus of text chunks to search through.\n    embeddings (List[dict]): Pre-computed embeddings for each text chunk.\n    k (int): The number of top relevant chunks to retrieve. This parameter balances:\n             - Too low: May miss relevant information\n             - Too high: May include irrelevant information and exceed context limits\n\n    Returns:\n    List[str]: The top k most semantically relevant text chunks for the query,\n               which will be used as context for the LLM to generate a response.\n    \"\"\"\n    # Create an embedding for the query\n    query_embedding = create_embeddings(query).data[0].embedding\n    similarity_scores = []  # Initialize a list to store similarity scores\n\n    # Calculate similarity scores between the query embedding and each text chunk embedding\n    for i, chunk_embedding in enumerate(embeddings):\n        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n\n    # Sort the similarity scores in descending order\n    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n    # Get the indices of the top k most similar text chunks\n    top_indices = [index for index, _ in similarity_scores[:k]]\n    # Return the top k most relevant text chunks\n    return [text_chunks[index] for index in top_indices]\n</pre> def semantic_search(query, text_chunks, embeddings, k=5):     \"\"\"     Performs semantic search to find the most relevant text chunks for a given query.     This is the core retrieval component of the RAG system, responsible for:     1. Finding the most semantically relevant information from the knowledge base     2. Filtering out irrelevant content to improve generation quality     3. Providing the context that will be used by the LLM for response generation          The quality of retrieval directly impacts the quality of the final generated response,     as the LLM can only work with the context it's provided.      Args:     query (str): The user's question or query text.     text_chunks (List[str]): The corpus of text chunks to search through.     embeddings (List[dict]): Pre-computed embeddings for each text chunk.     k (int): The number of top relevant chunks to retrieve. This parameter balances:              - Too low: May miss relevant information              - Too high: May include irrelevant information and exceed context limits      Returns:     List[str]: The top k most semantically relevant text chunks for the query,                which will be used as context for the LLM to generate a response.     \"\"\"     # Create an embedding for the query     query_embedding = create_embeddings(query).data[0].embedding     similarity_scores = []  # Initialize a list to store similarity scores      # Calculate similarity scores between the query embedding and each text chunk embedding     for i, chunk_embedding in enumerate(embeddings):         similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))         similarity_scores.append((i, similarity_score))  # Append the index and similarity score      # Sort the similarity scores in descending order     similarity_scores.sort(key=lambda x: x[1], reverse=True)     # Get the indices of the top k most similar text chunks     top_indices = [index for index, _ in similarity_scores[:k]]     # Return the top k most relevant text chunks     return [text_chunks[index] for index in top_indices]  In\u00a0[\u00a0]: Copied! <pre># Load the validation data from a JSON file\nwith open('data/val.json') as f:\n    data = json.load(f)\n\n# Extract the first query from the validation data\nquery = data[0]['question']\n\n# Perform semantic search to find the top 2 most relevant text chunks for the query\ntop_chunks = semantic_search(query, text_chunks, response.data, k=2)\n\n# Print the query\nprint(\"Query:\", query)\n\n# Print the top 2 most relevant text chunks\nfor i, chunk in enumerate(top_chunks):\n    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")\n</pre> # Load the validation data from a JSON file with open('data/val.json') as f:     data = json.load(f)  # Extract the first query from the validation data query = data[0]['question']  # Perform semantic search to find the top 2 most relevant text chunks for the query top_chunks = semantic_search(query, text_chunks, response.data, k=2)  # Print the query print(\"Query:\", query)  # Print the top 2 most relevant text chunks for i, chunk in enumerate(top_chunks):     print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\") In\u00a0[\u00a0]: Copied! <pre># Define the system prompt for the AI assistant\nsystem_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n\ndef generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n    \"\"\"\n    Generates a contextually informed response using an LLM with the retrieved information.\n    This is the 'Generation' part of Retrieval-Augmented Generation, where:\n    1. The retrieved context is combined with the user query\n    2. The LLM synthesizes this information to produce a coherent, accurate response\n    3. The system prompt guides the model to stay faithful to the provided context\n    \n    By using retrieved information as context, the RAG system can:\n    - Provide up-to-date information beyond the LLM's training data\n    - Cite specific sources for its claims\n    - Reduce hallucination by grounding responses in retrieved facts\n    - Answer domain-specific questions with greater accuracy\n\n    Args:\n    system_prompt (str): Instructions that guide the AI's behavior and response style.\n                         In RAG, this typically instructs the model to use only the provided context.\n    user_message (str): The combined context and query to be sent to the LLM.\n                        This includes both the retrieved text chunks and the original user question.\n    model (str): The LLM to use for response generation. Default is \"meta-llama/Llama-3.2-3B-Instruct\".\n\n    Returns:\n    dict: The complete response from the LLM, containing the generated answer based on\n          the retrieved context and original query.\n    \"\"\"\n    response = completion(model=model, messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_message}\n    ], temperature=0)\n    \n    # response = client.chat.completions.create(\n    #     model=model,\n    #     temperature=0,\n    #     messages=[\n    #         {\"role\": \"system\", \"content\": system_prompt},\n    #         {\"role\": \"user\", \"content\": user_message}\n    #     ]\n    # )\n    return response\n\n# Create the user prompt based on the top chunks\nuser_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\nuser_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n\n# Generate AI response\nai_response = generate_response(system_prompt, user_prompt)\n</pre> # Define the system prompt for the AI assistant system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"  def generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):     \"\"\"     Generates a contextually informed response using an LLM with the retrieved information.     This is the 'Generation' part of Retrieval-Augmented Generation, where:     1. The retrieved context is combined with the user query     2. The LLM synthesizes this information to produce a coherent, accurate response     3. The system prompt guides the model to stay faithful to the provided context          By using retrieved information as context, the RAG system can:     - Provide up-to-date information beyond the LLM's training data     - Cite specific sources for its claims     - Reduce hallucination by grounding responses in retrieved facts     - Answer domain-specific questions with greater accuracy      Args:     system_prompt (str): Instructions that guide the AI's behavior and response style.                          In RAG, this typically instructs the model to use only the provided context.     user_message (str): The combined context and query to be sent to the LLM.                         This includes both the retrieved text chunks and the original user question.     model (str): The LLM to use for response generation. Default is \"meta-llama/Llama-3.2-3B-Instruct\".      Returns:     dict: The complete response from the LLM, containing the generated answer based on           the retrieved context and original query.     \"\"\"     response = completion(model=model, messages=[         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": user_message}     ], temperature=0)          # response = client.chat.completions.create(     #     model=model,     #     temperature=0,     #     messages=[     #         {\"role\": \"system\", \"content\": system_prompt},     #         {\"role\": \"user\", \"content\": user_message}     #     ]     # )     return response  # Create the user prompt based on the top chunks user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)]) user_prompt = f\"{user_prompt}\\nQuestion: {query}\"  # Generate AI response ai_response = generate_response(system_prompt, user_prompt)"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#basic-rag-from-scratch","title":"Basic RAG from Scratch\u00b6","text":"<p>This notebook implements a basic Retrieval-Augmented Generation (RAG) system from scratch, without relying on external libraries except for essential system-level functionalities. This approach focuses on demonstrating the core concepts of RAG using fundamental Python operations.</p> <p>Core Steps:</p> <ol> <li>Data Loading: Read text data from a file.</li> <li>Chunking: Split the text into manageable chunks.</li> <li>Embedding Simulation: Create simple numerical representations (simulated embeddings).</li> <li>Semantic Search (Similarity): Implement a basic similarity calculation.</li> <li>Response Generation (Placeholder): Use a simple string concatenation as a placeholder for LLM response.</li> <li>Evaluation (Basic String Matching): Evaluate the generated response against a known answer.</li> </ol>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#setting-up-the-environment","title":"Setting Up the Environment\u00b6","text":"<p>We begin by importing necessary libraries.</p>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#extracting-text-from-a-pdf","title":"Extracting Text from a PDF\u00b6","text":"<p>We extract text from a PDF file using PyMuPDF. This process involves opening the PDF, reading its contents, and converting them into a format suitable for further processing.</p>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#chunking-the-extracted-text","title":"Chunking the Extracted Text\u00b6","text":"<p>Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy.</p>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#extracting-and-chunking-text-from-a-pdf-file","title":"Extracting and Chunking Text from a PDF File\u00b6","text":"<p>Now, we load the PDF, extract text, and split it into chunks.</p>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#creating-embeddings-for-text-chunks","title":"Creating Embeddings for Text Chunks\u00b6","text":"<p>Embeddings transform text into numerical vectors, which allow for efficient similarity search.</p>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#performing-semantic-search","title":"Performing Semantic Search\u00b6","text":"<p>We implement cosine similarity to find the most relevant text chunks for a user query.</p>"},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#running-a-query-on-extracted-chunks","title":"Running a Query on Extracted Chunks\u00b6","text":""},{"location":"RAG/01_Basic_RAG/basic_rag_scratch/#generating-a-response-based-on-retrieved-chunks","title":"Generating a Response Based on Retrieved Chunks\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/","title":"Hands on with RAG(llamaindex)","text":"Basic RAG In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n!pip install llama-index-vector-stores-qdrant\n!pip install llama-index-readers-file\n!pip install llama-index-embeddings-fastembed\n!pip install llama-index-llms-openai\n!pip install llama-index-llms-groq\n!pip install -U qdrant_client fastembed\n!pip install python-dotenv\n!pip install gradio\n</pre> !pip install llama-index !pip install llama-index-vector-stores-qdrant !pip install llama-index-readers-file !pip install llama-index-embeddings-fastembed !pip install llama-index-llms-openai !pip install llama-index-llms-groq !pip install -U qdrant_client fastembed !pip install python-dotenv !pip install gradio In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.llms.groq import Groq\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGROK_API_KEY = os.getenv(\"GROK_API_KEYs\")\n\n# Setting up Base LLM\n# Settings.llm = OpenAI(\n#     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True\n# )\n\nSettings.llm = Groq(model=\"llama-3.1-70b-versatile\" , api_key=GROK_API_KEY)\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\nSettings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\n# Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI from llama_index.llms.groq import Groq # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") GROK_API_KEY = os.getenv(\"GROK_API_KEYs\")  # Setting up Base LLM # Settings.llm = OpenAI( #     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True # )  Settings.llm = Groq(model=\"llama-3.1-70b-versatile\" , api_key=GROK_API_KEY)  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: # Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version In\u00a0[\u00a0]: Copied! <pre># lets loading the documents using SimpleDirectoryReader\n\nprint(\"\ud83d\udd03 Loading Data\")\n\nfrom llama_index.core import Document\nreader = SimpleDirectoryReader(\"/content/data\" , recursive=True)\ndocuments = reader.load_data(show_progress=True)\n</pre> # lets loading the documents using SimpleDirectoryReader  print(\"\ud83d\udd03 Loading Data\")  from llama_index.core import Document reader = SimpleDirectoryReader(\"/content/data\" , recursive=True) documents = reader.load_data(show_progress=True) In\u00a0[\u00a0]: Copied! <pre># creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    # host=\"localhost\",\n    # port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\")\n</pre> # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     # host=\"localhost\",     # port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\") In\u00a0[\u00a0]: Copied! <pre>## ingesting data into vector database\n\n## lets set up an ingestion pipeline\n\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        # MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ],\n    vector_store=vector_store,\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=documents , show_progress=True)\nprint(\"Number of chunks added to vector DB :\",len(nodes))\n</pre> ## ingesting data into vector database  ## lets set up an ingestion pipeline  from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         # MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ],     vector_store=vector_store, )  # Ingest directly into a vector db nodes = pipeline.run(documents=documents , show_progress=True) print(\"Number of chunks added to vector DB :\",len(nodes)) In\u00a0[\u00a0]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store=vector_store) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import ChatPromptTemplate\n\nqa_prompt_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the question: {query_str}\\n\"\n)\n\nrefine_prompt_str = (\n    \"We have the opportunity to refine the original answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question: {query_str}. \"\n    \"If the context isn't useful, output the original answer again.\\n\"\n    \"Original Answer: {existing_answer}\"\n)\n\n# Text QA Prompt\nchat_text_qa_msgs = [\n    (\"system\",\"You are a AI assistant who is well versed with answering questions from the provided context\"),\n    (\"user\", qa_prompt_str),\n]\ntext_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n\n# Refine Prompt\nchat_refine_msgs = [\n    (\"system\",\"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", refine_prompt_str),\n]\nrefine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)\n</pre> from llama_index.core import ChatPromptTemplate  qa_prompt_str = (     \"Context information is below.\\n\"     \"---------------------\\n\"     \"{context_str}\\n\"     \"---------------------\\n\"     \"Given the context information and not prior knowledge, \"     \"answer the question: {query_str}\\n\" )  refine_prompt_str = (     \"We have the opportunity to refine the original answer \"     \"(only if needed) with some more context below.\\n\"     \"------------\\n\"     \"{context_msg}\\n\"     \"------------\\n\"     \"Given the new context, refine the original answer to better \"     \"answer the question: {query_str}. \"     \"If the context isn't useful, output the original answer again.\\n\"     \"Original Answer: {existing_answer}\" )  # Text QA Prompt chat_text_qa_msgs = [     (\"system\",\"You are a AI assistant who is well versed with answering questions from the provided context\"),     (\"user\", qa_prompt_str), ] text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)  # Refine Prompt chat_refine_msgs = [     (\"system\",\"Always answer the question, even if the context isn't helpful.\",),     (\"user\", refine_prompt_str), ] refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs) In\u00a0[\u00a0]: Copied! <pre>chat_query = \"What is in this document\"\n\n# Setting up Chat Engine\nBASE_RAG_CHAT_ENGINE = index.as_chat_engine()\n\nresponse = BASE_RAG_CHAT_ENGINE.chat(chat_query)\ndisplay(Markdown(str(response)))\n</pre> chat_query = \"What is in this document\"  # Setting up Chat Engine BASE_RAG_CHAT_ENGINE = index.as_chat_engine()  response = BASE_RAG_CHAT_ENGINE.chat(chat_query) display(Markdown(str(response))) In\u00a0[\u00a0]: Copied! <pre>from typing import List\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\n\nclass ChatEngineInterface:\n    def __init__(self, index):\n        self.chat_engine = index.as_chat_engine()\n        self.chat_history: List[ChatMessage] = []\n\n    def display_message(self, role: str, content: str):\n        if role == \"USER\":\n            display(Markdown(f\"**Human:** {content}\"))\n        else:\n            display(Markdown(f\"**AI:** {content}\"))\n\n    def chat(self, message: str) -&gt; str:\n        # Create a ChatMessage for the user input\n        user_message = ChatMessage(role=MessageRole.USER, content=message)\n        self.chat_history.append(user_message)\n\n        # Get response from the chat engine\n        response = self.chat_engine.chat(message, chat_history=self.chat_history)\n\n        # Create a ChatMessage for the AI response\n        ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=str(response))\n        self.chat_history.append(ai_message)\n\n        # Display the conversation\n        self.display_message(\"USER\", message)\n        self.display_message(\"ASSISTANT\", str(response))\n\n        print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability\n\n        return str(response)\n\n    def get_chat_history(self) -&gt; List[ChatMessage]:\n        return self.chat_history\n</pre> from typing import List from llama_index.core.base.llms.types import ChatMessage, MessageRole  class ChatEngineInterface:     def __init__(self, index):         self.chat_engine = index.as_chat_engine()         self.chat_history: List[ChatMessage] = []      def display_message(self, role: str, content: str):         if role == \"USER\":             display(Markdown(f\"**Human:** {content}\"))         else:             display(Markdown(f\"**AI:** {content}\"))      def chat(self, message: str) -&gt; str:         # Create a ChatMessage for the user input         user_message = ChatMessage(role=MessageRole.USER, content=message)         self.chat_history.append(user_message)          # Get response from the chat engine         response = self.chat_engine.chat(message, chat_history=self.chat_history)          # Create a ChatMessage for the AI response         ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=str(response))         self.chat_history.append(ai_message)          # Display the conversation         self.display_message(\"USER\", message)         self.display_message(\"ASSISTANT\", str(response))          print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability          return str(response)      def get_chat_history(self) -&gt; List[ChatMessage]:         return self.chat_history In\u00a0[\u00a0]: Copied! <pre>chat_interface = ChatEngineInterface(index)\nwhile True:\n    user_input = input(\"You: \").strip()\n    if user_input.lower() == 'exit':\n        print(\"Thank you for chatting! Goodbye.\")\n        break\n    chat_interface.chat(user_input)\n</pre> chat_interface = ChatEngineInterface(index) while True:     user_input = input(\"You: \").strip()     if user_input.lower() == 'exit':         print(\"Thank you for chatting! Goodbye.\")         break     chat_interface.chat(user_input) In\u00a0[\u00a0]: Copied! <pre># To view chat history:\nhistory = chat_interface.get_chat_history()\nfor message in history:\n    print(f\"{message.role}: {message.content}\")\n</pre> # To view chat history: history = chat_interface.get_chat_history() for message in history:     print(f\"{message.role}: {message.content}\") In\u00a0[\u00a0]: Copied! <pre>import gradio as gr\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, Settings\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nimport qdrant_client\nimport os\nimport tempfile\nimport shutil\nfrom typing import List\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\n\nclass RAGChatbot:\n    def __init__(self):\n        self.client = qdrant_client.QdrantClient(path=\"./Demo_RAG\")\n        self.vector_store = None\n        self.index = None\n        self.chat_engine = None\n        self.chat_history = []\n        # Initialize vector store and index\n        self.vector_store = QdrantVectorStore(\n            client=self.client,\n            collection_name=\"Demo_RAG\"\n        )\n\n        # Create the index and ingest documents\n        self.index = VectorStoreIndex.from_vector_store(\n            vector_store=self.vector_store\n        )\n\n        # Initialize chat engine\n        self.chat_engine = self.index.as_chat_engine(\n            streaming=True,\n            verbose=True\n        )\n\n\n    def process_uploaded_files(self, files) -&gt; str:\n        try:\n            # Create a temporary directory for processing\n            with tempfile.TemporaryDirectory() as temp_dir:\n                # Save uploaded files to temporary directory\n                for file in files:\n                    shutil.copy(file.name, temp_dir)\n\n                # Load documents\n                reader = SimpleDirectoryReader(temp_dir)\n                documents = reader.load_data()\n\n                pipeline = IngestionPipeline(\n                    transformations=[\n                        # MarkdownNodeParser(include_metadata=True),\n                        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n                        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n                        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n                        Settings.embed_model,\n                    ],\n                    vector_store=self.vector_store,\n                )\n\n                # Ingest directly into a vector db\n                nodes = pipeline.run(documents=documents , show_progress=True)\n\n                return f\"Successfully processed {len(documents)} documents. Ready to chat! and inserted {len(nodes)} into the database\"\n\n        except Exception as e:\n            return f\"Error processing files: {str(e)}\"\n\n    def chat(self, message: str, history: List[List[str]]) -&gt; List[List[str]]:\n        if self.chat_engine is None:\n            return history + [[message, \"Please upload documents first before starting the chat.\"]]\n\n        try:\n            # Convert history to ChatMessage format\n            chat_history = []\n            for h in history:\n                chat_history.extend([\n                    ChatMessage(role=MessageRole.USER, content=h[0]),\n                    ChatMessage(role=MessageRole.ASSISTANT, content=h[1])\n                ])\n\n            # Add current message to history\n            chat_history.append(ChatMessage(role=MessageRole.USER, content=message))\n\n            # Get response from chat engine\n            response = self.chat_engine.chat(message, chat_history=chat_history)\n\n            # Return the updated history with the new message pair\n            return history + [[message, str(response)]]\n\n        except Exception as e:\n            return history + [[message, f\"Error generating response: {str(e)}\"]]\n\ndef create_demo():\n    # Initialize the chatbot\n    chatbot = RAGChatbot()\n\n    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n        gr.Markdown(\"# RAG Chatbot\")\n        gr.Markdown(\"Upload your documents and start chatting!\")\n\n        with gr.Row():\n            with gr.Column(scale=1):\n                file_output = gr.File(\n                    file_count=\"multiple\",\n                    label=\"Upload Documents\",\n                    file_types=[\".txt\", \".pdf\", \".docx\", \".md\"]\n                )\n                upload_button = gr.Button(\"Process Documents\")\n                status_box = gr.Textbox(label=\"Status\", interactive=False)\n\n            with gr.Column(scale=2):\n                chatbot_interface = gr.Chatbot(\n                    label=\"Chat History\",\n                    height=400,\n                    bubble_full_width=False,\n                )\n                with gr.Row():\n                    msg = gr.Textbox(\n                        label=\"Type your message\",\n                        placeholder=\"Ask me anything about the uploaded documents...\",\n                        lines=2,\n                        scale=4\n                    )\n                    submit_button = gr.Button(\"Submit\", scale=1)\n                clear = gr.Button(\"Clear\")\n\n        # Event handlers\n        upload_button.click(\n            fn=chatbot.process_uploaded_files,\n            inputs=[file_output],\n            outputs=[status_box],\n        )\n        submit_button.click(\n            fn=chatbot.chat,\n            inputs=[msg, chatbot_interface],\n            outputs=[chatbot_interface],\n        )\n\n        clear.click(\n            lambda: None,\n            None,\n            chatbot_interface,\n            queue=False\n        )\n\n    return demo\n\nif __name__ == \"__main__\":\n    demo = create_demo()\n    demo.launch(share=True)\n</pre> import gradio as gr from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, Settings from llama_index.vector_stores.qdrant import QdrantVectorStore from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.llms.openai import OpenAI import qdrant_client import os import tempfile import shutil from typing import List from llama_index.core.base.llms.types import ChatMessage, MessageRole  class RAGChatbot:     def __init__(self):         self.client = qdrant_client.QdrantClient(path=\"./Demo_RAG\")         self.vector_store = None         self.index = None         self.chat_engine = None         self.chat_history = []         # Initialize vector store and index         self.vector_store = QdrantVectorStore(             client=self.client,             collection_name=\"Demo_RAG\"         )          # Create the index and ingest documents         self.index = VectorStoreIndex.from_vector_store(             vector_store=self.vector_store         )          # Initialize chat engine         self.chat_engine = self.index.as_chat_engine(             streaming=True,             verbose=True         )       def process_uploaded_files(self, files) -&gt; str:         try:             # Create a temporary directory for processing             with tempfile.TemporaryDirectory() as temp_dir:                 # Save uploaded files to temporary directory                 for file in files:                     shutil.copy(file.name, temp_dir)                  # Load documents                 reader = SimpleDirectoryReader(temp_dir)                 documents = reader.load_data()                  pipeline = IngestionPipeline(                     transformations=[                         # MarkdownNodeParser(include_metadata=True),                         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),                         SentenceSplitter(chunk_size=1024, chunk_overlap=20),                         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),                         Settings.embed_model,                     ],                     vector_store=self.vector_store,                 )                  # Ingest directly into a vector db                 nodes = pipeline.run(documents=documents , show_progress=True)                  return f\"Successfully processed {len(documents)} documents. Ready to chat! and inserted {len(nodes)} into the database\"          except Exception as e:             return f\"Error processing files: {str(e)}\"      def chat(self, message: str, history: List[List[str]]) -&gt; List[List[str]]:         if self.chat_engine is None:             return history + [[message, \"Please upload documents first before starting the chat.\"]]          try:             # Convert history to ChatMessage format             chat_history = []             for h in history:                 chat_history.extend([                     ChatMessage(role=MessageRole.USER, content=h[0]),                     ChatMessage(role=MessageRole.ASSISTANT, content=h[1])                 ])              # Add current message to history             chat_history.append(ChatMessage(role=MessageRole.USER, content=message))              # Get response from chat engine             response = self.chat_engine.chat(message, chat_history=chat_history)              # Return the updated history with the new message pair             return history + [[message, str(response)]]          except Exception as e:             return history + [[message, f\"Error generating response: {str(e)}\"]]  def create_demo():     # Initialize the chatbot     chatbot = RAGChatbot()      with gr.Blocks(theme=gr.themes.Soft()) as demo:         gr.Markdown(\"# RAG Chatbot\")         gr.Markdown(\"Upload your documents and start chatting!\")          with gr.Row():             with gr.Column(scale=1):                 file_output = gr.File(                     file_count=\"multiple\",                     label=\"Upload Documents\",                     file_types=[\".txt\", \".pdf\", \".docx\", \".md\"]                 )                 upload_button = gr.Button(\"Process Documents\")                 status_box = gr.Textbox(label=\"Status\", interactive=False)              with gr.Column(scale=2):                 chatbot_interface = gr.Chatbot(                     label=\"Chat History\",                     height=400,                     bubble_full_width=False,                 )                 with gr.Row():                     msg = gr.Textbox(                         label=\"Type your message\",                         placeholder=\"Ask me anything about the uploaded documents...\",                         lines=2,                         scale=4                     )                     submit_button = gr.Button(\"Submit\", scale=1)                 clear = gr.Button(\"Clear\")          # Event handlers         upload_button.click(             fn=chatbot.process_uploaded_files,             inputs=[file_output],             outputs=[status_box],         )         submit_button.click(             fn=chatbot.chat,             inputs=[msg, chatbot_interface],             outputs=[chatbot_interface],         )          clear.click(             lambda: None,             None,             chatbot_interface,             queue=False         )      return demo  if __name__ == \"__main__\":     demo = create_demo()     demo.launch(share=True)"},{"location":"RAG/01_Basic_RAG/notebook/#ai-engineering-academy","title":"AI Engineering Academy\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#introduction","title":"Introduction\u00b6","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of large language models with the ability to retrieve relevant information from a knowledge base. This approach enhances the quality and accuracy of generated responses by grounding them in specific, retrieved information.</p> <p>This notebook aims to provide a clear and concise introduction to RAG, suitable for beginners who want to understand and implement this technology.</p>"},{"location":"RAG/01_Basic_RAG/notebook/#motivation","title":"Motivation\u00b6","text":"<p>Traditional language models generate text based on learned patterns from training data. However, when they are presented with queries that require specific, updated, or niche information, they may struggle to provide accurate responses. RAG addresses this limitation by incorporating a retrieval step that provides the language model with relevant context to generate more informed answers.</p>"},{"location":"RAG/01_Basic_RAG/notebook/#method-details","title":"Method Details\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation\u00b6","text":"<ol> <li><p>Document Chunking: The knowledge base documents (e.g., PDFs, articles) are preprocessed and split into manageable chunks. This is done to create a searchable corpus that can be efficiently used in the retrieval process.</p> </li> <li><p>Embedding Generation: Each chunk is converted into a vector representation using pre-trained embeddings (e.g., OpenAI's embeddings). This allows the documents to be stored in a vector database, such as Qdrant, enabling efficient similarity searches.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow\u00b6","text":"<ol> <li><p>Query Input: A user provides a query that needs to be answered.</p> </li> <li><p>Retrieval Step: The query is embedded into a vector using the same embedding model that was used for the documents. A similarity search is then performed in the vector database to find the most relevant document chunks.</p> </li> <li><p>Generation Step: The retrieved document chunks are passed to a large language model (e.g., GPT-4) as additional context. The model uses this context to generate a more accurate and relevant response.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook/#key-features-of-rag","title":"Key Features of RAG\u00b6","text":"<ol> <li><p>Contextual Relevance: By grounding responses in actual retrieved information, RAG models can produce more contextually relevant and accurate answers.</p> </li> <li><p>Scalability: The retrieval step can scale to handle large knowledge bases, allowing the model to draw from vast amounts of information.</p> </li> <li><p>Flexibility in Use Cases: RAG can be adapted for a variety of applications, including question answering, summarization, recommendation systems, and more.</p> </li> <li><p>Improved Accuracy: Combining generation with retrieval often yields more precise results, especially for queries requiring specific or lesser-known information.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook/#benefits-of-this-approach","title":"Benefits of this Approach\u00b6","text":"<ol> <li><p>Combines Strengths of Both Retrieval and Generation: RAG effectively merges retrieval-based methods with generative models, allowing for both precise fact-finding and natural language generation.</p> </li> <li><p>Enhanced Handling of Long-Tail Queries: It is particularly effective for queries where specific and less frequently occurring information is needed.</p> </li> <li><p>Domain Adaptability: The retrieval mechanism can be tuned to specific domains, ensuring that the generated responses are grounded in the most relevant and accurate domain-specific information.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook/#conclusion","title":"Conclusion\u00b6","text":"<p>Retrieval-Augmented Generation (RAG) represents an innovative fusion of retrieval and generation techniques, significantly enhancing the capabilities of language models by grounding their outputs in relevant external information. This approach can be particularly valuable in scenarios requiring precise, context-aware responses, such as customer support, academic research, and more. As AI continues to evolve, RAG stands out as a powerful method for building more reliable and context-sensitive AI systems.</p>"},{"location":"RAG/01_Basic_RAG/notebook/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Preferably Python 3.11</li> <li>Jupyter Notebook or JupyterLab</li> <li>LLM API Key<ul> <li>You can use any llm of your choice in this notebook we have use OpenAI and Gpt-4o-mini</li> </ul> </li> </ul> <p>With these steps, you can implement a basic RAG system to enhance the capabilities of language models by incorporating real-world, up-to-date information, improving their effectiveness in various applications.</p>"},{"location":"RAG/01_Basic_RAG/notebook/#setting-up-the-environment","title":"Setting up the Environment\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#load-the-data","title":"Load the Data\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/01_Basic_RAG/notebook/#ingest-data-into-vector-db","title":"Ingest Data into vector DB\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#setting-up-index","title":"Setting Up Index\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#modifying-prompts-and-prompt-tuning","title":"Modifying Prompts and Prompt Tuning\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#example-of-retrivers","title":"Example of Retrivers\u00b6","text":"<ul> <li>Query Engine</li> <li>Chat Engine</li> </ul>"},{"location":"RAG/01_Basic_RAG/notebook/#simple-chat-application-with-rag","title":"Simple Chat Application with RAG\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook/#gradio-applicaiton","title":"Gradio Applicaiton\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/","title":"RAG Evaluation","text":"Basic RAG AI Engineering.academy In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n!pip install llama-index-vector-stores-qdrant \n!pip install llama-index-readers-file \n!pip install llama-index-embeddings-fastembed \n!pip install llama-index-llms-openai\n!pip install llama-index-llms-groq\n!pip install -U qdrant_client fastembed\n!pip install python-dotenv\n</pre> !pip install llama-index !pip install llama-index-vector-stores-qdrant  !pip install llama-index-readers-file  !pip install llama-index-embeddings-fastembed  !pip install llama-index-llms-openai !pip install llama-index-llms-groq !pip install -U qdrant_client fastembed !pip install python-dotenv In\u00a0[1]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.llms.groq import Groq\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGROK_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\n# Setting up Base LLM\nSettings.llm = OpenAI(\n    model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True\n)\n\n# Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\n# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\nSettings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI from llama_index.llms.groq import Groq # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Setting up Base LLM Settings.llm = OpenAI(     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True )  # Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) # Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version <pre>/home/adithya/miniconda3/envs/01_Basic_RAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNone of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n</pre> In\u00a0[\u00a0]: Copied! <pre># !pip install arize-phoenix\n# !pip install openinference-instrumentation-llama-index\n# !pip install -U llama-index-callbacks-arize-phoenix\n</pre> # !pip install arize-phoenix # !pip install openinference-instrumentation-llama-index # !pip install -U llama-index-callbacks-arize-phoenix In\u00a0[\u00a0]: Copied! <pre># import phoenix as px\n\n# (session := px.launch_app()).view()\n</pre> # import phoenix as px  # (session := px.launch_app()).view() In\u00a0[\u00a0]: Copied! <pre># from openinference.instrumentation.langchain import LangChainInstrumentor\n# from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n# from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n# from opentelemetry.sdk.trace import SpanLimits, TracerProvider\n# from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\n# endpoint = \"http://127.0.0.1:6006/v1/traces\"\n# tracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=100_000))\n# tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\n# LlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n# # LangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n</pre> # from openinference.instrumentation.langchain import LangChainInstrumentor # from openinference.instrumentation.llama_index import LlamaIndexInstrumentor # from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter # from opentelemetry.sdk.trace import SpanLimits, TracerProvider # from opentelemetry.sdk.trace.export import SimpleSpanProcessor  # endpoint = \"http://127.0.0.1:6006/v1/traces\" # tracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=100_000)) # tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))  # LlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider) # # LangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider) In\u00a0[2]: Copied! <pre># lets loading the documents using SimpleDirectoryReader\n\nprint(\"\ud83d\udd03 Loading Data\")\n\nfrom llama_index.core import Document\nreader = SimpleDirectoryReader(\"../data/\" , recursive=True)\ndocuments = reader.load_data(show_progress=True)\n</pre> # lets loading the documents using SimpleDirectoryReader  print(\"\ud83d\udd03 Loading Data\")  from llama_index.core import Document reader = SimpleDirectoryReader(\"../data/\" , recursive=True) documents = reader.load_data(show_progress=True) <pre>\ud83d\udd03 Loading Data\n</pre> <pre>Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  4.27file/s]\n</pre> In\u00a0[3]: Copied! <pre># creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    host=\"localhost\",\n    port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    # path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\")\n</pre> # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     host=\"localhost\",     port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     # path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\") In\u00a0[5]: Copied! <pre>## ingesting data into vector database\n\n## lets set up an ingestion pipeline\n\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        # MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ],\n    vector_store=vector_store,\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=documents , show_progress=True)\nprint(\"Number of chunks added to vector DB :\",len(nodes))\n</pre> ## ingesting data into vector database  ## lets set up an ingestion pipeline  from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         # MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ],     vector_store=vector_store, )  # Ingest directly into a vector db nodes = pipeline.run(documents=documents , show_progress=True) print(\"Number of chunks added to vector DB :\",len(nodes)) <pre>Parsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00&lt;00:00, 555.31it/s]\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:08&lt;00:00,  7.04it/s]\n</pre> <pre>Number of chunks added to vector DB : 58\n</pre> In\u00a0[6]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store=vector_store) In\u00a0[7]: Copied! <pre>from llama_index.core import ChatPromptTemplate\n\nqa_prompt_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the question: {query_str}\\n\"\n)\n\nrefine_prompt_str = (\n    \"We have the opportunity to refine the original answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question: {query_str}. \"\n    \"If the context isn't useful, output the original answer again.\\n\"\n    \"Original Answer: {existing_answer}\"\n)\n\n# Text QA Prompt\nchat_text_qa_msgs = [\n    (\"system\",\"You are a AI assistant who is well versed with answering questions from the provided context\"),\n    (\"user\", qa_prompt_str),\n]\ntext_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n\n# Refine Prompt\nchat_refine_msgs = [\n    (\"system\",\"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", refine_prompt_str),\n]\nrefine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)\n</pre> from llama_index.core import ChatPromptTemplate  qa_prompt_str = (     \"Context information is below.\\n\"     \"---------------------\\n\"     \"{context_str}\\n\"     \"---------------------\\n\"     \"Given the context information and not prior knowledge, \"     \"answer the question: {query_str}\\n\" )  refine_prompt_str = (     \"We have the opportunity to refine the original answer \"     \"(only if needed) with some more context below.\\n\"     \"------------\\n\"     \"{context_msg}\\n\"     \"------------\\n\"     \"Given the new context, refine the original answer to better \"     \"answer the question: {query_str}. \"     \"If the context isn't useful, output the original answer again.\\n\"     \"Original Answer: {existing_answer}\" )  # Text QA Prompt chat_text_qa_msgs = [     (\"system\",\"You are a AI assistant who is well versed with answering questions from the provided context\"),     (\"user\", qa_prompt_str), ] text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)  # Refine Prompt chat_refine_msgs = [     (\"system\",\"Always answer the question, even if the context isn't helpful.\",),     (\"user\", refine_prompt_str), ] refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs) In\u00a0[11]: Copied! <pre># Setting up Query Engine\nBASE_RAG_QUERY_ENGINE = index.as_query_engine(\n        similarity_top_k=5,\n        text_qa_template=text_qa_template,\n        refine_template=refine_template,)\n\n\nresponse = BASE_RAG_QUERY_ENGINE.query(\"How many encoders are stacked in the encoder?\")\ndisplay(Markdown(str(response)))\n</pre> # Setting up Query Engine BASE_RAG_QUERY_ENGINE = index.as_query_engine(         similarity_top_k=5,         text_qa_template=text_qa_template,         refine_template=refine_template,)   response = BASE_RAG_QUERY_ENGINE.query(\"How many encoders are stacked in the encoder?\") display(Markdown(str(response))) <p>According to the context information, the encoder is composed of a stack of N = 6 identical layers.</p> In\u00a0[12]: Copied! <pre># Setting up Chat Engine\nBASE_RAG_CHAT_ENGINE = index.as_chat_engine()\n\nresponse = BASE_RAG_CHAT_ENGINE.chat(\"How many encoders are stacked in the encoder?\")\ndisplay(Markdown(str(response)))\n</pre> # Setting up Chat Engine BASE_RAG_CHAT_ENGINE = index.as_chat_engine()  response = BASE_RAG_CHAT_ENGINE.chat(\"How many encoders are stacked in the encoder?\") display(Markdown(str(response))) <p>The number of encoders stacked in the encoder is 6.</p> In\u00a0[13]: Copied! <pre>from typing import List\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\n\nclass ChatEngineInterface:\n    def __init__(self, index):\n        self.chat_engine = index.as_chat_engine()\n        self.chat_history: List[ChatMessage] = []\n\n    def display_message(self, role: str, content: str):\n        if role == \"USER\":\n            display(Markdown(f\"**Human:** {content}\"))\n        else:\n            display(Markdown(f\"**AI:** {content}\"))\n\n    def chat(self, message: str) -&gt; str:\n        # Create a ChatMessage for the user input\n        user_message = ChatMessage(role=MessageRole.USER, content=message)\n        self.chat_history.append(user_message)\n        \n        # Get response from the chat engine\n        response = self.chat_engine.chat(message, chat_history=self.chat_history)\n        \n        # Create a ChatMessage for the AI response\n        ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=str(response))\n        self.chat_history.append(ai_message)\n        \n        # Display the conversation\n        self.display_message(\"USER\", message)\n        self.display_message(\"ASSISTANT\", str(response))\n        \n        print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability\n\n        return str(response)\n\n    def get_chat_history(self) -&gt; List[ChatMessage]:\n        return self.chat_history\n</pre> from typing import List from llama_index.core.base.llms.types import ChatMessage, MessageRole  class ChatEngineInterface:     def __init__(self, index):         self.chat_engine = index.as_chat_engine()         self.chat_history: List[ChatMessage] = []      def display_message(self, role: str, content: str):         if role == \"USER\":             display(Markdown(f\"**Human:** {content}\"))         else:             display(Markdown(f\"**AI:** {content}\"))      def chat(self, message: str) -&gt; str:         # Create a ChatMessage for the user input         user_message = ChatMessage(role=MessageRole.USER, content=message)         self.chat_history.append(user_message)                  # Get response from the chat engine         response = self.chat_engine.chat(message, chat_history=self.chat_history)                  # Create a ChatMessage for the AI response         ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=str(response))         self.chat_history.append(ai_message)                  # Display the conversation         self.display_message(\"USER\", message)         self.display_message(\"ASSISTANT\", str(response))                  print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability          return str(response)      def get_chat_history(self) -&gt; List[ChatMessage]:         return self.chat_history In\u00a0[\u00a0]: Copied! <pre>chat_interface = ChatEngineInterface(index)\nwhile True:\n    user_input = input(\"You: \").strip()\n    if user_input.lower() == 'exit':\n        print(\"Thank you for chatting! Goodbye.\")\n        break\n    chat_interface.chat(user_input)\n</pre> chat_interface = ChatEngineInterface(index) while True:     user_input = input(\"You: \").strip()     if user_input.lower() == 'exit':         print(\"Thank you for chatting! Goodbye.\")         break     chat_interface.chat(user_input) In\u00a0[\u00a0]: Copied! <pre># To view chat history:\nhistory = chat_interface.get_chat_history()\nfor message in history:\n    print(f\"{message.role}: {message.content}\")\n</pre> # To view chat history: history = chat_interface.get_chat_history() for message in history:     print(f\"{message.role}: {message.content}\")"},{"location":"RAG/01_Basic_RAG/notebook_eval/#introduction","title":"Introduction\u00b6","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of large language models with the ability to retrieve relevant information from a knowledge base. This approach enhances the quality and accuracy of generated responses by grounding them in specific, retrieved information.</p> <p>This notebook aims to provide a clear and concise introduction to RAG, suitable for beginners who want to understand and implement this technology.</p>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#motivation","title":"Motivation\u00b6","text":"<p>Traditional language models generate text based on learned patterns from training data. However, when they are presented with queries that require specific, updated, or niche information, they may struggle to provide accurate responses. RAG addresses this limitation by incorporating a retrieval step that provides the language model with relevant context to generate more informed answers.</p>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#method-details","title":"Method Details\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation\u00b6","text":"<ol> <li><p>Document Chunking: The knowledge base documents (e.g., PDFs, articles) are preprocessed and split into manageable chunks. This is done to create a searchable corpus that can be efficiently used in the retrieval process.</p> </li> <li><p>Embedding Generation: Each chunk is converted into a vector representation using pre-trained embeddings (e.g., OpenAI's embeddings). This allows the documents to be stored in a vector database, such as Qdrant, enabling efficient similarity searches.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow\u00b6","text":"<ol> <li><p>Query Input: A user provides a query that needs to be answered.</p> </li> <li><p>Retrieval Step: The query is embedded into a vector using the same embedding model that was used for the documents. A similarity search is then performed in the vector database to find the most relevant document chunks.</p> </li> <li><p>Generation Step: The retrieved document chunks are passed to a large language model (e.g., GPT-4) as additional context. The model uses this context to generate a more accurate and relevant response.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#key-features-of-rag","title":"Key Features of RAG\u00b6","text":"<ol> <li><p>Contextual Relevance: By grounding responses in actual retrieved information, RAG models can produce more contextually relevant and accurate answers.</p> </li> <li><p>Scalability: The retrieval step can scale to handle large knowledge bases, allowing the model to draw from vast amounts of information.</p> </li> <li><p>Flexibility in Use Cases: RAG can be adapted for a variety of applications, including question answering, summarization, recommendation systems, and more.</p> </li> <li><p>Improved Accuracy: Combining generation with retrieval often yields more precise results, especially for queries requiring specific or lesser-known information.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#benefits-of-this-approach","title":"Benefits of this Approach\u00b6","text":"<ol> <li><p>Combines Strengths of Both Retrieval and Generation: RAG effectively merges retrieval-based methods with generative models, allowing for both precise fact-finding and natural language generation.</p> </li> <li><p>Enhanced Handling of Long-Tail Queries: It is particularly effective for queries where specific and less frequently occurring information is needed.</p> </li> <li><p>Domain Adaptability: The retrieval mechanism can be tuned to specific domains, ensuring that the generated responses are grounded in the most relevant and accurate domain-specific information.</p> </li> </ol>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#conclusion","title":"Conclusion\u00b6","text":"<p>Retrieval-Augmented Generation (RAG) represents an innovative fusion of retrieval and generation techniques, significantly enhancing the capabilities of language models by grounding their outputs in relevant external information. This approach can be particularly valuable in scenarios requiring precise, context-aware responses, such as customer support, academic research, and more. As AI continues to evolve, RAG stands out as a powerful method for building more reliable and context-sensitive AI systems.</p>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Preferably Python 3.11</li> <li>Jupyter Notebook or JupyterLab</li> <li>LLM API Key<ul> <li>You can use any llm of your choice in this notebook we have use OpenAI and Gpt-4o-mini</li> </ul> </li> </ul> <p>With these steps, you can implement a basic RAG system to enhance the capabilities of language models by incorporating real-world, up-to-date information, improving their effectiveness in various applications.</p>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#setting-up-the-environment","title":"Setting up the Environment\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#setting-up-observability","title":"Setting Up Observability\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#load-the-data","title":"Load the Data\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#ingest-data-into-vector-db","title":"Ingest Data into vector DB\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#setting-up-index","title":"Setting Up Index\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#modifying-prompts-and-prompt-tuning","title":"Modifying Prompts and Prompt Tuning\u00b6","text":""},{"location":"RAG/01_Basic_RAG/notebook_eval/#example-of-retrivers","title":"Example of Retrivers\u00b6","text":"<ul> <li>Query Engine</li> <li>Chat Engine</li> </ul>"},{"location":"RAG/01_Basic_RAG/notebook_eval/#simple-chat-application-with-rag","title":"Simple Chat Application with RAG\u00b6","text":""},{"location":"RAG/01_Basic_RAG/server/","title":"Server","text":"<p>File: app.py</p> In\u00a0[\u00a0]: Copied! <pre>from fastapi import FastAPI, HTTPException, File, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom typing import List\nimport os\nimport tempfile\nimport shutil\nfrom dotenv import load_dotenv\nimport qdrant_client\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom chainlit.utils import mount_chainlit\n</pre> from fastapi import FastAPI, HTTPException, File, UploadFile from fastapi.responses import JSONResponse from pydantic import BaseModel from typing import List import os import tempfile import shutil from dotenv import load_dotenv import qdrant_client from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings from llama_index.vector_stores.qdrant import QdrantVectorStore from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.llms.openai import OpenAI from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline from chainlit.utils import mount_chainlit In\u00a0[\u00a0]: Copied! <pre># Load environment variables\nload_dotenv()\n</pre> # Load environment variables load_dotenv() In\u00a0[\u00a0]: Copied! <pre># Configuration\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nQDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")\nQDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", 6333))\n</pre> # Configuration OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\") QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", 6333)) In\u00a0[\u00a0]: Copied! <pre># Set up LlamaIndex\nSettings.llm = OpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)\nSettings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY)\n</pre> # Set up LlamaIndex Settings.llm = OpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY) Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY) In\u00a0[\u00a0]: Copied! <pre># Set up Qdrant client\nclient = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\")\n</pre> # Set up Qdrant client client = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT) vector_store = QdrantVectorStore(client=client, collection_name=\"01_Basic_RAG\") In\u00a0[\u00a0]: Copied! <pre># Create index\nindex = VectorStoreIndex.from_vector_store(vector_store)\n</pre> # Create index index = VectorStoreIndex.from_vector_store(vector_store) In\u00a0[\u00a0]: Copied! <pre>def get_ingestion_pipeline():\n    return IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n            Settings.embed_model,\n        ],\n        vector_store=vector_store,\n    )\n</pre> def get_ingestion_pipeline():     return IngestionPipeline(         transformations=[             SentenceSplitter(chunk_size=1024, chunk_overlap=20),             Settings.embed_model,         ],         vector_store=vector_store,     ) In\u00a0[\u00a0]: Copied! <pre>def get_query_engine():\n    return index.as_query_engine(similarity_top_k=5)\n</pre> def get_query_engine():     return index.as_query_engine(similarity_top_k=5) In\u00a0[\u00a0]: Copied! <pre>class QueryRequest(BaseModel):\n    query: str\n</pre> class QueryRequest(BaseModel):     query: str In\u00a0[\u00a0]: Copied! <pre>app = FastAPI()\n</pre> app = FastAPI() In\u00a0[\u00a0]: Copied! <pre>@app.post(\"/api/ingest\")\nasync def ingest_files(files: List[UploadFile] = File(...)):\n    try:\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Save uploaded files to the temporary directory\n            for file in files:\n                file_path = os.path.join(temp_dir, file.filename)\n                with open(file_path, \"wb\") as buffer:\n                    shutil.copyfileobj(file.file, buffer)\n            \n            # Use SimpleDirectoryReader to load documents\n            reader = SimpleDirectoryReader(temp_dir, recursive=True)\n            documents = reader.load_data()\n            \n            # Process documents through the ingestion pipeline\n            pipeline = get_ingestion_pipeline()\n            nodes = pipeline.run(documents=documents)\n            \n            return JSONResponse(content={\n                \"message\": f\"Successfully ingested {len(files)} files\",\n                \"ingested_files\": [file.filename for file in files],\n                \"total_nodes\": len(nodes)\n            })\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</pre> @app.post(\"/api/ingest\") async def ingest_files(files: List[UploadFile] = File(...)):     try:         with tempfile.TemporaryDirectory() as temp_dir:             # Save uploaded files to the temporary directory             for file in files:                 file_path = os.path.join(temp_dir, file.filename)                 with open(file_path, \"wb\") as buffer:                     shutil.copyfileobj(file.file, buffer)                          # Use SimpleDirectoryReader to load documents             reader = SimpleDirectoryReader(temp_dir, recursive=True)             documents = reader.load_data()                          # Process documents through the ingestion pipeline             pipeline = get_ingestion_pipeline()             nodes = pipeline.run(documents=documents)                          return JSONResponse(content={                 \"message\": f\"Successfully ingested {len(files)} files\",                 \"ingested_files\": [file.filename for file in files],                 \"total_nodes\": len(nodes)             })     except Exception as e:         raise HTTPException(status_code=500, detail=str(e)) In\u00a0[\u00a0]: Copied! <pre>@app.post(\"/api/query\")\nasync def query_documents(request: QueryRequest):\n    try:\n        query_engine = get_query_engine()\n        response = query_engine.query(request.query)        \n        return {\"response\": str(response)}\n    except Exception as e:\n        print(e)\n        raise HTTPException(status_code=500, detail=str(e))\n</pre> @app.post(\"/api/query\") async def query_documents(request: QueryRequest):     try:         query_engine = get_query_engine()         response = query_engine.query(request.query)                 return {\"response\": str(response)}     except Exception as e:         print(e)         raise HTTPException(status_code=500, detail=str(e)) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</pre> if __name__ == \"__main__\":     import uvicorn     uvicorn.run(app, host=\"0.0.0.0\", port=8000)"},{"location":"RAG/01_Data_Ingestion/","title":"Data Chunking for RAG Systems","text":""},{"location":"RAG/01_Data_Ingestion/#data-chunking-for-rag-systems","title":"Data Chunking for RAG Systems","text":"[![GitHub Stars](https://img.shields.io/github/stars/adithya-s-k/AI-Engineering.academy?style=social)](https://github.com/adithya-s-k/AI-Engineering.academy/stargazers) [![GitHub Forks](https://img.shields.io/github/forks/adithya-s-k/AI-Engineering.academy?style=social)](https://github.com/adithya-s-k/AI-Engineering.academy/network/members) [![GitHub Issues](https://img.shields.io/github/issues/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/issues) [![GitHub Pull Requests](https://img.shields.io/github/issues-pr/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/pulls) [![License](https://img.shields.io/github/license/adithya-s-k/AI-Engineering.academy)](https://github.com/adithya-s-k/AI-Engineering.academy/blob/main/LICENSE)"},{"location":"RAG/01_Data_Ingestion/#introduction","title":"Introduction","text":"<p>Data chunking is a crucial step in Retrieval-Augmented Generation (RAG) systems. It involves breaking down large documents into smaller, manageable pieces that can be efficiently indexed, retrieved, and processed. This README provides an overview of various chunking methods that can be used in RAG pipelines.</p>"},{"location":"RAG/01_Data_Ingestion/#importance-of-chunking-in-rag","title":"Importance of Chunking in RAG","text":"<p>Effective chunking is essential for RAG systems because it: 1. Improves retrieval accuracy by creating coherent, self-contained units of information. 2. Enhances the efficiency of embedding generation and similarity search. 3. Allows for more precise context selection when generating responses. 4. Helps manage token limits in language models and embedding systems.</p>"},{"location":"RAG/01_Data_Ingestion/#chunking-methods","title":"Chunking Methods","text":"<p>We have implemented six different chunking methods, each with its own strengths and use cases:</p> <ol> <li>RecursiveCharacterTextSplitter</li> <li>TokenTextSplitter</li> <li>KamradtSemanticChunker</li> <li>KamradtModifiedChunker</li> <li>ClusterSemanticChunker</li> <li>LLMSemanticChunker</li> </ol>"},{"location":"RAG/01_Data_Ingestion/#chunking-workflows","title":"Chunking Workflows","text":""},{"location":"RAG/01_Data_Ingestion/#1-recursivecharactertextsplitter","title":"1. RecursiveCharacterTextSplitter","text":"<pre><code>flowchart TB\n    A[Document] --&gt; B[Split by separators]\n    B --&gt; C[\"Priority: &lt;br/&gt;\\n\\n, \\n, ., ?, !, space\"]\n    C --&gt; D[Merge splits until max length]\n    D --&gt; E[Optional: Add chunk overlap]</code></pre>"},{"location":"RAG/01_Data_Ingestion/#2-tokentextsplitter","title":"2. TokenTextSplitter","text":"<pre><code>flowchart TB\n    A[Document] --&gt; B[Tokenize text]\n    B --&gt; C[Split by fixed token count]\n    C --&gt; D[Ensure splits at token boundaries]\n    D --&gt; E[Optional: Add chunk overlap]</code></pre>"},{"location":"RAG/01_Data_Ingestion/#3-kamradtsemanticchunker","title":"3. KamradtSemanticChunker","text":"<pre><code>flowchart TB\n    A[Document] --&gt; B[Split by sentence]\n    B --&gt; C[Compute embeddings&lt;br/&gt;for sliding window]\n    C --&gt; D[Calculate cosine distances&lt;br/&gt;between consecutive windows]\n    D --&gt; E[Find discontinuities&lt;br/&gt;&gt; 95th percentile]\n    E --&gt; F[Split at discontinuities]</code></pre>"},{"location":"RAG/01_Data_Ingestion/#4-kamradtmodifiedchunker","title":"4. KamradtModifiedChunker","text":"<pre><code>flowchart TB\n    A[Document] --&gt; B[Split by sentence]\n    B --&gt; C[Compute embeddings&lt;br/&gt;for sliding window]\n    C --&gt; D[Calculate cosine distances&lt;br/&gt;between consecutive windows]\n    D --&gt; E[Binary search for&lt;br/&gt;optimal threshold]\n    E --&gt; F[Ensure largest chunk&lt;br/&gt;&lt; specified length]\n    F --&gt; G[Split at determined&lt;br/&gt;discontinuities]</code></pre>"},{"location":"RAG/01_Data_Ingestion/#5-clustersemanticchunker","title":"5. ClusterSemanticChunker","text":"<pre><code>flowchart TB\n    A[Document] --&gt; B[Split into 50-token pieces]\n    B --&gt; C[Compute embeddings&lt;br/&gt;for each piece]\n    C --&gt; D[Calculate pairwise&lt;br/&gt;cosine similarities]\n    D --&gt; E[Use dynamic programming&lt;br/&gt;to maximize similarity]\n    E --&gt; F[Ensure chunks &lt;= max length]\n    F --&gt; G[Merge pieces into&lt;br/&gt;optimal chunks]</code></pre>"},{"location":"RAG/01_Data_Ingestion/#6-llmsemanticchunker","title":"6. LLMSemanticChunker","text":"<pre><code>flowchart TB\n    A[Document] --&gt; B[Split into 50-token pieces]\n    B --&gt; C[Surround with&lt;br/&gt;&lt;start_chunk_X&gt; tags]\n    C --&gt; D[Prompt LLM with tagged text]\n    D --&gt; E[LLM returns split indexes]\n    E --&gt; F[Process indexes to&lt;br/&gt;create chunks]\n    F --&gt; G[Ensure chunks &lt;= max length]</code></pre>"},{"location":"RAG/01_Data_Ingestion/#method-descriptions","title":"Method Descriptions","text":"<ol> <li> <p>RecursiveCharacterTextSplitter: Splits text based on a hierarchy of separators, prioritizing natural breaks in the document.</p> </li> <li> <p>TokenTextSplitter: Splits text into chunks of a fixed number of tokens, ensuring that splits occur at token boundaries.</p> </li> <li> <p>KamradtSemanticChunker: Uses sliding window embeddings to identify semantic discontinuities and split the text accordingly.</p> </li> <li> <p>KamradtModifiedChunker: An improved version of KamradtSemanticChunker that uses binary search to find an optimal threshold for splitting.</p> </li> <li> <p>ClusterSemanticChunker: Splits text into small pieces, computes embeddings, and uses dynamic programming to create optimal chunks based on semantic similarity.</p> </li> <li> <p>LLMSemanticChunker: Utilizes a language model to determine appropriate split points in the text.</p> </li> </ol>"},{"location":"RAG/01_Data_Ingestion/#usage","title":"Usage","text":"<p>To use these chunking methods in your RAG pipeline:</p> <ol> <li>Import the desired chunker from the <code>chunkers</code> module.</li> <li>Initialize the chunker with appropriate parameters (e.g., max chunk size, overlap).</li> <li>Pass your document through the chunker to obtain the chunks.</li> </ol> <p>Example:</p> <pre><code>from chunkers import RecursiveCharacterTextSplitter\n\nchunker = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = chunker.split_text(your_document)\n</code></pre>"},{"location":"RAG/01_Data_Ingestion/#choosing-a-chunking-method","title":"Choosing a Chunking Method","text":"<p>The choice of chunking method depends on your specific use case:</p> <ul> <li>For simple text splitting, use RecursiveCharacterTextSplitter or TokenTextSplitter.</li> <li>For semantic-aware splitting, consider KamradtSemanticChunker or KamradtModifiedChunker.</li> <li>For more advanced semantic chunking, use ClusterSemanticChunker or LLMSemanticChunker.</li> </ul> <p>Factors to consider when choosing a method: - Document structure and content type - Desired chunk size and overlap - Computational resources available - Specific requirements of your retrieval system (e.g., vector vs. keyword-based)</p> <p>Experiment with different methods to find the one that works best for your documents and retrieval needs.</p>"},{"location":"RAG/01_Data_Ingestion/#integration-with-rag-systems","title":"Integration with RAG Systems","text":"<p>After chunking, typically you would: 1. Generate embeddings for each chunk (for vector-based retrieval systems). 2. Index the chunks in your chosen retrieval system (e.g., vector database, inverted index). 3. Use the indexed chunks in your retrieval step when answering queries.</p>"},{"location":"RAG/01_Data_Ingestion/#contributing","title":"Contributing","text":"<p>We welcome contributions to improve existing chunking methods or add new ones. Please refer to our contributing guidelines for more information.</p>"},{"location":"RAG/01_Data_Ingestion/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p> <pre><code>flowchart TB\n    Document --&gt; RecursiveCharacterTextSplitter &amp; TokenTextSplitter &amp; KamradtSemanticChunker &amp; KamradtModifiedChunker &amp; ClusterSemanticChunker &amp; LLMSemanticChunker\n\n    subgraph \"1. RecursiveCharacterTextSplitter\"\n        RecursiveCharacterTextSplitter --&gt; RCS1[Split by separators]\n        RCS1 --&gt; RCS2[\"Priority: &lt;br/&gt;\\n\\n, \\n, ., ?, !, space\"]\n        RCS2 --&gt; RCS3[Merge splits until max length]\n        RCS3 --&gt; RCS4[Optional: Add chunk overlap]\n    end\n\n    subgraph \"2. TokenTextSplitter\"\n        TokenTextSplitter --&gt; TTS1[Tokenize text]\n        TTS1 --&gt; TTS2[Split by fixed token count]\n        TTS2 --&gt; TTS3[Ensure splits at token boundaries]\n        TTS3 --&gt; TTS4[Optional: Add chunk overlap]\n    end\n\n    subgraph \"3. KamradtSemanticChunker\"\n        KamradtSemanticChunker --&gt; KSC1[Split by sentence]\n        KSC1 --&gt; KSC2[Compute embeddings&lt;br/&gt;for sliding window]\n        KSC2 --&gt; KSC3[Calculate cosine distances&lt;br/&gt;between consecutive windows]\n        KSC3 --&gt; KSC4[Find discontinuities&lt;br/&gt;&gt; 95th percentile]\n        KSC4 --&gt; KSC5[Split at discontinuities]\n    end\n\n    subgraph \"4. KamradtModifiedChunker\"\n        KamradtModifiedChunker --&gt; KMC1[Split by sentence]\n        KMC1 --&gt; KMC2[Compute embeddings&lt;br/&gt;for sliding window]\n        KMC2 --&gt; KMC3[Calculate cosine distances&lt;br/&gt;between consecutive windows]\n        KMC3 --&gt; KMC4[Binary search for&lt;br/&gt;optimal threshold]\n        KMC4 --&gt; KMC5[Ensure largest chunk&lt;br/&gt;&lt; specified length]\n        KMC5 --&gt; KMC6[Split at determined&lt;br/&gt;discontinuities]\n    end\n\n    subgraph \"5. ClusterSemanticChunker\"\n        ClusterSemanticChunker --&gt; CSC1[Split into 50-token pieces]\n        CSC1 --&gt; CSC2[Compute embeddings&lt;br/&gt;for each piece]\n        CSC2 --&gt; CSC3[Calculate pairwise&lt;br/&gt;cosine similarities]\n        CSC3 --&gt; CSC4[Use dynamic programming&lt;br/&gt;to maximize similarity]\n        CSC4 --&gt; CSC5[Ensure chunks &lt;= max length]\n        CSC5 --&gt; CSC6[Merge pieces into&lt;br/&gt;optimal chunks]\n    end\n\n    subgraph \"6. LLMSemanticChunker\"\n        LLMSemanticChunker --&gt; LSC1[Split into 50-token pieces]\n        LSC1 --&gt; LSC2[Surround with&lt;br/&gt;&lt;start_chunk_X&gt; tags]\n        LSC2 --&gt; LSC3[Prompt LLM with tagged text]\n        LSC3 --&gt; LSC4[LLM returns split indexes]\n        LSC4 --&gt; LSC5[Process indexes to&lt;br/&gt;create chunks]\n        LSC5 --&gt; LSC6[Ensure chunks &lt;= max length]\n    end\n\n    %% Force diagram to render left-to-right\n    RecursiveCharacterTextSplitter ~~~ TokenTextSplitter ~~~ KamradtSemanticChunker ~~~ KamradtModifiedChunker ~~~ ClusterSemanticChunker ~~~ LLMSemanticChunker</code></pre>"},{"location":"RAG/01_Data_Ingestion/data_chunking/","title":"RAG Data Chunking","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nfrom typing import List, Callable, Optional\nfrom abc import ABC, abstractmethod\nimport re\nimport tiktoken\n\n# Load the text file\nwith open('../data/paul_graham_essay.txt', 'r') as file:\n    text = file.read()\n\nprint(f\"Loaded text with {len(text)} characters\")\n</pre> import numpy as np from typing import List, Callable, Optional from abc import ABC, abstractmethod import re import tiktoken  # Load the text file with open('../data/paul_graham_essay.txt', 'r') as file:     text = file.read()  print(f\"Loaded text with {len(text)} characters\") <pre>Loaded text with 75014 characters\n</pre> In\u00a0[3]: Copied! <pre>class BaseChunker(ABC):\n    @abstractmethod\n    def split_text(self, text: str) -&gt; List[str]:\n        pass\n    \nclass TextSplitter(BaseChunker, ABC):\n    def __init__(\n        self,\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        length_function: Callable[[str], int] = len,\n    ) -&gt; None:\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self._length_function = length_function\n\n    def _merge_splits(self, splits: List[str], separator: str) -&gt; List[str]:\n        docs = []\n        current_doc = []\n        total = 0\n        for d in splits:\n            _len = self._length_function(d)\n            if total + _len &gt; self._chunk_size:\n                if total &gt; self._chunk_size:\n                    print(f\"Created a chunk of size {total}, which is longer than the specified {self._chunk_size}\")\n                if current_doc:\n                    doc = self._join_docs(current_doc, separator)\n                    if doc is not None:\n                        docs.append(doc)\n                    # Keep on popping if:\n                    # - we have a larger chunk than in the chunk overlap\n                    # - or if we still have any chunks and the length is long\n                    while total &gt; self._chunk_overlap or (\n                        total + _len &gt; self._chunk_size and total &gt; 0\n                    ):\n                        total -= self._length_function(current_doc[0])\n                        current_doc = current_doc[1:]\n            current_doc.append(d)\n            total += _len\n        doc = self._join_docs(current_doc, separator)\n        if doc is not None:\n            docs.append(doc)\n        return docs\n\n    def _join_docs(self, docs: List[str], separator: str) -&gt; Optional[str]:\n        text = separator.join(docs)\n        text = text.strip()\n        if text == \"\":\n            return None\n        else:\n            return text\n</pre> class BaseChunker(ABC):     @abstractmethod     def split_text(self, text: str) -&gt; List[str]:         pass      class TextSplitter(BaseChunker, ABC):     def __init__(         self,         chunk_size: int = 4000,         chunk_overlap: int = 200,         length_function: Callable[[str], int] = len,     ) -&gt; None:         self._chunk_size = chunk_size         self._chunk_overlap = chunk_overlap         self._length_function = length_function      def _merge_splits(self, splits: List[str], separator: str) -&gt; List[str]:         docs = []         current_doc = []         total = 0         for d in splits:             _len = self._length_function(d)             if total + _len &gt; self._chunk_size:                 if total &gt; self._chunk_size:                     print(f\"Created a chunk of size {total}, which is longer than the specified {self._chunk_size}\")                 if current_doc:                     doc = self._join_docs(current_doc, separator)                     if doc is not None:                         docs.append(doc)                     # Keep on popping if:                     # - we have a larger chunk than in the chunk overlap                     # - or if we still have any chunks and the length is long                     while total &gt; self._chunk_overlap or (                         total + _len &gt; self._chunk_size and total &gt; 0                     ):                         total -= self._length_function(current_doc[0])                         current_doc = current_doc[1:]             current_doc.append(d)             total += _len         doc = self._join_docs(current_doc, separator)         if doc is not None:             docs.append(doc)         return docs      def _join_docs(self, docs: List[str], separator: str) -&gt; Optional[str]:         text = separator.join(docs)         text = text.strip()         if text == \"\":             return None         else:             return text In\u00a0[8]: Copied! <pre>class FixedTokenChunker(TextSplitter):\n    def __init__(self, chunk_size: int = 100, chunk_overlap: int = 0):\n        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        self._tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n    def split_text(self, text: str) -&gt; List[str]:\n        tokens = self._tokenizer.encode(text)\n        chunks = []\n        for i in range(0, len(tokens), self._chunk_size - self._chunk_overlap):\n            chunk = self._tokenizer.decode(tokens[i:i + self._chunk_size])\n            chunks.append(chunk)\n        return chunks\n\n# Use FixedTokenChunker\nfixed_chunker = FixedTokenChunker(chunk_size=100, chunk_overlap=0)\nfixed_chunks = fixed_chunker.split_text(text)\n\nprint(f\"FixedTokenChunker: {len(fixed_chunks)} chunks\")\n\nprint(\"\\nFirst 5 chunks:\")\nfor i, chunk in enumerate(fixed_chunks[:5], 1):\n    print(f\"\\nChunk {i}:\\n\",\"-\"*100)\n    print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk)\n</pre> class FixedTokenChunker(TextSplitter):     def __init__(self, chunk_size: int = 100, chunk_overlap: int = 0):         super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)         self._tokenizer = tiktoken.get_encoding(\"cl100k_base\")      def split_text(self, text: str) -&gt; List[str]:         tokens = self._tokenizer.encode(text)         chunks = []         for i in range(0, len(tokens), self._chunk_size - self._chunk_overlap):             chunk = self._tokenizer.decode(tokens[i:i + self._chunk_size])             chunks.append(chunk)         return chunks  # Use FixedTokenChunker fixed_chunker = FixedTokenChunker(chunk_size=100, chunk_overlap=0) fixed_chunks = fixed_chunker.split_text(text)  print(f\"FixedTokenChunker: {len(fixed_chunks)} chunks\")  print(\"\\nFirst 5 chunks:\") for i, chunk in enumerate(fixed_chunks[:5], 1):     print(f\"\\nChunk {i}:\\n\",\"-\"*100)     print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk) <pre>FixedTokenChunker: 166 chunks\n\nFirst 5 chunks:\n\nChunk 1:\n ----------------------------------------------------------------------------------------------------\n\n\nWhat I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school...\n\nChunk 2:\n ----------------------------------------------------------------------------------------------------\n was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's ...\n\nChunk 3:\n ----------------------------------------------------------------------------------------------------\n we used was an early version of Fortran. You had to type programs on punch cards, then stack them i...\n\nChunk 4:\n ----------------------------------------------------------------------------------------------------\n punched cards, and I didn't have any data stored on punched cards. The only other option was to do ...\n\nChunk 5:\n ----------------------------------------------------------------------------------------------------\n. On a machine without time-sharing, this was a social as well as a technical error, as the data cen...\n</pre> In\u00a0[9]: Copied! <pre>class RecursiveTokenChunker(TextSplitter):\n    def __init__(\n        self,\n        chunk_size: int = 100,\n        chunk_overlap: int = 0,\n        separators: Optional[List[str]] = None,\n    ):\n        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        self._separators = separators or [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n\n    def split_text(self, text: str) -&gt; List[str]:\n        return self._split_text(text, self._separators)\n\n    def _split_text(self, text: str, separators: List[str]) -&gt; List[str]:\n        final_chunks = []\n        separator = separators[-1]\n        new_separators = []\n        for i, _s in enumerate(separators):\n            if _s == \"\":\n                separator = _s\n                break\n            if re.search(re.escape(_s), text):\n                separator = _s\n                new_separators = separators[i + 1:]\n                break\n\n        splits = re.split(f\"({re.escape(separator)})\", text)\n        splits = [s for s in splits if s != \"\"]\n\n        _good_splits = []\n        for s in splits:\n            if self._length_function(s) &lt; self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, \"\")\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                if not new_separators:\n                    final_chunks.append(s)\n                else:\n                    other_info = self._split_text(s, new_separators)\n                    final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, \"\")\n            final_chunks.extend(merged_text)\n        return final_chunks\n\n# Use RecursiveTokenChunker\nrecursive_chunker = RecursiveTokenChunker(chunk_size=100, chunk_overlap=0)\nrecursive_chunks = recursive_chunker.split_text(text)\n\nprint(f\"RecursiveTokenChunker: {len(recursive_chunks)} chunks\")\nprint(\"\\nFirst 5 chunks:\")\nfor i, chunk in enumerate(fixed_chunks[:5], 1):\n    print(f\"\\nChunk {i}:\\n\",\"-\"*100)\n    print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk)\n</pre> class RecursiveTokenChunker(TextSplitter):     def __init__(         self,         chunk_size: int = 100,         chunk_overlap: int = 0,         separators: Optional[List[str]] = None,     ):         super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)         self._separators = separators or [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]      def split_text(self, text: str) -&gt; List[str]:         return self._split_text(text, self._separators)      def _split_text(self, text: str, separators: List[str]) -&gt; List[str]:         final_chunks = []         separator = separators[-1]         new_separators = []         for i, _s in enumerate(separators):             if _s == \"\":                 separator = _s                 break             if re.search(re.escape(_s), text):                 separator = _s                 new_separators = separators[i + 1:]                 break          splits = re.split(f\"({re.escape(separator)})\", text)         splits = [s for s in splits if s != \"\"]          _good_splits = []         for s in splits:             if self._length_function(s) &lt; self._chunk_size:                 _good_splits.append(s)             else:                 if _good_splits:                     merged_text = self._merge_splits(_good_splits, \"\")                     final_chunks.extend(merged_text)                     _good_splits = []                 if not new_separators:                     final_chunks.append(s)                 else:                     other_info = self._split_text(s, new_separators)                     final_chunks.extend(other_info)         if _good_splits:             merged_text = self._merge_splits(_good_splits, \"\")             final_chunks.extend(merged_text)         return final_chunks  # Use RecursiveTokenChunker recursive_chunker = RecursiveTokenChunker(chunk_size=100, chunk_overlap=0) recursive_chunks = recursive_chunker.split_text(text)  print(f\"RecursiveTokenChunker: {len(recursive_chunks)} chunks\") print(\"\\nFirst 5 chunks:\") for i, chunk in enumerate(fixed_chunks[:5], 1):     print(f\"\\nChunk {i}:\\n\",\"-\"*100)     print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk) <pre>RecursiveTokenChunker: 1274 chunks\n\nFirst 5 chunks:\n\nChunk 1:\n ----------------------------------------------------------------------------------------------------\n\n\nWhat I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school...\n\nChunk 2:\n ----------------------------------------------------------------------------------------------------\n was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's ...\n\nChunk 3:\n ----------------------------------------------------------------------------------------------------\n we used was an early version of Fortran. You had to type programs on punch cards, then stack them i...\n\nChunk 4:\n ----------------------------------------------------------------------------------------------------\n punched cards, and I didn't have any data stored on punched cards. The only other option was to do ...\n\nChunk 5:\n ----------------------------------------------------------------------------------------------------\n. On a machine without time-sharing, this was a social as well as a technical error, as the data cen...\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install openai \n!pip install anthropic\n!pip install backoff\n</pre> !pip install openai  !pip install anthropic !pip install backoff In\u00a0[10]: Copied! <pre>import os\nimport openai\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n</pre> import os import openai from getpass import getpass  if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):     openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \") openai.api_key = openai_api_key os.environ[\"OPENAI_API_KEY\"] = openai_api_key In\u00a0[11]: Copied! <pre>import numpy as np\nfrom openai import OpenAI\n\ndef get_openai_embedding_function(api_key):\n    client = OpenAI(api_key=api_key)\n    def embedding_function(texts):\n        response = client.embeddings.create(input=texts, model=\"text-embedding-3-small\")\n        return [item.embedding for item in response.data]\n    return embedding_function\n\nclass ClusterSemanticChunker(BaseChunker):\n    def __init__(self, embedding_function, max_chunk_size=400, min_chunk_size=50):\n        self.splitter = RecursiveTokenChunker(chunk_size=min_chunk_size, chunk_overlap=0)\n        self.max_cluster = max_chunk_size // min_chunk_size\n        self.embedding_function = embedding_function\n        \n    def _get_similarity_matrix(self, sentences):\n        BATCH_SIZE = 500\n        N = len(sentences)\n        embedding_matrix = None\n        for i in range(0, N, BATCH_SIZE):\n            batch_sentences = sentences[i:i+BATCH_SIZE]\n            embeddings = self.embedding_function(batch_sentences)\n            batch_embedding_matrix = np.array(embeddings)\n            if embedding_matrix is None:\n                embedding_matrix = batch_embedding_matrix\n            else:\n                embedding_matrix = np.concatenate((embedding_matrix, batch_embedding_matrix), axis=0)\n        similarity_matrix = np.dot(embedding_matrix, embedding_matrix.T)\n        return similarity_matrix\n\n    def _calculate_reward(self, matrix, start, end):\n        sub_matrix = matrix[start:end+1, start:end+1]\n        return np.sum(sub_matrix)\n\n    def _optimal_segmentation(self, matrix, max_cluster_size):\n        mean_value = np.mean(matrix[np.triu_indices(matrix.shape[0], k=1)])\n        matrix = matrix - mean_value\n        np.fill_diagonal(matrix, 0)\n        n = matrix.shape[0]\n        dp = np.zeros(n)\n        segmentation = np.zeros(n, dtype=int)\n        for i in range(n):\n            for size in range(1, max_cluster_size + 1):\n                if i - size + 1 &gt;= 0:\n                    reward = self._calculate_reward(matrix, i - size + 1, i)\n                    adjusted_reward = reward\n                    if i - size &gt;= 0:\n                        adjusted_reward += dp[i - size]\n                    if adjusted_reward &gt; dp[i]:\n                        dp[i] = adjusted_reward\n                        segmentation[i] = i - size + 1\n        clusters = []\n        i = n - 1\n        while i &gt;= 0:\n            start = segmentation[i]\n            clusters.append((start, i))\n            i = start - 1\n        clusters.reverse()\n        return clusters\n        \n    def split_text(self, text: str) -&gt; List[str]:\n        sentences = self.splitter.split_text(text)\n        similarity_matrix = self._get_similarity_matrix(sentences)\n        clusters = self._optimal_segmentation(similarity_matrix, max_cluster_size=self.max_cluster)\n        docs = [' '.join(sentences[start:end+1]) for start, end in clusters]\n        return docs\n\n# Use ClusterSemanticChunker\n# api_key = \"your_openai_api_key_here\"  # Replace with your actual OpenAI API key\nembedding_function = get_openai_embedding_function(api_key = openai.api_key)\ncluster_chunker = ClusterSemanticChunker(embedding_function=embedding_function, max_chunk_size=400, min_chunk_size=50)\ncluster_chunks = cluster_chunker.split_text(text)\n\nprint(f\"ClusterSemanticChunker: {len(cluster_chunks)} chunks\")\nprint(\"\\nFirst 5 chunks:\")\nfor i, chunk in enumerate(fixed_chunks[:5], 1):\n    print(f\"\\nChunk {i}:\\n\",\"-\"*100)\n    print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk)\n</pre> import numpy as np from openai import OpenAI  def get_openai_embedding_function(api_key):     client = OpenAI(api_key=api_key)     def embedding_function(texts):         response = client.embeddings.create(input=texts, model=\"text-embedding-3-small\")         return [item.embedding for item in response.data]     return embedding_function  class ClusterSemanticChunker(BaseChunker):     def __init__(self, embedding_function, max_chunk_size=400, min_chunk_size=50):         self.splitter = RecursiveTokenChunker(chunk_size=min_chunk_size, chunk_overlap=0)         self.max_cluster = max_chunk_size // min_chunk_size         self.embedding_function = embedding_function              def _get_similarity_matrix(self, sentences):         BATCH_SIZE = 500         N = len(sentences)         embedding_matrix = None         for i in range(0, N, BATCH_SIZE):             batch_sentences = sentences[i:i+BATCH_SIZE]             embeddings = self.embedding_function(batch_sentences)             batch_embedding_matrix = np.array(embeddings)             if embedding_matrix is None:                 embedding_matrix = batch_embedding_matrix             else:                 embedding_matrix = np.concatenate((embedding_matrix, batch_embedding_matrix), axis=0)         similarity_matrix = np.dot(embedding_matrix, embedding_matrix.T)         return similarity_matrix      def _calculate_reward(self, matrix, start, end):         sub_matrix = matrix[start:end+1, start:end+1]         return np.sum(sub_matrix)      def _optimal_segmentation(self, matrix, max_cluster_size):         mean_value = np.mean(matrix[np.triu_indices(matrix.shape[0], k=1)])         matrix = matrix - mean_value         np.fill_diagonal(matrix, 0)         n = matrix.shape[0]         dp = np.zeros(n)         segmentation = np.zeros(n, dtype=int)         for i in range(n):             for size in range(1, max_cluster_size + 1):                 if i - size + 1 &gt;= 0:                     reward = self._calculate_reward(matrix, i - size + 1, i)                     adjusted_reward = reward                     if i - size &gt;= 0:                         adjusted_reward += dp[i - size]                     if adjusted_reward &gt; dp[i]:                         dp[i] = adjusted_reward                         segmentation[i] = i - size + 1         clusters = []         i = n - 1         while i &gt;= 0:             start = segmentation[i]             clusters.append((start, i))             i = start - 1         clusters.reverse()         return clusters              def split_text(self, text: str) -&gt; List[str]:         sentences = self.splitter.split_text(text)         similarity_matrix = self._get_similarity_matrix(sentences)         clusters = self._optimal_segmentation(similarity_matrix, max_cluster_size=self.max_cluster)         docs = [' '.join(sentences[start:end+1]) for start, end in clusters]         return docs  # Use ClusterSemanticChunker # api_key = \"your_openai_api_key_here\"  # Replace with your actual OpenAI API key embedding_function = get_openai_embedding_function(api_key = openai.api_key) cluster_chunker = ClusterSemanticChunker(embedding_function=embedding_function, max_chunk_size=400, min_chunk_size=50) cluster_chunks = cluster_chunker.split_text(text)  print(f\"ClusterSemanticChunker: {len(cluster_chunks)} chunks\") print(\"\\nFirst 5 chunks:\") for i, chunk in enumerate(fixed_chunks[:5], 1):     print(f\"\\nChunk {i}:\\n\",\"-\"*100)     print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk) <pre>ClusterSemanticChunker: 680 chunks\n\nFirst 5 chunks:\n\nChunk 1:\n ----------------------------------------------------------------------------------------------------\n\n\nWhat I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school...\n\nChunk 2:\n ----------------------------------------------------------------------------------------------------\n was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's ...\n\nChunk 3:\n ----------------------------------------------------------------------------------------------------\n we used was an early version of Fortran. You had to type programs on punch cards, then stack them i...\n\nChunk 4:\n ----------------------------------------------------------------------------------------------------\n punched cards, and I didn't have any data stored on punched cards. The only other option was to do ...\n\nChunk 5:\n ----------------------------------------------------------------------------------------------------\n. On a machine without time-sharing, this was a social as well as a technical error, as the data cen...\n</pre> In\u00a0[16]: Copied! <pre>import anthropic\nimport backoff\nfrom tqdm import tqdm\n\nclass AnthropicClient:\n    def __init__(self, model_name, api_key):\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self.model_name = model_name\n\n    @backoff.on_exception(backoff.expo, Exception, max_tries=3)\n    def create_message(self, system_prompt, messages, max_tokens=1000, temperature=1.0):\n        try:\n            message = self.client.messages.create(\n                model=self.model_name,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                system=system_prompt,\n                messages=messages\n            )\n            return message.content[0].text\n        except Exception as e:\n            print(f\"Error occurred: {e}, retrying...\")\n            raise e\n\nclass OpenAIClient:\n    def __init__(self, model_name, api_key):\n        self.client = OpenAI(api_key=api_key)\n        self.model_name = model_name\n\n    @backoff.on_exception(backoff.expo, Exception, max_tries=3)\n    def create_message(self, system_prompt, messages, max_tokens=1000, temperature=1.0):\n        try:\n            gpt_messages = [\n                {\"role\": \"system\", \"content\": system_prompt}\n            ] + messages\n\n            completion = self.client.chat.completions.create(\n                model=self.model_name,\n                max_tokens=max_tokens,\n                messages=gpt_messages,\n                temperature=temperature\n            )\n\n            return completion.choices[0].message.content\n        except Exception as e:\n            print(f\"Error occurred: {e}, retrying...\")\n            raise e\n\nclass LLMSemanticChunker(BaseChunker):\n    def __init__(self, organisation=\"openai\", api_key=None, model_name=None):\n        if organisation == \"openai\":\n            if model_name is None:\n                model_name = \"gpt-4\"\n            self.client = OpenAIClient(model_name, api_key=api_key)\n        elif organisation == \"anthropic\":\n            if model_name is None:\n                model_name = \"claude-3-opus-20240229\"\n            self.client = AnthropicClient(model_name, api_key=api_key)\n        else:\n            raise ValueError(\"Invalid organisation. Please choose either 'openai' or 'anthropic'.\")\n\n        self.splitter = RecursiveTokenChunker(chunk_size=50, chunk_overlap=0)\n\n    def get_prompt(self, chunked_input, current_chunk=0, invalid_response=None):\n        messages = [\n            {\n                \"role\": \"system\", \n                \"content\": (\n                    \"You are an assistant specialized in splitting text into thematically consistent sections. \"\n                    \"The text has been divided into chunks, each marked with &lt;|start_chunk_X|&gt; and &lt;|end_chunk_X|&gt; tags, where X is the chunk number. \"\n                    \"Your task is to identify the points where splits should occur, such that consecutive chunks of similar themes stay together. \"\n                    \"Respond with a list of chunk IDs where you believe a split should be made. For example, if chunks 1 and 2 belong together but chunk 3 starts a new topic, you would suggest a split after chunk 2. THE CHUNKS MUST BE IN ASCENDING ORDER.\"\n                    \"Your response should be in the form: 'split_after: 3, 5'.\"\n                )\n            },\n            {\n                \"role\": \"user\", \n                \"content\": (\n                    \"CHUNKED_TEXT: \" + chunked_input + \"\\n\\n\"\n                    \"Respond only with the IDs of the chunks where you believe a split should occur. YOU MUST RESPOND WITH AT LEAST ONE SPLIT. THESE SPLITS MUST BE IN ASCENDING ORDER AND EQUAL OR LARGER THAN: \" + str(current_chunk)+\".\" + (f\"\\n\\The previous response of {invalid_response} was invalid. DO NOT REPEAT THIS ARRAY OF NUMBERS. Please try again.\" if invalid_response else \"\")\n                )\n            },\n        ]\n        return messages\n\n    def split_text(self, text):\n        import re\n\n        chunks = self.splitter.split_text(text)\n\n        split_indices = []\n        current_chunk = 0\n\n        with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n            while True:\n                if current_chunk &gt;= len(chunks) - 4:\n                    break\n\n                token_count = 0\n                chunked_input = ''\n\n                for i in range(current_chunk, len(chunks)):\n                    token_count += len(chunks[i].split())\n                    chunked_input += f\"&lt;|start_chunk_{i+1}|&gt;{chunks[i]}&lt;|end_chunk_{i+1}|&gt;\"\n                    if token_count &gt; 800:\n                        break\n\n                messages = self.get_prompt(chunked_input, current_chunk)\n                while True:\n                    result_string = self.client.create_message(messages[0]['content'], messages[1:], max_tokens=200, temperature=0.2)\n                    split_after_line = [line for line in result_string.split('\\n') if 'split_after:' in line][0]\n                    numbers = re.findall(r'\\d+', split_after_line)\n                    numbers = list(map(int, numbers))\n\n                    if not (numbers != sorted(numbers) or any(number &lt; current_chunk for number in numbers)):\n                        break\n                    else:\n                        messages = self.get_prompt(chunked_input, current_chunk, numbers)\n                        print(\"Response: \", result_string)\n                        print(\"Invalid response. Please try again.\")\n\n                split_indices.extend(numbers)\n                current_chunk = numbers[-1]\n                pbar.update(current_chunk - pbar.n)\n\n        chunks_to_split_after = [i - 1 for i in split_indices]\n\n        docs = []\n        current_chunk = ''\n        for i, chunk in enumerate(chunks):\n            current_chunk += chunk + ' '\n            if i in chunks_to_split_after:\n                docs.append(current_chunk.strip())\n                current_chunk = ''\n        if current_chunk:\n            docs.append(current_chunk.strip())\n\n        return docs\n\n# Use LLMSemanticChunker\n# api_key = \"your_openai_api_key_here\"  # Replace with your actual OpenAI API key\napi_key = openai.api_key\nllm_chunker = LLMSemanticChunker(organisation=\"openai\", api_key=api_key)\nllm_chunks = llm_chunker.split_text(text)\n\nprint(f\"LLMSemanticChunker: {len(llm_chunks)} chunks\")\nprint(\"\\nFirst 5 chunks:\")\nfor i, chunk in enumerate(fixed_chunks[:5], 1):\n    print(f\"\\nChunk {i}:\\n\",\"-\"*100)\n    print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk)\n    \n</pre> import anthropic import backoff from tqdm import tqdm  class AnthropicClient:     def __init__(self, model_name, api_key):         self.client = anthropic.Anthropic(api_key=api_key)         self.model_name = model_name      @backoff.on_exception(backoff.expo, Exception, max_tries=3)     def create_message(self, system_prompt, messages, max_tokens=1000, temperature=1.0):         try:             message = self.client.messages.create(                 model=self.model_name,                 max_tokens=max_tokens,                 temperature=temperature,                 system=system_prompt,                 messages=messages             )             return message.content[0].text         except Exception as e:             print(f\"Error occurred: {e}, retrying...\")             raise e  class OpenAIClient:     def __init__(self, model_name, api_key):         self.client = OpenAI(api_key=api_key)         self.model_name = model_name      @backoff.on_exception(backoff.expo, Exception, max_tries=3)     def create_message(self, system_prompt, messages, max_tokens=1000, temperature=1.0):         try:             gpt_messages = [                 {\"role\": \"system\", \"content\": system_prompt}             ] + messages              completion = self.client.chat.completions.create(                 model=self.model_name,                 max_tokens=max_tokens,                 messages=gpt_messages,                 temperature=temperature             )              return completion.choices[0].message.content         except Exception as e:             print(f\"Error occurred: {e}, retrying...\")             raise e  class LLMSemanticChunker(BaseChunker):     def __init__(self, organisation=\"openai\", api_key=None, model_name=None):         if organisation == \"openai\":             if model_name is None:                 model_name = \"gpt-4\"             self.client = OpenAIClient(model_name, api_key=api_key)         elif organisation == \"anthropic\":             if model_name is None:                 model_name = \"claude-3-opus-20240229\"             self.client = AnthropicClient(model_name, api_key=api_key)         else:             raise ValueError(\"Invalid organisation. Please choose either 'openai' or 'anthropic'.\")          self.splitter = RecursiveTokenChunker(chunk_size=50, chunk_overlap=0)      def get_prompt(self, chunked_input, current_chunk=0, invalid_response=None):         messages = [             {                 \"role\": \"system\",                  \"content\": (                     \"You are an assistant specialized in splitting text into thematically consistent sections. \"                     \"The text has been divided into chunks, each marked with &lt;|start_chunk_X|&gt; and &lt;|end_chunk_X|&gt; tags, where X is the chunk number. \"                     \"Your task is to identify the points where splits should occur, such that consecutive chunks of similar themes stay together. \"                     \"Respond with a list of chunk IDs where you believe a split should be made. For example, if chunks 1 and 2 belong together but chunk 3 starts a new topic, you would suggest a split after chunk 2. THE CHUNKS MUST BE IN ASCENDING ORDER.\"                     \"Your response should be in the form: 'split_after: 3, 5'.\"                 )             },             {                 \"role\": \"user\",                  \"content\": (                     \"CHUNKED_TEXT: \" + chunked_input + \"\\n\\n\"                     \"Respond only with the IDs of the chunks where you believe a split should occur. YOU MUST RESPOND WITH AT LEAST ONE SPLIT. THESE SPLITS MUST BE IN ASCENDING ORDER AND EQUAL OR LARGER THAN: \" + str(current_chunk)+\".\" + (f\"\\n\\The previous response of {invalid_response} was invalid. DO NOT REPEAT THIS ARRAY OF NUMBERS. Please try again.\" if invalid_response else \"\")                 )             },         ]         return messages      def split_text(self, text):         import re          chunks = self.splitter.split_text(text)          split_indices = []         current_chunk = 0          with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:             while True:                 if current_chunk &gt;= len(chunks) - 4:                     break                  token_count = 0                 chunked_input = ''                  for i in range(current_chunk, len(chunks)):                     token_count += len(chunks[i].split())                     chunked_input += f\"&lt;|start_chunk_{i+1}|&gt;{chunks[i]}&lt;|end_chunk_{i+1}|&gt;\"                     if token_count &gt; 800:                         break                  messages = self.get_prompt(chunked_input, current_chunk)                 while True:                     result_string = self.client.create_message(messages[0]['content'], messages[1:], max_tokens=200, temperature=0.2)                     split_after_line = [line for line in result_string.split('\\n') if 'split_after:' in line][0]                     numbers = re.findall(r'\\d+', split_after_line)                     numbers = list(map(int, numbers))                      if not (numbers != sorted(numbers) or any(number &lt; current_chunk for number in numbers)):                         break                     else:                         messages = self.get_prompt(chunked_input, current_chunk, numbers)                         print(\"Response: \", result_string)                         print(\"Invalid response. Please try again.\")                  split_indices.extend(numbers)                 current_chunk = numbers[-1]                 pbar.update(current_chunk - pbar.n)          chunks_to_split_after = [i - 1 for i in split_indices]          docs = []         current_chunk = ''         for i, chunk in enumerate(chunks):             current_chunk += chunk + ' '             if i in chunks_to_split_after:                 docs.append(current_chunk.strip())                 current_chunk = ''         if current_chunk:             docs.append(current_chunk.strip())          return docs  # Use LLMSemanticChunker # api_key = \"your_openai_api_key_here\"  # Replace with your actual OpenAI API key api_key = openai.api_key llm_chunker = LLMSemanticChunker(organisation=\"openai\", api_key=api_key) llm_chunks = llm_chunker.split_text(text)  print(f\"LLMSemanticChunker: {len(llm_chunks)} chunks\") print(\"\\nFirst 5 chunks:\") for i, chunk in enumerate(fixed_chunks[:5], 1):     print(f\"\\nChunk {i}:\\n\",\"-\"*100)     print(chunk[:100] + \"...\" if len(chunk) &gt; 100 else chunk)       <pre>Processing chunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 2427/2428 [01:22&lt;00:00, 29.49it/s]</pre> <pre>LLMSemanticChunker: 355 chunks\n\nFirst 5 chunks:\n\nChunk 1:\n ----------------------------------------------------------------------------------------------------\n\n\nWhat I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school...\n\nChunk 2:\n ----------------------------------------------------------------------------------------------------\n was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's ...\n\nChunk 3:\n ----------------------------------------------------------------------------------------------------\n we used was an early version of Fortran. You had to type programs on punch cards, then stack them i...\n\nChunk 4:\n ----------------------------------------------------------------------------------------------------\n punched cards, and I didn't have any data stored on punched cards. The only other option was to do ...\n\nChunk 5:\n ----------------------------------------------------------------------------------------------------\n. On a machine without time-sharing, this was a social as well as a technical error, as the data cen...\n</pre> <pre>\n</pre> In\u00a0[18]: Copied! <pre>\n</pre> <pre>Output saved to llm_semantic_chunker_output.json\n</pre> In\u00a0[19]: Copied! <pre>class KamradtModifiedChunker(BaseChunker):\n    def __init__(self, avg_chunk_size=400, min_chunk_size=50, embedding_function=None):\n        self.splitter = RecursiveTokenChunker(\n            chunk_size=min_chunk_size,\n            chunk_overlap=0,\n        )\n        \n        self.avg_chunk_size = avg_chunk_size\n        if embedding_function is None:\n            embedding_function = get_openai_embedding_function(api_key)  # Use the same API key as before\n        self.embedding_function = embedding_function\n\n    def combine_sentences(self, sentences, buffer_size=1):\n        for i in range(len(sentences)):\n            combined_sentence = ''\n            for j in range(i - buffer_size, i + 1 + buffer_size):\n                if 0 &lt;= j &lt; len(sentences):\n                    combined_sentence += sentences[j]['sentence'] + ' '\n            sentences[i]['combined_sentence'] = combined_sentence.strip()\n        return sentences\n\n    def calculate_cosine_distances(self, sentences):\n        BATCH_SIZE = 500\n        distances = []\n        embedding_matrix = None\n        for i in range(0, len(sentences), BATCH_SIZE):\n            batch_sentences = sentences[i:i+BATCH_SIZE]\n            batch_sentences = [sentence['combined_sentence'] for sentence in batch_sentences]\n            embeddings = self.embedding_function(batch_sentences)\n            batch_embedding_matrix = np.array(embeddings)\n            if embedding_matrix is None:\n                embedding_matrix = batch_embedding_matrix\n            else:\n                embedding_matrix = np.concatenate((embedding_matrix, batch_embedding_matrix), axis=0)\n\n        norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n        embedding_matrix = embedding_matrix / norms\n        similarity_matrix = np.dot(embedding_matrix, embedding_matrix.T)\n        \n        for i in range(len(sentences) - 1):\n            similarity = similarity_matrix[i, i + 1]\n            distance = 1 - similarity\n            distances.append(distance)\n            sentences[i]['distance_to_next'] = distance\n\n        return distances, sentences\n\n    def split_text(self, text):\n        sentences_strips = self.splitter.split_text(text)\n        sentences = [{'sentence': x, 'index': i} for i, x in enumerate(sentences_strips)]\n        sentences = self.combine_sentences(sentences, 3)\n        distances, sentences = self.calculate_cosine_distances(sentences)\n\n        total_tokens = sum(len(sentence['sentence'].split()) for sentence in sentences)\n        number_of_cuts = total_tokens // self.avg_chunk_size\n\n        lower_limit, upper_limit = 0.0, 1.0\n        distances_np = np.array(distances)\n\n        while upper_limit - lower_limit &gt; 1e-6:\n            threshold = (upper_limit + lower_limit) / 2.0\n            num_points_above_threshold = np.sum(distances_np &gt; threshold)\n            \n            if num_points_above_threshold &gt; number_of_cuts:\n                lower_limit = threshold\n            else:\n                upper_limit = threshold\n\n        indices_above_thresh = [i for i, x in enumerate(distances) if x &gt; threshold]\n        \n        chunks = []\n        start_index = 0\n        for index in indices_above_thresh:\n            group = sentences[start_index:index + 1]\n            combined_text = ' '.join([d['sentence'] for d in group])\n            chunks.append(combined_text)\n            start_index = index + 1\n\n        if start_index &lt; len(sentences):\n            combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n            chunks.append(combined_text)\n\n        return chunks\n\n# Use KamradtModifiedChunker\nkamradt_chunker = KamradtModifiedChunker(avg_chunk_size=300, min_chunk_size=50)\nkamradt_chunks = kamradt_chunker.split_text(text)\n\nprint(f\"KamradtModifiedChunker: {len(kamradt_chunks)} chunks\")\nprint(\"First chunk:\", kamradt_chunks[0][:100] + \"...\" if len(kamradt_chunks[0]) &gt; 100 else kamradt_chunks[0])\n</pre> class KamradtModifiedChunker(BaseChunker):     def __init__(self, avg_chunk_size=400, min_chunk_size=50, embedding_function=None):         self.splitter = RecursiveTokenChunker(             chunk_size=min_chunk_size,             chunk_overlap=0,         )                  self.avg_chunk_size = avg_chunk_size         if embedding_function is None:             embedding_function = get_openai_embedding_function(api_key)  # Use the same API key as before         self.embedding_function = embedding_function      def combine_sentences(self, sentences, buffer_size=1):         for i in range(len(sentences)):             combined_sentence = ''             for j in range(i - buffer_size, i + 1 + buffer_size):                 if 0 &lt;= j &lt; len(sentences):                     combined_sentence += sentences[j]['sentence'] + ' '             sentences[i]['combined_sentence'] = combined_sentence.strip()         return sentences      def calculate_cosine_distances(self, sentences):         BATCH_SIZE = 500         distances = []         embedding_matrix = None         for i in range(0, len(sentences), BATCH_SIZE):             batch_sentences = sentences[i:i+BATCH_SIZE]             batch_sentences = [sentence['combined_sentence'] for sentence in batch_sentences]             embeddings = self.embedding_function(batch_sentences)             batch_embedding_matrix = np.array(embeddings)             if embedding_matrix is None:                 embedding_matrix = batch_embedding_matrix             else:                 embedding_matrix = np.concatenate((embedding_matrix, batch_embedding_matrix), axis=0)          norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)         embedding_matrix = embedding_matrix / norms         similarity_matrix = np.dot(embedding_matrix, embedding_matrix.T)                  for i in range(len(sentences) - 1):             similarity = similarity_matrix[i, i + 1]             distance = 1 - similarity             distances.append(distance)             sentences[i]['distance_to_next'] = distance          return distances, sentences      def split_text(self, text):         sentences_strips = self.splitter.split_text(text)         sentences = [{'sentence': x, 'index': i} for i, x in enumerate(sentences_strips)]         sentences = self.combine_sentences(sentences, 3)         distances, sentences = self.calculate_cosine_distances(sentences)          total_tokens = sum(len(sentence['sentence'].split()) for sentence in sentences)         number_of_cuts = total_tokens // self.avg_chunk_size          lower_limit, upper_limit = 0.0, 1.0         distances_np = np.array(distances)          while upper_limit - lower_limit &gt; 1e-6:             threshold = (upper_limit + lower_limit) / 2.0             num_points_above_threshold = np.sum(distances_np &gt; threshold)                          if num_points_above_threshold &gt; number_of_cuts:                 lower_limit = threshold             else:                 upper_limit = threshold          indices_above_thresh = [i for i, x in enumerate(distances) if x &gt; threshold]                  chunks = []         start_index = 0         for index in indices_above_thresh:             group = sentences[start_index:index + 1]             combined_text = ' '.join([d['sentence'] for d in group])             chunks.append(combined_text)             start_index = index + 1          if start_index &lt; len(sentences):             combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])             chunks.append(combined_text)          return chunks  # Use KamradtModifiedChunker kamradt_chunker = KamradtModifiedChunker(avg_chunk_size=300, min_chunk_size=50) kamradt_chunks = kamradt_chunker.split_text(text)  print(f\"KamradtModifiedChunker: {len(kamradt_chunks)} chunks\") print(\"First chunk:\", kamradt_chunks[0][:100] + \"...\" if len(kamradt_chunks[0]) &gt; 100 else kamradt_chunks[0]) <pre>KamradtModifiedChunker: 49 chunks\nFirst chunk: What I Worked On\n\nFebruary 2021 Before college the two main things I worked on, outside of school, w...\n</pre> In\u00a0[20]: Copied! <pre>import json\n\n# After running the LLMSemanticChunker\noutput = {\n    \"chunker\": \"LLMSemanticChunker\",\n    \"total_chunks\": len(llm_chunks),\n    \"first_5_chunks\": [\n        {\n            \"chunk_number\": i,\n            \"content\": chunk\n        }\n        for i, chunk in enumerate(kamradt_chunks, 1)\n    ]\n}\n\n# Save the output as a JSON file\nwith open('llm_semantic_chunker_output.json', 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint(\"Output saved to llm_semantic_chunker_output.json\")\n</pre> import json  # After running the LLMSemanticChunker output = {     \"chunker\": \"LLMSemanticChunker\",     \"total_chunks\": len(llm_chunks),     \"first_5_chunks\": [         {             \"chunk_number\": i,             \"content\": chunk         }         for i, chunk in enumerate(kamradt_chunks, 1)     ] }  # Save the output as a JSON file with open('llm_semantic_chunker_output.json', 'w') as f:     json.dump(output, f, indent=2)  print(\"Output saved to llm_semantic_chunker_output.json\") <pre>Output saved to llm_semantic_chunker_output.json\n</pre> In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\nSettings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\n# Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI  # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: # Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version In\u00a0[\u00a0]: Copied! <pre># lets loading the documents using SimpleDirectoryReader\nfrom llama_index.core import Document\nreader = SimpleDirectoryReader(\"../data/\" , recursive=True)\ndocuments = reader.load_data(show_progress=True)\n\n# combining all the documents into a single document for later chunking and splitting\ndocuments = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n</pre> # lets loading the documents using SimpleDirectoryReader from llama_index.core import Document reader = SimpleDirectoryReader(\"../data/\" , recursive=True) documents = reader.load_data(show_progress=True)  # combining all the documents into a single document for later chunking and splitting documents = Document(text=\"\\n\\n\".join([doc.text for doc in documents])) In\u00a0[\u00a0]: Copied! <pre>## ingesting data into vector database\n\n## lets set up an ingestion pipeline\n\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        # MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ]\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=[documents] , show_progress=True)\nprint(\"Number of chunks added to vector DB :\",len(nodes))\n</pre> ## ingesting data into vector database  ## lets set up an ingestion pipeline  from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         # MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ] )  # Ingest directly into a vector db nodes = pipeline.run(documents=[documents] , show_progress=True) print(\"Number of chunks added to vector DB :\",len(nodes)) In\u00a0[\u00a0]: Copied! <pre>print(nodes)\n</pre> print(nodes) <p>Reference:</p> <p>Smith, Brandon, and Anton Troynikov. \"Evaluating Chunking Strategies for Retrieval.\" Chroma, July 2024. https://research.trychroma.com/evaluating-chunking.</p>"},{"location":"RAG/01_Data_Ingestion/data_chunking/#implmenting-chunking-from-scratch","title":"Implmenting Chunking From Scratch\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_chunking/#fixed-token-chunker","title":"Fixed Token Chunker\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_chunking/#recursive-token-chunking","title":"Recursive Token Chunking\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_chunking/#cluster-semantic-chunking","title":"Cluster Semantic Chunking\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_chunking/#llm-semantic-chunking","title":"LLM Semantic Chunking\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_chunking/#kamradt-modified-chunker","title":"Kamradt Modified Chunker\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_chunking/#llama-index-based-chunking","title":"Llama Index Based Chunking\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_embedding/","title":"RAG  Data Embedding","text":"In\u00a0[17]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom rich import print\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\n</pre> import os from dotenv import load_dotenv from openai import OpenAI from rich import print  load_dotenv() OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  client = OpenAI(api_key=OPENAI_API_KEY) In\u00a0[18]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n\nbatch_input_file = client.files.create(\n  file=open(\"batchinput.jsonl\", \"rb\"),\n  purpose=\"batch\"\n)\n</pre> from openai import OpenAI client = OpenAI()  batch_input_file = client.files.create(   file=open(\"batchinput.jsonl\", \"rb\"),   purpose=\"batch\" )  In\u00a0[19]: Copied! <pre>print(batch_input_file)\n</pre> print(batch_input_file) <pre>FileObject(\n    id='file-5UhXBkBba7yLBUhEALqUQea3',\n    bytes=4758,\n    created_at=1726847544,\n    filename='batchinput.jsonl',\n    object='file',\n    purpose='batch',\n    status='processed',\n    status_details=None\n)\n</pre> In\u00a0[20]: Copied! <pre>batch_input_file_id = batch_input_file.id\n\nbatch = client.batches.create(\n    input_file_id=batch_input_file_id,\n    endpoint=\"/v1/embeddings\",\n    completion_window=\"24h\",\n    metadata={\n      \"description\": \"nightly eval job\"\n    }\n)\n</pre> batch_input_file_id = batch_input_file.id  batch = client.batches.create(     input_file_id=batch_input_file_id,     endpoint=\"/v1/embeddings\",     completion_window=\"24h\",     metadata={       \"description\": \"nightly eval job\"     } ) In\u00a0[21]: Copied! <pre>print(batch)\n</pre> print(batch) <pre>Batch(\n    id='batch_katOshqMSik1DI8qwXtuHaUj',\n    completion_window='24h',\n    created_at=1726847573,\n    endpoint='/v1/embeddings',\n    input_file_id='file-5UhXBkBba7yLBUhEALqUQea3',\n    object='batch',\n    status='validating',\n    cancelled_at=None,\n    cancelling_at=None,\n    completed_at=None,\n    error_file_id=None,\n    errors=None,\n    expired_at=None,\n    expires_at=1726933973,\n    failed_at=None,\n    finalizing_at=None,\n    in_progress_at=None,\n    metadata={'description': 'nightly eval job'},\n    output_file_id=None,\n    request_counts=BatchRequestCounts(completed=0, failed=0, total=0)\n)\n</pre> In\u00a0[22]: Copied! <pre>from openai import OpenAI\nclient = OpenAI()\n\nbatch_status = client.batches.retrieve(batch.id)\nprint(batch_status)\n</pre> from openai import OpenAI client = OpenAI()  batch_status = client.batches.retrieve(batch.id) print(batch_status) <pre>Batch(\n    id='batch_katOshqMSik1DI8qwXtuHaUj',\n    completion_window='24h',\n    created_at=1726847573,\n    endpoint='/v1/embeddings',\n    input_file_id='file-5UhXBkBba7yLBUhEALqUQea3',\n    object='batch',\n    status='completed',\n    cancelled_at=None,\n    cancelling_at=None,\n    completed_at=1726847576,\n    error_file_id=None,\n    errors=None,\n    expired_at=None,\n    expires_at=1726933973,\n    failed_at=None,\n    finalizing_at=1726847576,\n    in_progress_at=1726847574,\n    metadata={'description': 'nightly eval job'},\n    output_file_id='file-fWmorUHxdNWkFGNLreI8JCCE',\n    request_counts=BatchRequestCounts(completed=2, failed=0, total=2)\n)\n</pre> In\u00a0[23]: Copied! <pre>file_response = client.files.content(batch_status.output_file_id)\nfile_response.astream_to_file(\"result.json\")\n</pre> file_response = client.files.content(batch_status.output_file_id) file_response.astream_to_file(\"result.json\") <pre>/tmp/ipykernel_281948/1607969247.py:2: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  file_response.astream_to_file(\"result.json\")\n</pre> Out[23]: <pre>&lt;coroutine object HttpxBinaryResponseContent.astream_to_file at 0x762f9c6e1070&gt;</pre> <p>Set your OpenAI api key, and Pinecone api key and environment in the file we created.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n</pre> import os from dotenv import load_dotenv  load_dotenv() In\u00a0[\u00a0]: Copied! <pre>import qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    host=\"localhost\",\n    port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    # path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_Data_Ingestion\")\n</pre> import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     host=\"localhost\",     port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     # path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"01_Data_Ingestion\") In\u00a0[\u00a0]: Copied! <pre>!mkdir data\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"../data/llama2.pdf\"\n</pre> !mkdir data !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"../data/llama2.pdf\" <pre>--2023-10-13 01:45:14--  https://arxiv.org/pdf/2307.09288.pdf\nResolving arxiv.org (arxiv.org)... 128.84.21.199\nConnecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13661300 (13M) [application/pdf]\nSaving to: \u2018data/llama2.pdf\u2019\n\ndata/llama2.pdf     100%[===================&gt;]  13.03M  7.59MB/s    in 1.7s    \n\n2023-10-13 01:45:16 (7.59 MB/s) - \u2018data/llama2.pdf\u2019 saved [13661300/13661300]\n</pre> In\u00a0[\u00a0]: Copied! <pre>import fitz\n</pre> import fitz In\u00a0[\u00a0]: Copied! <pre>file_path = \"../data/llama2.pdf\"\ndoc = fitz.open(file_path)\n</pre> file_path = \"../data/llama2.pdf\" doc = fitz.open(file_path) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.node_parser import SentenceSplitter\n</pre> from llama_index.core.node_parser import SentenceSplitter In\u00a0[\u00a0]: Copied! <pre>text_parser = SentenceSplitter(\n    chunk_size=1024,\n    # separator=\" \",\n)\n</pre> text_parser = SentenceSplitter(     chunk_size=1024,     # separator=\" \", ) In\u00a0[\u00a0]: Copied! <pre>text_chunks = []\n# maintain relationship with source doc index, to help inject doc metadata in (3)\ndoc_idxs = []\nfor doc_idx, page in enumerate(doc):\n    page_text = page.get_text(\"text\")\n    cur_text_chunks = text_parser.split_text(page_text)\n    text_chunks.extend(cur_text_chunks)\n    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n</pre> text_chunks = [] # maintain relationship with source doc index, to help inject doc metadata in (3) doc_idxs = [] for doc_idx, page in enumerate(doc):     page_text = page.get_text(\"text\")     cur_text_chunks = text_parser.split_text(page_text)     text_chunks.extend(cur_text_chunks)     doc_idxs.extend([doc_idx] * len(cur_text_chunks)) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.schema import TextNode\n</pre> from llama_index.core.schema import TextNode In\u00a0[\u00a0]: Copied! <pre>nodes = []\nfor idx, text_chunk in enumerate(text_chunks):\n    node = TextNode(\n        text=text_chunk,\n    )\n    src_doc_idx = doc_idxs[idx]\n    src_page = doc[src_doc_idx]\n    nodes.append(node)\n</pre> nodes = [] for idx, text_chunk in enumerate(text_chunks):     node = TextNode(         text=text_chunk,     )     src_doc_idx = doc_idxs[idx]     src_page = doc[src_doc_idx]     nodes.append(node) In\u00a0[\u00a0]: Copied! <pre>print(nodes[0].metadata)\n</pre> print(nodes[0].metadata) In\u00a0[\u00a0]: Copied! <pre># print a sample node\nprint(nodes[0].get_content(metadata_mode=\"all\"))\n</pre> # print a sample node print(nodes[0].get_content(metadata_mode=\"all\")) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.extractors import (\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n)\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\nextractors = [\n    TitleExtractor(nodes=5, llm=llm),\n    QuestionsAnsweredExtractor(questions=3, llm=llm),\n]\n</pre> from llama_index.core.extractors import (     QuestionsAnsweredExtractor,     TitleExtractor, ) from llama_index.core.ingestion import IngestionPipeline from llama_index.llms.openai import OpenAI  llm = OpenAI(model=\"gpt-3.5-turbo\")  extractors = [     TitleExtractor(nodes=5, llm=llm),     QuestionsAnsweredExtractor(questions=3, llm=llm), ] In\u00a0[\u00a0]: Copied! <pre>pipeline = IngestionPipeline(\n    transformations=extractors,\n)\nnodes = await pipeline.arun(nodes=nodes, in_place=False)\n</pre> pipeline = IngestionPipeline(     transformations=extractors, ) nodes = await pipeline.arun(nodes=nodes, in_place=False) In\u00a0[\u00a0]: Copied! <pre>print(nodes[0].metadata)\n</pre> print(nodes[0].metadata) In\u00a0[\u00a0]: Copied! <pre>from llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\n</pre> from llama_index.embeddings.openai import OpenAIEmbedding  embed_model = OpenAIEmbedding() In\u00a0[\u00a0]: Copied! <pre>for node in nodes:\n    node_embedding = embed_model.get_text_embedding(\n        node.get_content(metadata_mode=\"all\")\n    )\n    node.embedding = node_embedding\n</pre> for node in nodes:     node_embedding = embed_model.get_text_embedding(         node.get_content(metadata_mode=\"all\")     )     node.embedding = node_embedding In\u00a0[\u00a0]: Copied! <pre>vector_store.add(nodes)\n</pre> vector_store.add(nodes) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex\nfrom llama_index.core import StorageContext\n</pre> from llama_index.core import VectorStoreIndex from llama_index.core import StorageContext In\u00a0[\u00a0]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store) In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine()\n</pre> query_engine = index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>query_str = \"Can you tell me about the key concepts for safety finetuning\"\n</pre> query_str = \"Can you tell me about the key concepts for safety finetuning\" In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(query_str)\n</pre> response = query_engine.query(query_str) In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response))"},{"location":"RAG/01_Data_Ingestion/data_embedding/#openai","title":"OpenAI\u00b6","text":"<p>You will need an OpenAI api key for this tutorial. Login to your platform.openai.com account, click on your profile picture in the upper right corner, and choose 'API Keys' from the menu. Create an API key for this tutorial and save it. You will need it below.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#build-an-ingestion-pipeline-from-scratch","title":"Build an Ingestion Pipeline from Scratch\u00b6","text":"<p>We show how to build an ingestion pipeline as mentioned in the introduction.</p> <p>Note that steps (2) and (3) can be handled via our <code>NodeParser</code> abstractions, which handle splitting and node creation.</p> <p>For the purposes of this tutorial, we show you how to create these objects manually.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#1-load-data","title":"1. Load Data\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_embedding/#2-use-a-text-splitter-to-split-documents","title":"2. Use a Text Splitter to Split Documents\u00b6","text":"<p>Here we import our <code>SentenceSplitter</code> to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#3-manually-construct-nodes-from-text-chunks","title":"3. Manually Construct Nodes from Text Chunks\u00b6","text":"<p>We convert each chunk into a <code>TextNode</code> object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.</p> <p>We inject metadata from the document into each node.</p> <p>This essentially replicates logic in our <code>SentenceSplitter</code>.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#optional-4-extract-metadata-from-each-node","title":"[Optional] 4. Extract Metadata from each Node\u00b6","text":"<p>We extract metadata from each Node using our Metadata extractors.</p> <p>This will add more metadata to each Node.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#5-generate-embeddings-for-each-node","title":"5. Generate Embeddings for each Node\u00b6","text":"<p>Generate document embeddings for each Node using our OpenAI embedding model (<code>text-embedding-ada-002</code>).</p> <p>Store these on the <code>embedding</code> property on each Node.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#6-load-nodes-into-a-vector-store","title":"6. Load Nodes into a Vector Store\u00b6","text":"<p>We now insert these nodes into our <code>PineconeVectorStore</code>.</p> <p>NOTE: We skip the VectorStoreIndex abstraction, which is a higher-level abstraction that handles ingestion as well. We use <code>VectorStoreIndex</code> in the next section to fast-track retrieval/querying.</p>"},{"location":"RAG/01_Data_Ingestion/data_embedding/#retrieve-and-query-from-the-vector-store","title":"Retrieve and Query from the Vector Store\u00b6","text":"<p>Now that our ingestion is complete, we can retrieve/query this vector store.</p> <p>NOTE: We can use our high-level <code>VectorStoreIndex</code> abstraction here. See the next section to see how to define retrieval at a lower-level!</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/","title":"RAG  Data Ingestion","text":"Building Data Ingestion from Scratch AI Engineering.academy <p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>!!pip install llama-index\n!pip install llama-index-llms-openai\n!pip install llama-index-embeddings-openai\n!pip install llama-index-vector-stores-qdrant\n!pip -q install python-dotenv \n!pip install -U qdrant_client fastembed\n!pip install pymupdf\n</pre> !!pip install llama-index !pip install llama-index-llms-openai !pip install llama-index-embeddings-openai !pip install llama-index-vector-stores-qdrant !pip -q install python-dotenv  !pip install -U qdrant_client fastembed !pip install pymupdf <p>Set your OpenAI api key, and Pinecone api key and environment in the file we created.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n</pre> import os from dotenv import load_dotenv  load_dotenv() In\u00a0[\u00a0]: Copied! <pre>import qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    host=\"localhost\",\n    port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    # path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_Data_Ingestion\")\n</pre> import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     host=\"localhost\",     port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     # path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"01_Data_Ingestion\") In\u00a0[\u00a0]: Copied! <pre>!mkdir data\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"../data/llama2.pdf\"\n</pre> !mkdir data !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"../data/llama2.pdf\" <pre>--2023-10-13 01:45:14--  https://arxiv.org/pdf/2307.09288.pdf\nResolving arxiv.org (arxiv.org)... 128.84.21.199\nConnecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13661300 (13M) [application/pdf]\nSaving to: \u2018data/llama2.pdf\u2019\n\ndata/llama2.pdf     100%[===================&gt;]  13.03M  7.59MB/s    in 1.7s    \n\n2023-10-13 01:45:16 (7.59 MB/s) - \u2018data/llama2.pdf\u2019 saved [13661300/13661300]\n</pre> In\u00a0[\u00a0]: Copied! <pre>import fitz\n</pre> import fitz In\u00a0[\u00a0]: Copied! <pre>file_path = \"../data/llama2.pdf\"\ndoc = fitz.open(file_path)\n</pre> file_path = \"../data/llama2.pdf\" doc = fitz.open(file_path) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.node_parser import SentenceSplitter\n</pre> from llama_index.core.node_parser import SentenceSplitter In\u00a0[\u00a0]: Copied! <pre>text_parser = SentenceSplitter(\n    chunk_size=1024,\n    # separator=\" \",\n)\n</pre> text_parser = SentenceSplitter(     chunk_size=1024,     # separator=\" \", ) In\u00a0[\u00a0]: Copied! <pre>text_chunks = []\n# maintain relationship with source doc index, to help inject doc metadata in (3)\ndoc_idxs = []\nfor doc_idx, page in enumerate(doc):\n    page_text = page.get_text(\"text\")\n    cur_text_chunks = text_parser.split_text(page_text)\n    text_chunks.extend(cur_text_chunks)\n    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n</pre> text_chunks = [] # maintain relationship with source doc index, to help inject doc metadata in (3) doc_idxs = [] for doc_idx, page in enumerate(doc):     page_text = page.get_text(\"text\")     cur_text_chunks = text_parser.split_text(page_text)     text_chunks.extend(cur_text_chunks)     doc_idxs.extend([doc_idx] * len(cur_text_chunks)) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.schema import TextNode\n</pre> from llama_index.core.schema import TextNode In\u00a0[\u00a0]: Copied! <pre>nodes = []\nfor idx, text_chunk in enumerate(text_chunks):\n    node = TextNode(\n        text=text_chunk,\n    )\n    src_doc_idx = doc_idxs[idx]\n    src_page = doc[src_doc_idx]\n    nodes.append(node)\n</pre> nodes = [] for idx, text_chunk in enumerate(text_chunks):     node = TextNode(         text=text_chunk,     )     src_doc_idx = doc_idxs[idx]     src_page = doc[src_doc_idx]     nodes.append(node) In\u00a0[\u00a0]: Copied! <pre>print(nodes[0].metadata)\n</pre> print(nodes[0].metadata) In\u00a0[\u00a0]: Copied! <pre># print a sample node\nprint(nodes[0].get_content(metadata_mode=\"all\"))\n</pre> # print a sample node print(nodes[0].get_content(metadata_mode=\"all\")) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.extractors import (\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n)\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\nextractors = [\n    TitleExtractor(nodes=5, llm=llm),\n    QuestionsAnsweredExtractor(questions=3, llm=llm),\n]\n</pre> from llama_index.core.extractors import (     QuestionsAnsweredExtractor,     TitleExtractor, ) from llama_index.core.ingestion import IngestionPipeline from llama_index.llms.openai import OpenAI  llm = OpenAI(model=\"gpt-3.5-turbo\")  extractors = [     TitleExtractor(nodes=5, llm=llm),     QuestionsAnsweredExtractor(questions=3, llm=llm), ] In\u00a0[\u00a0]: Copied! <pre>pipeline = IngestionPipeline(\n    transformations=extractors,\n)\nnodes = await pipeline.arun(nodes=nodes, in_place=False)\n</pre> pipeline = IngestionPipeline(     transformations=extractors, ) nodes = await pipeline.arun(nodes=nodes, in_place=False) In\u00a0[\u00a0]: Copied! <pre>print(nodes[0].metadata)\n</pre> print(nodes[0].metadata) In\u00a0[\u00a0]: Copied! <pre>from llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\n</pre> from llama_index.embeddings.openai import OpenAIEmbedding  embed_model = OpenAIEmbedding() In\u00a0[\u00a0]: Copied! <pre>for node in nodes:\n    node_embedding = embed_model.get_text_embedding(\n        node.get_content(metadata_mode=\"all\")\n    )\n    node.embedding = node_embedding\n</pre> for node in nodes:     node_embedding = embed_model.get_text_embedding(         node.get_content(metadata_mode=\"all\")     )     node.embedding = node_embedding In\u00a0[\u00a0]: Copied! <pre>vector_store.add(nodes)\n</pre> vector_store.add(nodes) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex\nfrom llama_index.core import StorageContext\n</pre> from llama_index.core import VectorStoreIndex from llama_index.core import StorageContext In\u00a0[\u00a0]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store) In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine()\n</pre> query_engine = index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>query_str = \"Can you tell me about the key concepts for safety finetuning\"\n</pre> query_str = \"Can you tell me about the key concepts for safety finetuning\" In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(query_str)\n</pre> response = query_engine.query(query_str) In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response))"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#introduction","title":"Introduction\u00b6","text":"<p>Data ingestion is a crucial first step in building effective Retrieval-Augmented Generation (RAG) systems. It involves the process of collecting, processing, and storing data in a format that can be efficiently retrieved and used by the RAG model. This README provides an overview of the data ingestion process for RAG systems.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#importance-of-data-ingestion-in-rag","title":"Importance of Data Ingestion in RAG\u00b6","text":"<p>Effective data ingestion is essential for RAG systems because it:</p> <ol> <li>Determines the quality and relevance of information available for retrieval.</li> <li>Affects the system's ability to understand and process queries accurately.</li> <li>Impacts the overall performance and efficiency of the RAG pipeline.</li> <li>Enables the system to handle diverse data sources and formats.</li> </ol>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#key-steps-in-data-ingestion","title":"Key Steps in Data Ingestion\u00b6","text":"<p>The data ingestion process typically involves the following steps:</p> <pre>\nflowchart TB\n    A[Data Collection] --&gt; B[Data Cleaning]\n    B --&gt; C[Document Splitting]\n    C --&gt; D[Metadata Extraction]\n    D --&gt; E[Embedding Generation]\n    E --&gt; F[Indexing and Storage]\n</pre><ol> <li><p>Data Collection: Gathering information from various sources such as databases, APIs, web scraping, or file systems.</p> </li> <li><p>Data Cleaning: Preprocessing the collected data to remove noise, handle missing values, and standardize formats.</p> </li> <li><p>Document Splitting: Breaking down large documents into smaller, manageable chunks for more effective retrieval.</p> </li> <li><p>Metadata Extraction: Identifying and extracting relevant metadata from the documents to enhance retrieval capabilities.</p> </li> <li><p>Embedding Generation: Creating vector representations of the text chunks to enable semantic search.</p> </li> <li><p>Indexing and Storage: Organizing and storing the processed data in a format optimized for quick retrieval, often using vector databases or search engines.</p> </li> </ol>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#challenges-in-data-ingestion","title":"Challenges in Data Ingestion\u00b6","text":"<ul> <li>Handling diverse data formats and sources</li> <li>Ensuring data quality and consistency</li> <li>Managing large volumes of data efficiently</li> <li>Updating and maintaining the knowledge base</li> <li>Balancing between chunk size and semantic coherence</li> </ul>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#best-practices","title":"Best Practices\u00b6","text":"<ol> <li>Data Quality: Implement robust data cleaning and validation processes.</li> <li>Scalability: Design the ingestion pipeline to handle growing data volumes.</li> <li>Metadata Enrichment: Extract and store relevant metadata to improve retrieval accuracy.</li> <li>Incremental Updates: Develop mechanisms for efficiently updating the knowledge base.</li> <li>Monitoring: Implement logging and monitoring to track ingestion performance and data quality.</li> </ol>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#conclusion","title":"Conclusion\u00b6","text":"<p>A well-designed data ingestion process is fundamental to the success of a RAG system. It ensures that the information retrieved is accurate, relevant, and up-to-date, ultimately leading to better-quality responses from the language model.</p> <p>In the following sections, we'll explore other crucial components of RAG systems, including data chunking, embedding generation, and retrieval mechanisms.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#openai","title":"OpenAI\u00b6","text":"<p>You will need an OpenAI api key for this tutorial. Login to your platform.openai.com account, click on your profile picture in the upper right corner, and choose 'API Keys' from the menu. Create an API key for this tutorial and save it. You will need it below.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#build-an-ingestion-pipeline-from-scratch","title":"Build an Ingestion Pipeline from Scratch\u00b6","text":"<p>We show how to build an ingestion pipeline as mentioned in the introduction.</p> <p>Note that steps (2) and (3) can be handled via our <code>NodeParser</code> abstractions, which handle splitting and node creation.</p> <p>For the purposes of this tutorial, we show you how to create these objects manually.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#1-load-data","title":"1. Load Data\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_ingestion/#2-use-a-text-splitter-to-split-documents","title":"2. Use a Text Splitter to Split Documents\u00b6","text":"<p>Here we import our <code>SentenceSplitter</code> to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#3-manually-construct-nodes-from-text-chunks","title":"3. Manually Construct Nodes from Text Chunks\u00b6","text":"<p>We convert each chunk into a <code>TextNode</code> object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.</p> <p>We inject metadata from the document into each node.</p> <p>This essentially replicates logic in our <code>SentenceSplitter</code>.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#optional-4-extract-metadata-from-each-node","title":"[Optional] 4. Extract Metadata from each Node\u00b6","text":"<p>We extract metadata from each Node using our Metadata extractors.</p> <p>This will add more metadata to each Node.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#5-generate-embeddings-for-each-node","title":"5. Generate Embeddings for each Node\u00b6","text":"<p>Generate document embeddings for each Node using our OpenAI embedding model (<code>text-embedding-ada-002</code>).</p> <p>Store these on the <code>embedding</code> property on each Node.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#6-load-nodes-into-a-vector-store","title":"6. Load Nodes into a Vector Store\u00b6","text":"<p>We now insert these nodes into our <code>PineconeVectorStore</code>.</p> <p>NOTE: We skip the VectorStoreIndex abstraction, which is a higher-level abstraction that handles ingestion as well. We use <code>VectorStoreIndex</code> in the next section to fast-track retrieval/querying.</p>"},{"location":"RAG/01_Data_Ingestion/data_ingestion/#retrieve-and-query-from-the-vector-store","title":"Retrieve and Query from the Vector Store\u00b6","text":"<p>Now that our ingestion is complete, we can retrieve/query this vector store.</p> <p>NOTE: We can use our high-level <code>VectorStoreIndex</code> abstraction here. See the next section to see how to define retrieval at a lower-level!</p>"},{"location":"RAG/01_Data_Ingestion/data_parsing/","title":"Data parsing","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre># !pip install omniparse-client\n</pre> # !pip install omniparse-client In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"RAG/01_Data_Ingestion/data_parsing/#comparing-different-parsingcoming-soon","title":"Comparing Different Parsing(Coming Soon)\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#omniparse","title":"OmniParse\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#pypdf2","title":"PyPDF2\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#pdfminersix","title":"pdfminer.six:\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#tabula-py","title":"Tabula-py\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#pymupdf","title":"PyMuPDF\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#camelot","title":"Camelot\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"RAG/01_Data_Ingestion/data_parsing/#text-extraction","title":"Text Extraction:\u00b6","text":"<ul> <li>PyPDF2: Good support for text extraction.</li> <li>pdfminer.six: Excellent support with advanced layout information extraction.</li> <li>Tabula-py: Limited support, mainly focused on tables.</li> <li>PyMuPDF: Strong text extraction capabilities.</li> <li>Camelot: Primarily designed for tabular data extraction and may not provide advanced text extraction capabilities for answering questions from the content.</li> </ul>"},{"location":"RAG/01_Data_Ingestion/data_parsing/#image-extraction","title":"Image Extraction:\u00b6","text":"<ul> <li>PyPDF2: Limited support.</li> <li>pdfminer.six: Limited or no support.</li> <li>Tabula-py: No built-in support.</li> <li>PyMuPDF: Strong image extraction capabilities.</li> <li>Camelot: no built-in support for image extraction.</li> </ul>"},{"location":"RAG/01_Data_Ingestion/data_parsing/#table-extraction","title":"Table Extraction:\u00b6","text":"<ul> <li>PyPDF2: No built-in support.</li> <li>pdfminer.six: No built-in support.</li> <li>Tabula-py: Excellent support for table extraction.</li> <li>PyMuPDF: Custom implementation required, but provides a foundation for table extraction.</li> <li>Camelot: Excels at extracting tabular data from PDFs, which can be useful for answering questions based on structured information.</li> </ul>"},{"location":"RAG/01_Data_Ingestion/data_parsing/#speed-of-execution","title":"Speed of Execution:\u00b6","text":"<ul> <li>PyPDF2: Speed is moderate as it may take longer for processing large PDF files.</li> <li>pdfminer.six: Moderate speed, depending on the complexity of the PDF.</li> <li>Tabula-py: Varies depending on the size and complexity of the tables.</li> <li>PyMuPDF: Known for its high-performance rendering and parsing.</li> <li>Camelot: execution speed is impressive, thanks to its efficient table extraction algorithms.</li> </ul>"},{"location":"RAG/01_Data_Ingestion/data_parsing/#ease-of-use","title":"Ease of Use:\u00b6","text":"<ul> <li>PyPDF2: Simple and easy to use.</li> <li>pdfminer.six: More complex compared to other libraries.</li> <li>Tabula-py: User-friendly interface, especially for table extraction.</li> <li>PyMuPDF: Provides a rich set of functionalities but may have a steeper learning curve.</li> <li>Camelot: Initial set up is tricky but otherwise good documentation for any specific use. Flexibilty to provide page numbers and page range to extrtact tables</li> </ul>"},{"location":"RAG/01_RAG_Evaluation/","title":"Intro to Evals","text":""},{"location":"RAG/01_RAG_Evaluation/#evaluation-of-rag-systems","title":"Evaluation of RAG Systems","text":""},{"location":"RAG/01_RAG_Evaluation/#introduction","title":"Introduction","text":"<p>Evaluation is a critical component in the development and optimization of Retrieval-Augmented Generation (RAG) systems. It involves assessing the performance, accuracy, and quality of various aspects of the RAG pipeline, from retrieval effectiveness to the relevance and faithfulness of generated responses.</p>"},{"location":"RAG/01_RAG_Evaluation/#importance-of-evaluation-in-rag","title":"Importance of Evaluation in RAG","text":"<p>Effective evaluation of RAG systems is essential because it:</p> <ol> <li>Helps identify strengths and weaknesses in the retrieval and generation processes.</li> <li>Guides improvements and optimizations across the RAG pipeline.</li> <li>Ensures the system meets quality standards and user expectations.</li> <li>Facilitates comparison between different RAG implementations or configurations.</li> <li>Helps detect issues such as hallucinations, biases, or irrelevant responses.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/#rag-evaluation-workflow","title":"RAG Evaluation Workflow","text":"<p>The evaluation process in RAG systems typically involves the following steps:</p> <pre><code>flowchart TB\n    subgraph \"1. Input\"\n        A[Query] --&gt; E[Evaluation Engine]\n        B[Retrieved Chunks] --&gt; E\n        C[Generated Response] --&gt; E\n    end\n\n    subgraph \"2. Evaluation Engine\"\n        E --&gt; F[Evaluation Libraries]\n        F --&gt; G[RAGAS Metrics]\n        F --&gt; H[DeepEval Metrics]\n        F --&gt; I[Trulens Metrics]\n    end\n\n    subgraph \"3. RAGAS Metrics\"\n        G --&gt; G1[Faithfulness]\n        G --&gt; G2[Answer Relevancy]\n        G --&gt; G3[Context Recall]\n        G --&gt; G4[Context Precision]\n        G --&gt; G5[Context Utilization]\n        G --&gt; G6[Context Entity Recall]\n        G --&gt; G7[Noise Sensitivity]\n        G --&gt; G8[Summarization Score]\n    end\n\n    subgraph \"4. DeepEval Metrics\"\n        H --&gt; H1[G-Eval]\n        H --&gt; H2[Summarization]\n        H --&gt; H3[Answer Relevancy]\n        H --&gt; H4[Faithfulness]\n        H --&gt; H5[Contextual Recall]\n        H --&gt; H6[Contextual Precision]\n        H --&gt; H7[RAGAS]\n        H --&gt; H8[Hallucination]\n        H --&gt; H9[Toxicity]\n        H --&gt; H10[Bias]\n    end\n\n    subgraph \"5. Trulens Metrics\"\n        I --&gt; I1[Context Relevance]\n        I --&gt; I2[Groundedness]\n        I --&gt; I3[Answer Relevance]\n        I --&gt; I4[Comprehensiveness]\n        I --&gt; I5[Harmful/Toxic Language]\n        I --&gt; I6[User Sentiment]\n        I --&gt; I7[Language Mismatch]\n        I --&gt; I8[Fairness and Bias]\n        I --&gt; I9[Custom Feedback Functions]\n    end</code></pre>"},{"location":"RAG/01_RAG_Evaluation/#key-evaluation-metrics","title":"Key Evaluation Metrics","text":""},{"location":"RAG/01_RAG_Evaluation/#ragas-metrics","title":"RAGAS Metrics","text":"<ol> <li>Faithfulness: Measures how well the generated response aligns with the retrieved context.</li> <li>Answer Relevancy: Assesses the relevance of the response to the query.</li> <li>Context Recall: Evaluates how well the retrieved chunks cover the information needed to answer the query.</li> <li>Context Precision: Measures the proportion of relevant information in the retrieved chunks.</li> <li>Context Utilization: Assesses how effectively the generated response uses the provided context.</li> <li>Context Entity Recall: Evaluates the coverage of important entities from the context in the response.</li> <li>Noise Sensitivity: Measures the system's robustness to irrelevant or noisy information.</li> <li>Summarization Score: Assesses the quality of summarization in the response.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/#deepeval-metrics","title":"DeepEval Metrics","text":"<ol> <li>G-Eval: A general evaluation metric for text generation tasks.</li> <li>Summarization: Assesses the quality of text summarization.</li> <li>Answer Relevancy: Measures how well the response answers the query.</li> <li>Faithfulness: Evaluates the accuracy of the response with respect to the source information.</li> <li>Contextual Recall and Precision: Measures the effectiveness of context retrieval.</li> <li>Hallucination: Detects fabricated or inaccurate information in the response.</li> <li>Toxicity: Identifies harmful or offensive content in the response.</li> <li>Bias: Detects unfair prejudice or favoritism in the generated content.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/#trulens-metrics","title":"Trulens Metrics","text":"<ol> <li>Context Relevance: Assesses how well the retrieved context matches the query.</li> <li>Groundedness: Measures how well the response is supported by the retrieved information.</li> <li>Answer Relevance: Evaluates how well the response addresses the query.</li> <li>Comprehensiveness: Assesses the completeness of the response.</li> <li>Harmful/Toxic Language: Identifies potentially offensive or dangerous content.</li> <li>User Sentiment: Analyzes the emotional tone of user interactions.</li> <li>Language Mismatch: Detects inconsistencies in language use between query and response.</li> <li>Fairness and Bias: Evaluates the system for equitable treatment across different groups.</li> <li>Custom Feedback Functions: Allows for tailored evaluation metrics specific to use cases.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/#best-practices-for-rag-evaluation","title":"Best Practices for RAG Evaluation","text":"<ol> <li>Comprehensive Evaluation: Use a combination of metrics to assess different aspects of the RAG system.</li> <li>Regular Benchmarking: Continuously evaluate the system as changes are made to the pipeline.</li> <li>Human-in-the-Loop: Incorporate human evaluation alongside automated metrics for a holistic assessment.</li> <li>Domain-Specific Metrics: Develop custom metrics relevant to your specific use case or domain.</li> <li>Error Analysis: Investigate patterns in low-scoring responses to identify areas for improvement.</li> <li>Comparative Evaluation: Benchmark your RAG system against baseline models and alternative implementations.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/#conclusion","title":"Conclusion","text":"<p>A robust evaluation framework is crucial for developing and maintaining high-quality RAG systems. By leveraging a diverse set of metrics and following best practices, developers can ensure their RAG systems deliver accurate, relevant, and trustworthy responses while continuously improving performance.</p>"},{"location":"RAG/01_RAG_Evaluation/RAGAS/","title":"RAGAS","text":"<p>Run the cell below to install Git LFS, which we use to download our dataset.</p> In\u00a0[\u00a0]: Copied! <pre>!git lfs install\n</pre> !git lfs install <p>Install and import Python dependencies.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install \"ragas==0.1.4\" pypdf \"arize-phoenix[llama-index]\" \"openai&gt;=1.0.0\" pandas\n</pre> !pip install \"ragas==0.1.4\" pypdf \"arize-phoenix[llama-index]\" \"openai&gt;=1.0.0\" pandas In\u00a0[1]: Copied! <pre>import pandas as pd\n\n# Display the complete contents of dataframe cells.\npd.set_option(\"display.max_colwidth\", None)\n</pre> import pandas as pd  # Display the complete contents of dataframe cells. pd.set_option(\"display.max_colwidth\", None) In\u00a0[2]: Copied! <pre>import os\nfrom getpass import getpass\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n</pre> import os from getpass import getpass  if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):     openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \") os.environ[\"OPENAI_API_KEY\"] = openai_api_key <p>Launch Phoenix in the background and setup auto-instrumentation for llama-index and LangChain so that your OpenInference spans and traces are sent to and collected by Phoenix. OpenInference is an open standard built atop OpenTelemetry that captures and stores LLM application executions. It is designed to be a category of telemetry data that is used to understand the execution of LLMs and the surrounding application context, such as retrieval from vector stores and the usage of external tools such as search engines or APIs.</p> In\u00a0[\u00a0]: Copied! <pre>import phoenix as px\n\n(session := px.launch_app()).view()\n</pre> import phoenix as px  (session := px.launch_app()).view() In\u00a0[\u00a0]: Copied! <pre>from openinference.instrumentation.langchain import LangChainInstrumentor\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import SpanLimits, TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=100_000))\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\nLangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n</pre> from openinference.instrumentation.langchain import LangChainInstrumentor from openinference.instrumentation.llama_index import LlamaIndexInstrumentor from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter from opentelemetry.sdk.trace import SpanLimits, TracerProvider from opentelemetry.sdk.trace.export import SimpleSpanProcessor  endpoint = \"http://127.0.0.1:6006/v1/traces\" tracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=100_000)) tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))  LlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider) LangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider) <p>Curating a golden test dataset for evaluation can be a long, tedious, and expensive process that is not pragmatic \u2014 especially when starting out or when data sources keep changing. This can be solved by synthetically generating high quality data points, which then can be verified by developers. This can reduce the time and effort in curating test data by 90%.</p> <p>Run the cell below to download a dataset of prompt engineering papers in PDF format from arXiv and read these documents using LlamaIndex.</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers\n</pre> !git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\ndir_path = \"./prompt-engineering-papers\"\nreader = SimpleDirectoryReader(dir_path, num_files_limit=2)\ndocuments = reader.load_data()\n</pre> from llama_index.core import SimpleDirectoryReader  dir_path = \"./prompt-engineering-papers\" reader = SimpleDirectoryReader(dir_path, num_files_limit=2) documents = reader.load_data() <p>An ideal test dataset should contain data points of high quality and diverse nature from a similar distribution to the one observed during production. Ragas uses a unique evolution-based synthetic data generation paradigm to generate questions that are of the highest quality which also ensures diversity of questions generated.  Ragas by default uses OpenAI models under the hood, but you\u2019re free to use any model of your choice. Let\u2019s generate 100 data points using Ragas.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace import using_project\nfrom ragas.testset.evolutions import multi_context, reasoning, simple\nfrom ragas.testset.generator import TestsetGenerator\n\nTEST_SIZE = 5\n\n# generator with openai models\ngenerator = TestsetGenerator.with_openai()\n\n# set question type distribution\ndistribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n\n# generate testset\nwith using_project(\"ragas-testset\"):\n    testset = generator.generate_with_llamaindex_docs(\n        documents, test_size=TEST_SIZE, distributions=distribution\n    )\ntest_df = (\n    testset.to_pandas().sort_values(\"question\").drop_duplicates(subset=[\"question\"], keep=\"first\")\n)\ntest_df.head(2)\n</pre> from phoenix.trace import using_project from ragas.testset.evolutions import multi_context, reasoning, simple from ragas.testset.generator import TestsetGenerator  TEST_SIZE = 5  # generator with openai models generator = TestsetGenerator.with_openai()  # set question type distribution distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}  # generate testset with using_project(\"ragas-testset\"):     testset = generator.generate_with_llamaindex_docs(         documents, test_size=TEST_SIZE, distributions=distribution     ) test_df = (     testset.to_pandas().sort_values(\"question\").drop_duplicates(subset=[\"question\"], keep=\"first\") ) test_df.head(2) <p>You are free to change the question type distribution according to your needs. Since we now have our test dataset ready, let\u2019s move on and build a simple RAG pipeline using LlamaIndex.</p> <p>LlamaIndex is an easy to use and flexible framework for building RAG applications. For the sake of simplicity, we use the default LLM (gpt-3.5-turbo) and embedding models (openai-ada-2).</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom phoenix.trace import using_project\n\n\ndef build_query_engine(documents):\n    vector_index = VectorStoreIndex.from_documents(\n        documents,\n        embed_model=OpenAIEmbedding(),\n    )\n    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n    return query_engine\n\n\nwith using_project(\"indexing\"):\n    # By assigning a project name, the instrumentation will send all the embeddings to the indexing project\n    query_engine = build_query_engine(documents)\n</pre> from llama_index.core import VectorStoreIndex from llama_index.embeddings.openai import OpenAIEmbedding from phoenix.trace import using_project   def build_query_engine(documents):     vector_index = VectorStoreIndex.from_documents(         documents,         embed_model=OpenAIEmbedding(),     )     query_engine = vector_index.as_query_engine(similarity_top_k=2)     return query_engine   with using_project(\"indexing\"):     # By assigning a project name, the instrumentation will send all the embeddings to the indexing project     query_engine = build_query_engine(documents) <p>If you check Phoenix, you should see embedding spans from when your corpus data was indexed. Export and save those embeddings into a dataframe for visualization later in the notebook.</p> In\u00a0[\u00a0]: Copied! <pre>from time import sleep\n\n# Wait a little bit in case data hasn't beomme fully available\nsleep(2)\n</pre> from time import sleep  # Wait a little bit in case data hasn't beomme fully available sleep(2) In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace.dsl.helpers import SpanQuery\n\nclient = px.Client()\ncorpus_df = px.Client().query_spans(\n    SpanQuery().explode(\n        \"embedding.embeddings\",\n        text=\"embedding.text\",\n        vector=\"embedding.vector\",\n    ),\n    project_name=\"indexing\",\n)\ncorpus_df.head(2)\n</pre> from phoenix.trace.dsl.helpers import SpanQuery  client = px.Client() corpus_df = px.Client().query_spans(     SpanQuery().explode(         \"embedding.embeddings\",         text=\"embedding.text\",         vector=\"embedding.vector\",     ),     project_name=\"indexing\", ) corpus_df.head(2) <p>Ragas provides a comprehensive list of metrics that can be used to evaluate RAG pipelines both component-wise and end-to-end.</p> <p>To use Ragas, we first form an evaluation dataset comprised of a question, generated answer, retrieved context, and ground-truth answer (the actual expected answer for the given question).</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import Dataset\nfrom phoenix.trace import using_project\nfrom tqdm.auto import tqdm\n\n\ndef generate_response(query_engine, question):\n    response = query_engine.query(question)\n    return {\n        \"answer\": response.response,\n        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n    }\n\n\ndef generate_ragas_dataset(query_engine, test_df):\n    test_questions = test_df[\"question\"].values\n    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n\n    dataset_dict = {\n        \"question\": test_questions,\n        \"answer\": [response[\"answer\"] for response in responses],\n        \"contexts\": [response[\"contexts\"] for response in responses],\n        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n    }\n    ds = Dataset.from_dict(dataset_dict)\n    return ds\n\n\nwith using_project(\"llama-index\"):\n    ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)\n\nragas_evals_df = pd.DataFrame(ragas_eval_dataset)\nragas_evals_df.head(2)\n</pre> import pandas as pd from datasets import Dataset from phoenix.trace import using_project from tqdm.auto import tqdm   def generate_response(query_engine, question):     response = query_engine.query(question)     return {         \"answer\": response.response,         \"contexts\": [c.node.get_content() for c in response.source_nodes],     }   def generate_ragas_dataset(query_engine, test_df):     test_questions = test_df[\"question\"].values     responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]      dataset_dict = {         \"question\": test_questions,         \"answer\": [response[\"answer\"] for response in responses],         \"contexts\": [response[\"contexts\"] for response in responses],         \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),     }     ds = Dataset.from_dict(dataset_dict)     return ds   with using_project(\"llama-index\"):     ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)  ragas_evals_df = pd.DataFrame(ragas_eval_dataset) ragas_evals_df.head(2) <p>Check out Phoenix to view your LlamaIndex application traces.</p> In\u00a0[\u00a0]: Copied! <pre>print(session.url)\n</pre> print(session.url) <p></p> <p>We save out a couple of dataframes, one containing embedding data that we'll visualize later, and another containing our exported traces and spans that we plan to evaluate using Ragas.</p> In\u00a0[\u00a0]: Copied! <pre>from time import sleep\n\n# Wait a bit in case data hasn't beomme fully available\nsleep(2)\n</pre> from time import sleep  # Wait a bit in case data hasn't beomme fully available sleep(2) In\u00a0[\u00a0]: Copied! <pre># dataset containing embeddings for visualization\nquery_embeddings_df = px.Client().query_spans(\n    SpanQuery().explode(\"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"),\n    project_name=\"llama-index\",\n)\nquery_embeddings_df.head(2)\n</pre> # dataset containing embeddings for visualization query_embeddings_df = px.Client().query_spans(     SpanQuery().explode(\"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"),     project_name=\"llama-index\", ) query_embeddings_df.head(2) In\u00a0[\u00a0]: Copied! <pre>from phoenix.session.evaluation import get_qa_with_reference\n\n# dataset containing span data for evaluation with Ragas\nspans_dataframe = get_qa_with_reference(client, project_name=\"llama-index\")\nspans_dataframe.head(2)\n</pre> from phoenix.session.evaluation import get_qa_with_reference  # dataset containing span data for evaluation with Ragas spans_dataframe = get_qa_with_reference(client, project_name=\"llama-index\") spans_dataframe.head(2) <p>Ragas uses LangChain to evaluate your LLM application data. Since we initialized the LangChain instrumentation above we can see what's going on under the hood when we evaluate our LLM application.</p> <p>Evaluate your LLM traces and view the evaluation scores in dataframe format.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace import using_project\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_correctness,\n    context_precision,\n    context_recall,\n    faithfulness,\n)\n\n# Log the traces to the project \"ragas-evals\" just to view\n# how Ragas works under the hood\nwith using_project(\"ragas-evals\"):\n    evaluation_result = evaluate(\n        dataset=ragas_eval_dataset,\n        metrics=[faithfulness, answer_correctness, context_recall, context_precision],\n    )\neval_scores_df = pd.DataFrame(evaluation_result.scores)\n</pre> from phoenix.trace import using_project from ragas import evaluate from ragas.metrics import (     answer_correctness,     context_precision,     context_recall,     faithfulness, )  # Log the traces to the project \"ragas-evals\" just to view # how Ragas works under the hood with using_project(\"ragas-evals\"):     evaluation_result = evaluate(         dataset=ragas_eval_dataset,         metrics=[faithfulness, answer_correctness, context_recall, context_precision],     ) eval_scores_df = pd.DataFrame(evaluation_result.scores) <p>Submit your evaluations to Phoenix so they are visible as annotations on your spans.</p> In\u00a0[\u00a0]: Copied! <pre># Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans).\nspan_questions = (\n    spans_dataframe[[\"input\"]]\n    .sort_values(\"input\")\n    .drop_duplicates(subset=[\"input\"], keep=\"first\")\n    .reset_index()\n    .rename({\"input\": \"question\"}, axis=1)\n)\nragas_evals_df = ragas_evals_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\ntest_df = test_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\neval_data_df = pd.DataFrame(evaluation_result.dataset)\neval_data_df = eval_data_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\neval_scores_df.index = eval_data_df.index\n\nquery_embeddings_df = (\n    query_embeddings_df.sort_values(\"text\")\n    .drop_duplicates(subset=[\"text\"])\n    .rename({\"text\": \"question\"}, axis=1)\n    .merge(span_questions, on=\"question\")\n    .set_index(\"context.span_id\")\n)\n</pre> # Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans). span_questions = (     spans_dataframe[[\"input\"]]     .sort_values(\"input\")     .drop_duplicates(subset=[\"input\"], keep=\"first\")     .reset_index()     .rename({\"input\": \"question\"}, axis=1) ) ragas_evals_df = ragas_evals_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\") test_df = test_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\") eval_data_df = pd.DataFrame(evaluation_result.dataset) eval_data_df = eval_data_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\") eval_scores_df.index = eval_data_df.index  query_embeddings_df = (     query_embeddings_df.sort_values(\"text\")     .drop_duplicates(subset=[\"text\"])     .rename({\"text\": \"question\"}, axis=1)     .merge(span_questions, on=\"question\")     .set_index(\"context.span_id\") ) In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace import SpanEvaluations\n\n# Log the evaluations to Phoenix under the project \"llama-index\"\n# This will allow you to visualize the scores alongside the spans in the UI\nfor eval_name in eval_scores_df.columns:\n    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})\n    evals = SpanEvaluations(eval_name, evals_df)\n    px.Client().log_evaluations(evals)\n</pre> from phoenix.trace import SpanEvaluations  # Log the evaluations to Phoenix under the project \"llama-index\" # This will allow you to visualize the scores alongside the spans in the UI for eval_name in eval_scores_df.columns:     evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})     evals = SpanEvaluations(eval_name, evals_df)     px.Client().log_evaluations(evals) <p>If you check out Phoenix, you'll see your Ragas evaluations as annotations on your application spans.</p> In\u00a0[\u00a0]: Copied! <pre>print(session.url)\n</pre> print(session.url) <p></p>"},{"location":"RAG/01_RAG_Evaluation/RAGAS/#2-install-dependencies-and-import-libraries","title":"2. Install Dependencies and Import Libraries\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/RAGAS/#3-setup","title":"3. Setup\u00b6","text":"<p>Set your OpenAI API key if it is not already set as an environment variable.</p>"},{"location":"RAG/01_RAG_Evaluation/RAGAS/#4-generate-your-synthetic-test-dataset","title":"4. Generate Your Synthetic Test Dataset\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/RAGAS/#5-build-your-rag-application-with-llamaindex","title":"5. Build Your RAG Application With LlamaIndex\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/RAGAS/#6-evaluate-your-llm-application","title":"6. Evaluate Your LLM Application\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/RAGAS/#8-recap","title":"8. Recap\u00b6","text":"<p>Congrats! You built and evaluated a LlamaIndex query engine using Ragas and Phoenix. Let's recap what we learned:</p> <ul> <li>With Ragas, you bootstraped a test dataset and computed metrics such as faithfulness and answer correctness to evaluate your LlamaIndex query engine.</li> <li>With OpenInference, you instrumented your query engine so you could observe the inner workings of both LlamaIndex and Ragas.</li> <li>With Phoenix, you collected your spans and traces, imported your evaluations for easy inspection, and visualized your embedded queries and retrieved documents to identify pockets of poor performance.</li> </ul> <p>This notebook is just an introduction to the capabilities of Ragas and Phoenix. To learn more, see the Ragas and Phoenix docs.</p> <p>If you enjoyed this tutorial, please leave a \u2b50 on GitHub:</p> <ul> <li>Ragas</li> <li>Phoenix</li> <li>OpenInference</li> </ul>"},{"location":"RAG/01_RAG_Evaluation/deepeval/","title":"DeepEval","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -q llama-index\n!pip install -U -q deepeval\n</pre> !pip install -q -q llama-index !pip install -U -q deepeval In\u00a0[8]: Copied! <pre>import os\nfrom getpass import getpass\nimport openai\nimport nest_asyncio\nnest_asyncio.apply() \n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n</pre> import os from getpass import getpass import openai import nest_asyncio nest_asyncio.apply()   if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):     openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \") openai.api_key = openai_api_key os.environ[\"OPENAI_API_KEY\"] = openai_api_key In\u00a0[5]: Copied! <pre>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Read LlamaIndex's quickstart on more details, you will need to store your data in \"YOUR_DATA_DIRECTORY\" beforehand\ndocuments = SimpleDirectoryReader(\"../data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nrag_application = index.as_query_engine()\n</pre> from llama_index.core import VectorStoreIndex, SimpleDirectoryReader  # Read LlamaIndex's quickstart on more details, you will need to store your data in \"YOUR_DATA_DIRECTORY\" beforehand documents = SimpleDirectoryReader(\"../data\").load_data() index = VectorStoreIndex.from_documents(documents) rag_application = index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>from deepeval.integrations.llama_index import DeepEvalFaithfulnessEvaluator\n\n# An example input to your RAG application\nuser_input = \"What is LlamaIndex?\"\n\n# LlamaIndex returns a response object that contains\n# both the output string and retrieved nodes\nresponse_object = rag_application.query(user_input)\n\nevaluator = DeepEvalFaithfulnessEvaluator()\nevaluation_result = evaluator.evaluate_response(\n    query=user_input, response=response_object\n)\nprint(evaluation_result)\n</pre> from deepeval.integrations.llama_index import DeepEvalFaithfulnessEvaluator  # An example input to your RAG application user_input = \"What is LlamaIndex?\"  # LlamaIndex returns a response object that contains # both the output string and retrieved nodes response_object = rag_application.query(user_input)  evaluator = DeepEvalFaithfulnessEvaluator() evaluation_result = evaluator.evaluate_response(     query=user_input, response=response_object ) print(evaluation_result) In\u00a0[\u00a0]: Copied! <pre>from deepeval.integrations.llama_index import (\n    DeepEvalAnswerRelevancyEvaluator,\n    DeepEvalFaithfulnessEvaluator,\n    DeepEvalContextualRelevancyEvaluator,\n    DeepEvalSummarizationEvaluator,\n    DeepEvalBiasEvaluator,\n    DeepEvalToxicityEvaluator,\n)\n</pre> from deepeval.integrations.llama_index import (     DeepEvalAnswerRelevancyEvaluator,     DeepEvalFaithfulnessEvaluator,     DeepEvalContextualRelevancyEvaluator,     DeepEvalSummarizationEvaluator,     DeepEvalBiasEvaluator,     DeepEvalToxicityEvaluator, ) In\u00a0[\u00a0]: Copied! <pre>evaluator = DeepEvalAnswerRelevancyEvaluator()\nevaluation_result = evaluator.evaluate_response(\n    query=user_input, response=response_object\n)\nprint(evaluation_result)\n</pre> evaluator = DeepEvalAnswerRelevancyEvaluator() evaluation_result = evaluator.evaluate_response(     query=user_input, response=response_object ) print(evaluation_result)"},{"location":"RAG/01_RAG_Evaluation/notebook/","title":"Notebook","text":"Evaluating RAG AI Engineering.academy In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n!pip install llama-index-vector-stores-qdrant \n!pip install llama-index-readers-file \n!pip install llama-index-embeddings-fastembed \n!pip install llama-index-llms-openai\n!pip install llama-index-llms-groq\n!pip install -U qdrant_client fastembed\n!pip install python-dotenv\n</pre> !pip install llama-index !pip install llama-index-vector-stores-qdrant  !pip install llama-index-readers-file  !pip install llama-index-embeddings-fastembed  !pip install llama-index-llms-openai !pip install llama-index-llms-groq !pip install -U qdrant_client fastembed !pip install python-dotenv In\u00a0[\u00a0]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\n# from llama_index.llms.groq import Groq\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGROK_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\n# Setting up Base LLM\nSettings.llm = OpenAI(\n    model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True\n)\n\n# Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\n# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\nSettings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI # from llama_index.llms.groq import Groq # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Setting up Base LLM Settings.llm = OpenAI(     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True )  # Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) # Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version In\u00a0[\u00a0]: Copied! <pre># lets loading the documents using SimpleDirectoryReader\n\nprint(\"\ud83d\udd03 Loading Data\")\n\nfrom llama_index.core import Document\nreader = SimpleDirectoryReader(\"../data/\" , recursive=True)\ndocuments = reader.load_data(show_progress=True)\n</pre> # lets loading the documents using SimpleDirectoryReader  print(\"\ud83d\udd03 Loading Data\")  from llama_index.core import Document reader = SimpleDirectoryReader(\"../data/\" , recursive=True) documents = reader.load_data(show_progress=True) In\u00a0[\u00a0]: Copied! <pre># creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    host=\"localhost\",\n    port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    # path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"01_RAG_Evaluation\")\n</pre> # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     host=\"localhost\",     port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     # path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"01_RAG_Evaluation\") In\u00a0[\u00a0]: Copied! <pre>## ingesting data into vector database\n\n## lets set up an ingestion pipeline\n\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        # MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ],\n    vector_store=vector_store,\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=documents , show_progress=True)\nprint(\"Number of chunks added to vector DB :\",len(nodes))\n</pre> ## ingesting data into vector database  ## lets set up an ingestion pipeline  from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         # MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ],     vector_store=vector_store, )  # Ingest directly into a vector db nodes = pipeline.run(documents=documents , show_progress=True) print(\"Number of chunks added to vector DB :\",len(nodes)) In\u00a0[4]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store=vector_store) In\u00a0[5]: Copied! <pre>from llama_index.core import ChatPromptTemplate\n\nqa_prompt_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the question: {query_str}\\n\"\n)\n\nrefine_prompt_str = (\n    \"We have the opportunity to refine the original answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question: {query_str}. \"\n    \"If the context isn't useful, output the original answer again.\\n\"\n    \"Original Answer: {existing_answer}\"\n)\n\n# Text QA Prompt\nchat_text_qa_msgs = [\n    (\"system\",\"You are a AI assistant who is well versed with answering questions from the provided context\"),\n    (\"user\", qa_prompt_str),\n]\ntext_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n\n# Refine Prompt\nchat_refine_msgs = [\n    (\"system\",\"Always answer the question, even if the context isn't helpful.\",),\n    (\"user\", refine_prompt_str),\n]\nrefine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)\n</pre> from llama_index.core import ChatPromptTemplate  qa_prompt_str = (     \"Context information is below.\\n\"     \"---------------------\\n\"     \"{context_str}\\n\"     \"---------------------\\n\"     \"Given the context information and not prior knowledge, \"     \"answer the question: {query_str}\\n\" )  refine_prompt_str = (     \"We have the opportunity to refine the original answer \"     \"(only if needed) with some more context below.\\n\"     \"------------\\n\"     \"{context_msg}\\n\"     \"------------\\n\"     \"Given the new context, refine the original answer to better \"     \"answer the question: {query_str}. \"     \"If the context isn't useful, output the original answer again.\\n\"     \"Original Answer: {existing_answer}\" )  # Text QA Prompt chat_text_qa_msgs = [     (\"system\",\"You are a AI assistant who is well versed with answering questions from the provided context\"),     (\"user\", qa_prompt_str), ] text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)  # Refine Prompt chat_refine_msgs = [     (\"system\",\"Always answer the question, even if the context isn't helpful.\",),     (\"user\", refine_prompt_str), ] refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs) In\u00a0[\u00a0]: Copied! <pre># Setting up Query Engine\nBASE_RAG_QUERY_ENGINE = index.as_query_engine(\n        similarity_top_k=5,\n        text_qa_template=text_qa_template,\n        refine_template=refine_template,)\n\nresponse = BASE_RAG_QUERY_ENGINE.query(\"How many encoders are stacked in the encoder?\")\ndisplay(Markdown(str(response)))\n</pre> # Setting up Query Engine BASE_RAG_QUERY_ENGINE = index.as_query_engine(         similarity_top_k=5,         text_qa_template=text_qa_template,         refine_template=refine_template,)  response = BASE_RAG_QUERY_ENGINE.query(\"How many encoders are stacked in the encoder?\") display(Markdown(str(response))) In\u00a0[\u00a0]: Copied! <pre># Setting up Chat Engine\nBASE_RAG_CHAT_ENGINE = index.as_chat_engine()\n\nresponse = BASE_RAG_CHAT_ENGINE.chat(\"How many encoders are stacked in the encoder?\")\ndisplay(Markdown(str(response)))\n</pre> # Setting up Chat Engine BASE_RAG_CHAT_ENGINE = index.as_chat_engine()  response = BASE_RAG_CHAT_ENGINE.chat(\"How many encoders are stacked in the encoder?\") display(Markdown(str(response))) In\u00a0[\u00a0]: Copied! <pre>!pip install arize-phoenix\n!pip install openinference-instrumentation-llama-index\n!pip install -U llama-index-callbacks-arize-phoenix\n</pre> !pip install arize-phoenix !pip install openinference-instrumentation-llama-index !pip install -U llama-index-callbacks-arize-phoenix In\u00a0[1]: Copied! <pre>import phoenix as px\n\n# (session := px.launch_app()).view()\n</pre> import phoenix as px  # (session := px.launch_app()).view() In\u00a0[2]: Copied! <pre>from openinference.instrumentation.langchain import LangChainInstrumentor\nfrom openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import SpanLimits, TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\nendpoint = \"http://127.0.0.1:6006/v1/traces\"\ntracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=100_000))\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\nLlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n# LangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)\n</pre> from openinference.instrumentation.langchain import LangChainInstrumentor from openinference.instrumentation.llama_index import LlamaIndexInstrumentor from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter from opentelemetry.sdk.trace import SpanLimits, TracerProvider from opentelemetry.sdk.trace.export import SimpleSpanProcessor  endpoint = \"http://127.0.0.1:6006/v1/traces\" tracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=100_000)) tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))  LlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider) # LangChainInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider) <p>Curating a golden test dataset for evaluation can be a long, tedious, and expensive process that is not pragmatic \u2014 especially when starting out or when data sources keep changing. This can be solved by synthetically generating high quality data points, which then can be verified by developers. This can reduce the time and effort in curating test data by 90%.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install ragas\n</pre> !pip install ragas In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nfrom phoenix.trace import using_project\nfrom ragas.testset.evolutions import multi_context, reasoning, simple\nfrom ragas.testset.generator import TestsetGenerator\n\nTEST_SIZE = 5\nCACHE_FILE = \"eval_testset.csv\"\n\ndef generate_and_save_testset():\n    # Generator with openai models\n    generator = TestsetGenerator.with_openai()\n\n    # Set question type distribution\n    distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n\n    # Generate testset\n    with using_project(\"ragas-testset\"):\n        testset = generator.generate_with_llamaindex_docs(\n            documents, test_size=TEST_SIZE, distributions=distribution\n        )\n    \n    test_df = (\n        testset.to_pandas()\n        .sort_values(\"question\")\n        .drop_duplicates(subset=[\"question\"], keep=\"first\")\n    )\n    \n    # Save the dataset locally\n    test_df.to_csv(CACHE_FILE, index=False)\n    print(f\"Test dataset saved to {CACHE_FILE}\")\n    \n    return test_df\n\ndef load_or_generate_testset():\n    if os.path.exists(CACHE_FILE):\n        print(f\"Loading existing test dataset from {CACHE_FILE}\")\n        test_df = pd.read_csv(CACHE_FILE)\n    else:\n        print(\"Generating new test dataset...\")\n        test_df = generate_and_save_testset()\n    \n    return test_df\n\n# Main execution\ntest_df = load_or_generate_testset()\nprint(test_df.head(5))\n</pre> import os import pandas as pd from phoenix.trace import using_project from ragas.testset.evolutions import multi_context, reasoning, simple from ragas.testset.generator import TestsetGenerator  TEST_SIZE = 5 CACHE_FILE = \"eval_testset.csv\"  def generate_and_save_testset():     # Generator with openai models     generator = TestsetGenerator.with_openai()      # Set question type distribution     distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}      # Generate testset     with using_project(\"ragas-testset\"):         testset = generator.generate_with_llamaindex_docs(             documents, test_size=TEST_SIZE, distributions=distribution         )          test_df = (         testset.to_pandas()         .sort_values(\"question\")         .drop_duplicates(subset=[\"question\"], keep=\"first\")     )          # Save the dataset locally     test_df.to_csv(CACHE_FILE, index=False)     print(f\"Test dataset saved to {CACHE_FILE}\")          return test_df  def load_or_generate_testset():     if os.path.exists(CACHE_FILE):         print(f\"Loading existing test dataset from {CACHE_FILE}\")         test_df = pd.read_csv(CACHE_FILE)     else:         print(\"Generating new test dataset...\")         test_df = generate_and_save_testset()          return test_df  # Main execution test_df = load_or_generate_testset() print(test_df.head(5)) <p>You are free to change the question type distribution according to your needs. Since we now have our test dataset ready, let\u2019s move on and build a simple RAG pipeline using LlamaIndex.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace.dsl.helpers import SpanQuery\n\nclient = px.Client()\ncorpus_df = px.Client().query_spans(\n    SpanQuery().explode(\n        \"embedding.embeddings\",\n        text=\"embedding.text\",\n        vector=\"embedding.vector\",\n    ),\n    project_name=\"indexing\",\n)\ncorpus_df.head(2)\n</pre> from phoenix.trace.dsl.helpers import SpanQuery  client = px.Client() corpus_df = px.Client().query_spans(     SpanQuery().explode(         \"embedding.embeddings\",         text=\"embedding.text\",         vector=\"embedding.vector\",     ),     project_name=\"indexing\", ) corpus_df.head(2) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import Dataset\nfrom phoenix.trace import using_project\nfrom tqdm.auto import tqdm\n\n\ndef generate_response(query_engine, question):\n    response = query_engine.query(question)\n    return {\n        \"answer\": response.response,\n        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n    }\n\n\ndef generate_ragas_dataset(query_engine, test_df):\n    test_questions = test_df[\"question\"].values\n    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n\n    dataset_dict = {\n        \"question\": test_questions,\n        \"answer\": [response[\"answer\"] for response in responses],\n        \"contexts\": [response[\"contexts\"] for response in responses],\n        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n    }\n    ds = Dataset.from_dict(dataset_dict)\n    return ds\n\n\nwith using_project(\"llama-index\"):\n    ragas_eval_dataset = generate_ragas_dataset(BASE_RAG_QUERY_ENGINE, test_df)\n\nragas_evals_df = pd.DataFrame(ragas_eval_dataset)\nragas_evals_df.head(2)\n</pre> import pandas as pd from datasets import Dataset from phoenix.trace import using_project from tqdm.auto import tqdm   def generate_response(query_engine, question):     response = query_engine.query(question)     return {         \"answer\": response.response,         \"contexts\": [c.node.get_content() for c in response.source_nodes],     }   def generate_ragas_dataset(query_engine, test_df):     test_questions = test_df[\"question\"].values     responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]      dataset_dict = {         \"question\": test_questions,         \"answer\": [response[\"answer\"] for response in responses],         \"contexts\": [response[\"contexts\"] for response in responses],         \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),     }     ds = Dataset.from_dict(dataset_dict)     return ds   with using_project(\"llama-index\"):     ragas_eval_dataset = generate_ragas_dataset(BASE_RAG_QUERY_ENGINE, test_df)  ragas_evals_df = pd.DataFrame(ragas_eval_dataset) ragas_evals_df.head(2) In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace.dsl.helpers import SpanQuery\n# dataset containing embeddings for visualization\nquery_embeddings_df = px.Client().query_spans(\n    SpanQuery().explode(\"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"),\n    project_name=\"llama-index\",\n)\nquery_embeddings_df.head(2)\n</pre> from phoenix.trace.dsl.helpers import SpanQuery # dataset containing embeddings for visualization query_embeddings_df = px.Client().query_spans(     SpanQuery().explode(\"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"),     project_name=\"llama-index\", ) query_embeddings_df.head(2) In\u00a0[\u00a0]: Copied! <pre>from phoenix.session.evaluation import get_qa_with_reference\n\n# dataset containing span data for evaluation with Ragas\nspans_dataframe = get_qa_with_reference(client, project_name=\"llama-index\")\nspans_dataframe.head(2)\n</pre> from phoenix.session.evaluation import get_qa_with_reference  # dataset containing span data for evaluation with Ragas spans_dataframe = get_qa_with_reference(client, project_name=\"llama-index\") spans_dataframe.head(2) <p>Ragas uses LangChain to evaluate your LLM application data. Since we initialized the LangChain instrumentation above we can see what's going on under the hood when we evaluate our LLM application.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace import using_project\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_correctness,\n    context_precision,\n    context_recall,\n    faithfulness,\n)\n\n# Log the traces to the project \"ragas-evals\" just to view\n# how Ragas works under the hood\nwith using_project(\"ragas-evals\"):\n    evaluation_result = evaluate(\n        dataset=ragas_eval_dataset,\n        metrics=[faithfulness, answer_correctness, context_recall, context_precision],\n    )\neval_scores_df = pd.DataFrame(evaluation_result.scores)\n</pre> from phoenix.trace import using_project from ragas import evaluate from ragas.metrics import (     answer_correctness,     context_precision,     context_recall,     faithfulness, )  # Log the traces to the project \"ragas-evals\" just to view # how Ragas works under the hood with using_project(\"ragas-evals\"):     evaluation_result = evaluate(         dataset=ragas_eval_dataset,         metrics=[faithfulness, answer_correctness, context_recall, context_precision],     ) eval_scores_df = pd.DataFrame(evaluation_result.scores) In\u00a0[21]: Copied! <pre># Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans).\nspan_questions = (\n    spans_dataframe[[\"input\"]]\n    .sort_values(\"input\")\n    .drop_duplicates(subset=[\"input\"], keep=\"first\")\n    .reset_index()\n    .rename({\"input\": \"question\"}, axis=1)\n)\nragas_evals_df = ragas_evals_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\ntest_df = test_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\neval_data_df = pd.DataFrame(evaluation_result.dataset)\neval_data_df = eval_data_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\neval_scores_df.index = eval_data_df.index\n\nquery_embeddings_df = (\n    query_embeddings_df.sort_values(\"text\")\n    .drop_duplicates(subset=[\"text\"])\n    .rename({\"text\": \"question\"}, axis=1)\n    .merge(span_questions, on=\"question\")\n    .set_index(\"context.span_id\")\n)\n</pre> # Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans). span_questions = (     spans_dataframe[[\"input\"]]     .sort_values(\"input\")     .drop_duplicates(subset=[\"input\"], keep=\"first\")     .reset_index()     .rename({\"input\": \"question\"}, axis=1) ) ragas_evals_df = ragas_evals_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\") test_df = test_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\") eval_data_df = pd.DataFrame(evaluation_result.dataset) eval_data_df = eval_data_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\") eval_scores_df.index = eval_data_df.index  query_embeddings_df = (     query_embeddings_df.sort_values(\"text\")     .drop_duplicates(subset=[\"text\"])     .rename({\"text\": \"question\"}, axis=1)     .merge(span_questions, on=\"question\")     .set_index(\"context.span_id\") ) In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace import SpanEvaluations\n\n# Log the evaluations to Phoenix under the project \"llama-index\"\n# This will allow you to visualize the scores alongside the spans in the UI\nfor eval_name in eval_scores_df.columns:\n    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})\n    evals = SpanEvaluations(eval_name, evals_df)\n    px.Client().log_evaluations(evals)\n</pre> from phoenix.trace import SpanEvaluations  # Log the evaluations to Phoenix under the project \"llama-index\" # This will allow you to visualize the scores alongside the spans in the UI for eval_name in eval_scores_df.columns:     evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})     evals = SpanEvaluations(eval_name, evals_df)     px.Client().log_evaluations(evals) In\u00a0[\u00a0]: Copied! <pre>!pip install deepeval\n</pre> !pip install deepeval In\u00a0[\u00a0]: Copied! <pre>from deepeval.integrations.llama_index import (\n    DeepEvalAnswerRelevancyEvaluator,\n    DeepEvalFaithfulnessEvaluator,\n    DeepEvalContextualRelevancyEvaluator,\n    DeepEvalSummarizationEvaluator,\n    DeepEvalBiasEvaluator,\n    DeepEvalToxicityEvaluator,\n)\n\n# An example input to your RAG application\ntest_questions = test_df[\"question\"].values\nfor q in tqdm(test_questions):\n\n    # LlamaIndex returns a response object that contains\n    # both the output string and retrieved nodes\n    response_object = BASE_RAG_QUERY_ENGINE.query(q)\n\n    # Create a list of all evaluators\n    evaluators = [\n        DeepEvalAnswerRelevancyEvaluator(model=\"gpt-4o-mini\"),\n        DeepEvalFaithfulnessEvaluator(model=\"gpt-4o-mini\"),\n        DeepEvalContextualRelevancyEvaluator(model=\"gpt-4o-mini\"),\n        DeepEvalSummarizationEvaluator(model=\"gpt-4o-mini\"),\n        DeepEvalBiasEvaluator(model=\"gpt-4o-mini\"),\n        DeepEvalToxicityEvaluator(model=\"gpt-4o-mini\"),\n    ]\n\n    # Evaluate the response using all evaluators\n    for evaluator in evaluators:\n        evaluation_result = evaluator.evaluate_response(\n            query=q, response=response_object\n        )\n        print(f\"{evaluator.__class__.__name__} Result:\")\n        print(evaluation_result)\n        print(\"\\n\" + \"=\"*50 + \"\\n\") \n</pre>  from deepeval.integrations.llama_index import (     DeepEvalAnswerRelevancyEvaluator,     DeepEvalFaithfulnessEvaluator,     DeepEvalContextualRelevancyEvaluator,     DeepEvalSummarizationEvaluator,     DeepEvalBiasEvaluator,     DeepEvalToxicityEvaluator, )  # An example input to your RAG application test_questions = test_df[\"question\"].values for q in tqdm(test_questions):      # LlamaIndex returns a response object that contains     # both the output string and retrieved nodes     response_object = BASE_RAG_QUERY_ENGINE.query(q)      # Create a list of all evaluators     evaluators = [         DeepEvalAnswerRelevancyEvaluator(model=\"gpt-4o-mini\"),         DeepEvalFaithfulnessEvaluator(model=\"gpt-4o-mini\"),         DeepEvalContextualRelevancyEvaluator(model=\"gpt-4o-mini\"),         DeepEvalSummarizationEvaluator(model=\"gpt-4o-mini\"),         DeepEvalBiasEvaluator(model=\"gpt-4o-mini\"),         DeepEvalToxicityEvaluator(model=\"gpt-4o-mini\"),     ]      # Evaluate the response using all evaluators     for evaluator in evaluators:         evaluation_result = evaluator.evaluate_response(             query=q, response=response_object         )         print(f\"{evaluator.__class__.__name__} Result:\")         print(evaluation_result)         print(\"\\n\" + \"=\"*50 + \"\\n\")"},{"location":"RAG/01_RAG_Evaluation/notebook/#introduction","title":"Introduction\u00b6","text":"<p>Evaluation is a critical component in the development and optimization of Retrieval-Augmented Generation (RAG) systems. It involves assessing the performance, accuracy, and quality of various aspects of the RAG pipeline, from retrieval effectiveness to the relevance and faithfulness of generated responses.</p>"},{"location":"RAG/01_RAG_Evaluation/notebook/#importance-of-evaluation-in-rag","title":"Importance of Evaluation in RAG\u00b6","text":"<p>Effective evaluation of RAG systems is essential because it:</p> <ol> <li>Helps identify strengths and weaknesses in the retrieval and generation processes.</li> <li>Guides improvements and optimizations across the RAG pipeline.</li> <li>Ensures the system meets quality standards and user expectations.</li> <li>Facilitates comparison between different RAG implementations or configurations.</li> <li>Helps detect issues such as hallucinations, biases, or irrelevant responses.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/notebook/#key-evaluation-metrics","title":"Key Evaluation Metrics\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#ragas-metrics","title":"RAGAS Metrics\u00b6","text":"<ol> <li>Faithfulness: Measures how well the generated response aligns with the retrieved context.</li> <li>Answer Relevancy: Assesses the relevance of the response to the query.</li> <li>Context Recall: Evaluates how well the retrieved chunks cover the information needed to answer the query.</li> <li>Context Precision: Measures the proportion of relevant information in the retrieved chunks.</li> <li>Context Utilization: Assesses how effectively the generated response uses the provided context.</li> <li>Context Entity Recall: Evaluates the coverage of important entities from the context in the response.</li> <li>Noise Sensitivity: Measures the system's robustness to irrelevant or noisy information.</li> <li>Summarization Score: Assesses the quality of summarization in the response.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/notebook/#deepeval-metrics","title":"DeepEval Metrics\u00b6","text":"<ol> <li>G-Eval: A general evaluation metric for text generation tasks.</li> <li>Summarization: Assesses the quality of text summarization.</li> <li>Answer Relevancy: Measures how well the response answers the query.</li> <li>Faithfulness: Evaluates the accuracy of the response with respect to the source information.</li> <li>Contextual Recall and Precision: Measures the effectiveness of context retrieval.</li> <li>Hallucination: Detects fabricated or inaccurate information in the response.</li> <li>Toxicity: Identifies harmful or offensive content in the response.</li> <li>Bias: Detects unfair prejudice or favoritism in the generated content.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/notebook/#trulens-metrics","title":"Trulens Metrics\u00b6","text":"<ol> <li>Context Relevance: Assesses how well the retrieved context matches the query.</li> <li>Groundedness: Measures how well the response is supported by the retrieved information.</li> <li>Answer Relevance: Evaluates how well the response addresses the query.</li> <li>Comprehensiveness: Assesses the completeness of the response.</li> <li>Harmful/Toxic Language: Identifies potentially offensive or dangerous content.</li> <li>User Sentiment: Analyzes the emotional tone of user interactions.</li> <li>Language Mismatch: Detects inconsistencies in language use between query and response.</li> <li>Fairness and Bias: Evaluates the system for equitable treatment across different groups.</li> <li>Custom Feedback Functions: Allows for tailored evaluation metrics specific to use cases.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/notebook/#best-practices-for-rag-evaluation","title":"Best Practices for RAG Evaluation\u00b6","text":"<ol> <li>Comprehensive Evaluation: Use a combination of metrics to assess different aspects of the RAG system.</li> <li>Regular Benchmarking: Continuously evaluate the system as changes are made to the pipeline.</li> <li>Human-in-the-Loop: Incorporate human evaluation alongside automated metrics for a holistic assessment.</li> <li>Domain-Specific Metrics: Develop custom metrics relevant to your specific use case or domain.</li> <li>Error Analysis: Investigate patterns in low-scoring responses to identify areas for improvement.</li> <li>Comparative Evaluation: Benchmark your RAG system against baseline models and alternative implementations.</li> </ol>"},{"location":"RAG/01_RAG_Evaluation/notebook/#conclusion","title":"Conclusion\u00b6","text":"<p>A robust evaluation framework is crucial for developing and maintaining high-quality RAG systems. By leveraging a diverse set of metrics and following best practices, developers can ensure their RAG systems deliver accurate, relevant, and trustworthy responses while continuously improving performance.</p>"},{"location":"RAG/01_RAG_Evaluation/notebook/#setting-up-the-environment","title":"Setting up the Environment\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#load-the-data","title":"Load the Data\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/01_RAG_Evaluation/notebook/#ingest-data-into-vector-db","title":"Ingest Data into vector DB\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#setting-up-index","title":"Setting Up Index\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#modifying-prompts-and-prompt-tuning","title":"Modifying Prompts and Prompt Tuning\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#example-of-retrivers","title":"Example of Retrivers\u00b6","text":"<ul> <li>Query Engine</li> <li>Chat Engine</li> </ul>"},{"location":"RAG/01_RAG_Evaluation/notebook/#setup-observability","title":"Setup Observability\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#generating-test-dataset","title":"Generating Test Dataset\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#ragas","title":"RAGAS\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/notebook/#deep-eval","title":"Deep Eval\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/","title":"TruLens","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install trulens trulens-apps-llamaindex trulens-providers-openai llama_index openai\n</pre> # !pip install trulens trulens-apps-llamaindex trulens-providers-openai llama_index openai In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>from trulens.core import TruSession\n\nsession = TruSession()\nsession.reset_database()\n</pre> from trulens.core import TruSession  session = TruSession() session.reset_database() In\u00a0[\u00a0]: Copied! <pre>import os\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\nfile_path = \"data/paul_graham_essay.txt\"\n\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n\nif not os.path.exists(file_path):\n    urllib.request.urlretrieve(url, file_path)\n</pre> import os import urllib.request  url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" file_path = \"data/paul_graham_essay.txt\"  if not os.path.exists(\"data\"):     os.makedirs(\"data\")  if not os.path.exists(file_path):     urllib.request.urlretrieve(url, file_path) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import Settings\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.llms.openai import OpenAI\n\nSettings.chunk_size = 128\nSettings.chunk_overlap = 16\nSettings.llm = OpenAI()\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(similarity_top_k=3)\n</pre> from llama_index.core import Settings from llama_index.core import SimpleDirectoryReader from llama_index.core import VectorStoreIndex from llama_index.llms.openai import OpenAI  Settings.chunk_size = 128 Settings.chunk_overlap = 16 Settings.llm = OpenAI()  documents = SimpleDirectoryReader(\"data\").load_data() index = VectorStoreIndex.from_documents(documents)  query_engine = index.as_query_engine(similarity_top_k=3) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n</pre> response = query_engine.query(\"What did the author do growing up?\") print(response) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom trulens.apps.llamaindex import TruLlama\nfrom trulens.core import Feedback\nfrom trulens.providers.openai import OpenAI\n\n# Initialize provider class\nprovider = OpenAI()\n\n# select context to be used in feedback. the location of context is app specific.\n\ncontext = TruLlama.select_context(query_engine)\n\n# Define a groundedness feedback function\nf_groundedness = (\n    Feedback(\n        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n    )\n    .on(context.collect())  # collect context chunks into a list\n    .on_output()\n)\n\n# Question/answer relevance between overall question and answer.\nf_answer_relevance = Feedback(\n    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n).on_input_output()\n# Question/statement relevance between question and each context chunk.\nf_context_relevance = (\n    Feedback(\n        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n    )\n    .on_input()\n    .on(context)\n    .aggregate(np.mean)\n)\n</pre> import numpy as np from trulens.apps.llamaindex import TruLlama from trulens.core import Feedback from trulens.providers.openai import OpenAI  # Initialize provider class provider = OpenAI()  # select context to be used in feedback. the location of context is app specific.  context = TruLlama.select_context(query_engine)  # Define a groundedness feedback function f_groundedness = (     Feedback(         provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"     )     .on(context.collect())  # collect context chunks into a list     .on_output() )  # Question/answer relevance between overall question and answer. f_answer_relevance = Feedback(     provider.relevance_with_cot_reasons, name=\"Answer Relevance\" ).on_input_output() # Question/statement relevance between question and each context chunk. f_context_relevance = (     Feedback(         provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"     )     .on_input()     .on(context)     .aggregate(np.mean) ) In\u00a0[\u00a0]: Copied! <pre>tru_query_engine_recorder = TruLlama(\n    query_engine,\n    app_name=\"LlamaIndex_App\",\n    app_version=\"base\",\n    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n)\n</pre> tru_query_engine_recorder = TruLlama(     query_engine,     app_name=\"LlamaIndex_App\",     app_version=\"base\",     feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance], ) In\u00a0[\u00a0]: Copied! <pre># or as context manager\nwith tru_query_engine_recorder as recording:\n    query_engine.query(\"What did the author do growing up?\")\n</pre> # or as context manager with tru_query_engine_recorder as recording:     query_engine.query(\"What did the author do growing up?\") In\u00a0[\u00a0]: Copied! <pre>from trulens.dashboard.display import get_feedback_result\n\nlast_record = recording.records[-1]\nget_feedback_result(last_record, \"Context Relevance\")\n</pre> from trulens.dashboard.display import get_feedback_result  last_record = recording.records[-1] get_feedback_result(last_record, \"Context Relevance\") <p>Wouldn't it be great if we could automatically filter out context chunks with relevance scores below 0.5?</p> <p>We can do so with the TruLens guardrail, WithFeedbackFilterNodes. All we have to do is use the method <code>of_query_engine</code> to create a new filtered retriever, passing in the original retriever along with the feedback function and threshold we want to use.</p> In\u00a0[\u00a0]: Copied! <pre>from trulens.apps.llamaindex.guardrails import WithFeedbackFilterNodes\n\n# note: feedback function used for guardrail must only return a score, not also reasons\nf_context_relevance_score = Feedback(provider.context_relevance)\n\nfiltered_query_engine = WithFeedbackFilterNodes(\n    query_engine, feedback=f_context_relevance_score, threshold=0.5\n)\n</pre> from trulens.apps.llamaindex.guardrails import WithFeedbackFilterNodes  # note: feedback function used for guardrail must only return a score, not also reasons f_context_relevance_score = Feedback(provider.context_relevance)  filtered_query_engine = WithFeedbackFilterNodes(     query_engine, feedback=f_context_relevance_score, threshold=0.5 ) <p>Then we can operate as normal</p> In\u00a0[\u00a0]: Copied! <pre>tru_recorder = TruLlama(\n    filtered_query_engine,\n    app_name=\"LlamaIndex_App\",\n    app_version=\"filtered\",\n    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness],\n)\n\nwith tru_recorder as recording:\n    llm_response = filtered_query_engine.query(\n        \"What did the author do growing up?\"\n    )\n\ndisplay(llm_response)\n</pre> tru_recorder = TruLlama(     filtered_query_engine,     app_name=\"LlamaIndex_App\",     app_version=\"filtered\",     feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness], )  with tru_recorder as recording:     llm_response = filtered_query_engine.query(         \"What did the author do growing up?\"     )  display(llm_response) In\u00a0[\u00a0]: Copied! <pre>from trulens.dashboard.display import get_feedback_result\n\nlast_record = recording.records[-1]\nget_feedback_result(last_record, \"Context Relevance\")\n</pre> from trulens.dashboard.display import get_feedback_result  last_record = recording.records[-1] get_feedback_result(last_record, \"Context Relevance\") In\u00a0[\u00a0]: Copied! <pre>session.get_leaderboard()\n</pre> session.get_leaderboard() In\u00a0[\u00a0]: Copied! <pre># The record of the app invocation can be retrieved from the `recording`:\n\nrec = recording.get()  # use .get if only one record\n# recs = recording.records # use .records if multiple\n\ndisplay(rec)\n</pre> # The record of the app invocation can be retrieved from the `recording`:  rec = recording.get()  # use .get if only one record # recs = recording.records # use .records if multiple  display(rec) In\u00a0[\u00a0]: Copied! <pre>from trulens.dashboard import run_dashboard\n\nrun_dashboard(session)\n</pre> from trulens.dashboard import run_dashboard  run_dashboard(session) In\u00a0[\u00a0]: Copied! <pre># The results of the feedback functions can be rertireved from\n# `Record.feedback_results` or using the `wait_for_feedback_result` method. The\n# results if retrieved directly are `Future` instances (see\n# `concurrent.futures`). You can use `as_completed` to wait until they have\n# finished evaluating or use the utility method:\n\nfor feedback, feedback_result in rec.wait_for_feedback_results().items():\n    print(feedback.name, feedback_result.result)\n\n# See more about wait_for_feedback_results:\n# help(rec.wait_for_feedback_results)\n</pre> # The results of the feedback functions can be rertireved from # `Record.feedback_results` or using the `wait_for_feedback_result` method. The # results if retrieved directly are `Future` instances (see # `concurrent.futures`). You can use `as_completed` to wait until they have # finished evaluating or use the utility method:  for feedback, feedback_result in rec.wait_for_feedback_results().items():     print(feedback.name, feedback_result.result)  # See more about wait_for_feedback_results: # help(rec.wait_for_feedback_results) In\u00a0[\u00a0]: Copied! <pre>records, feedback = session.get_records_and_feedback()\n\nrecords.head()\n</pre> records, feedback = session.get_records_and_feedback()  records.head() In\u00a0[\u00a0]: Copied! <pre>session.get_leaderboard()\n</pre> session.get_leaderboard() In\u00a0[\u00a0]: Copied! <pre>run_dashboard(session)  # open a local streamlit app to explore\n\n# stop_dashboard(session) # stop if needed\n</pre> run_dashboard(session)  # open a local streamlit app to explore  # stop_dashboard(session) # stop if needed <p>Alternatively, you can run <code>trulens</code> from a command line in the same folder to start the dashboard.</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#llamaindex-quickstart","title":"\ud83d\udcd3 LlamaIndex Quickstart\u00b6","text":"<p>In this quickstart you will create a simple Llama Index app and learn how to log it and get feedback on an LLM response.</p> <p>You'll also learn how to use feedbacks for guardrails, via filtering retrieved context.</p> <p>For evaluation, we will leverage the RAG triad of groundedness, context relevance and answer relevance.</p> <p></p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#setup","title":"Setup\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Let's install some of the dependencies for this notebook if we don't have them already</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#add-api-keys","title":"Add API keys\u00b6","text":"<p>For this quickstart, you will need an Open AI key. The OpenAI key is used for embeddings, completion and evaluation.</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#import-from-trulens","title":"Import from TruLens\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/#download-data","title":"Download data\u00b6","text":"<p>This example uses the text of Paul Graham\u2019s essay, \u201cWhat I Worked On\u201d, and is the canonical llama-index example.</p> <p>The easiest way to get it is to download it via this link and save it in a folder called data. You can do so with the following command:</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#create-simple-llm-application","title":"Create Simple LLM Application\u00b6","text":"<p>This example uses LlamaIndex which internally uses an OpenAI LLM.</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#send-your-first-request","title":"Send your first request\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/#initialize-feedback-functions","title":"Initialize Feedback Function(s)\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/#instrument-app-for-logging-with-trulens","title":"Instrument app for logging with TruLens\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/#use-guardrails","title":"Use guardrails\u00b6","text":"<p>In addition to making informed iteration, we can also directly use feedback results as guardrails at inference time. In particular, here we show how to use the context relevance score as a guardrail to filter out irrelevant context before it gets passed to the LLM. This both reduces hallucination and improves efficiency.</p> <p>Below, you can see the TruLens feedback display of each context relevance chunk retrieved by our RAG.</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#see-the-power-of-context-filters","title":"See the power of context filters!\u00b6","text":"<p>If we inspect the context relevance of our retrieval now, you see only relevant context chunks!</p>"},{"location":"RAG/01_RAG_Evaluation/trulens/#retrieve-records-and-feedback","title":"Retrieve records and feedback\u00b6","text":""},{"location":"RAG/01_RAG_Evaluation/trulens/#explore-in-a-dashboard","title":"Explore in a Dashboard\u00b6","text":""},{"location":"RAG/01_RAG_Observability/","title":"Intro to Observability","text":""},{"location":"RAG/01_RAG_Observability/#rag-observability-arize-phoenix-setup","title":"RAG Observability - Arize Phoenix Setup","text":"<p>Welcome to this notebook, where we explore the setup and observation of a Retrieval-Augmented Generation (RAG) pipeline using Llama Index.</p>"},{"location":"RAG/01_RAG_Observability/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Getting Started</li> <li>Usage</li> <li>Conclusion</li> </ol>"},{"location":"RAG/01_RAG_Observability/#introduction","title":"Introduction","text":"<p>This guide provides a comprehensive walkthrough for configuring the necessary tools and libraries, including embedding models and vector store indexing, to enable efficient document retrieval and query processing. We\u2019ll cover everything from installation and setup to querying and retrieving relevant information, equipping you with the knowledge to harness the power of RAG pipelines for advanced search capabilities.</p>"},{"location":"RAG/01_RAG_Observability/#getting-started","title":"Getting Started","text":"<p>To get started with this notebook, you'll need to have a basic understanding of Python and some familiarity with machine learning concepts. Don't worry if you're new to some of these ideas \u2013 we'll guide you through each step!</p>"},{"location":"RAG/01_RAG_Observability/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7+</li> <li>Jupyter Notebook or JupyterLab</li> <li>Basic knowledge of Python and machine learning concepts</li> </ul>"},{"location":"RAG/01_RAG_Observability/#usage","title":"Usage","text":""},{"location":"RAG/01_RAG_Observability/#1-setup","title":"1. Setup","text":""},{"location":"RAG/01_RAG_Observability/#11-install-required-packages","title":"1.1 Install required packages","text":"<p>To get started with setting up Arize Phoenix, you'll need to install the required packages.</p> <p>Arize Phoenix is a comprehensive tool designed for observability and monitoring in machine learning and AI systems. It provides functionalities for tracking and analyzing various aspects of machine learning models and data pipelines.</p> <pre><code>!pip install arize-phoenix\n!pip install openinference-instrumentation-openai\n</code></pre> <p>These commands will install:</p> <ul> <li><code>arize-phoenix</code>: A tool for observability in machine learning workflows.</li> <li><code>openinference-instrumentation-openai</code>: A package to instrument OpenAI models with observability tools like Arize Phoenix.</li> </ul>"},{"location":"RAG/01_RAG_Observability/#12-setting-up-arize-phoenix","title":"1.2 Setting up Arize Phoenix","text":"<p>There are 3 ways to do this:</p> <p>Read more here.</p> <ul> <li>Command Line</li> </ul> <pre><code>python3 -m phoenix.server.main serve\n</code></pre> <ul> <li>Docker</li> </ul> <p>Launch the phoenix docker image using:</p> <pre><code>docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest\n</code></pre> <p>This will expose the Phoenix UI and REST API on localhost:6006 and exposes the gRPC endpoint for spans on localhost:4317.</p> <ul> <li>Notebook</li> </ul> <pre><code>import phoenix as px\npx.launch_app()\n</code></pre>"},{"location":"RAG/01_RAG_Observability/#13-import-required-libraries-and-configure-the-environment","title":"1.3 Import Required Libraries and Configure the Environment","text":"<p>Before proceeding with data processing and evaluation, import the necessary libraries and set up the environment:</p> <pre><code>import json\nimport os\nfrom getpass import getpass\nimport nest_asyncio\nimport pandas as pd\nfrom tqdm import tqdm\nimport phoenix as px\n\n# Allows concurrent evaluations in notebook environments\nnest_asyncio.apply()\n\n# Set display options for pandas DataFrames to show more content\npd.set_option(\"display.max_colwidth\", 1000)\n</code></pre> <ul> <li> <p><code>json</code>, <code>os</code>: Standard Python libraries for handling JSON data and operating system interactions.</p> </li> <li> <p><code>getpass</code>: A utility for securely capturing password input.   <code>nest_asyncio</code>: Allows the usage of asyncio within Jupyter notebooks.</p> </li> <li> <p><code>pandas</code> (<code>pd</code>): A powerful data manipulation library for Python.</p> </li> <li> <p><code>tqdm</code>: Provides progress bars for loops, useful for tracking the progress of data processing.</p> </li> <li> <p><code>phoenix</code> (<code>px</code>): The phoenix library is part of Arize's observability tools. It provides an interactive UI for exploring data and monitoring machine learning models.</p> </li> </ul> <p>Configure <code>nest_asyncio</code> to allow concurrent evaluations in notebook environments and set the maximum column width for pandas DataFrames to ensure better readability.</p>"},{"location":"RAG/01_RAG_Observability/#14-launch-the-phoenix-app","title":"1.4 Launch the Phoenix App","text":"<pre><code>px.launch_app()\n</code></pre> <p>This function initializes and launches the Phoenix app, which opens in a new tab in your default web browser. It provides an interactive interface for exploring datasets, visualizing model performance, and debugging.</p>"},{"location":"RAG/01_RAG_Observability/#15-view-the-phoenix-app-session","title":"1.5 View the Phoenix App Session","text":"<p>Once the Phoenix app is launched, you can use the session object to interact with the app directly in the notebook. Run the following code to launch the Phoenix app and view it in the current session:</p> <pre><code># Launch and view the Phoenix app session\n(session := px.launch_app()).view()\n</code></pre> <p>This line launches the Phoenix app and assigns the session to a variable named session, with the <code>view()</code> method allowing you to display the Phoenix app directly within the notebook interface, providing a more integrated experience without switching between the browser and the notebook.</p>"},{"location":"RAG/01_RAG_Observability/#16-set-up-the-endpoint-for-traces","title":"1.6 Set Up the Endpoint for Traces","text":"<p>To send traces to the Phoenix app for analysis and observability, define the endpoint URL where the Phoenix app is listening for incoming data.</p> <pre><code>endpoint = \"http://127.0.0.1:6006/v1/traces\"\n</code></pre> <p>The <code>endpoint</code> variable stores the URL of the Phoenix app's endpoint that listens for incoming traces.</p>"},{"location":"RAG/01_RAG_Observability/#2-trace-open-ai","title":"2. Trace Open AI","text":"<p>For more integration, read.</p>"},{"location":"RAG/01_RAG_Observability/#21-install-and-import-the-openai-package","title":"2.1 Install and Import the OpenAI Package","text":"<pre><code>!pip install openai\nimport openai\n</code></pre> <p><code>openai</code>: The Python client library for OpenAI's API. It enables you to make requests to OpenAI's models, including GPT-3 and GPT-4, for various tasks.</p>"},{"location":"RAG/01_RAG_Observability/#22-configure-the-openai-api-key","title":"2.2 Configure the OpenAI API Key","text":"<pre><code>import openai\nimport os\nfrom getpass import getpass\n\n# Retrieve API key from environment variable or prompt user if not set\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")\n\n# Set the API key for the OpenAI client\nopenai.api_key = openai_api_key\n\n# Store the API key in environment variables for future use\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n</code></pre> <ul> <li> <p>Retrieve API Key: The code first attempts to get the API key from an environment variable (OPENAI_API_KEY). If the key is not found, it prompts the user to enter it securely using getpass.</p> </li> <li> <p>Set API Key: The retrieved or provided API key is then set for the openai client library.</p> </li> <li> <p>Store API Key: Finally, the API key is stored in the environment variables to ensure it is available for future use within the session.</p> </li> </ul>"},{"location":"RAG/01_RAG_Observability/#23-set-up-opentelemetry-for-tracing","title":"2.3 Set Up OpenTelemetry for Tracing","text":"<p>To enable tracing for your OpenAI interactions, configure OpenTelemetry with the necessary components.</p> <pre><code>from opentelemetry import trace as trace_api\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\n# Set up the Tracer Provider\ntracer_provider = trace_sdk.TracerProvider()\n\n# Define the OTLP Span Exporter with the endpoint\nspan_exporter = OTLPSpanExporter(endpoint)\n\n# Set up the Span Processor to process and export spans\nspan_processor = SimpleSpanProcessor(span_exporter)\n\n# Add the Span Processor to the Tracer Provider\ntracer_provider.add_span_processor(span_processor)\n\n# Set the global Tracer Provider\ntrace_api.set_tracer_provider(tracer_provider)\n</code></pre> <p>OpenTelemetry Libraries</p> <p>In the provided code, several OpenTelemetry libraries are used to set up tracing. Here's an overview of each:</p> <ul> <li><code>opentelemetry</code>:</li> </ul> <p>**Purpose**: The core library for OpenTelemetry, providing APIs for tracing and metrics.</p> <p>Usage: It includes the trace module, which is used to create and manage traces.</p> <ul> <li><code>opentelemetry.exporter.otlp.proto.http.trace_exporter</code>:</li> </ul> <p>Purpose: Provides the OTLP (OpenTelemetry Protocol) exporter for traces using HTTP.</p> <p>Usage: The <code>OTLPSpanExporter</code> class in this module sends trace data to an OTLP-compatible backend. This exporter is configured with an endpoint where trace data will be sent.</p> <ul> <li><code>opentelemetry.sdk.trace</code>:</li> </ul> <p>Purpose: Contains the SDK implementations for tracing, including the <code>TracerProvider</code>.</p> <p>Usage:</p> <ul> <li> <p><code>TracerProvider</code>: Manages Tracer instances and is responsible for exporting spans (units of work) collected during tracing.</p> </li> <li> <p><code>SimpleSpanProcessor</code>: A span processor that exports spans synchronously, used to process and send trace data to the exporter.</p> </li> <li> <p><code>opentelemetry.sdk.trace.export</code>:</p> </li> </ul> <p>Purpose: Provides classes for exporting trace data.</p> <p>Usage:</p> <ul> <li><code>SimpleSpanProcessor</code>: Processes spans and exports them using the specified exporter. It ensures that spans are sent to the backend for analysis.</li> </ul>"},{"location":"RAG/01_RAG_Observability/#24-instrument-openai-with-openinference","title":"2.4 Instrument OpenAI with OpenInference","text":"<p>To integrate OpenTelemetry with OpenAI and enable tracing for OpenAI model interactions, use the <code>OpenAIInstrumentor</code> from the <code>openinference</code> library.</p> <pre><code>from openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Instantiate and apply instrumentation for OpenAI\nOpenAIInstrumentor().instrument()\n</code></pre> <ul> <li> <p><code>OpenAIInstrumentor</code>: A class from the openinference library designed to instrument OpenAI's API calls, enabling tracing and observability.</p> </li> <li> <p><code>instrument()</code>: This method configures the OpenAI API client to automatically generate and send trace data to the OpenTelemetry backend. It integrates with the tracing setup you have configured, allowing you to monitor and analyze interactions with OpenAI's models.</p> </li> </ul> <p>By running this code, you ensure that all OpenAI API calls are traced, allowing you to capture detailed insights into model usage and performance.</p>"},{"location":"RAG/01_RAG_Observability/#25-make-a-request-to-openai-api","title":"2.5 Make a Request to OpenAI API","text":"<p>To interact with OpenAI\u2019s API and obtain a response, use the following code. This example demonstrates how to create a chat completion using the OpenAI API and print the result:</p> <pre><code>import openai\n\n# Create an OpenAI client instance\nclient = openai.OpenAI()\n\n# Make a request to the OpenAI API for a chat completion\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n)\n\n# Print the content of the response\nprint(response.choices[0].message.content)\n</code></pre> <ul> <li> <p><code>openai.OpenAI()</code>: Initializes an OpenAI client instance that can be used to interact with the OpenAI API.</p> </li> <li> <p><code>client.chat.completions.create()</code>: Sends a request to the OpenAI API to create a chat completion using the specified model.</p> </li> <li> <p><code>model=\"gpt-4o\"</code>: Specifies the model to use for generating completions. Ensure the model name is correct and available in your OpenAI API account.</p> </li> <li> <p><code>messages</code>: A list of message objects representing the conversation history. In this case, it includes a single message from the user asking to \"Write a haiku.\"</p> </li> </ul> <p><code>response.choices[0].message.content</code>: Extracts and prints the content of the completion response generated by the model.</p>"},{"location":"RAG/01_RAG_Observability/#3-trace-llama-index","title":"3. Trace Llama index","text":""},{"location":"RAG/01_RAG_Observability/#31-install-and-import-the-required-libraries","title":"3.1 Install and Import the Required Libraries","text":"<pre><code>!pip install llama-index\n!pip install llama-index-core\n!pip install llama-index-llms-openai\n!pip install openinference-instrumentation-llama-index==2.2.4\n!pip install -U llama-index-callbacks-arize-phoenix\n!pip install \"arize-phoenix[llama-index]\"\n</code></pre> <ul> <li> <p><code>llama-index</code>: Core package for Llama Index functionality.</p> </li> <li> <p><code>llama-index-core</code>: Provides core features and utilities for Llama Index.</p> </li> <li> <p><code>llama-index-llms-openai</code>: Integration package for Llama Index and OpenAI models.</p> </li> <li> <p><code>openinference-instrumentation-llama-index==2.2.4</code>: Provides instrumentation for tracing Llama Index interactions.</p> </li> <li> <p><code>llama-index-callbacks-arize-phoenix</code>: Callback integration for Arize Phoenix with Llama Index.</p> </li> <li> <p><code>arize-phoenix[llama-index]</code>: Extends Arize Phoenix to support Llama Index tracing.</p> </li> </ul>"},{"location":"RAG/01_RAG_Observability/#32-retrieve-the-url-of-the-active-phoenix-session","title":"3.2 Retrieve the URL of the Active Phoenix Session","text":"<pre><code># Retrieve the URL of the active Phoenix session\npx.active_session().url\n</code></pre> <p>Accesses the current active session of the Phoenix app and retrieves its URL, allowing you to view or share the link to the Phoenix interface where you can monitor and analyze trace data.</p>"},{"location":"RAG/01_RAG_Observability/#33-set-up-tracing-for-llama-index","title":"3.3 Set Up Tracing for Llama Index","text":"<p>To instrument Llama Index for tracing with OpenTelemetry, configure the tracer provider and integrate the Llama Index instrumentor.</p> <pre><code>from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\n# Set up the Tracer Provider\ntracer_provider = trace_sdk.TracerProvider()\n\n# Add Span Processor to the Tracer Provider\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\n# Instrument Llama Index with the Tracer Provider\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n</code></pre> <ul> <li> <p><code>LlamaIndexInstrumentor</code>: This class from openinference.instrumentation.llama_index instruments Llama Index to support tracing and observability.</p> </li> <li> <p><code>trace_sdk.TracerProvider()</code>: Initializes a new Tracer Provider responsible for creating and managing trace data.   OTLPSpanExporter(endpoint): Configures the OTLP exporter to send trace data to the specified endpoint.</p> </li> <li> <p><code>SimpleSpanProcessor</code>: Processes and exports spans synchronously.</p> </li> <li> <p><code>tracer_provider.add_span_processor</code>: Adds the span processor to the Tracer Provider.</p> </li> <li> <p><code>LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)</code>: Applies the instrumentation to Llama Index, using the provided Tracer Provider for tracing.</p> </li> </ul>"},{"location":"RAG/01_RAG_Observability/#34-interact-with-llama-index-using-openai","title":"3.4 Interact with Llama Index Using OpenAI","text":"<p>To perform a completion request with Llama Index using an OpenAI model, use the following code:</p> <pre><code>from llama_index.llms.openai import OpenAI\n\n# Initialize the OpenAI model\nllm = OpenAI(model=\"gpt-4o-mini\")\n\n# Make a completion request\nresp = llm.complete(\"Paul Graham is \")\n\n# Print the response\nprint(resp)\n</code></pre> <ul> <li> <p><code>from llama_index.llms.openai import OpenAI</code>: Imports the OpenAI class from the llama_index package, allowing interaction with OpenAI models.</p> </li> <li> <p><code>OpenAI(model=\"gpt-4o-mini\")</code>: Initializes an instance of the OpenAI class with the specified model (e.g., gpt-4).</p> </li> <li> <p><code>llm.complete(...)</code>: Sends a prompt to the model to generate a completion based on the input text.</p> </li> </ul>"},{"location":"RAG/01_RAG_Observability/#35-perform-a-chat-interaction-with-llama-index-using-openai","title":"3.5 Perform a Chat Interaction with Llama Index Using OpenAI","text":"<pre><code>from llama_index.llms.openai import OpenAI\nfrom llama_index.core.llms import ChatMessage\n\n# Initialize the OpenAI model\nllm = OpenAI()\n\n# Define the chat messages\nmessages = [\n    ChatMessage(\n        role=\"system\", content=\"You are a pirate with a colorful personality\"\n    ),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\n\n# Get the response from the model\nresp = llm.chat(messages)\n</code></pre> <ul> <li> <p><code>OpenAI</code>: A class for interacting with OpenAI models.</p> </li> <li> <p><code>ChatMessage</code>: A class to format chat messages.</p> </li> <li> <p><code>OpenAI()</code>: Initializes an instance of the OpenAI class.</p> </li> <li> <p><code>ChatMessage</code>: Creates chat message objects with a specified role (e.g., \"system\", \"user\") and content.</p> </li> <li> <p><code>role=\"system\"</code>: Defines the system message to set the context or personality of the model.</p> </li> <li> <p><code>role=\"user\"</code>: Represents a user message in the chat.</p> </li> <li> <p><code>llm.chat(messages)</code>: Sends the defined messages to the model and retrieves the response.</p> </li> </ul> <p>This code sets up a chat with an OpenAI model, specifying system and user messages to guide the interaction.</p>"},{"location":"RAG/01_RAG_Observability/#4-observe-rag-pipeline","title":"4. Observe RAG Pipeline","text":""},{"location":"RAG/01_RAG_Observability/#41-setup-an-environment-for-observing-a-rag-piepline","title":"4.1 Setup an environment for observing a RAG piepline","text":"<pre><code>!pip install llama-index\n!pip install llama-index-vector-stores-qdrant\n!pip install llama-index-readers-file\n!pip install llama-index-embeddings-fastembed\n!pip install llama-index-llms-openai\n!pip install -U qdrant_client fastembed\n</code></pre> <ul> <li> <p><code>llama-index</code>: Core package for Llama Index functionality.</p> </li> <li> <p><code>llama-index-vector-stores-qdrant</code>: Integration for using Qdrant as a vector store with Llama Index.</p> </li> <li> <p><code>llama-index-readers-file</code>: Provides file reading capabilities for Llama Index.</p> </li> <li> <p><code>llama-index-embeddings-fastembed</code>: Adds FastEmbed support for generating embeddings with Llama Index.</p> </li> <li> <p><code>llama-index-llms-openai</code>: Integration for using OpenAI models with Llama Index.</p> </li> <li> <p><code>qdrant_client</code>: Client library for interacting with Qdrant, a vector search engine.</p> </li> <li> <p><code>fastembed</code>: Library for generating embeddings quickly.</p> </li> </ul>"},{"location":"RAG/01_RAG_Observability/#42-prepare-rag-pipeline-with-embeddings-and-document-indexing","title":"4.2 Prepare RAG Pipeline with Embeddings and Document Indexing","text":"<pre><code>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\n# from llama_index.embeddings.openai import OpenAIEmbedding\n\nfrom llama_index.core.settings import Settings\n\nSettings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n# Settings.embed_model = OpenAIEmbedding(embed_batch_size=10)\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n</code></pre> <ul> <li> <p><code>VectorStoreIndex</code>: A class used to create and manage a vector store index. This index allows for efficient similarity search and retrieval based on vector representations of documents.</p> </li> <li> <p><code>SimpleDirectoryReader</code>: A class for loading documents from a specified directory. It reads and preprocesses files from the directory \"sample_data\" to be used in the indexing process.</p> </li> <li> <p><code>FastEmbedEmbedding</code>: Provides an embedding model for generating vector representations of text using the FastEmbed library. The model specified (\"BAAI/bge-base-en-v1.5\") is used to convert documents into embeddings, which are then used for similarity search within the vector store index.</p> </li> <li> <p><code>from llama_index.embeddings.openai import OpenAIEmbedding</code>:</p> </li> </ul> <p><code>OpenAIEmbedding</code>: (Commented out) Provides an embedding model for generating vector representations using OpenAI\u2019s embeddings. Uncomment this line if you wish to use OpenAI\u2019s model instead of FastEmbed. This model can be configured with parameters like <code>embed_batch_size</code> for batch processing.</p> <ul> <li> <p><code>Settings</code>: A configuration class used to set global settings for embedding models. By assigning the embed_model attribute, you specify which embedding model to use for the RAG pipeline.</p> </li> <li> <p><code>Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")</code>   Configures the Settings class to use the FastEmbed model for generating embeddings. This step is crucial for defining how text data will be represented in the vector store.</p> </li> <li> <p><code>documents = SimpleDirectoryReader(\"data\").load_data()</code>   Loads and preprocesses documents (in this case) from the \"data\" directory. Please ensure to tweak the directory name according to your project. The <code>load_data()</code> method reads all files in the specified directory and prepares them for indexing.</p> </li> <li> <p><code>index = VectorStoreIndex.from_documents(documents)</code>   Creates a VectorStoreIndex from the preprocessed documents. This index allows for efficient querying and retrieval based on the vector representations generated by the embedding model.</p> </li> </ul>"},{"location":"RAG/01_RAG_Observability/#43-query-the-vector-store-index","title":"4.3 Query the Vector Store Index","text":"<p>Once the vector store index is set up, you can use it to perform queries and retrieve relevant information.</p> <pre><code>query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n</code></pre> <ul> <li> <p><code>as_query_engine()</code>: Converts the <code>VectorStoreIndex</code> into a query engine. This engine allows you to perform searches and retrieve information based on the vector representations of documents stored in the index.</p> </li> <li> <p><code>query()</code>: Executes a query against the vector store index. The query string \"What did the author do growing up?\" is used to search for relevant documents and retrieve information based on the context provided by the vector embeddings.</p> </li> </ul> <p>Finally, the <code>response</code> containing the information retrieved from the vector store index, which is based on the query and the indexed documents is output.</p>"},{"location":"RAG/01_RAG_Observability/#conclusion","title":"Conclusion","text":"<p>In this guide, we have set up a Retrieval-Augmented Generation (RAG) pipeline using Llama Index and integrated it with various components to observe its functionality. We began by configuring and installing the necessary libraries, including Llama Index, OpenTelemetry, and various embedding models.</p> <p>We then proceeded to:</p> <ul> <li>Initialize and configure the embedding models, using FastEmbed or OpenAI models as needed.</li> <li>Load and index documents from a directory to prepare the data for querying.</li> <li>Set up a query engine to perform searches and retrieve relevant information based on the indexed documents.</li> </ul> <p>By following these steps, you have successfully prepared a RAG pipeline capable of efficient document retrieval and query processing. This setup enables advanced search and information retrieval capabilities, leveraging the power of vector-based embeddings and indexing.</p> <p>Feel free to experiment with different configurations and queries to further explore the capabilities of the RAG pipeline. For any questions or additional customization, consult the documentation of the libraries used or seek further guidance.</p> <p>If you find this guide helpful, please consider giving us a star on GitHub! \u2b50</p> <p></p> <p>Thank you for following this guide, and happy querying!</p>"},{"location":"RAG/01_RAG_Observability/notebook/","title":"Observability(Arize Phoenix)","text":"RAG Observability - Arize Phoenix Setup AI Engineering.academy <p>Welcome to this notebook, where we explore the setup and observation of a Retrieval-Augmented Generation (RAG) pipeline using Llama Index.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arize-phoenix\n!pip install openinference-instrumentation-openai\n</pre> !pip install arize-phoenix !pip install openinference-instrumentation-openai <p>These commands will install:</p> <ul> <li><code>arize-phoenix</code>: A tool for observability in machine learning workflows.</li> <li><code>openinference-instrumentation-openai</code>: A package to instrument OpenAI models with observability tools like Arize Phoenix.</li> </ul> In\u00a0[6]: Copied! <pre>import json\nimport os\nfrom getpass import getpass\nimport nest_asyncio\nimport pandas as pd\nfrom tqdm import tqdm\nimport phoenix as px\n\n# Allows concurrent evaluations in notebook environments\nnest_asyncio.apply()\n\n# Set display options for pandas DataFrames to show more content\npd.set_option(\"display.max_colwidth\", 1000)\n</pre> import json import os from getpass import getpass import nest_asyncio import pandas as pd from tqdm import tqdm import phoenix as px  # Allows concurrent evaluations in notebook environments nest_asyncio.apply()  # Set display options for pandas DataFrames to show more content pd.set_option(\"display.max_colwidth\", 1000) <ul> <li><p><code>getpass</code>: A utility for securely capturing password input. <code>nest_asyncio</code>: Allows the usage of asyncio within Jupyter notebooks.</p> </li> <li><p><code>pandas</code> (<code>pd</code>): A powerful data manipulation library for Python.</p> </li> <li><p><code>tqdm</code>: Provides progress bars for loops, useful for tracking the progress of data processing.</p> </li> <li><p><code>phoenix</code> (<code>px</code>): The phoenix library is part of Arize's observability tools. It provides an interactive UI for exploring data and monitoring machine learning models.</p> </li> </ul> <p>Configure <code>nest_asyncio</code> to allow concurrent evaluations in notebook environments and set the maximum column width for pandas DataFrames to ensure better readability.</p> In\u00a0[2]: Copied! <pre>import phoenix as px\npx.launch_app()\n</pre> import phoenix as px px.launch_app() <pre>\ud83c\udf0d To view the Phoenix app in your browser, visit http://localhost:6006/\n\ud83d\udcd6 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n</pre> Out[2]: <pre>&lt;phoenix.session.session.ThreadSession at 0x243f0fb3650&gt;</pre> <p>This function initializes and launches the Phoenix app, which opens in a new tab in your default web browser. It provides an interactive interface for exploring datasets, visualizing model performance, and debugging.</p> In\u00a0[\u00a0]: Copied! <pre># Launch and view the Phoenix app session\n(session := px.launch_app()).view()\n</pre> # Launch and view the Phoenix app session (session := px.launch_app()).view() <p>This line launches the Phoenix app and assigns the session to a variable named session, with the <code>view()</code> method allowing you to display the Phoenix app directly within the notebook interface, providing a more integrated experience without switching between the browser and the notebook.</p> In\u00a0[9]: Copied! <pre>endpoint = \"http://127.0.0.1:6006/v1/traces\"\n</pre> endpoint = \"http://127.0.0.1:6006/v1/traces\" <p>The <code>endpoint</code> variable stores the URL of the Phoenix app's endpoint that listens for incoming traces.</p> In\u00a0[\u00a0]: Copied! <pre># !pip install openai\n</pre> # !pip install openai In\u00a0[11]: Copied! <pre>import openai\n</pre> import openai <p><code>openai</code>: The Python client library for OpenAI's API. It enables you to make requests to OpenAI's models, including GPT-3 and GPT-4, for various tasks.</p> In\u00a0[1]: Copied! <pre># Retrieve API key from environment variable or prompt user if not set\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")\n\n# Set the API key for the OpenAI client\nopenai.api_key = openai_api_key\n\n# Store the API key in environment variables for future use\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n</pre> # Retrieve API key from environment variable or prompt user if not set if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):     openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")  # Set the API key for the OpenAI client openai.api_key = openai_api_key  # Store the API key in environment variables for future use os.environ[\"OPENAI_API_KEY\"] = openai_api_key <ul> <li><p>Retrieve API Key: The code first attempts to get the API key from an environment variable (OPENAI_API_KEY). If the key is not found, it prompts the user to enter it securely using getpass.</p> </li> <li><p>Set API Key: The retrieved or provided API key is then set for the openai client library.</p> </li> <li><p>Store API Key: Finally, the API key is stored in the environment variables to ensure it is available for future use within the session.</p> </li> </ul> In\u00a0[13]: Copied! <pre>from opentelemetry import trace as trace_api\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\n# Set up the Tracer Provider\ntracer_provider = trace_sdk.TracerProvider()\n\n# Define the OTLP Span Exporter with the endpoint\nspan_exporter = OTLPSpanExporter(endpoint)\n\n# Set up the Span Processor to process and export spans\nspan_processor = SimpleSpanProcessor(span_exporter)\n\n# Add the Span Processor to the Tracer Provider\ntracer_provider.add_span_processor(span_processor)\n\n# Set the global Tracer Provider\ntrace_api.set_tracer_provider(tracer_provider)\n</pre> from opentelemetry import trace as trace_api from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter from opentelemetry.sdk import trace as trace_sdk from opentelemetry.sdk.trace.export import SimpleSpanProcessor  # Set up the Tracer Provider tracer_provider = trace_sdk.TracerProvider()  # Define the OTLP Span Exporter with the endpoint span_exporter = OTLPSpanExporter(endpoint)  # Set up the Span Processor to process and export spans span_processor = SimpleSpanProcessor(span_exporter)  # Add the Span Processor to the Tracer Provider tracer_provider.add_span_processor(span_processor)  # Set the global Tracer Provider trace_api.set_tracer_provider(tracer_provider) <p>OpenTelemetry Libraries</p> <p>In the provided code, several OpenTelemetry libraries are used to set up tracing. Here's an overview of each:</p> <ul> <li><p><code>opentelemetry</code>:</p> <p>**Purpose**: The core library for OpenTelemetry, providing APIs for tracing and metrics.</p> <p>Usage: It includes the trace module, which is used to create and manage traces.</p> </li> <li><p><code>opentelemetry.exporter.otlp.proto.http.trace_exporter</code>:</p> <p>Purpose: Provides the OTLP (OpenTelemetry Protocol) exporter for traces using HTTP.</p> <p>Usage: The <code>OTLPSpanExporter</code> class in this module sends trace data to an OTLP-compatible backend. This exporter is configured with an endpoint where trace data will be sent.</p> </li> <li><p><code>opentelemetry.sdk.trace</code>:</p> <p>Purpose: Contains the SDK implementations for tracing, including the <code>TracerProvider</code>.</p> <p>Usage:</p> <ul> <li><p><code>TracerProvider</code>: Manages Tracer instances and is responsible for exporting spans (units of work) collected during tracing.</p> </li> <li><p><code>SimpleSpanProcessor</code>: A span processor that exports spans synchronously, used to process and send trace data to the exporter.</p> </li> </ul> </li> <li><p><code>opentelemetry.sdk.trace.export</code>:</p> <p>Purpose: Provides classes for exporting trace data.</p> <p>Usage:</p> <ul> <li><code>SimpleSpanProcessor</code>: Processes spans and exports them using the specified exporter. It ensures that spans are sent to the backend for analysis.</li> </ul> </li> </ul> In\u00a0[14]: Copied! <pre>from openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Instantiate and apply instrumentation for OpenAI\nOpenAIInstrumentor().instrument()\n</pre> from openinference.instrumentation.openai import OpenAIInstrumentor  # Instantiate and apply instrumentation for OpenAI OpenAIInstrumentor().instrument() <ul> <li><p><code>OpenAIInstrumentor</code>: A class from the openinference library designed to instrument OpenAI's API calls, enabling tracing and observability.</p> </li> <li><p><code>instrument()</code>: This method configures the OpenAI API client to automatically generate and send trace data to the OpenTelemetry backend. It integrates with the tracing setup you have configured, allowing you to monitor and analyze interactions with OpenAI's models.</p> </li> </ul> <p>By running this code, you ensure that all OpenAI API calls are traced, allowing you to capture detailed insights into model usage and performance.</p> In\u00a0[\u00a0]: Copied! <pre># Create an OpenAI client instance\nclient = openai.OpenAI()\n\n# Make a request to the OpenAI API for a chat completion\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}],\n)\n\n# Print the content of the response\nprint(response.choices[0].message.content)\n</pre> # Create an OpenAI client instance client = openai.OpenAI()  # Make a request to the OpenAI API for a chat completion response = client.chat.completions.create(     model=\"gpt-4o\",     messages=[{\"role\": \"user\", \"content\": \"Write a haiku.\"}], )  # Print the content of the response print(response.choices[0].message.content) <ul> <li><p><code>openai.OpenAI()</code>: Initializes an OpenAI client instance that can be used to interact with the OpenAI API.</p> </li> <li><p><code>client.chat.completions.create()</code>: Sends a request to the OpenAI API to create a chat completion using the specified model.</p> <ul> <li><p><code>model=\"gpt-4o\"</code>: Specifies the model to use for generating completions. Ensure the model name is correct and available in your OpenAI API account.</p> </li> <li><p><code>messages</code>: A list of message objects representing the conversation history. In this case, it includes a single message from the user asking to \"Write a haiku.\"</p> </li> </ul> </li> </ul> <p><code>response.choices[0].message.content</code>: Extracts and prints the content of the completion response generated by the model.</p> In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index\n# !pip install llama-index-core\n# !pip install llama-index-llms-openai\n# !pip install openinference-instrumentation-llama-index==2.2.4\n# !pip install -U llama-index-callbacks-arize-phoenix\n# !pip install \"arize-phoenix[llama-index]\"\n</pre> # !pip install llama-index # !pip install llama-index-core # !pip install llama-index-llms-openai # !pip install openinference-instrumentation-llama-index==2.2.4 # !pip install -U llama-index-callbacks-arize-phoenix # !pip install \"arize-phoenix[llama-index]\" <ul> <li><p><code>llama-index</code>: Core package for Llama Index functionality.</p> </li> <li><p><code>llama-index-core</code>: Provides core features and utilities for Llama Index.</p> </li> <li><p><code>llama-index-llms-openai</code>: Integration package for Llama Index and OpenAI models.</p> </li> <li><p><code>openinference-instrumentation-llama-index==2.2.4</code>: Provides instrumentation for tracing Llama Index interactions.</p> </li> <li><p><code>llama-index-callbacks-arize-phoenix</code>: Callback integration for Arize Phoenix with Llama Index.</p> </li> <li><p><code>arize-phoenix[llama-index]</code>: Extends Arize Phoenix to support Llama Index tracing.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># Retrieve the URL of the active Phoenix session\npx.active_session().url\n</pre> # Retrieve the URL of the active Phoenix session px.active_session().url <p>Accesses the current active session of the Phoenix app and retrieves its URL, allowing you to view or share the link to the Phoenix interface where you can monitor and analyze trace data.</p> In\u00a0[\u00a0]: Copied! <pre>from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\n# Set up the Tracer Provider\ntracer_provider = trace_sdk.TracerProvider()\n\n# Add Span Processor to the Tracer Provider\ntracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\n# Instrument Llama Index with the Tracer Provider\nLlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n</pre> from openinference.instrumentation.llama_index import LlamaIndexInstrumentor from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter from opentelemetry.sdk import trace as trace_sdk from opentelemetry.sdk.trace.export import SimpleSpanProcessor  # Set up the Tracer Provider tracer_provider = trace_sdk.TracerProvider()  # Add Span Processor to the Tracer Provider tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))  # Instrument Llama Index with the Tracer Provider LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider) <ul> <li><p><code>LlamaIndexInstrumentor</code>: This class from openinference.instrumentation.llama_index instruments Llama Index to support tracing and observability.</p> </li> <li><p><code>trace_sdk.TracerProvider()</code>: Initializes a new Tracer Provider responsible for creating and managing trace data. OTLPSpanExporter(endpoint): Configures the OTLP exporter to send trace data to the specified endpoint.</p> </li> <li><p><code>SimpleSpanProcessor</code>: Processes and exports spans synchronously.</p> </li> <li><p><code>tracer_provider.add_span_processor</code>: Adds the span processor to the Tracer Provider.</p> </li> <li><p><code>LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)</code>: Applies the instrumentation to Llama Index, using the provided Tracer Provider for tracing.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\n\n# Initialize the OpenAI model\nllm = OpenAI(model=\"gpt-4o-mini\")\n\n# Make a completion request\nresp = llm.complete(\"Paul Graham is \")\n\n# Print the response\nprint(resp)\n</pre> from llama_index.llms.openai import OpenAI  # Initialize the OpenAI model llm = OpenAI(model=\"gpt-4o-mini\")  # Make a completion request resp = llm.complete(\"Paul Graham is \")  # Print the response print(resp) <ul> <li><p><code>from llama_index.llms.openai import OpenAI</code>: Imports the OpenAI class from the llama_index package, allowing interaction with OpenAI models.</p> </li> <li><p><code>OpenAI(model=\"gpt-4o-mini\")</code>: Initializes an instance of the OpenAI class with the specified model (e.g., gpt-4).</p> </li> <li><p><code>llm.complete(...)</code>: Sends a prompt to the model to generate a completion based on the input text.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.core.llms import ChatMessage\n\n# Initialize the OpenAI model\nllm = OpenAI()\n\n# Define the chat messages\nmessages = [\n    ChatMessage(\n        role=\"system\", content=\"You are a pirate with a colorful personality\"\n    ),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\n\n# Get the response from the model\nresp = llm.chat(messages)\n</pre> from llama_index.llms.openai import OpenAI from llama_index.core.llms import ChatMessage  # Initialize the OpenAI model llm = OpenAI()  # Define the chat messages messages = [     ChatMessage(         role=\"system\", content=\"You are a pirate with a colorful personality\"     ),     ChatMessage(role=\"user\", content=\"What is your name\"), ]  # Get the response from the model resp = llm.chat(messages) <ul> <li><p><code>OpenAI</code>: A class for interacting with OpenAI models.</p> </li> <li><p><code>ChatMessage</code>: A class to format chat messages.</p> </li> <li><p><code>OpenAI()</code>: Initializes an instance of the OpenAI class.</p> </li> <li><p><code>ChatMessage</code>: Creates chat message objects with a specified role (e.g., \"system\", \"user\") and content.</p> <ul> <li><code>role=\"system\"</code>: Defines the system message to set the context or personality of the model.</li> <li><code>role=\"user\"</code>: Represents a user message in the chat.</li> </ul> </li> <li><p><code>llm.chat(messages)</code>: Sends the defined messages to the model and retrieves the response.</p> </li> </ul> <p>This code sets up a chat with an OpenAI model, specifying system and user messages to guide the interaction.</p> In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index\n# !pip install llama-index-vector-stores-qdrant \n# !pip install llama-index-readers-file \n# !pip install llama-index-embeddings-fastembed \n# !pip install llama-index-llms-openai\n# !pip install -U qdrant_client fastembed\n</pre> # !pip install llama-index # !pip install llama-index-vector-stores-qdrant  # !pip install llama-index-readers-file  # !pip install llama-index-embeddings-fastembed  # !pip install llama-index-llms-openai # !pip install -U qdrant_client fastembed <ul> <li><p><code>llama-index</code>: Core package for Llama Index functionality.</p> </li> <li><p><code>llama-index-vector-stores-qdrant</code>: Integration for using Qdrant as a vector store with Llama Index.</p> </li> <li><p><code>llama-index-readers-file</code>: Provides file reading capabilities for Llama Index.</p> </li> <li><p><code>llama-index-embeddings-fastembed</code>: Adds FastEmbed support for generating embeddings with Llama Index.</p> </li> <li><p><code>llama-index-llms-openai</code>: Integration for using OpenAI models with Llama Index.</p> </li> <li><p><code>qdrant_client</code>: Client library for interacting with Qdrant, a vector search engine.</p> </li> <li><p><code>fastembed</code>: Library for generating embeddings quickly.</p> </li> </ul> In\u00a0[20]: Copied! <pre>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\n# from llama_index.embeddings.openai import OpenAIEmbedding\n\nfrom llama_index.core.settings import Settings\n\nSettings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n# Settings.embed_model = OpenAIEmbedding(embed_batch_size=10)\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n</pre> from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.embeddings.fastembed import FastEmbedEmbedding # from llama_index.embeddings.openai import OpenAIEmbedding  from llama_index.core.settings import Settings  Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\") # Settings.embed_model = OpenAIEmbedding(embed_batch_size=10)  documents = SimpleDirectoryReader(\"data\").load_data() index = VectorStoreIndex.from_documents(documents) <ul> <li><p><code>VectorStoreIndex</code>: A class used to create and manage a vector store index. This index allows for efficient similarity search and retrieval based on vector representations of documents.</p> </li> <li><p><code>SimpleDirectoryReader</code>: A class for loading documents from a specified directory. It reads and preprocesses files from the directory \"sample_data\" to be used in the indexing process.</p> </li> <li><p><code>FastEmbedEmbedding</code>: Provides an embedding model for generating vector representations of text using the FastEmbed library. The model specified (\"BAAI/bge-base-en-v1.5\") is used to convert documents into embeddings, which are then used for similarity search within the vector store index.</p> </li> <li><p><code>from llama_index.embeddings.openai import OpenAIEmbedding</code>:</p> <p><code>OpenAIEmbedding</code>: (Commented out) Provides an embedding model for generating vector representations using OpenAI\u2019s embeddings. Uncomment this line if you wish to use OpenAI\u2019s model instead of FastEmbed. This model can be configured with parameters like <code>embed_batch_size</code> for batch processing.</p> </li> <li><p><code>Settings</code>: A configuration class used to set global settings for embedding models. By assigning the embed_model attribute, you specify which embedding model to use for the RAG pipeline.</p> </li> <li><p><code>Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")</code>: Configures the Settings class to use the FastEmbed model for generating embeddings. This step is crucial for defining how text data will be represented in the vector store.</p> </li> <li><p><code>documents = SimpleDirectoryReader(\"data\").load_data()</code>: Loads and preprocesses documents (in this case) from the \"data\" directory. Please ensure to tweak the directory name according to your project. The <code>load_data()</code> method reads all files in the specified directory and prepares them for indexing.</p> </li> <li><p><code>index = VectorStoreIndex.from_documents(documents)</code>: Creates a VectorStoreIndex from the preprocessed documents. This index allows for efficient querying and retrieval based on the vector representations generated by the embedding model.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n</pre> query_engine = index.as_query_engine() response = query_engine.query(\"What did the author do growing up?\") print(response) <ul> <li><p><code>as_query_engine()</code>: Converts the <code>VectorStoreIndex</code> into a query engine. This engine allows you to perform searches and retrieve information based on the vector representations of documents stored in the index.</p> </li> <li><p><code>query()</code>: Executes a query against the vector store index. The query string \"What did the author do growing up?\" is used to search for relevant documents and retrieve information based on the context provided by the vector embeddings.</p> </li> </ul> <p>Finally, the <code>response</code> containing the information retrieved from the vector store index, which is based on the query and the indexed documents is output.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li>Introduction</li> <li>Getting Started</li> <li>Usage</li> <li>Conclusion</li> </ol>"},{"location":"RAG/01_RAG_Observability/notebook/#introduction","title":"Introduction\u00b6","text":"<p>This guide provides a comprehensive walkthrough for configuring the necessary tools and libraries, including embedding models and vector store indexing, to enable efficient document retrieval and query processing. We\u2019ll cover everything from installation and setup to querying and retrieving relevant information, equipping you with the knowledge to harness the power of RAG pipelines for advanced search capabilities.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#getting-started","title":"Getting Started\u00b6","text":"<p>To get started with this notebook, you'll need to have a basic understanding of Python and some familiarity with machine learning concepts. Don't worry if you're new to some of these ideas \u2013 we'll guide you through each step!</p>"},{"location":"RAG/01_RAG_Observability/notebook/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Python 3.7+</li> <li>Jupyter Notebook or JupyterLab</li> <li>Basic knowledge of Python and machine learning concepts</li> </ul>"},{"location":"RAG/01_RAG_Observability/notebook/#usage","title":"Usage\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#1-setup","title":"1. Setup\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#11-install-required-packages","title":"1.1 Install required packages\u00b6","text":"<p>To get started with setting up Arize Phoenix, you'll need to install the required packages.</p> <p>Arize Phoenix is a comprehensive tool designed for observability and monitoring in machine learning and AI systems. It provides functionalities for tracking and analyzing various aspects of machine learning models and data pipelines.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#12-setting-up-arize-phoenix","title":"1.2 Setting up Arize Phoenix\u00b6","text":"<p>There are 3 ways to do this:</p> <p>Read more here.</p> <ul> <li><p>Command Line</p> <pre>python3 -m phoenix.server.main serve\n</pre> </li> <li><p>Docker</p> <p>Launch the phoenix docker image using:</p> <pre>docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest\n</pre> <p>This will expose the Phoenix UI and REST API on localhost:6006 and exposes the gRPC endpoint for spans on localhost:4317.</p> </li> <li><p>Notebook</p> <pre>import phoenix as px\npx.launch_app()\n</pre> </li> </ul>"},{"location":"RAG/01_RAG_Observability/notebook/#13-import-required-libraries-and-configure-the-environment","title":"1.3 Import Required Libraries and Configure the Environment\u00b6","text":"<p>Before proceeding with data processing and evaluation, import the necessary libraries and set up the environment:</p>"},{"location":"RAG/01_RAG_Observability/notebook/#14-launch-the-phoenix-app","title":"1.4 Launch the Phoenix App\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#15-view-the-phoenix-app-session","title":"1.5 View the Phoenix App Session\u00b6","text":"<p>Once the Phoenix app is launched, you can use the session object to interact with the app directly in the notebook. Run the following code to launch the Phoenix app and view it in the current session:</p>"},{"location":"RAG/01_RAG_Observability/notebook/#16-set-up-the-endpoint-for-traces","title":"1.6 Set Up the Endpoint for Traces\u00b6","text":"<p>To send traces to the Phoenix app for analysis and observability, define the endpoint URL where the Phoenix app is listening for incoming data.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#2-trace-open-ai","title":"2. Trace Open AI\u00b6","text":"<p>For more integration, read.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#21-install-and-import-the-openai-package","title":"2.1 Install and Import the OpenAI Package\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#22-configure-the-openai-api-key","title":"2.2 Configure the OpenAI API Key\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#23-set-up-opentelemetry-for-tracing","title":"2.3 Set Up OpenTelemetry for Tracing\u00b6","text":"<p>To enable tracing for your OpenAI interactions, configure OpenTelemetry with the necessary components.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#24-instrument-openai-with-openinference","title":"2.4 Instrument OpenAI with OpenInference\u00b6","text":"<p>To integrate OpenTelemetry with OpenAI and enable tracing for OpenAI model interactions, use the <code>OpenAIInstrumentor</code> from the <code>openinference</code> library.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#25-make-a-request-to-openai-api","title":"2.5 Make a Request to OpenAI API\u00b6","text":"<p>To interact with OpenAI\u2019s API and obtain a response, use the following code. This example demonstrates how to create a chat completion using the OpenAI API and print the result:</p>"},{"location":"RAG/01_RAG_Observability/notebook/#3-trace-llama-index","title":"3. Trace Llama index\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#31-install-and-import-the-required-libraries","title":"3.1 Install and Import the Required Libraries\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#32-retrieve-the-url-of-the-active-phoenix-session","title":"3.2 Retrieve the URL of the Active Phoenix Session\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#33-set-up-tracing-for-llama-index","title":"3.3 Set Up Tracing for Llama Index\u00b6","text":"<p>To instrument Llama Index for tracing with OpenTelemetry, configure the tracer provider and integrate the Llama Index instrumentor.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#34-interact-with-llama-index-using-openai","title":"3.4 Interact with Llama Index Using OpenAI\u00b6","text":"<p>To perform a completion request with Llama Index using an OpenAI model, use the following code:</p>"},{"location":"RAG/01_RAG_Observability/notebook/#35-perform-a-chat-interaction-with-llama-index-using-openai","title":"3.5 Perform a Chat Interaction with Llama Index Using OpenAI\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#4-observe-rag-pipeline","title":"4. Observe RAG Pipeline\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#41-setup-an-environment-for-observing-a-rag-piepline","title":"4.1 Setup an environment for observing a RAG piepline\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#42-prepare-rag-pipeline-with-embeddings-and-document-indexing","title":"4.2 Prepare RAG Pipeline with Embeddings and Document Indexing\u00b6","text":""},{"location":"RAG/01_RAG_Observability/notebook/#43-query-the-vector-store-index","title":"4.3 Query the Vector Store Index\u00b6","text":"<p>Once the vector store index is set up, you can use it to perform queries and retrieve relevant information.</p>"},{"location":"RAG/01_RAG_Observability/notebook/#conclusion","title":"Conclusion\u00b6","text":"<p>In this guide, we have set up a Retrieval-Augmented Generation (RAG) pipeline using Llama Index and integrated it with various components to observe its functionality. We began by configuring and installing the necessary libraries, including Llama Index, OpenTelemetry, and various embedding models.</p> <p>We then proceeded to:</p> <ul> <li>Initialize and configure the embedding models, using FastEmbed or OpenAI models as needed.</li> <li>Load and index documents from a directory to prepare the data for querying.</li> <li>Set up a query engine to perform searches and retrieve relevant information based on the indexed documents.</li> </ul> <p>By following these steps, you have successfully prepared a RAG pipeline capable of efficient document retrieval and query processing. This setup enables advanced search and information retrieval capabilities, leveraging the power of vector-based embeddings and indexing.</p> <p>Feel free to experiment with different configurations and queries to further explore the capabilities of the RAG pipeline. For any questions or additional customization, consult the documentation of the libraries used or seek further guidance.</p> <p>If you find this guide helpful, please consider giving us a star on GitHub! \u2b50</p> <p></p> <p>Thank you for following this guide, and happy querying!</p>"},{"location":"RAG/02_ReRanker_RAG/","title":"Index","text":"<p>Re Ranker</p> <pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Document Embedding\"\n        EM1{{Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; EM1\n        EM1 --&gt; D1[Embedding-1] &amp; D2[Embedding-2] &amp; D3[Embedding-3]\n    end\n\n    subgraph \"3. Indexing\"\n        D1 &amp; D2 &amp; D3 --&gt; E[(VectorDB)]\n    end\n\n    subgraph \"4. Query Processing\"\n        F[Query] --&gt; EM2{{Embedding Model}}\n        EM2 --&gt; G[Query Embedding]\n    end\n\n    subgraph \"5. Retrieval\"\n        G --&gt;|Similarity Search| E\n        E --&gt;|Top-K Retrieval| H[Top-K Chunks]\n    end\n\n    subgraph \"6. ReRanking\"\n        H --&gt; RR{{ReRanker Model}}\n        RR --&gt; I[Reranked Chunks]\n    end\n\n    subgraph \"7. Context Formation\"\n        I --&gt; J[Query + Reranked Chunks]\n    end\n\n    subgraph \"8. Generation\"\n        J --&gt; K[LLM]\n        K --&gt; L[Response]\n    end\n\n    F --&gt; J\n\n    %% Highlighting the difference between Top-K and Reranked chunks\n    H -.-&gt; |Before ReRanking|M([Top-K: Chunk-2, Chunk-5, Chunk-1, Chunk-7, Chunk-3])\n    I -.-&gt; |After ReRanking|N([Reranked: Chunk-5, Chunk-1, Chunk-7, Chunk-2, Chunk-3])\n</code></pre>"},{"location":"RAG/02_ReRanker_RAG/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport openai\nimport chainlit as cl\nimport argparse\nfrom dotenv import load_dotenv\nfrom llama_index.core import (\n    Settings,\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n)\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.postprocessor import LLMRerank\n</pre> import os import openai import chainlit as cl import argparse from dotenv import load_dotenv from llama_index.core import (     Settings,     VectorStoreIndex,     SimpleDirectoryReader, ) from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.core.callbacks import CallbackManager from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline from llama_index.core.postprocessor import LLMRerank In\u00a0[\u00a0]: Copied! <pre># LlamaIndex vector store import\nimport qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n</pre> # LlamaIndex vector store import import qdrant_client from llama_index.vector_stores.qdrant import QdrantVectorStore In\u00a0[\u00a0]: Copied! <pre># Load environment variables from .env file\nprint(\"Loading Environment variables\")\nload_dotenv()\n</pre> # Load environment variables from .env file print(\"Loading Environment variables\") load_dotenv() In\u00a0[\u00a0]: Copied! <pre># Set OpenAI API key\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = OPENAI_API_KEY\n</pre> # Set OpenAI API key OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") openai.api_key = OPENAI_API_KEY In\u00a0[\u00a0]: Copied! <pre># Configure LLM settings\nSettings.llm = OpenAI(\n    model=\"gpt-4-1106-preview\",\n    temperature=0.1,\n    max_tokens=1024,\n    streaming=True,\n    api_key=OPENAI_API_KEY,\n)\n</pre> # Configure LLM settings Settings.llm = OpenAI(     model=\"gpt-4-1106-preview\",     temperature=0.1,     max_tokens=1024,     streaming=True,     api_key=OPENAI_API_KEY, ) In\u00a0[\u00a0]: Copied! <pre>Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\nSettings.context_window = 4096\nSettings.callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()])\n</pre> Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY) Settings.context_window = 4096 Settings.callback_manager = CallbackManager([cl.LlamaIndexCallbackHandler()]) In\u00a0[\u00a0]: Copied! <pre># Connect to the Vector Database\nprint(\"Connecting to Vector Database\")\nclient = qdrant_client.QdrantClient(\n    host=\"localhost\",\n    port=6333\n)\n</pre> # Connect to the Vector Database print(\"Connecting to Vector Database\") client = qdrant_client.QdrantClient(     host=\"localhost\",     port=6333 ) In\u00a0[\u00a0]: Copied! <pre># Initialize the vector store\nvector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\")\n</pre> # Initialize the vector store vector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\") In\u00a0[\u00a0]: Copied! <pre>def ingest_documents(data_dir):\n    documents = SimpleDirectoryReader(data_dir, recursive=True).load_data(\n        show_progress=True\n    )\n    print(\"Ingesting Data\")\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n            Settings.embed_model,\n        ],\n        vector_store=vector_store,\n    )\n    nodes = pipeline.run(documents=documents, show_progress=True)\n    print(\"Number of chunks added to vector DB:\", len(nodes))\n</pre> def ingest_documents(data_dir):     documents = SimpleDirectoryReader(data_dir, recursive=True).load_data(         show_progress=True     )     print(\"Ingesting Data\")     pipeline = IngestionPipeline(         transformations=[             SentenceSplitter(chunk_size=1024, chunk_overlap=20),             Settings.embed_model,         ],         vector_store=vector_store,     )     nodes = pipeline.run(documents=documents, show_progress=True)     print(\"Number of chunks added to vector DB:\", len(nodes)) In\u00a0[\u00a0]: Copied! <pre># Global variable to store the index\nindex = None\n</pre> # Global variable to store the index index = None In\u00a0[\u00a0]: Copied! <pre>def get_reranker(rerank_method):\n    if rerank_method == \"llm\":\n        return LLMRerank(choice_batch_size=5, top_n=2)\n    elif rerank_method == \"cohere\":\n        try:\n            from llama_index.postprocessor.cohere_rerank import CohereRerank\n        except:\n            raise \"Cohere reranker package missiong please install : pip install llama-index-postprocessor-cohere-rerank\"\n        cohere_api_key = os.environ.get(\"COHERE_API_KEY\")\n        if not cohere_api_key:\n            raise ValueError(\"COHERE_API_KEY not found in environment variables\")\n        return CohereRerank(api_key=cohere_api_key, top_n=2)\n    elif rerank_method == \"colbert\":\n        try:\n            from llama_index.postprocessor.colbert_rerank import ColbertRerank\n        except:\n            raise \"colbertreranker package missiong please install : pip install llama-index-postprocessor-colbert-rerank\"\n        return ColbertRerank(\n            top_n=5,\n            model=\"colbert-ir/colbertv2.0\",\n            tokenizer=\"colbert-ir/colbertv2.0\",\n            keep_retrieval_score=True,\n        )\n    elif rerank_method == \"sentence_transformer\":\n        from llama_index.core.postprocessor import SentenceTransformerRerank\n        return SentenceTransformerRerank(\n            model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n        )\n    else:\n        return None\n</pre> def get_reranker(rerank_method):     if rerank_method == \"llm\":         return LLMRerank(choice_batch_size=5, top_n=2)     elif rerank_method == \"cohere\":         try:             from llama_index.postprocessor.cohere_rerank import CohereRerank         except:             raise \"Cohere reranker package missiong please install : pip install llama-index-postprocessor-cohere-rerank\"         cohere_api_key = os.environ.get(\"COHERE_API_KEY\")         if not cohere_api_key:             raise ValueError(\"COHERE_API_KEY not found in environment variables\")         return CohereRerank(api_key=cohere_api_key, top_n=2)     elif rerank_method == \"colbert\":         try:             from llama_index.postprocessor.colbert_rerank import ColbertRerank         except:             raise \"colbertreranker package missiong please install : pip install llama-index-postprocessor-colbert-rerank\"         return ColbertRerank(             top_n=5,             model=\"colbert-ir/colbertv2.0\",             tokenizer=\"colbert-ir/colbertv2.0\",             keep_retrieval_score=True,         )     elif rerank_method == \"sentence_transformer\":         from llama_index.core.postprocessor import SentenceTransformerRerank         return SentenceTransformerRerank(             model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3         )     else:         return None In\u00a0[\u00a0]: Copied! <pre>@cl.on_chat_start\nasync def start():\n    global index\n    if index is None:\n        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n    rerank_method = cl.user_session.get(\"rerank_method\")\n    reranker = get_reranker(rerank_method)\n\n    query_engine = index.as_query_engine(\n        streaming=True,\n        similarity_top_k=10,\n        node_postprocessors=[reranker] if reranker else [],\n    )\n    cl.user_session.set(\"query_engine\", query_engine)\n\n    await cl.Message(\n        author=\"Assistant\", content=\"Hello! I'm an AI assistant. How may I help you?\"\n    ).send()\n</pre> @cl.on_chat_start async def start():     global index     if index is None:         index = VectorStoreIndex.from_vector_store(vector_store=vector_store)      rerank_method = cl.user_session.get(\"rerank_method\")     reranker = get_reranker(rerank_method)      query_engine = index.as_query_engine(         streaming=True,         similarity_top_k=10,         node_postprocessors=[reranker] if reranker else [],     )     cl.user_session.set(\"query_engine\", query_engine)      await cl.Message(         author=\"Assistant\", content=\"Hello! I'm an AI assistant. How may I help you?\"     ).send() In\u00a0[\u00a0]: Copied! <pre>@cl.on_message\nasync def handle_message(message: cl.Message):\n    global index\n    if message.elements:\n        for file in message.elements:\n            if file.type == \"file\":\n                documents = SimpleDirectoryReader(input_files=[file.path]).load_data()\n                pipeline = IngestionPipeline(\n                    transformations=[\n                        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n                        Settings.embed_model,\n                    ],\n                    vector_store=vector_store,\n                )\n                nodes = pipeline.run(documents=documents, show_progress=True)\n                index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n                \n                rerank_method = cl.user_session.get(\"rerank_method\")\n                reranker = get_reranker(rerank_method)\n                \n                query_engine = index.as_query_engine(\n                    streaming=True,\n                    similarity_top_k=10,\n                    node_postprocessors=[reranker] if reranker else [],\n                )\n                cl.user_session.set(\"query_engine\", query_engine)\n\n                await cl.Message(\n                    content=f\"Processed {len(nodes)} chunks from the uploaded file.\"\n                ).send()\n\n    query_engine = cl.user_session.get(\"query_engine\")\n    msg = cl.Message(content=\"\", author=\"Assistant\")\n    res = await cl.make_async(query_engine.query)(message.content)\n\n    for token in res.response_gen:\n        await msg.stream_token(token)\n    await msg.send()\n</pre> @cl.on_message async def handle_message(message: cl.Message):     global index     if message.elements:         for file in message.elements:             if file.type == \"file\":                 documents = SimpleDirectoryReader(input_files=[file.path]).load_data()                 pipeline = IngestionPipeline(                     transformations=[                         SentenceSplitter(chunk_size=1024, chunk_overlap=20),                         Settings.embed_model,                     ],                     vector_store=vector_store,                 )                 nodes = pipeline.run(documents=documents, show_progress=True)                 index = VectorStoreIndex.from_vector_store(vector_store=vector_store)                                  rerank_method = cl.user_session.get(\"rerank_method\")                 reranker = get_reranker(rerank_method)                                  query_engine = index.as_query_engine(                     streaming=True,                     similarity_top_k=10,                     node_postprocessors=[reranker] if reranker else [],                 )                 cl.user_session.set(\"query_engine\", query_engine)                  await cl.Message(                     content=f\"Processed {len(nodes)} chunks from the uploaded file.\"                 ).send()      query_engine = cl.user_session.get(\"query_engine\")     msg = cl.Message(content=\"\", author=\"Assistant\")     res = await cl.make_async(query_engine.query)(message.content)      for token in res.response_gen:         await msg.stream_token(token)     await msg.send() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    import sys\n    import subprocess\n\n    parser = argparse.ArgumentParser(description=\"RAG Script with ingestion and reranking options\")\n    parser.add_argument(\"--host\", default=\"localhost\", help='Host IP address')\n    parser.add_argument(\"--port\", type=int, default=8000, help='Port number')\n    parser.add_argument('--ingest', action='store_true', help='Ingest documents before starting the chat')\n    parser.add_argument('--data_dir', type=str, default=\"../data\", help='Directory containing documents to ingest')\n    parser.add_argument('--rerank', choices=['llm', 'cohere', 'colbert', 'sentence_transformer'], \n                        default=None, help='Choose reranking method')\n    args = parser.parse_args()\n\n    if args.ingest:\n        ingest_documents(args.data_dir)\n\n    # Set the rerank method in the environment for Chainlit to access\n    os.environ[\"RERANK_METHOD\"] = args.rerank if args.rerank else \"\"\n\n    # Run the Chainlit app with specified host and port\n    subprocess.run([\n        \"chainlit\", \"run\", sys.argv[0],\n        \"--host\", args.host,\n        \"--port\", str(args.port)\n    ], check=True)\n</pre> if __name__ == \"__main__\":     import sys     import subprocess      parser = argparse.ArgumentParser(description=\"RAG Script with ingestion and reranking options\")     parser.add_argument(\"--host\", default=\"localhost\", help='Host IP address')     parser.add_argument(\"--port\", type=int, default=8000, help='Port number')     parser.add_argument('--ingest', action='store_true', help='Ingest documents before starting the chat')     parser.add_argument('--data_dir', type=str, default=\"../data\", help='Directory containing documents to ingest')     parser.add_argument('--rerank', choices=['llm', 'cohere', 'colbert', 'sentence_transformer'],                          default=None, help='Choose reranking method')     args = parser.parse_args()      if args.ingest:         ingest_documents(args.data_dir)      # Set the rerank method in the environment for Chainlit to access     os.environ[\"RERANK_METHOD\"] = args.rerank if args.rerank else \"\"      # Run the Chainlit app with specified host and port     subprocess.run([         \"chainlit\", \"run\", sys.argv[0],         \"--host\", args.host,         \"--port\", str(args.port)     ], check=True)"},{"location":"RAG/02_ReRanker_RAG/notebook/","title":"ReRanker RAG(Llamaindex)","text":"ReRanker RAG AI Engineering.academy In\u00a0[3]: Copied! <pre>!pip install -U nest-asyncio\n!pip install -U llama-index\n!pip install -U llama-index-vector-stores-qdrant \n!pip install -U llama-index-readers-file \n!pip install -U llama-index-embeddings-fastembed \n!pip install -U llama-index-llms-openai\n!pip install -U llama-index-llms-groq\n!pip install -U qdrant_client fastembed\n!pip install -U python-dotenv\n</pre> !pip install -U nest-asyncio !pip install -U llama-index !pip install -U llama-index-vector-stores-qdrant  !pip install -U llama-index-readers-file  !pip install -U llama-index-embeddings-fastembed  !pip install -U llama-index-llms-openai !pip install -U llama-index-llms-groq !pip install -U qdrant_client fastembed !pip install -U python-dotenv <pre>Requirement already satisfied: nest-asyncio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (1.6.0)\nRequirement already satisfied: llama-index in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.10.51)\nCollecting llama-index\n  Using cached llama_index-0.11.10-py3-none-any.whl.metadata (11 kB)\nCollecting llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1 (from llama-index)\n  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-cli&lt;0.4.0,&gt;=0.3.1 (from llama-index)\n  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.10 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index) (0.11.10)\nCollecting llama-index-embeddings-openai&lt;0.3.0,&gt;=0.2.4 (from llama-index)\n  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\nCollecting llama-index-indices-managed-llama-cloud&gt;=0.3.0 (from llama-index)\n  Using cached llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: llama-index-legacy&lt;0.10.0,&gt;=0.9.48 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index) (0.9.48.post3)\nCollecting llama-index-llms-openai&lt;0.3.0,&gt;=0.2.3 (from llama-index)\n  Using cached llama_index_llms_openai-0.2.9-py3-none-any.whl.metadata (648 bytes)\nCollecting llama-index-multi-modal-llms-openai&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-program-openai&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\nCollecting llama-index-question-gen-openai&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\nCollecting llama-index-readers-file&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse&gt;=0.3.0 (from llama-index)\n  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index) (3.9.1)\nRequirement already satisfied: openai&gt;=1.14.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1-&gt;llama-index) (1.44.0)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.3)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.16.0)\nCollecting llama-cloud&gt;=0.0.11 (from llama-index-indices-managed-llama-cloud&gt;=0.3.0-&gt;llama-index)\n  Using cached llama_cloud-0.0.17-py3-none-any.whl.metadata (751 bytes)\nRequirement already satisfied: pandas in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2.2.2)\nRequirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (4.12.3)\nRequirement already satisfied: pypdf&lt;5.0.0,&gt;=4.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (4.3.1)\nRequirement already satisfied: striprtf&lt;0.0.27,&gt;=0.0.26 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (0.0.26)\nCollecting llama-parse&gt;=0.5.0 (from llama-index-readers-llama-parse&gt;=0.3.0-&gt;llama-index)\n  Using cached llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index) (2024.7.24)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.0.3)\nRequirement already satisfied: soupsieve&gt;1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.3-&gt;llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (2.6)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.4.0)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.0.5)\nRequirement already satisfied: idna in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.8)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.14.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&gt;=1.14.0-&gt;llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1-&gt;llama-index) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&gt;=1.14.0-&gt;llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1-&gt;llama-index) (0.5.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.3.2)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.22.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2024.1)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.2.2)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (23.2)\nRequirement already satisfied: six&gt;=1.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (1.16.0)\nUsing cached llama_index-0.11.10-py3-none-any.whl (6.8 kB)\nUsing cached llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\nUsing cached llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\nUsing cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\nUsing cached llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl (10 kB)\nUsing cached llama_index_llms_openai-0.2.9-py3-none-any.whl (12 kB)\nUsing cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl (5.9 kB)\nUsing cached llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\nUsing cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\nUsing cached llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\nUsing cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\nUsing cached llama_cloud-0.0.17-py3-none-any.whl (187 kB)\nUsing cached llama_parse-0.5.6-py3-none-any.whl (10 kB)\nInstalling collected packages: llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n  Attempting uninstall: llama-cloud\n    Found existing installation: llama-cloud 0.0.6\n    Uninstalling llama-cloud-0.0.6:\n      Successfully uninstalled llama-cloud-0.0.6\n  Attempting uninstall: llama-parse\n    Found existing installation: llama-parse 0.4.9\n    Uninstalling llama-parse-0.4.9:\n      Successfully uninstalled llama-parse-0.4.9\n  Attempting uninstall: llama-index-readers-file\n    Found existing installation: llama-index-readers-file 0.1.25\n    Uninstalling llama-index-readers-file-0.1.25:\n      Successfully uninstalled llama-index-readers-file-0.1.25\n  Attempting uninstall: llama-index-llms-openai\n    Found existing installation: llama-index-llms-openai 0.1.24\n    Uninstalling llama-index-llms-openai-0.1.24:\n      Successfully uninstalled llama-index-llms-openai-0.1.24\n  Attempting uninstall: llama-index-indices-managed-llama-cloud\n    Found existing installation: llama-index-indices-managed-llama-cloud 0.2.4\n    Uninstalling llama-index-indices-managed-llama-cloud-0.2.4:\n      Successfully uninstalled llama-index-indices-managed-llama-cloud-0.2.4\n  Attempting uninstall: llama-index-embeddings-openai\n    Found existing installation: llama-index-embeddings-openai 0.1.10\n    Uninstalling llama-index-embeddings-openai-0.1.10:\n      Successfully uninstalled llama-index-embeddings-openai-0.1.10\n  Attempting uninstall: llama-index-readers-llama-parse\n    Found existing installation: llama-index-readers-llama-parse 0.1.6\n    Uninstalling llama-index-readers-llama-parse-0.1.6:\n      Successfully uninstalled llama-index-readers-llama-parse-0.1.6\n  Attempting uninstall: llama-index-multi-modal-llms-openai\n    Found existing installation: llama-index-multi-modal-llms-openai 0.1.9\n    Uninstalling llama-index-multi-modal-llms-openai-0.1.9:\n      Successfully uninstalled llama-index-multi-modal-llms-openai-0.1.9\n  Attempting uninstall: llama-index-cli\n    Found existing installation: llama-index-cli 0.1.13\n    Uninstalling llama-index-cli-0.1.13:\n      Successfully uninstalled llama-index-cli-0.1.13\n  Attempting uninstall: llama-index-agent-openai\n    Found existing installation: llama-index-agent-openai 0.2.7\n    Uninstalling llama-index-agent-openai-0.2.7:\n      Successfully uninstalled llama-index-agent-openai-0.2.7\n  Attempting uninstall: llama-index-program-openai\n    Found existing installation: llama-index-program-openai 0.1.6\n    Uninstalling llama-index-program-openai-0.1.6:\n      Successfully uninstalled llama-index-program-openai-0.1.6\n  Attempting uninstall: llama-index-question-gen-openai\n    Found existing installation: llama-index-question-gen-openai 0.1.3\n    Uninstalling llama-index-question-gen-openai-0.1.3:\n      Successfully uninstalled llama-index-question-gen-openai-0.1.3\n  Attempting uninstall: llama-index\n    Found existing installation: llama-index 0.10.51\n    Uninstalling llama-index-0.10.51:\n      Successfully uninstalled llama-index-0.10.51\nSuccessfully installed llama-cloud-0.0.17 llama-index-0.11.10 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.3.1 llama-index-llms-openai-0.2.9 llama-index-multi-modal-llms-openai-0.2.1 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.6\nRequirement already satisfied: llama-index-vector-stores-qdrant in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.3.0)\nRequirement already satisfied: grpcio&lt;2.0.0,&gt;=1.60.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-vector-stores-qdrant) (1.63.2)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-vector-stores-qdrant) (0.11.10)\nRequirement already satisfied: qdrant-client&gt;=1.7.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-vector-stores-qdrant) (1.11.2)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.16.0)\nRequirement already satisfied: grpcio-tools&gt;=1.41.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (1.62.3)\nRequirement already satisfied: portalocker&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (2.10.1)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.14 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (2.2.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.0.3)\nRequirement already satisfied: protobuf&lt;5.0dev,&gt;=4.21.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (4.25.4)\nRequirement already satisfied: setuptools in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (72.1.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.4.0)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.0.5)\nRequirement already satisfied: idna in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.8)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.14.0)\nRequirement already satisfied: h2&lt;5,&gt;=3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx[http2]&gt;=0.20.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (4.1.0)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.3.2)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.22.0)\nRequirement already satisfied: hyperframe&lt;7,&gt;=6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (6.0.1)\nRequirement already satisfied: hpack&lt;5,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (4.0.0)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (23.2)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.2.2)\nRequirement already satisfied: llama-index-readers-file in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.2)\nRequirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (4.12.3)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (0.11.10)\nRequirement already satisfied: pandas in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (2.2.2)\nRequirement already satisfied: pypdf&lt;5.0.0,&gt;=4.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (4.3.1)\nRequirement already satisfied: striprtf&lt;0.0.27,&gt;=0.0.26 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (0.0.26)\nRequirement already satisfied: soupsieve&gt;1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.3-&gt;llama-index-readers-file) (2.6)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.16.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-readers-file) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-readers-file) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-readers-file) (2024.1)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.0.3)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.23.3)\nRequirement already satisfied: six&gt;=1.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;llama-index-readers-file) (1.16.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.2.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.22.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.0.5)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.14.0)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (23.2)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.2.2)\nRequirement already satisfied: llama-index-embeddings-fastembed in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: fastembed&gt;=0.2.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-embeddings-fastembed) (0.3.6)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-embeddings-fastembed) (0.11.10)\nRequirement already satisfied: PyStemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.2.0.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.20 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (0.24.6)\nRequirement already satisfied: loguru&lt;0.8.0,&gt;=0.7.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (0.7.2)\nRequirement already satisfied: mmh3&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (4.1.0)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.26.4)\nRequirement already satisfied: onnx&lt;2.0.0,&gt;=1.15.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.16.2)\nRequirement already satisfied: onnxruntime&lt;2.0.0,&gt;=1.17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.19.2)\nRequirement already satisfied: pillow&lt;11.0.0,&gt;=10.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (10.4.0)\nRequirement already satisfied: requests&lt;3.0,&gt;=2.31 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.32.3)\nRequirement already satisfied: snowballstemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.2.0)\nRequirement already satisfied: tokenizers&lt;1.0,&gt;=0.15 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (0.19.1)\nRequirement already satisfied: tqdm&lt;5.0,&gt;=4.66 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (4.66.5)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.9.1)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.9.1)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.7.0)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.16.0)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (4.0.3)\nRequirement already satisfied: filelock in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (3.16.0)\nRequirement already satisfied: packaging&gt;=20.9 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (23.2)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2024.7.24)\nRequirement already satisfied: protobuf&gt;=3.20.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnx&lt;2.0.0,&gt;=1.15.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (4.25.4)\nRequirement already satisfied: coloredlogs in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (15.0.1)\nRequirement already satisfied: flatbuffers in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (24.3.25)\nRequirement already satisfied: sympy in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.13.2)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.2.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.22.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.0.5)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.14.0)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.2.2)\nRequirement already satisfied: humanfriendly&gt;=9.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from coloredlogs-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (10.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from sympy-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.3.0)\nRequirement already satisfied: llama-index-llms-openai in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.9)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.7 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai) (0.11.10)\nRequirement already satisfied: openai&lt;2.0.0,&gt;=1.40.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai) (1.44.0)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.16.0)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (4.4.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (0.5.0)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (1.3.1)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (4.0.3)\nRequirement already satisfied: idna&gt;=2.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (3.8)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (1.2.2)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.0.5)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.14.0)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.3.2)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.22.0)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (23.2)\nRequirement already satisfied: llama-index-llms-groq in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-groq) (0.11.10)\nRequirement already satisfied: llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-groq) (0.2.0)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.16.0)\nRequirement already satisfied: llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.2.9)\nRequirement already satisfied: transformers&lt;5.0.0,&gt;=4.37.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (4.44.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.0.3)\nRequirement already satisfied: openai&lt;2.0.0,&gt;=1.40.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (1.44.0)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.2.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.0.3)\nRequirement already satisfied: filelock in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (3.16.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.23.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.24.6)\nRequirement already satisfied: packaging&gt;=20.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (23.2)\nRequirement already satisfied: safetensors&gt;=0.4.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.4.5)\nRequirement already satisfied: tokenizers&lt;0.20,&gt;=0.19 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.19.1)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.22.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.0.5)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.14.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.5.0)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.2.2)\nRequirement already satisfied: qdrant_client in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: fastembed in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.3.6)\nRequirement already satisfied: grpcio&gt;=1.41.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (1.63.2)\nRequirement already satisfied: grpcio-tools&gt;=1.41.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (1.62.3)\nRequirement already satisfied: httpx&gt;=0.20.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (0.27.2)\nRequirement already satisfied: numpy&gt;=1.21 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (1.26.4)\nRequirement already satisfied: portalocker&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (2.10.1)\nRequirement already satisfied: pydantic&gt;=1.10.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (2.9.1)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.14 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (2.2.2)\nRequirement already satisfied: PyStemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (2.2.0.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.20 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (0.24.6)\nRequirement already satisfied: loguru&lt;0.8.0,&gt;=0.7.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (0.7.2)\nRequirement already satisfied: mmh3&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (4.1.0)\nRequirement already satisfied: onnx&lt;2.0.0,&gt;=1.15.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (1.16.2)\nRequirement already satisfied: onnxruntime&lt;2.0.0,&gt;=1.17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (1.19.2)\nRequirement already satisfied: pillow&lt;11.0.0,&gt;=10.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (10.4.0)\nRequirement already satisfied: requests&lt;3.0,&gt;=2.31 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (2.32.3)\nRequirement already satisfied: snowballstemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (2.2.0)\nRequirement already satisfied: tokenizers&lt;1.0,&gt;=0.15 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (0.19.1)\nRequirement already satisfied: tqdm&lt;5.0,&gt;=4.66 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (4.66.5)\nRequirement already satisfied: protobuf&lt;5.0dev,&gt;=4.21.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant_client) (4.25.4)\nRequirement already satisfied: setuptools in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant_client) (72.1.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (4.4.0)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (1.0.5)\nRequirement already satisfied: idna in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (3.8)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (0.14.0)\nRequirement already satisfied: h2&lt;5,&gt;=3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (4.1.0)\nRequirement already satisfied: filelock in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (3.16.0)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (2024.6.1)\nRequirement already satisfied: packaging&gt;=20.9 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (6.0.2)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (4.12.2)\nRequirement already satisfied: coloredlogs in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (15.0.1)\nRequirement already satisfied: flatbuffers in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (24.3.25)\nRequirement already satisfied: sympy in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (1.13.2)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&gt;=1.10.8-&gt;qdrant_client) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&gt;=1.10.8-&gt;qdrant_client) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed) (3.3.2)\nRequirement already satisfied: hyperframe&lt;7,&gt;=6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (6.0.1)\nRequirement already satisfied: hpack&lt;5,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (4.0.0)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (1.2.2)\nRequirement already satisfied: humanfriendly&gt;=9.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from coloredlogs-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (10.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from sympy-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (1.3.0)\nRequirement already satisfied: python-dotenv in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (1.0.1)\n</pre> In\u00a0[1]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[2]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.llms.groq import Groq\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGROK_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\n# Setting up Base LLM\nSettings.llm = OpenAI(\n    model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True\n)\n\n# Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\n# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\nSettings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI from llama_index.llms.groq import Groq # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Setting up Base LLM Settings.llm = OpenAI(     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True )  # Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) # Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version In\u00a0[4]: Copied! <pre># lets loading the documents using SimpleDirectoryReader\n\nprint(\"\ud83d\udd03 Loading Data\")\n\nfrom llama_index.core import Document\nreader = SimpleDirectoryReader(\"../data/md/\" , recursive=True)\ndocuments = reader.load_data(show_progress=True)\n</pre> # lets loading the documents using SimpleDirectoryReader  print(\"\ud83d\udd03 Loading Data\")  from llama_index.core import Document reader = SimpleDirectoryReader(\"../data/md/\" , recursive=True) documents = reader.load_data(show_progress=True) <pre>\ud83d\udd03 Loading Data\n</pre> <pre>Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 41.67file/s]\n</pre> In\u00a0[6]: Copied! <pre># creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    host=\"localhost\",\n    port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    # path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\")\n</pre> # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     host=\"localhost\",     port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     # path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\") In\u00a0[\u00a0]: Copied! <pre>## ingesting data into vector database\n\n## lets set up an ingestion pipeline\n\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        # MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ],\n    vector_store=vector_store,\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=documents , show_progress=True)\nprint(\"Number of chunks added to vector DB :\",len(nodes))\n</pre> ## ingesting data into vector database  ## lets set up an ingestion pipeline  from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         # MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ],     vector_store=vector_store, )  # Ingest directly into a vector db nodes = pipeline.run(documents=documents , show_progress=True) print(\"Number of chunks added to vector DB :\",len(nodes)) In\u00a0[7]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store=vector_store) In\u00a0[8]: Copied! <pre>from llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core import QueryBundle\nimport pandas as pd\nfrom IPython.display import display, HTML\nfrom copy import deepcopy\nfrom llama_index.core.postprocessor import LLMRerank\n\n\ndef get_retrieved_nodes(\n    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n):\n    query_bundle = QueryBundle(query_str)\n    # configure retriever\n    retriever = VectorIndexRetriever(\n        index=index,\n        similarity_top_k=vector_top_k,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n\n    if with_reranker:\n        # configure reranker\n        reranker = LLMRerank(\n            choice_batch_size=5,\n            top_n=reranker_top_n,\n        )\n        retrieved_nodes = reranker.postprocess_nodes(\n            retrieved_nodes, query_bundle\n        )\n\n    return retrieved_nodes\n\n\ndef pretty_print(df):\n    return display(HTML(df.to_html().replace(\"\\\\n\", \"&lt;br&gt;\")))\n\n\ndef visualize_retrieved_nodes(nodes) -&gt; None:\n    result_dicts = []\n    for node in nodes:\n        # node = deepcopy(node)\n        # node.node.metadata = None\n        node_text = node.node.get_text()\n        node_text = node_text.replace(\"\\n\", \" \")\n\n        result_dict = {\"Score\": node.score, \"Text\": node_text}\n        result_dicts.append(result_dict)\n\n    pretty_print(pd.DataFrame(result_dicts))\n</pre> from llama_index.core.retrievers import VectorIndexRetriever from llama_index.core import QueryBundle import pandas as pd from IPython.display import display, HTML from copy import deepcopy from llama_index.core.postprocessor import LLMRerank   def get_retrieved_nodes(     query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False ):     query_bundle = QueryBundle(query_str)     # configure retriever     retriever = VectorIndexRetriever(         index=index,         similarity_top_k=vector_top_k,     )     retrieved_nodes = retriever.retrieve(query_bundle)      if with_reranker:         # configure reranker         reranker = LLMRerank(             choice_batch_size=5,             top_n=reranker_top_n,         )         retrieved_nodes = reranker.postprocess_nodes(             retrieved_nodes, query_bundle         )      return retrieved_nodes   def pretty_print(df):     return display(HTML(df.to_html().replace(\"\\\\n\", \"\")))   def visualize_retrieved_nodes(nodes) -&gt; None:     result_dicts = []     for node in nodes:         # node = deepcopy(node)         # node.node.metadata = None         node_text = node.node.get_text()         node_text = node_text.replace(\"\\n\", \" \")          result_dict = {\"Score\": node.score, \"Text\": node_text}         result_dicts.append(result_dict)      pretty_print(pd.DataFrame(result_dicts)) In\u00a0[9]: Copied! <pre>new_nodes = get_retrieved_nodes(\n    \"What is Attention\", vector_top_k=5, with_reranker=False\n)\n</pre> new_nodes = get_retrieved_nodes(     \"What is Attention\", vector_top_k=5, with_reranker=False ) In\u00a0[10]: Copied! <pre>visualize_retrieved_nodes(new_nodes)\n</pre> visualize_retrieved_nodes(new_nodes) Score Text 0 0.828242 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 1 0.828206 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 2 0.825941 Attention Visualizations  It is this spirit that a majority of American governments have passed new laws since 2009.  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color.  Voting process more difficult. --- 3 0.816966 Attention Visualizations  It is this spirit that a majority of American governments have passed new laws since 2009.  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.  Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color. --- 4 0.808573 Figure 4  Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5 and 6. Note that the attentions are very sharp for this word.  14 --- In\u00a0[11]: Copied! <pre>new_nodes = get_retrieved_nodes(\n    \"What is Attention\",\n    vector_top_k=20,\n    reranker_top_n=5,\n    with_reranker=True,\n)\n</pre> new_nodes = get_retrieved_nodes(     \"What is Attention\",     vector_top_k=20,     reranker_top_n=5,     with_reranker=True, ) In\u00a0[12]: Copied! <pre>visualize_retrieved_nodes(new_nodes)\n</pre> visualize_retrieved_nodes(new_nodes) Score Text 0 9.0 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 1 9.0 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 2 9.0 2 Background  The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.  Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].  To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 9.0 3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the values.  In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:  Attention(Q, K, V) = softmax( \u221adk Q KT ) V (1)  The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor \u221a1/dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by \u221a1/dk. 4 9.0 1 Introduction  Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].  Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.  Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.  In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. In\u00a0[13]: Copied! <pre>query_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=[\n        LLMRerank(\n            choice_batch_size=5,\n            top_n=2,\n        )\n    ],\n    response_mode=\"tree_summarize\",\n)\n</pre> query_engine = index.as_query_engine(     similarity_top_k=10,     node_postprocessors=[         LLMRerank(             choice_batch_size=5,             top_n=2,         )     ],     response_mode=\"tree_summarize\", ) In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-postprocessor-cohere-rerank\n</pre> # !pip install llama-index-postprocessor-cohere-rerank In\u00a0[\u00a0]: Copied! <pre># from llama_index.postprocessor.cohere_rerank import CohereRerank\n\n# cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n# cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10,\n#     node_postprocessors=[cohere_rerank],\n# )\n</pre> # from llama_index.postprocessor.cohere_rerank import CohereRerank  # cohere_api_key = os.environ[\"COHERE_API_KEY\"] # cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)  # query_engine = index.as_query_engine( #     similarity_top_k=10, #     node_postprocessors=[cohere_rerank], # ) In\u00a0[\u00a0]: Copied! <pre># !pip install -U llama-index-postprocessor-colbert-rerank\n</pre> # !pip install -U llama-index-postprocessor-colbert-rerank In\u00a0[\u00a0]: Copied! <pre># from llama_index.postprocessor.colbert_rerank import ColbertRerank\n\n# colbert_reranker = ColbertRerank(\n#     top_n=5,\n#     model=\"colbert-ir/colbertv2.0\",\n#     tokenizer=\"colbert-ir/colbertv2.0\",\n#     keep_retrieval_score=True,\n# )\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10,\n#     node_postprocessors=[colbert_reranker],\n# )\n</pre> # from llama_index.postprocessor.colbert_rerank import ColbertRerank  # colbert_reranker = ColbertRerank( #     top_n=5, #     model=\"colbert-ir/colbertv2.0\", #     tokenizer=\"colbert-ir/colbertv2.0\", #     keep_retrieval_score=True, # )  # query_engine = index.as_query_engine( #     similarity_top_k=10, #     node_postprocessors=[colbert_reranker], # ) In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-postprocessor-flag-embedding-reranker\n# !pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n</pre> # !pip install llama-index-postprocessor-flag-embedding-reranker # !pip install git+https://github.com/FlagOpen/FlagEmbedding.git In\u00a0[5]: Copied! <pre># from llama_index.postprocessor.flag_embedding_reranker import (\n#     FlagEmbeddingReranker,\n# )\n\n# rerank = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=5)\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10, node_postprocessors=[rerank]\n# )\n</pre> # from llama_index.postprocessor.flag_embedding_reranker import ( #     FlagEmbeddingReranker, # )  # rerank = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=5)  # query_engine = index.as_query_engine( #     similarity_top_k=10, node_postprocessors=[rerank] # ) In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-embeddings-huggingface\n</pre> # !pip install llama-index-embeddings-huggingface In\u00a0[\u00a0]: Copied! <pre># from llama_index.core.postprocessor import SentenceTransformerRerank\n\n# rerank = SentenceTransformerRerank(\n#     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n# )\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10, node_postprocessors=[rerank]\n# )\n</pre> # from llama_index.core.postprocessor import SentenceTransformerRerank  # rerank = SentenceTransformerRerank( #     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3 # )  # query_engine = index.as_query_engine( #     similarity_top_k=10, node_postprocessors=[rerank] # )  In\u00a0[14]: Copied! <pre>response = query_engine.query(\n    \"What is Attention\"\n)\n\ndisplay(Markdown(str(response)))\n</pre> response = query_engine.query(     \"What is Attention\" )  display(Markdown(str(response))) <p>Attention is a function that maps a query and a set of key-value pairs to an output. In this process, the query, keys, values, and output are all represented as vectors. The resulting output is calculated as a weighted sum of these vectors.</p>"},{"location":"RAG/02_ReRanker_RAG/notebook/#setting-up-the-environment","title":"Setting up the Environment\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/02_ReRanker_RAG/notebook/#ingest-data-into-vector-db","title":"Ingest Data into vector DB\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#setting-up-index","title":"Setting Up Index\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#retrieval-comparisons","title":"Retrieval Comparisons\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#llm-reranker","title":"LLM ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#cohere-reranker","title":"Cohere ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#colber-reranker","title":"Colber ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#flag-embedding-reranker","title":"Flag Embedding ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#sentence-transformer-reranker","title":"Sentence Transformer ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook/#set-up-query-engine","title":"Set Up Query Engine\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/","title":"Evaluation(RAGAS)","text":"ReRanker RAG AI Engineering.academy In\u00a0[3]: Copied! <pre>!pip install -U nest-asyncio\n!pip install -U llama-index\n!pip install -U llama-index-vector-stores-qdrant \n!pip install -U llama-index-readers-file \n!pip install -U llama-index-embeddings-fastembed \n!pip install -U llama-index-llms-openai\n!pip install -U llama-index-llms-groq\n!pip install -U qdrant_client fastembed\n!pip install -U python-dotenv\n</pre> !pip install -U nest-asyncio !pip install -U llama-index !pip install -U llama-index-vector-stores-qdrant  !pip install -U llama-index-readers-file  !pip install -U llama-index-embeddings-fastembed  !pip install -U llama-index-llms-openai !pip install -U llama-index-llms-groq !pip install -U qdrant_client fastembed !pip install -U python-dotenv <pre>Requirement already satisfied: nest-asyncio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (1.6.0)\nRequirement already satisfied: llama-index in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.10.51)\nCollecting llama-index\n  Using cached llama_index-0.11.10-py3-none-any.whl.metadata (11 kB)\nCollecting llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1 (from llama-index)\n  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-cli&lt;0.4.0,&gt;=0.3.1 (from llama-index)\n  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.10 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index) (0.11.10)\nCollecting llama-index-embeddings-openai&lt;0.3.0,&gt;=0.2.4 (from llama-index)\n  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\nCollecting llama-index-indices-managed-llama-cloud&gt;=0.3.0 (from llama-index)\n  Using cached llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: llama-index-legacy&lt;0.10.0,&gt;=0.9.48 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index) (0.9.48.post3)\nCollecting llama-index-llms-openai&lt;0.3.0,&gt;=0.2.3 (from llama-index)\n  Using cached llama_index_llms_openai-0.2.9-py3-none-any.whl.metadata (648 bytes)\nCollecting llama-index-multi-modal-llms-openai&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-program-openai&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\nCollecting llama-index-question-gen-openai&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\nCollecting llama-index-readers-file&lt;0.3.0,&gt;=0.2.0 (from llama-index)\n  Using cached llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse&gt;=0.3.0 (from llama-index)\n  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index) (3.9.1)\nRequirement already satisfied: openai&gt;=1.14.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1-&gt;llama-index) (1.44.0)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.3)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.16.0)\nCollecting llama-cloud&gt;=0.0.11 (from llama-index-indices-managed-llama-cloud&gt;=0.3.0-&gt;llama-index)\n  Using cached llama_cloud-0.0.17-py3-none-any.whl.metadata (751 bytes)\nRequirement already satisfied: pandas in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2.2.2)\nRequirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (4.12.3)\nRequirement already satisfied: pypdf&lt;5.0.0,&gt;=4.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (4.3.1)\nRequirement already satisfied: striprtf&lt;0.0.27,&gt;=0.0.26 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (0.0.26)\nCollecting llama-parse&gt;=0.5.0 (from llama-index-readers-llama-parse&gt;=0.3.0-&gt;llama-index)\n  Using cached llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index) (2024.7.24)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.0.3)\nRequirement already satisfied: soupsieve&gt;1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.3-&gt;llama-index-readers-file&lt;0.3.0,&gt;=0.2.0-&gt;llama-index) (2.6)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (4.4.0)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.0.5)\nRequirement already satisfied: idna in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.8)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.14.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&gt;=1.14.0-&gt;llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1-&gt;llama-index) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&gt;=1.14.0-&gt;llama-index-agent-openai&lt;0.4.0,&gt;=0.3.1-&gt;llama-index) (0.5.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.3.2)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (2.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (3.22.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (2024.1)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (1.2.2)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.10-&gt;llama-index) (23.2)\nRequirement already satisfied: six&gt;=1.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;llama-index-legacy&lt;0.10.0,&gt;=0.9.48-&gt;llama-index) (1.16.0)\nUsing cached llama_index-0.11.10-py3-none-any.whl (6.8 kB)\nUsing cached llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\nUsing cached llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\nUsing cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\nUsing cached llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl (10 kB)\nUsing cached llama_index_llms_openai-0.2.9-py3-none-any.whl (12 kB)\nUsing cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl (5.9 kB)\nUsing cached llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\nUsing cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\nUsing cached llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\nUsing cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\nUsing cached llama_cloud-0.0.17-py3-none-any.whl (187 kB)\nUsing cached llama_parse-0.5.6-py3-none-any.whl (10 kB)\nInstalling collected packages: llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n  Attempting uninstall: llama-cloud\n    Found existing installation: llama-cloud 0.0.6\n    Uninstalling llama-cloud-0.0.6:\n      Successfully uninstalled llama-cloud-0.0.6\n  Attempting uninstall: llama-parse\n    Found existing installation: llama-parse 0.4.9\n    Uninstalling llama-parse-0.4.9:\n      Successfully uninstalled llama-parse-0.4.9\n  Attempting uninstall: llama-index-readers-file\n    Found existing installation: llama-index-readers-file 0.1.25\n    Uninstalling llama-index-readers-file-0.1.25:\n      Successfully uninstalled llama-index-readers-file-0.1.25\n  Attempting uninstall: llama-index-llms-openai\n    Found existing installation: llama-index-llms-openai 0.1.24\n    Uninstalling llama-index-llms-openai-0.1.24:\n      Successfully uninstalled llama-index-llms-openai-0.1.24\n  Attempting uninstall: llama-index-indices-managed-llama-cloud\n    Found existing installation: llama-index-indices-managed-llama-cloud 0.2.4\n    Uninstalling llama-index-indices-managed-llama-cloud-0.2.4:\n      Successfully uninstalled llama-index-indices-managed-llama-cloud-0.2.4\n  Attempting uninstall: llama-index-embeddings-openai\n    Found existing installation: llama-index-embeddings-openai 0.1.10\n    Uninstalling llama-index-embeddings-openai-0.1.10:\n      Successfully uninstalled llama-index-embeddings-openai-0.1.10\n  Attempting uninstall: llama-index-readers-llama-parse\n    Found existing installation: llama-index-readers-llama-parse 0.1.6\n    Uninstalling llama-index-readers-llama-parse-0.1.6:\n      Successfully uninstalled llama-index-readers-llama-parse-0.1.6\n  Attempting uninstall: llama-index-multi-modal-llms-openai\n    Found existing installation: llama-index-multi-modal-llms-openai 0.1.9\n    Uninstalling llama-index-multi-modal-llms-openai-0.1.9:\n      Successfully uninstalled llama-index-multi-modal-llms-openai-0.1.9\n  Attempting uninstall: llama-index-cli\n    Found existing installation: llama-index-cli 0.1.13\n    Uninstalling llama-index-cli-0.1.13:\n      Successfully uninstalled llama-index-cli-0.1.13\n  Attempting uninstall: llama-index-agent-openai\n    Found existing installation: llama-index-agent-openai 0.2.7\n    Uninstalling llama-index-agent-openai-0.2.7:\n      Successfully uninstalled llama-index-agent-openai-0.2.7\n  Attempting uninstall: llama-index-program-openai\n    Found existing installation: llama-index-program-openai 0.1.6\n    Uninstalling llama-index-program-openai-0.1.6:\n      Successfully uninstalled llama-index-program-openai-0.1.6\n  Attempting uninstall: llama-index-question-gen-openai\n    Found existing installation: llama-index-question-gen-openai 0.1.3\n    Uninstalling llama-index-question-gen-openai-0.1.3:\n      Successfully uninstalled llama-index-question-gen-openai-0.1.3\n  Attempting uninstall: llama-index\n    Found existing installation: llama-index 0.10.51\n    Uninstalling llama-index-0.10.51:\n      Successfully uninstalled llama-index-0.10.51\nSuccessfully installed llama-cloud-0.0.17 llama-index-0.11.10 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.3.1 llama-index-llms-openai-0.2.9 llama-index-multi-modal-llms-openai-0.2.1 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.6\nRequirement already satisfied: llama-index-vector-stores-qdrant in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.3.0)\nRequirement already satisfied: grpcio&lt;2.0.0,&gt;=1.60.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-vector-stores-qdrant) (1.63.2)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-vector-stores-qdrant) (0.11.10)\nRequirement already satisfied: qdrant-client&gt;=1.7.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-vector-stores-qdrant) (1.11.2)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.16.0)\nRequirement already satisfied: grpcio-tools&gt;=1.41.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (1.62.3)\nRequirement already satisfied: portalocker&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (2.10.1)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.14 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (2.2.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.0.3)\nRequirement already satisfied: protobuf&lt;5.0dev,&gt;=4.21.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (4.25.4)\nRequirement already satisfied: setuptools in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (72.1.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (4.4.0)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.0.5)\nRequirement already satisfied: idna in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.8)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.14.0)\nRequirement already satisfied: h2&lt;5,&gt;=3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx[http2]&gt;=0.20.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (4.1.0)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.3.2)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (3.22.0)\nRequirement already satisfied: hyperframe&lt;7,&gt;=6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (6.0.1)\nRequirement already satisfied: hpack&lt;5,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant-client&gt;=1.7.1-&gt;llama-index-vector-stores-qdrant) (4.0.0)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (23.2)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-qdrant) (1.2.2)\nRequirement already satisfied: llama-index-readers-file in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.2)\nRequirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (4.12.3)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (0.11.10)\nRequirement already satisfied: pandas in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (2.2.2)\nRequirement already satisfied: pypdf&lt;5.0.0,&gt;=4.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (4.3.1)\nRequirement already satisfied: striprtf&lt;0.0.27,&gt;=0.0.26 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-readers-file) (0.0.26)\nRequirement already satisfied: soupsieve&gt;1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.3-&gt;llama-index-readers-file) (2.6)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.16.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-readers-file) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-readers-file) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pandas-&gt;llama-index-readers-file) (2024.1)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.0.3)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.23.3)\nRequirement already satisfied: six&gt;=1.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;llama-index-readers-file) (1.16.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2.2.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (3.22.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.0.5)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (0.14.0)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (23.2)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-readers-file) (1.2.2)\nRequirement already satisfied: llama-index-embeddings-fastembed in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: fastembed&gt;=0.2.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-embeddings-fastembed) (0.3.6)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-embeddings-fastembed) (0.11.10)\nRequirement already satisfied: PyStemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.2.0.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.20 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (0.24.6)\nRequirement already satisfied: loguru&lt;0.8.0,&gt;=0.7.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (0.7.2)\nRequirement already satisfied: mmh3&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (4.1.0)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.26.4)\nRequirement already satisfied: onnx&lt;2.0.0,&gt;=1.15.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.16.2)\nRequirement already satisfied: onnxruntime&lt;2.0.0,&gt;=1.17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.19.2)\nRequirement already satisfied: pillow&lt;11.0.0,&gt;=10.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (10.4.0)\nRequirement already satisfied: requests&lt;3.0,&gt;=2.31 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.32.3)\nRequirement already satisfied: snowballstemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.2.0)\nRequirement already satisfied: tokenizers&lt;1.0,&gt;=0.15 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (0.19.1)\nRequirement already satisfied: tqdm&lt;5.0,&gt;=4.66 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (4.66.5)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.9.1)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.9.1)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.7.0)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.16.0)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (4.0.3)\nRequirement already satisfied: filelock in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (3.16.0)\nRequirement already satisfied: packaging&gt;=20.9 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (23.2)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2024.7.24)\nRequirement already satisfied: protobuf&gt;=3.20.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnx&lt;2.0.0,&gt;=1.15.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (4.25.4)\nRequirement already satisfied: coloredlogs in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (15.0.1)\nRequirement already satisfied: flatbuffers in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (24.3.25)\nRequirement already satisfied: sympy in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.13.2)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2.2.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (3.22.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.0.5)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (0.14.0)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-fastembed) (1.2.2)\nRequirement already satisfied: humanfriendly&gt;=9.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from coloredlogs-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (10.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from sympy-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed&gt;=0.2.2-&gt;llama-index-embeddings-fastembed) (1.3.0)\nRequirement already satisfied: llama-index-llms-openai in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.9)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.7 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai) (0.11.10)\nRequirement already satisfied: openai&lt;2.0.0,&gt;=1.40.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai) (1.44.0)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.16.0)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (4.4.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (0.5.0)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (1.3.1)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (4.0.3)\nRequirement already satisfied: idna&gt;=2.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (3.8)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai) (1.2.2)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.0.5)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.14.0)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.3.2)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (2.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.0.3)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (3.22.0)\nRequirement already satisfied: packaging&gt;=17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.7-&gt;llama-index-llms-openai) (23.2)\nRequirement already satisfied: llama-index-llms-groq in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-groq) (0.11.10)\nRequirement already satisfied: llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-groq) (0.2.0)\nRequirement already satisfied: PyYAML&gt;=6.0.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (6.0.2)\nRequirement already satisfied: SQLAlchemy&gt;=1.4.49 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.0.34)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.10.5)\nRequirement already satisfied: dataclasses-json in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.5.14)\nRequirement already satisfied: deprecated&gt;=1.2.9.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.2.14)\nRequirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.0.8)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2024.6.1)\nRequirement already satisfied: httpx in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.27.2)\nRequirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.6.0)\nRequirement already satisfied: networkx&gt;=3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.3)\nRequirement already satisfied: nltk&gt;3.8.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.9.1)\nRequirement already satisfied: numpy&lt;2.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.26.4)\nRequirement already satisfied: pillow&gt;=9.0.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (10.4.0)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.9.1)\nRequirement already satisfied: requests&gt;=2.31.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (8.4.2)\nRequirement already satisfied: tiktoken&gt;=0.3.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.66.5)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.12.2)\nRequirement already satisfied: typing-inspect&gt;=0.8.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.9.0)\nRequirement already satisfied: wrapt in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.16.0)\nRequirement already satisfied: llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.2.9)\nRequirement already satisfied: transformers&lt;5.0.0,&gt;=4.37.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (4.44.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.11.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.0.3)\nRequirement already satisfied: openai&lt;2.0.0,&gt;=1.40.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (1.44.0)\nRequirement already satisfied: click in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (8.1.7)\nRequirement already satisfied: joblib in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2024.7.24)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.8)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2.2.2)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.0.3)\nRequirement already satisfied: filelock in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (3.16.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.23.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.24.6)\nRequirement already satisfied: packaging&gt;=20.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (23.2)\nRequirement already satisfied: safetensors&gt;=0.4.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.4.5)\nRequirement already satisfied: tokenizers&lt;0.20,&gt;=0.19 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from transformers&lt;5.0.0,&gt;=4.37.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.19.1)\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.0.0)\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (3.22.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.0.5)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (0.14.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from openai&lt;2.0.0,&gt;=1.40.0-&gt;llama-index-llms-openai&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-openai-like&lt;0.3.0,&gt;=0.2.0-&gt;llama-index-llms-groq) (0.5.0)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-groq) (1.2.2)\nRequirement already satisfied: qdrant_client in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (1.11.2)\nRequirement already satisfied: fastembed in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (0.3.6)\nRequirement already satisfied: grpcio&gt;=1.41.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (1.63.2)\nRequirement already satisfied: grpcio-tools&gt;=1.41.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (1.62.3)\nRequirement already satisfied: httpx&gt;=0.20.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (0.27.2)\nRequirement already satisfied: numpy&gt;=1.21 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (1.26.4)\nRequirement already satisfied: portalocker&lt;3.0.0,&gt;=2.7.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (2.10.1)\nRequirement already satisfied: pydantic&gt;=1.10.8 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (2.9.1)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.14 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from qdrant_client) (2.2.2)\nRequirement already satisfied: PyStemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (2.2.0.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.20 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (0.24.6)\nRequirement already satisfied: loguru&lt;0.8.0,&gt;=0.7.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (0.7.2)\nRequirement already satisfied: mmh3&lt;5.0,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (4.1.0)\nRequirement already satisfied: onnx&lt;2.0.0,&gt;=1.15.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (1.16.2)\nRequirement already satisfied: onnxruntime&lt;2.0.0,&gt;=1.17.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (1.19.2)\nRequirement already satisfied: pillow&lt;11.0.0,&gt;=10.3.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (10.4.0)\nRequirement already satisfied: requests&lt;3.0,&gt;=2.31 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (2.32.3)\nRequirement already satisfied: snowballstemmer&lt;3.0.0,&gt;=2.2.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (2.2.0)\nRequirement already satisfied: tokenizers&lt;1.0,&gt;=0.15 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (0.19.1)\nRequirement already satisfied: tqdm&lt;5.0,&gt;=4.66 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from fastembed) (4.66.5)\nRequirement already satisfied: protobuf&lt;5.0dev,&gt;=4.21.6 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant_client) (4.25.4)\nRequirement already satisfied: setuptools in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant_client) (72.1.0)\nRequirement already satisfied: anyio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (4.4.0)\nRequirement already satisfied: certifi in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (1.0.5)\nRequirement already satisfied: idna in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (3.8)\nRequirement already satisfied: sniffio in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (1.3.1)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (0.14.0)\nRequirement already satisfied: h2&lt;5,&gt;=3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (4.1.0)\nRequirement already satisfied: filelock in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (3.16.0)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (2024.6.1)\nRequirement already satisfied: packaging&gt;=20.9 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (6.0.2)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.20-&gt;fastembed) (4.12.2)\nRequirement already satisfied: coloredlogs in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (15.0.1)\nRequirement already satisfied: flatbuffers in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (24.3.25)\nRequirement already satisfied: sympy in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (1.13.2)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&gt;=1.10.8-&gt;qdrant_client) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.3 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from pydantic&gt;=1.10.8-&gt;qdrant_client) (2.23.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed) (3.3.2)\nRequirement already satisfied: hyperframe&lt;7,&gt;=6.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (6.0.1)\nRequirement already satisfied: hpack&lt;5,&gt;=4.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (4.0.0)\nRequirement already satisfied: exceptiongroup&gt;=1.0.2 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from anyio-&gt;httpx&gt;=0.20.0-&gt;httpx[http2]&gt;=0.20.0-&gt;qdrant_client) (1.2.2)\nRequirement already satisfied: humanfriendly&gt;=9.1 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from coloredlogs-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (10.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (from sympy-&gt;onnxruntime&lt;2.0.0,&gt;=1.17.0-&gt;fastembed) (1.3.0)\nRequirement already satisfied: python-dotenv in /home/adithya/miniconda3/envs/basic-rag/lib/python3.10/site-packages (1.0.1)\n</pre> In\u00a0[1]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[2]: Copied! <pre># Standard library imports\nimport logging\nimport sys\nimport os\n\n# Third-party imports\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# Qdrant client import\nimport qdrant_client\n\n# LlamaIndex core imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core import Settings\n\n# LlamaIndex vector store import\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Embedding model imports\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# LLM import\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.llms.groq import Groq\n# Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGROK_API_KEY = os.getenv(\"GROQ_API_KEY\")\n\n# Setting up Base LLM\nSettings.llm = OpenAI(\n    model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True\n)\n\n# Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)\n\n# Set the embedding model\n# Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default)\n# Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# Option 2: Use OpenAI's embedding model (commented out)\n# If you want to use OpenAI's embedding model, uncomment the following line:\nSettings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)\n\n# Qdrant configuration (commented out)\n# If you're using Qdrant, uncomment and set these variables:\n# QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\")\n# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n\n# Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version\n</pre> # Standard library imports import logging import sys import os  # Third-party imports from dotenv import load_dotenv from IPython.display import Markdown, display  # Qdrant client import import qdrant_client  # LlamaIndex core imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core import Settings  # LlamaIndex vector store import from llama_index.vector_stores.qdrant import QdrantVectorStore  # Embedding model imports from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.embeddings.openai import OpenAIEmbedding  # LLM import from llama_index.llms.openai import OpenAI from llama_index.llms.groq import Groq # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") GROK_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Setting up Base LLM Settings.llm = OpenAI(     model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True )  # Settings.llm = Groq(model=\"llama3-70b-8192\" , api_key=GROK_API_KEY)  # Set the embedding model # Option 1: Use FastEmbed with BAAI/bge-base-en-v1.5 model (default) # Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  # Option 2: Use OpenAI's embedding model (commented out) # If you want to use OpenAI's embedding model, uncomment the following line: Settings.embed_model = OpenAIEmbedding(embed_batch_size=10, api_key=OPENAI_API_KEY)  # Qdrant configuration (commented out) # If you're using Qdrant, uncomment and set these variables: # QDRANT_CLOUD_ENDPOINT = os.getenv(\"QDRANT_CLOUD_ENDPOINT\") # QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Note: Remember to add QDRANT_CLOUD_ENDPOINT and QDRANT_API_KEY to your .env file if using Qdrant Hosted version In\u00a0[\u00a0]: Copied! <pre># !pip install arize-phoenix\n# !pip install openinference-instrumentation-llama-index\n# !pip install -U llama-index-callbacks-arize-phoenix\n</pre> # !pip install arize-phoenix # !pip install openinference-instrumentation-llama-index # !pip install -U llama-index-callbacks-arize-phoenix In\u00a0[3]: Copied! <pre># from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n# from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n# from opentelemetry.sdk import trace as trace_sdk\n# from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n\n# endpoint = \"http://127.0.0.1:6006/v1/traces\"\n\n# # Set up the Tracer Provider\n# tracer_provider = trace_sdk.TracerProvider()\n\n# # Add Span Processor to the Tracer Provider\n# tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n\n# # Instrument Llama Index with the Tracer Provider\n# LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n</pre> # from openinference.instrumentation.llama_index import LlamaIndexInstrumentor # from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter # from opentelemetry.sdk import trace as trace_sdk # from opentelemetry.sdk.trace.export import SimpleSpanProcessor  # endpoint = \"http://127.0.0.1:6006/v1/traces\"  # # Set up the Tracer Provider # tracer_provider = trace_sdk.TracerProvider()  # # Add Span Processor to the Tracer Provider # tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))  # # Instrument Llama Index with the Tracer Provider # LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider) In\u00a0[4]: Copied! <pre># lets loading the documents using SimpleDirectoryReader\n\nprint(\"\ud83d\udd03 Loading Data\")\n\nfrom llama_index.core import Document\nreader = SimpleDirectoryReader(\"../data/md/\" , recursive=True)\ndocuments = reader.load_data(show_progress=True)\n</pre> # lets loading the documents using SimpleDirectoryReader  print(\"\ud83d\udd03 Loading Data\")  from llama_index.core import Document reader = SimpleDirectoryReader(\"../data/md/\" , recursive=True) documents = reader.load_data(show_progress=True) <pre>\ud83d\udd03 Loading Data\n</pre> <pre>Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00&lt;00:00, 41.67file/s]\n</pre> In\u00a0[6]: Copied! <pre># creating a qdrant client instance\n\nclient = qdrant_client.QdrantClient(\n    # you can use :memory: mode for fast and light-weight experiments,\n    # it does not require to have Qdrant deployed anywhere\n    # but requires qdrant-client &gt;= 1.1.1\n    # location=\":memory:\"\n    # otherwise set Qdrant instance address with:\n    # url=QDRANT_CLOUD_ENDPOINT,\n    # otherwise set Qdrant instance with host and port:\n    host=\"localhost\",\n    port=6333\n    # set API KEY for Qdrant Cloud\n    # api_key=QDRANT_API_KEY,\n    # path=\"./db/\"\n)\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\")\n</pre> # creating a qdrant client instance  client = qdrant_client.QdrantClient(     # you can use :memory: mode for fast and light-weight experiments,     # it does not require to have Qdrant deployed anywhere     # but requires qdrant-client &gt;= 1.1.1     # location=\":memory:\"     # otherwise set Qdrant instance address with:     # url=QDRANT_CLOUD_ENDPOINT,     # otherwise set Qdrant instance with host and port:     host=\"localhost\",     port=6333     # set API KEY for Qdrant Cloud     # api_key=QDRANT_API_KEY,     # path=\"./db/\" )  vector_store = QdrantVectorStore(client=client, collection_name=\"02_ReRanker_RAG\") In\u00a0[\u00a0]: Copied! <pre>## ingesting data into vector database\n\n## lets set up an ingestion pipeline\n\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[\n        # MarkdownNodeParser(include_metadata=True),\n        # TokenTextSplitter(chunk_size=500, chunk_overlap=20),\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),\n        Settings.embed_model,\n    ],\n    vector_store=vector_store,\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=documents , show_progress=True)\nprint(\"Number of chunks added to vector DB :\",len(nodes))\n</pre> ## ingesting data into vector database  ## lets set up an ingestion pipeline  from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.node_parser import SentenceSplitter from llama_index.core.node_parser import MarkdownNodeParser from llama_index.core.node_parser import SemanticSplitterNodeParser from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(     transformations=[         # MarkdownNodeParser(include_metadata=True),         # TokenTextSplitter(chunk_size=500, chunk_overlap=20),         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         # SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95 , embed_model=Settings.embed_model),         Settings.embed_model,     ],     vector_store=vector_store, )  # Ingest directly into a vector db nodes = pipeline.run(documents=documents , show_progress=True) print(\"Number of chunks added to vector DB :\",len(nodes)) In\u00a0[7]: Copied! <pre>index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n</pre> index = VectorStoreIndex.from_vector_store(vector_store=vector_store) In\u00a0[8]: Copied! <pre>from llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core import QueryBundle\nimport pandas as pd\nfrom IPython.display import display, HTML\nfrom copy import deepcopy\nfrom llama_index.core.postprocessor import LLMRerank\n\n\ndef get_retrieved_nodes(\n    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n):\n    query_bundle = QueryBundle(query_str)\n    # configure retriever\n    retriever = VectorIndexRetriever(\n        index=index,\n        similarity_top_k=vector_top_k,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n\n    if with_reranker:\n        # configure reranker\n        reranker = LLMRerank(\n            choice_batch_size=5,\n            top_n=reranker_top_n,\n        )\n        retrieved_nodes = reranker.postprocess_nodes(\n            retrieved_nodes, query_bundle\n        )\n\n    return retrieved_nodes\n\n\ndef pretty_print(df):\n    return display(HTML(df.to_html().replace(\"\\\\n\", \"&lt;br&gt;\")))\n\n\ndef visualize_retrieved_nodes(nodes) -&gt; None:\n    result_dicts = []\n    for node in nodes:\n        # node = deepcopy(node)\n        # node.node.metadata = None\n        node_text = node.node.get_text()\n        node_text = node_text.replace(\"\\n\", \" \")\n\n        result_dict = {\"Score\": node.score, \"Text\": node_text}\n        result_dicts.append(result_dict)\n\n    pretty_print(pd.DataFrame(result_dicts))\n</pre> from llama_index.core.retrievers import VectorIndexRetriever from llama_index.core import QueryBundle import pandas as pd from IPython.display import display, HTML from copy import deepcopy from llama_index.core.postprocessor import LLMRerank   def get_retrieved_nodes(     query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False ):     query_bundle = QueryBundle(query_str)     # configure retriever     retriever = VectorIndexRetriever(         index=index,         similarity_top_k=vector_top_k,     )     retrieved_nodes = retriever.retrieve(query_bundle)      if with_reranker:         # configure reranker         reranker = LLMRerank(             choice_batch_size=5,             top_n=reranker_top_n,         )         retrieved_nodes = reranker.postprocess_nodes(             retrieved_nodes, query_bundle         )      return retrieved_nodes   def pretty_print(df):     return display(HTML(df.to_html().replace(\"\\\\n\", \"\")))   def visualize_retrieved_nodes(nodes) -&gt; None:     result_dicts = []     for node in nodes:         # node = deepcopy(node)         # node.node.metadata = None         node_text = node.node.get_text()         node_text = node_text.replace(\"\\n\", \" \")          result_dict = {\"Score\": node.score, \"Text\": node_text}         result_dicts.append(result_dict)      pretty_print(pd.DataFrame(result_dicts)) In\u00a0[9]: Copied! <pre>new_nodes = get_retrieved_nodes(\n    \"What is Attention\", vector_top_k=5, with_reranker=False\n)\n</pre> new_nodes = get_retrieved_nodes(     \"What is Attention\", vector_top_k=5, with_reranker=False ) In\u00a0[10]: Copied! <pre>visualize_retrieved_nodes(new_nodes)\n</pre> visualize_retrieved_nodes(new_nodes) Score Text 0 0.828242 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 1 0.828206 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 2 0.825941 Attention Visualizations  It is this spirit that a majority of American governments have passed new laws since 2009.  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color.  Voting process more difficult. --- 3 0.816966 Attention Visualizations  It is this spirit that a majority of American governments have passed new laws since 2009.  Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.  Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color. --- 4 0.808573 Figure 4  Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5 and 6. Note that the attentions are very sharp for this word.  14 --- In\u00a0[11]: Copied! <pre>new_nodes = get_retrieved_nodes(\n    \"What is Attention\",\n    vector_top_k=20,\n    reranker_top_n=5,\n    with_reranker=True,\n)\n</pre> new_nodes = get_retrieved_nodes(     \"What is Attention\",     vector_top_k=20,     reranker_top_n=5,     with_reranker=True, ) In\u00a0[12]: Copied! <pre>visualize_retrieved_nodes(new_nodes)\n</pre> visualize_retrieved_nodes(new_nodes) Score Text 0 9.0 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 1 9.0 3.2 Attention  An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum. --- 2 9.0 2 Background  The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.  Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].  To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 9.0 3.2.1 Scaled Dot-Product Attention  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the values.  In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:  Attention(Q, K, V) = softmax( \u221adk Q KT ) V (1)  The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor \u221a1/dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by \u221a1/dk. 4 9.0 1 Introduction  Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].  Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.  Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.  In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. In\u00a0[13]: Copied! <pre>query_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=[\n        LLMRerank(\n            choice_batch_size=5,\n            top_n=2,\n        )\n    ],\n    response_mode=\"tree_summarize\",\n)\n</pre> query_engine = index.as_query_engine(     similarity_top_k=10,     node_postprocessors=[         LLMRerank(             choice_batch_size=5,             top_n=2,         )     ],     response_mode=\"tree_summarize\", ) In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-postprocessor-cohere-rerank\n</pre> # !pip install llama-index-postprocessor-cohere-rerank In\u00a0[\u00a0]: Copied! <pre># from llama_index.postprocessor.cohere_rerank import CohereRerank\n\n# cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n# cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10,\n#     node_postprocessors=[cohere_rerank],\n# )\n</pre> # from llama_index.postprocessor.cohere_rerank import CohereRerank  # cohere_api_key = os.environ[\"COHERE_API_KEY\"] # cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)  # query_engine = index.as_query_engine( #     similarity_top_k=10, #     node_postprocessors=[cohere_rerank], # ) In\u00a0[\u00a0]: Copied! <pre># !pip install -U llama-index-postprocessor-colbert-rerank\n</pre> # !pip install -U llama-index-postprocessor-colbert-rerank In\u00a0[\u00a0]: Copied! <pre># from llama_index.postprocessor.colbert_rerank import ColbertRerank\n\n# colbert_reranker = ColbertRerank(\n#     top_n=5,\n#     model=\"colbert-ir/colbertv2.0\",\n#     tokenizer=\"colbert-ir/colbertv2.0\",\n#     keep_retrieval_score=True,\n# )\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10,\n#     node_postprocessors=[colbert_reranker],\n# )\n</pre> # from llama_index.postprocessor.colbert_rerank import ColbertRerank  # colbert_reranker = ColbertRerank( #     top_n=5, #     model=\"colbert-ir/colbertv2.0\", #     tokenizer=\"colbert-ir/colbertv2.0\", #     keep_retrieval_score=True, # )  # query_engine = index.as_query_engine( #     similarity_top_k=10, #     node_postprocessors=[colbert_reranker], # ) In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-postprocessor-flag-embedding-reranker\n# !pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n</pre> # !pip install llama-index-postprocessor-flag-embedding-reranker # !pip install git+https://github.com/FlagOpen/FlagEmbedding.git In\u00a0[5]: Copied! <pre># from llama_index.postprocessor.flag_embedding_reranker import (\n#     FlagEmbeddingReranker,\n# )\n\n# rerank = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=5)\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10, node_postprocessors=[rerank]\n# )\n</pre> # from llama_index.postprocessor.flag_embedding_reranker import ( #     FlagEmbeddingReranker, # )  # rerank = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=5)  # query_engine = index.as_query_engine( #     similarity_top_k=10, node_postprocessors=[rerank] # ) In\u00a0[\u00a0]: Copied! <pre># !pip install llama-index-embeddings-huggingface\n</pre> # !pip install llama-index-embeddings-huggingface In\u00a0[\u00a0]: Copied! <pre># from llama_index.core.postprocessor import SentenceTransformerRerank\n\n# rerank = SentenceTransformerRerank(\n#     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n# )\n\n# query_engine = index.as_query_engine(\n#     similarity_top_k=10, node_postprocessors=[rerank]\n# )\n</pre> # from llama_index.core.postprocessor import SentenceTransformerRerank  # rerank = SentenceTransformerRerank( #     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3 # )  # query_engine = index.as_query_engine( #     similarity_top_k=10, node_postprocessors=[rerank] # )  In\u00a0[14]: Copied! <pre>response = query_engine.query(\n    \"What is Attention\"\n)\n\ndisplay(Markdown(str(response)))\n</pre> response = query_engine.query(     \"What is Attention\" )  display(Markdown(str(response))) <p>Attention is a function that maps a query and a set of key-value pairs to an output. In this process, the query, keys, values, and output are all represented as vectors. The resulting output is calculated as a weighted sum of these vectors.</p>"},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#setting-up-the-environment","title":"Setting up the Environment\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#setting-up-observability","title":"Setting Up observability\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#ingest-data-into-vector-db","title":"Ingest Data into vector DB\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#setting-up-index","title":"Setting Up Index\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#retrieval-comparisons","title":"Retrieval Comparisons\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#llm-reranker","title":"LLM ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#cohere-reranker","title":"Cohere ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#colber-reranker","title":"Colber ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#flag-embedding-reranker","title":"Flag Embedding ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#sentence-transformer-reranker","title":"Sentence Transformer ReRanker\u00b6","text":""},{"location":"RAG/02_ReRanker_RAG/notebook_eval/#set-up-query-engine","title":"Set Up Query Engine\u00b6","text":""},{"location":"RAG/03_Hybrid_RAG/","title":"Index","text":"<pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Dual Embedding\"\n        C1 &amp; C2 &amp; C3 --&gt; DEM{{Dense Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; SEM{{Sparse Embedding Model}}\n        DEM --&gt; DE1[Dense Embedding-1] &amp; DE2[Dense Embedding-2] &amp; DE3[Dense Embedding-n]\n        SEM --&gt; SE1[Sparse Embedding-1] &amp; SE2[Sparse Embedding-2] &amp; SE3[Sparse Embedding-n]\n    end\n\n    subgraph \"3. Unified Indexing\"\n        DE1 &amp; SE1 --&gt; UI1[Unified Index Entry-1]\n        DE2 &amp; SE2 --&gt; UI2[Unified Index Entry-2]\n        DE3 &amp; SE3 --&gt; UI3[Unified Index Entry-n]\n        UI1 &amp; UI2 &amp; UI3 --&gt; UDB[(Unified VectorDB)]\n    end\n\n    subgraph \"4. Query Processing\"\n        Q[Query] --&gt; QDEM{{Dense Embedding Model}}\n        Q --&gt; QSEM{{Sparse Embedding Model}}\n        QDEM --&gt; QDE[Query Dense Embedding]\n        QSEM --&gt; QSE[Query Sparse Embedding]\n    end\n\n    subgraph \"5. Two-Step Hybrid Retrieval\"\n        QSE --&gt; SS{Sparse Search}\n        SS --&gt; UDB\n        UDB --&gt;|Top K1 Sparse Results| SR[Sparse Retrieved Chunks]\n        SR --&gt; DS{Dense Search}\n        QDE --&gt; DS\n        DS --&gt;|Top K2 Dense Results| DR[Final Retrieved Chunks]\n    end\n\n    subgraph \"6. Context Formation\"\n        DR --&gt; CF[Query + Retrieved Chunks]\n    end\n\n    subgraph \"7. Generation\"\n        CF --&gt; LLM[LLM]\n        LLM --&gt; R[Response]\n    end\n\n    Q --&gt; CF</code></pre>"},{"location":"RAG/03_Hybrid_RAG/#sentence-window-retriever-based-rag-approach","title":"Sentence Window Retriever-Based RAG Approach","text":""},{"location":"RAG/03_Hybrid_RAG/#introduction","title":"Introduction","text":"<p>The Sentence Window Retriever-Based RAG (Retrieval-Augmented Generation) approach is an advanced implementation of the RAG framework, designed to enhance the context-awareness and coherence of AI-generated responses. This method combines the power of large language models with efficient information retrieval techniques, providing a robust solution for generating high-quality, context-rich responses.</p>"},{"location":"RAG/03_Hybrid_RAG/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with maintaining coherence across larger contexts or when dealing with information that spans multiple chunks of text. The Sentence Window Retriever-Based approach addresses this limitation by preserving the contextual relationships between chunks during the indexing process and leveraging this information during retrieval and generation.</p>"},{"location":"RAG/03_Hybrid_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/03_Hybrid_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Document Splitting: The input document is split into sentences.</li> <li>Chunk Creation: Sentences are grouped into manageable chunks.</li> <li>Embedding: Each chunk is processed through an embedding model to create vector representations.</li> <li>Vector Database Indexing: Chunk IDs, text, and embeddings are stored in a vector database for efficient similarity search.</li> <li>Document Structure Indexing: A separate database stores the relationships between chunks, including references to previous and next k chunks for each chunk.</li> </ol>"},{"location":"RAG/03_Hybrid_RAG/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Processing: The user query is embedded using the same embedding model used for chunks.</li> <li>Similarity Search: The query embedding is used to find the most relevant chunks in the vector database.</li> <li>Context Expansion: For each retrieved chunk, the system fetches the previous and next k chunks using the document structure database.</li> <li>Context Formation: The retrieved chunks and their expanded context are combined with the original query.</li> <li>Generation: The expanded context and query are passed to a large language model to generate a response.</li> </ol>"},{"location":"RAG/03_Hybrid_RAG/#flow-chart","title":"Flow Chart","text":"<p>The following flow chart illustrates the Sentence Window Retriever-Based RAG approach:</p> <pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Document] --&gt; B[Split into Sentences]\n        B --&gt; C[Group Sentences into Chunks]\n    end\n\n    subgraph \"2. Chunk Processing\"\n        C --&gt; D1[Chunk 1]\n        C --&gt; D2[Chunk 2]\n        C --&gt; D3[Chunk 3]\n        C --&gt; D4[...]\n        C --&gt; Dn[Chunk n]\n    end\n\n    subgraph \"3. Embedding\"\n        D1 &amp; D2 &amp; D3 &amp; D4 &amp; Dn --&gt; E{Embedding Model}\n        E --&gt; F1[Embedding 1]\n        E --&gt; F2[Embedding 2]\n        E --&gt; F3[Embedding 3]\n        E --&gt; F4[...]\n        E --&gt; Fn[Embedding n]\n    end\n\n    subgraph \"4. Indexing\"\n        F1 &amp; F2 &amp; F3 &amp; F4 &amp; Fn --&gt; G[(Vector Database)]\n        G --&gt;|Store| H[Chunk ID]\n        G --&gt;|Store| I[Chunk Text]\n        G --&gt;|Store| J[Embedding]\n    end\n\n    subgraph \"5. Document Structure Store\"\n        C --&gt; K[(Document Structure DB)]\n        K --&gt;|Store| L[Chunk ID]\n        K --&gt;|Store| M[Previous k Chunk IDs]\n        K --&gt;|Store| N[Next k Chunk IDs]\n    end\n\n    subgraph \"6. Retrieval\"\n        O[Query] --&gt; P{Embedding Model}\n        P --&gt; Q[Query Embedding]\n        Q --&gt; R{Similarity Search}\n        R --&gt; G\n        G --&gt; S[Retrieved Chunks]\n    end\n\n    subgraph \"7. Context Expansion\"\n        S --&gt; T{Expand Context}\n        T --&gt; K\n        K --&gt; U[Previous k Chunks]\n        K --&gt; V[Next k Chunks]\n        S &amp; U &amp; V --&gt; W[Expanded Context]\n    end\n\n    subgraph \"8. Generation\"\n        W --&gt; X[Query + Expanded Context]\n        X --&gt; Y[LLM]\n        Y --&gt; Z[Response]\n    end</code></pre>"},{"location":"RAG/03_Hybrid_RAG/#key-features-of-rag","title":"Key Features of RAG","text":"<ul> <li>Efficient Retrieval: Utilizes vector similarity search for fast and accurate information retrieval.</li> <li>Context Preservation: Maintains document structure and chunk relationships during indexing.</li> <li>Flexible Context Window: Allows for adjustable context expansion at retrieval time.</li> <li>Scalability: Capable of handling large document collections and diverse query types.</li> </ul>"},{"location":"RAG/03_Hybrid_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Coherence: By including surrounding chunks, the system can generate more coherent and contextually accurate responses.</li> <li>Reduced Hallucination: Access to expanded context helps the model ground its responses in retrieved information, reducing the likelihood of generating false or irrelevant content.</li> <li>Efficient Storage: Only stores necessary information in the vector database, optimizing storage usage.</li> <li>Adaptable Context: The size of the context window can be adjusted based on the specific needs of different queries or applications.</li> <li>Preservation of Document Structure: Maintains the original structure and flow of the document, allowing for more nuanced understanding and generation.</li> </ol>"},{"location":"RAG/03_Hybrid_RAG/#conclusion","title":"Conclusion","text":"<p>The Sentence Window Retriever-Based RAG approach offers a powerful solution for enhancing the quality and contextual relevance of AI-generated responses. By preserving document structure and allowing for flexible context expansion, this method addresses key limitations of traditional RAG systems. It provides a robust framework for building advanced question-answering, document analysis, and content generation applications.</p>"},{"location":"RAG/03_Hybrid_RAG/_Qdrant_Hybrid_Search/","title":"Qdrant Hybrid Search","text":"In\u00a0[6]: Copied! <pre>from llama_index.core import SummaryIndex\nfrom llama_index.readers.web import SimpleWebPageReader\ndocuments = SimpleWebPageReader(html_to_text=True).load_data(\n    [\"https://www.thoughtworks.com/en-in/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products\"]\n)\n</pre> from llama_index.core import SummaryIndex from llama_index.readers.web import SimpleWebPageReader documents = SimpleWebPageReader(html_to_text=True).load_data(     [\"https://www.thoughtworks.com/en-in/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products\"] ) In\u00a0[3]: Copied! <pre>documents[0]\n</pre> documents[0] Out[3]: <pre>Document(id_='https://www.thoughtworks.com/en-in/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n  * [What we do  ](/en-in/what-we-do \"What we do\")\\n\\n    * [ Go to overview ](/en-in/what-we-do)\\n      * ### Services\\n\\n        * [ Artificial Intelligence  ](/en-in/what-we-do/ai)\\n        * [ Cloud  ](/en-in/what-we-do/cloud)\\n        * [ Customer Experience and Products  ](/en-in/what-we-do/customer-experience-product-design)\\n        * [ Data and Analytics  ](/en-in/what-we-do/data)\\n        * [ Managed Services  ](/en-in/what-we-do/digital-application-management-and-operations)\\n        * [ Modernization  ](/en-in/what-we-do/modernization)\\n        * [ Platforms  ](/en-in/what-we-do/platforms)\\n\\n  * [Who we work with  ](/en-in/clients \"Who we work with\")\\n\\n    * [ Go to overview ](/en-in/clients)\\n    * [Automotive  ](/en-in/clients/automotive \"Automotive\")\\n    * [Healthcare and Life Sciences  ](/en-in/clients/healthcare \"Healthcare and Life Sciences\")\\n    * [Public Sector  ](/en-in/clients/public-sector \"Public Sector\")\\n    * [Cleantech, Energy and Utilities  ](/en-in/clients/cleantech-energy-utilities \"Cleantech, Energy and Utilities\")\\n    * [Media and Publishing  ](/en-in/clients/media-publishing \"Media and Publishing\")\\n    * [Retail and E-commerce  ](/en-in/clients/retail-ecommerce \"Retail and E-commerce\")\\n    * [Financial Services and Insurance  ](/en-in/clients/financial-services-insurance \"Financial Services and Insurance\")\\n    * [Not-for-profit  ](/en-in/clients/not-for-profit \"Not-for-profit\")\\n    * [Travel and Transport  ](/en-in/clients/travel-transport \"Travel and Transport\")\\n\\n  * [Insights  ](/en-in/insights \"Insights\")\\n\\n    * [ Go to overview ](/en-in/insights)\\n      * Loading\\n\\n###\\n\\n      * ### Resource Hubs\\n\\n        * [ Technology \\n\\nEnterprise technology and engineering excellence\\n\\n](/en-in/insights/technology)\\n\\n        * [ Business \\n\\nBusiness and industry insights for digital leaders\\n\\n](/en-in/insights/business)\\n\\n        * [ Culture \\n\\nExplore what it means to be a Thoughtworker\\n\\n](/en-in/insights/culture)\\n\\n      * ### Publications and Tools\\n\\n        * [ Technology Radar \\n\\nAn opinionated guide to today\\'s technology landscape\\n\\n](/en-in/radar)\\n\\n        * [ Perspectives \\n\\nA no-nonsense publication for digital leaders\\n\\n](/en-in/perspectives)\\n\\n        * [ Digital Fluency Model \\n\\nA model to help you build a resilient business\\n\\n](/en-in/digital-fluency)\\n\\n        * [ Decoder \\n\\nThe business execs\\' A-Z guide to technology\\n\\n](/en-in/insights/decoder)\\n\\n        * [ Looking Glass \\n\\nBringing the tech-led business changes into focus\\n\\n](/en-in/insights/looking-glass)\\n\\n      * ### All Insights\\n\\n        * [ Articles \\n\\nIn-depth insights to help your business grow\\n\\n](/en-in/insights/articles)\\n\\n        * [ Blogs \\n\\nExpert advice on strategy, design, engineering, and careers\\n\\n](/en-in/insights/blog)\\n\\n        * [ Books \\n\\nExplore our extensive library to keep learning\\n\\n](/en-in/insights/books)\\n\\n        * [ Podcasts \\n\\nConversations on the latest in business and tech\\n\\n](/en-in/insights/podcasts)\\n\\n  * [Careers  ](/en-in/careers \"Careers\")\\n\\n    * [ Go to overview ](/en-in/careers)\\n    * [Application Process \\n\\nWhat to expect as you interview with us\\n\\n](/en-in/careers/our-process \"Application Process\")\\n\\n    * [Consultant Life \\n\\nLearn what life is like as a Thoughtworker\\n\\n](/en-in/careers/consultant-life \"Consultant Life\")\\n\\n    * [Thoughtworks India graduates hiring  ](/en-in/careers/graduates \"Thoughtworks India graduates hiring\")\\n    * [Search Jobs \\n\\nFind open positions in your region\\n\\n](/en-in/careers/jobs \"Search Jobs\")\\n\\n    * [Stay Connected \\n\\nSign up for our monthly newsletter\\n\\n](/en-in/careers/access \"Stay Connected\")\\n\\n    * [Learning and Development \\n\\nExplore how we support career growth\\n\\n](/en-in/careers/learning-and-development \"Learning and Development\")\\n\\n    * [Benefits \\n\\nSee how we take care of our people\\n\\n](/en-in/careers/benefits \"Benefits\")\\n\\n  * [About  ](/en-in/about-us \"About\")\\n\\n    * [ Go to overview ](/en-in/about-us)\\n    * [Our Purpose  ](/en-in/about-us/our-purpose \"Our Purpose\")\\n    * [Diversity, Equity and Inclusion  ](/en-in/about-us/diversity-and-inclusion \"Diversity, Equity and Inclusion\")\\n    * [Our History  ](/en-in/about-us/history \"Our History\")\\n    * [Our Leaders  ](/en-in/about-us/leaders \"Our Leaders\")\\n    * [Social Change  ](/en-in/about-us/social-change \"Social Change\")\\n    * [News  ](/en-in/about-us/news \"News\")\\n    * [Partnerships  ](/en-in/about-us/partnerships \"Partnerships\")\\n    * [Sustainability  ](/en-in/about-us/sustainability \"Sustainability\")\\n    * [Conferences and Events  ](/en-in/about-us/events \"Conferences and Events\")\\n    * [Our Brand  ](/en-in/about-us/brand \"Our Brand\")\\n    * [Awards and Recognition  ](/en-in/about-us/awards-recognition \"Awards and Recognition\")\\n\\n  * [Investors  ](https://investors.thoughtworks.com/ \"Investors\")\\n  * [Contact  ](/en-in/contact-us \"Contact\")\\n\\nSearch\\n\\nClose\\n\\n![](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/in.svg) India | English\\n\\n  * ![Australia flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/au.svg) Australia\\n\\n[English](/en-au/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![Brazil flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/br.svg) Brazil\\n\\n[English](/en-br/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\") | [Portugu\u00eas](/pt-br \"Portugu\u00eas\")\\n\\n  * ![Canada flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/ca.svg) Canada\\n\\n[English](/en-ca/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![Chile flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/cl.svg) Chile\\n\\n[English](/en-cl/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\") | [ Espa\u00f1ol](/es-cl \" Espa\u00f1ol\")\\n\\n  * ![China flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/cn.svg) China\\n\\n[Hong Kong SAR (English)](/en-cn/insights/blog/data-strategy/building-an-\\namazon-com-for-your-data-products \"Hong Kong SAR \\\\(English\\\\)\") | [Mainland\\n(Chinese)](/zh-cn \"Mainland \\\\(Chinese\\\\)\")\\n\\n  * ![Ecuador flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/ec.svg) Ecuador\\n\\n[English](/en-ec/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\") | [ Espa\u00f1ol](/es-ec \" Espa\u00f1ol\")\\n\\n  * ![Germany flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/de.svg) Germany\\n\\n[English](/en-de/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\") | [Deutsch](/de-de \"Deutsch\")\\n\\n  * ![India flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/in.svg) India\\n\\n[English](/en-in/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![Singapore flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/sg.svg) Singapore\\n\\n[English](/en-sg/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![Spain flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/es.svg) Spain\\n\\n[English](/en-es/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\") | [ Espa\u00f1ol](/es-es \" Espa\u00f1ol\")\\n\\n  * ![Thailand flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/th.svg) Thailand\\n\\n[English](/en-th/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![United Kingdom flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/gb.svg) United Kingdom\\n\\n[English](/en-gb/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![United States flag](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/us.svg) United States\\n\\n[English](/en-us/insights/blog/data-strategy/building-an-amazon-com-for-your-\\ndata-products \"English\")\\n\\n  * ![Worldwide icon](/etc.clientlibs/thoughtworks/clientlibs/clientlib-site/resources/images/global.svg) Worldwide\\n\\n[English](/insights/blog/data-strategy/building-an-amazon-com-for-your-data-\\nproducts \"English\")\\n\\n![](/content/dam/thoughtworks/images/photography/banner-\\nimage/insights/in_banner_blogs.jpg)\\n![](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n#  Building An \u201cAmazon.com\u201d For Your Data Products\\n\\nSurfacing reliability SLIs and SLOs can boost adoption. Here\u2019s how.\\n\\n[ Blogs Back ](/en-in/insights/blog) [ Blogs Back ](/en-in/insights/blog)\\n\\n![Social share button](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/share-fill.svg)\\n\\nClose\\n\\n[ Data strategy ](https://www.thoughtworks.com/insights/topic/data-strategy) [\\nData mesh ](https://www.thoughtworks.com/insights/topic/data-mesh) [ Blog\\n](/en-in/insights/blog)\\n\\nBy\\n\\n[Barr Moses](/en-in/profiles/b/barr-moses) ,\\n\\n[Manisha Jain](/en-in/profiles/m/manisha-jain)  and\\n\\n[Pablo Porto](/en-in/profiles/p/pablo-porto)\\n\\nPublished: June 20, 2023\\n\\n![Customer 360 Data\\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n![Customer 360 Data\\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nHave you ever come across an internal [data\\nproduct](https://www.thoughtworks.com/en-us/what-we-do/data-and-ai/modern-\\ndata-engineering-playbook/data-as-a-product) and side-eyed it like it\u2019s your\\nkid\u2019s prom date? While it _seems_ like it fits the requirements, you don\u2019t\\nquite trust it \u2014 who knows where the data in this shifty table has been. Will\\nit be reliable and safe even after you turn your focus elsewhere? Will the\\nschema stay true?\\n\\nThis project is your baby; you just can\u2019t risk it. So, just to be safe you\\ntake the extra time to recreate the dataset.\\n\\n## Data products and trustworthiness\\n\\nAccording to Zhamak Dehgahi, data products should be discoverable,\\naddressable, trustworthy, self-describing, interoperable and secure. In our\\nexperience, most data products only support one or two use cases. That\u2019s a\\nlost opportunity experienced by too many data teams, especially those with\\ndecentralized organizational structures or implementing [data\\nmesh](https://www.montecarlodata.com/blog-what-is-a-data-mesh-and-how-not-to-\\nmesh-it-up/).\\n\\n![Data product characteristics as originally defined by Zhamak Dehghani.\\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nData product characteristics as originally defined by Zhamak Dehghani.\\n\\n![Data product characteristics as originally defined by Zhamak Dehghani.\\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nData product characteristics as originally defined by Zhamak Dehghani.\\n\\nIn the focus on building data trust with business stakeholders, it\u2019s easy to\\nlose sight of the importance of also building trust with data teams across\\ndifferent domains. However, a data product must be trustworthy if it\u2019s to\\nencourage the reuse of data products. This is what ultimately **separates data\\nmesh from data silo.**\\n\\nThe data product is trustworthy if data consumers are confident in the\\naccuracy and reliability of the data. Data products should be transparent with\\nregards to information quality metrics and performance promises.\\n\\nCreating a central marketplace or catalog of internal data products is a great\\nfirst step to raising awareness, but more is needed to convince skeptical data\\nconsumers to actually start using them.\\n\\nFor this, we can take a page out of Amazon.com\u2019s playbook. Amazon provides an\\nincredible amount of detail to help consumers purchase products from unknown\\nthird-parties. Take the example of something as simple as a wrench:\\n\\n[\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\\n](https://www.amazon.com/Amazon-Brand-Denali-8-Inch-\\nAdjustable/dp/B091BLK385/ref=sr_1_1_ffob_sspa?crid=39GIJHE50YBB1&amp;keywords=wrench&amp;qid=1681395714&amp;sprefix=wrench%2Caps%2C70&amp;sr=8-1-spons&amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUE1RDdMRDJXTFMxWEkmZW5jcnlwdGVkSWQ9QTAzMzY4NDQzT0NYSFNPR1A3OFZOJmVuY3J5cHRlZEFkSWQ9QTAxODYxODQxMVZDUzkyNlM4TFFRJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&amp;th=1)\\n\\nIt\u2019s not just a _wrench_ \u2014 it\u2019s an adjustable Denali, 7.7 inch, 4.4 ounce,\\nrust resistant steel, adjustable wrench for repairs, maintenance and general\\nuse, covered by a limited lifetime warranty. Oh, and here are similar products\\nand reviews from users like yourself.\\n\\nData teams and data product owners need to be as capable of marketing data\\nproducts as they are at building them. Otherwise, you\u2019re not going to see the\\nadoption levels that justify the value of your data initiative.\\n\\nThe central \u201cstore\u201d for your data products needs to include not just\\ninformation about the data, but information about the context of how it can be\\nused. In other words, it needs to provide metrics such as uptime or data\\nfreshness; these are commonly referred to as service level objectives (SLO)\\n\\n  \\n  \\n\\nThoughtworks has helped create one of the more [advanced deployments of Monte\\nCarlo \u2014 ](https://www.thoughtworks.com/en-th/insights/blog/data-strategy/dev-\\nexperience-data-mesh-platform)a data observability platform that monitors the\\nhealth and quality of data \u2014[ within a data mesh\\nimplementation](https://www.thoughtworks.com/en-th/insights/blog/data-\\nstrategy/dev-experience-data-mesh-platform).\\n\\nIn this post, we will explore the process of implementation and go further by\\nexploring what else is possible.\\n\\n##  \\n  \\nWhere to start: Identifying reusable data products\\n\\nThe two best ways to fail at creating valuable, reusable data products are to\\ndevelop them without any sense of who they are for and to make them more\\ncomplicated than they need to be.\\n\\nOne of the best ways to succeed is by involving business and product\\nleadership and identifying the most valuable and shared use cases.\\nThoughtworks, for example, often identifies potential data products by working\\nbackwards from the use case using the [Jobs to be done\\n(JTBD)](https://jtbd.info/2-what-is-jobs-to-be-done-jtbd-796b82081cca)\\nframework created by Clayton Christensen.\\n\\n![Example JTBD framework for a Customer 360 data product. Image courtesy of\\nthe\\nauthors.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_4.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nExample JTBD framework for a Customer 360 data product. Image courtesy of the\\nauthors.\\n\\n![Example JTBD framework for a Customer 360 data product. Image courtesy of\\nthe\\nauthors.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_4.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nExample JTBD framework for a Customer 360 data product. Image courtesy of the\\nauthors.\\n\\nAnother strategy is to evaluate the [data\\nlineage](https://www.montecarlodata.com/blog-data-lineage/) within your\\ncurrent environment. It\u2019s likely that your tables will follow some sort of\\nPareto distribution where 20% will have 80% of the queries run against them\\n(or power 80% of the most-visited dashboards).\\n\\nFor example, if the table customer_accounts is constantly being queried by\\nmarketing, finance, support and other domains, that can be taken as a signal\\nthat building a data product that consolidates the necessary information into\\na full 360 view may have shared utility.\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_5.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_5.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n## Second step: Creating data product SLOs\\n\\n##\\n\\nA key part of data product thinking is keeping the consumers at the center and\\nconsidering what provides the most value for them. The only way to ensure we\\nare delivering high-quality data products is to identify those consumers,\\nunderstand their requirements and codify their expectations within a [SLO/SLI\\nframework](https://www.thoughtworks.com/en-us/insights/articles/data-mesh-in-\\npractice-product-thinking-and-development).\\n\\nYou can think of SLOs as measures that remove uncertainty surrounding the data\\nand serve as a primary way to define trustworthiness for its consumers.\\n\\nAs explained in [Zhamak\u2019s Data Mesh\\nbook](https://www.oreilly.com/library/view/data-mesh/9781492092384/), in\\ncontrast to previous approaches to data management, data mesh introduces a\\nfundamental shift in that the owners of data products must communicate and\\nguarantee an acceptable level of quality and trust\u2010worthiness as it is an\\nimportant characteristic of the data product. This means cleansing and running\\nautomated data integrity tests or data quality monitors at the point the data\\nproducts are created.\\n\\nIf SLOs are breached, the data product team must be notified so they can take\\nremediation measures. Like a typical business contract, data product SLOs will\\nlikely evolve over time based on changing circumstances.\\n\\nThoughtworks uses a discovery exercise during its [data mesh\\nacceleration](https://martinfowler.com/articles/data-mesh-accelerate-\\nworkshop.html#DiscoveringDataProducts) workshop on product usage patterns.\\nThis helps teams collectively brainstorm and understand usage, expectations,\\ntrade-offs and business impact. The outcomes of the exercise are then used to\\ndetermine the various SLOs that need to be set for individual products.\\n\\n![Product usage pattern exercise template. Courtesy of\\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\\n\\n![Product usage pattern exercise template. Courtesy of\\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\\n\\n## Third step: Implementing the SLOs\\n\\n##\\n\\nDuring the implementation phase of the data product, the data product team\\nwill start by defining the metrics (SLIs) used to measure the SLO.\\n\\nOne common SLI for data products is freshness. In the example from the\\nprevious section, the exercise may reveal the marketing team relies heavily on\\na particular dashboard that supports the monitoring of daily campaign and\\npurchasing behaviors, which means the data needs to be updated every day.\\n\\nThe customer service team, on the other hand, may require hourly updates to\\nbetter engage with customers in real time. In this scenario, it is almost\\ncertainly more efficient to build the data product to be updated hourly to\\nserve both consumer groups rather than build two different data products. The\\nmarketing team isn\u2019t going to complain about having data that is more\\nfrequently updated than they requested after all!\\n\\nSLIs are typically expressed as a percentage over a period of time. In the\\nexample presented earlier, 99% freshness over an hourly interval is the SLI in\\nplace for the Customer 360 data product.\\n\\nIn our example, the team has decided to track data freshness checks based on\\nthe processing timestamp attribute present in the dataset that is served by\\nthe data product: processing_timestamp. To do this, they start by defining a\\n[monitor as code](https://docs.getmontecarlo.com/docs/monitors-as-code) that\\nwill become part of the data product which will support the implementation of\\nthe freshness SLO:\\n\\n    \\n    \\n    namespace: customer-domain\\n    montecarlo:\\n      freshness:\\n        - description: Customer 360 Data Product Freshness Monitor\\n          name:\\xa0 Freshness - Customer 360 Data Product\\n          table: analytics:prod.customer_360_dp.customers\\n          freshness_threshold: 240\\n          schedule:\\n            type: fixed\\n            interval_minutes: 240\\n            start_time: \"2022-09-15T01:00:00\"\\n    \\n\\nThe data team can then automate the deployment of this monitor via the CI/CD\\npipeline using the Monte Carlo CLI:\\n\\n    \\n    \\n    montecarlo monitors apply --namespace customer-domain\\n    \\n\\nThis ensures the monitor to support the SLO is implemented and deployed every\\ntime there is a change via the CI/CD pipeline. The monitor as code\\nfunctionality improves the experience of the data product developer in\\nmaintaining and deploying these monitors at scale using version control  \\n\\nThe stakeholder exercise may also reveal that the Customer 360 data product\\nshould not contain deleted rows in the final table as customers will be marked\\nas active or inactive rather than removed entirely. To ensure this, a custom\\nvolume SLI can be set to monitor and ensure the data product follows this\\nbehavior.\\n\\nFinally, data product users need to be alerted whenever any changes are made\\nto the schema of any tables within or upstream of the data product. This is\\nbecause such changes could break processes downstream; there could be new\\nfields that can enable new use cases. This can be covered by an automated\\nschema monitor which sends alerts via the appropriate communication channel.\\n\\n## Going beyond basic SLOs\\n\\n##\\n\\nSo far we have covered three basic dimensions that can be used as SLOs. There\\nare several other dimensions improving data product trust such as accuracy and\\navailability. These and others are described in the [Implementing Service\\nLevel Objectives book](https://www.oreilly.com/library/view/implementing-\\nservice-level/9781492076803/).\\n\\nMore advanced SLOs can better validate data product quality and encourage\\nwider use throughout the organization.\\n\\nFor example, let\\'s imagine the data in our Customer 360 data product is not\\ncomplete. Perhaps our stakeholder exercise revealed the channel and region\\nwhere the customer buys the product is important for the marketing team\u2019s\\ndigital advertising decisions while the customer service team cares deeply\\nthat every customer has a profile in the system.\\n\\nWe could use field health monitors on relevant fields within the data product\\nsuch as region and purchase_channel to surface the number of anomalies over a\\ncertain time period on the attributes the marketing team needs to segment\\nusers. If any of these fields experience anomalous NULL rates or values\\noutside the typical distribution, remediations can be launched to ensure\\ncompliance with stated SLOs. Similarly, we could place field health monitors\\non the account_id field to ensure it is never NULL so that the data product\\nperforms to the customer service team\u2019s standards.\\n\\nDeploying field health monitors has the added benefit of profiling the data,\\nwhich can provide additional context that helps encourage adoption for those\\nnot as familiar with the data or the data product.\\n\\nWhat the field profile feature looks like:\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_7.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_7.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nLet\u2019s look at another possible SLO related to data quality. Consider a\\npurchase order data product tracking the purchases/transactions made by the\\ncustomer. This data product is used as a source for Customer 360 data product\\nto understand the purchase patterns of the customer based on a\\npurchase_timestamp.\\n\\n  \\nUsing a [dimension distribution\\nmonitor](https://docs.getmontecarlo.com/docs/understanding-dimension-tracking-\\nmonitors), we can identify a potential anomaly when an active customer does\\nnot have any purchases made in the recent timeline, highlighting the lack of\\ndata trust/quality on the upstream purchase order data product.\\n\\n### Other indicators to build data trust\\n\\n###\\n\\nAt this point, we have reassured any potential data product users that there\\nare no freshness, volume, schema, or data quality issues that will prevent\\nthem from benefiting from its use. But what other information can we surface\\nthat will speak to the data product\u2019s trustworthiness?\\n\\nOne idea is to go beyond describing the data itself to surfacing information\\non its level of support and consumption. To harken back to our Denali wrench\\nexample, the Amazon.com page doesn\u2019t just describe the product itself, it also\\nincludes information on the lifetime warranty. Netflix doesn\u2019t just tell its\\nviewers the plot of the movie, it also has a list of the top ten most popular.\\n\\nThe data product equivalents of this are:\\n\\n  * **Total tests or custom monitors** : If there are more dbt tests across a pipeline or it has more Monte Carlo custom monitors set, this indicates more granular support and reliability in depth.\\n\\n  * **Coverage percentage:** Data products typically involve a series of complex, interdependent operations upstream. Data moves and is transformed from table to table. Understanding that a data product has basic data monitoring coverage across its [data lineage](https://www.montecarlodata.com/blog-data-lineage/) helps further build trust.\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_8.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_8.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n  * **Average time to response/fixed:** The overview above, currently at the domain level, highlights important metrics to consider when monitoring the health of the data products. Similar to the stability metrics in the DORA 4 key metrics framework, the metrics shown in this overview, \u201cTime to response\u201d and \u201cTime to Fixed,\u201d indicate how long it takes a data product team to spot and recover from any type of incident that could lead to breaching the SLOs. Faster response and fix times indicate data product stability and highlights the maturity of the supporting data product teams thus increasing the trustworthiness over time. \\n\\n  * **Key asset score:** An all too common story is when a member of the data team leverages a seemingly ideal table or data product as part of their task, only later to find out it\u2019s been deprecated or an older version. Monte Carlo\\'s [Key Asset Score, calculated by the ](https://docs.getmontecarlo.com/docs/key-assets-importance-score)reads and writes and the downstream consumption on each dataset part of the data product, can give data product users (and re-users) confidence the asset is safe to use. It can also be helpful for data product owners to measure their success, in a data mesh context, based on the satisfaction and growth of their data product consumers.\\n\\n## Fourth step: Monitoring and visualizing data product SLO health\\n\\n##\\n\\nThe data product teams select what SLOs their data products guarantee, and\\nultimately they are responsible for the satisfaction of their data products\u2019\\nconsumers. To succeed on this, they need the right tools to monitor and track\\nthe SLOs over time.\\n\\nMonte Carlo\\'s notification mechanism enables this by notifying the data\\nproduct teams on any SLO breach incident. To improve the developer experience,\\nthese notifications can also be defined as\\n[code](https://docs.getmontecarlo.com/docs/notifications-as-code) in the\\nlatest version of Monte Carlo and be included as part of the CI/CD pipeline.\\n\\nMonte Carlo also provides functionality to extract some or all of this\\nmonitoring metadata via APIs to publish them in catalogs like\\n[Collibra](https://www.collibra.com/us/en), [dataworld](https://data.world),\\nor [Atlan](https://atlan.com). This is critical for making data products\\ndiscoverable. It\u2019s also where all of the work your team has done to create and\\nautomatically monitor SLOs and SLIs comes together and is put on display.\\n\\nData product owners and data platform teams can leverage these APIs to\\nvisualize the health of the data products in the marketplace via custom\\nintegrations similar to the solution shared in a [past\\nwebinar](https://vimeo.com/765878759).\\n\\n![Service Example - \\\\(4\\\\) Check and show service levels Delivering data\\nproduct health information as part of the user\\nexperience](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_9.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\n![Service Example - \\\\(4\\\\) Check and show service levels Delivering data\\nproduct health information as part of the user\\nexperience](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_9.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nFinally, if you are using dbt for modeling and transforming for your data,\\n[Monte Carlo offers a dbt integration\\n](https://docs.getmontecarlo.com/docs/dbt-integration)that automates the\\nincident creation on every dbt test failure. This provides a holistic view of\\nincidents created due to data quality tests failing for a data served by the\\ndata product, provides our data quality health of the data product and also\\neases debugging. By enabling this integration, the team can leverage Monte\\nCarlo\u2019s notification channel to also receive alerts on data quality issues.\\n\\nTo implement this, the data product team can run the dbt data quality test as\\npart of their data pipeline and upload the results to Monte Carlo with a\\nsimple CLI command.\\n\\n    \\n    \\n    &gt; dbt test\\n    &gt; montecarlo import dbt-run \\\\\\n            --manifest ./target/manifest.json\\xa0 \\\\\\n            --run-results ./target/run_results.json \\\\\\n            --project-name customer-360-data-product\\n    \\n\\n## Putting it all together\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_10.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nSource: Montecarlo\\n\\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_10.png)\\n\\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/pause-icon.svg)\\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/play-icon.svg)\\n\\nSource: Montecarlo\\n\\nThe data mesh principles, especially the data as a product concept can create\\ntremendous business value.\\n\\nDefining SLOs will help build reliable data products that fit business user\\nneeds and surfacing their value will create the level of data trust required\\nfor data driven organizations to thrive.\\n\\nUltimately, the more product context you provide for data consumers and team\\nmembers across the organization, the more efficiencies and value you will be\\nable to derive from a \u201cbuild once use many times\u201d approach. Good luck!\\n\\n## _Appendix:_\\n\\nData Mesh Accelerated workshop formulated by Paulo Caroli as explained in this\\narticle &lt;https://martinfowler.com/articles/data-mesh-accelerate-workshop.html&gt;\\n\\nhelps teams and organizations accelerate their Data Mesh transformation, by\\nunderstanding their current state and exploring what the next steps will look\\nlike.\\n\\nDisclaimer: The statements and opinions expressed in this article are those of\\nthe author(s) and do not necessarily reflect the positions of Thoughtworks.\\n\\n## Related blogs\\n\\n[\\n![](/content/dam/thoughtworks/images/photography/abstract/insights/blog/abs_blogs_006.jpg)\\n![](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)\\nData mesh Data Mesh in practice: Getting off to the right start Learn more\\n](/en-in/insights/articles/data-mesh-in-practice-getting-off-to-the-right-\\nstart)\\n\\n[\\n![](/content/dam/thoughtworks/images/photography/abstract/insights/blog/abs_blogs_057.jpg)\\n![](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)\\nData strategy Data Mesh in practice: Organizational operating model Learn more\\n](/en-in/insights/articles/data-mesh-in-practice-organizational-operating-\\nmodel)\\n\\n[\\n![](/content/dam/thoughtworks/images/photography/abstract/insights/blog/abs_blogs_001.jpg)\\n![](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)\\nData engineering Data Mesh at Glovo  Learn more ](/en-in/insights/blog/data-\\nengineering/data-mesh-at-glovo)\\n\\n## How can you achieve faster growth?\\n\\n[ Connect with us ](/en-in/contact-us)\\n\\nCompany\\n\\n  * [About us](/en-in/about-us)\\n  * [What we do](/en-in/what-we-do)\\n  * [Partnerships](/en-in/about-us/partnerships)\\n  * [Who we work with](/en-in/clients)\\n  * [News](/en-in/about-us/news)\\n  * [Diversity, Equity and Inclusion](/en-in/about-us/diversity-and-inclusion)\\n  * [Careers](/en-in/careers)\\n  * [Investors](https://investors.thoughtworks.com/)\\n  * [Contact us](/en-in/contact-us)\\n\\nInsights\\n\\n  * [Articles](/en-in/insights/articles)\\n  * [Blogs](/en-in/insights/blog)\\n  * [Books](/en-in/insights/books)\\n  * [Podcasts](/en-in/insights/podcasts)\\n\\nSite info\\n\\n  * [Privacy policy](/en-in/about-us/privacy-policy)\\n  * [Accessibility statement](/en-in/about-us/accessibility)\\n  * [Modern slavery statement](/content/dam/thoughtworks/documents/guide/tw_guide_modern_slavery_statement.pdf)\\n  * [Corporate Social Responsibility Policy](/content/dam/thoughtworks/documents/guide/tw_guide_csrpolicy_india.pdf)\\n  * [Policy of Equal Opportunity, Non-Discrimination and Anti-Harassment at the Workplace](/content/dam/thoughtworks/documents/guide/tw_guide_policy_of%20_equal_opportunity_non_discrimination_anti_harassment_india.pdf)\\n  * [Code of conduct](/content/dam/thoughtworks/documents/guide/tw_guide_code_of_conduct_en.pdf)\\n  * [Integrity helpline](https://integrity.thoughtworks.com)\\n\\nConnect with us\\n\\n[ ](https://www.linkedin.com/company/thoughtworks \"Link to Thoughtworks\\nLinkedin page\") [ ](https://www.facebook.com/Thoughtworks \"Link to\\nThoughtworks Facebook page\") [ ](https://www.twitter.com/thoughtworks \"Link to\\nThoughtworks Twitter account\") [ ](javascript: \"Link to Thoughtworks China\\nWeChat subscription account QR code\")\\n\\n[\u00d7](javascript:void\\\\(0\\\\);) WeChat\\n\\n![QR code to Thoughtworks China WeChat subscription\\naccount](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/wechat_qr_code.jpg)\\n\\n[ ](https://www.youtube.com/user/thoughtworks \"Link to Thoughtworks Youtube\\npage\") [ ](https://www.instagram.com/thoughtworks/ \"Link to Thoughtworks\\nInstagram page\")\\n\\n\u00a9 2024 Thoughtworks, Inc.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')</pre> In\u00a0[5]: Copied! <pre>documents[0]\n</pre> documents[0] Out[5]: <pre>Document(id_='83e760c3-aa01-433a-a603-019d12eee223', embedding=None, metadata={'page_label': '1', 'file_name': 'llama2.pdf', 'file_path': '/Users/samvardhan/Desktop/DataEngineer/opensearch_rag/data_pdf/llama2.pdf', 'file_type': 'application/pdf', 'file_size': 13661300, 'creation_date': '2024-04-21', 'last_modified_date': '2024-04-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron\u2217Louis Martin\u2020Kevin Stone\u2020\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\u2217\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n\u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n\u2020Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')</pre> In\u00a0[7]: Copied! <pre>from llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.core import Settings\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\n\n# creates a persistant index to disk\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n</pre> from llama_index.core import VectorStoreIndex, StorageContext from llama_index.core import Settings from llama_index.vector_stores.qdrant import QdrantVectorStore from qdrant_client import QdrantClient  # creates a persistant index to disk  client = QdrantClient(url=\"http://localhost:6333\") In\u00a0[8]: Copied! <pre>from llama_index.core.node_parser import SentenceSplitter\ntext_parser = SentenceSplitter(\n chunk_size=1024,\n)\ntext_chunks = []\ndoc_idxs = []\nfor doc_idx, doc in enumerate(documents):\n    cur_text_chunks = text_parser.split_text(doc.text)\n    text_chunks.extend(cur_text_chunks)\n    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n</pre> from llama_index.core.node_parser import SentenceSplitter text_parser = SentenceSplitter(  chunk_size=1024, ) text_chunks = [] doc_idxs = [] for doc_idx, doc in enumerate(documents):     cur_text_chunks = text_parser.split_text(doc.text)     text_chunks.extend(cur_text_chunks)     doc_idxs.extend([doc_idx] * len(cur_text_chunks)) In\u00a0[9]: Copied! <pre>from llama_index.core.schema import TextNode, IndexNode\nnodes = []\nfor idx, text_chunk in enumerate(text_chunks):\n node = TextNode(\n text=text_chunk,\n )\n src_doc = documents[doc_idxs[idx]]\n node.metadata = src_doc.metadata\n nodes.append(node)\n</pre> from llama_index.core.schema import TextNode, IndexNode nodes = [] for idx, text_chunk in enumerate(text_chunks):  node = TextNode(  text=text_chunk,  )  src_doc = documents[doc_idxs[idx]]  node.metadata = src_doc.metadata  nodes.append(node) In\u00a0[10]: Copied! <pre>from llama_index.embeddings.huggingface import HuggingFaceEmbedding\nembed_model = HuggingFaceEmbedding(model_name=\"avsolatorio/GIST-Embedding-v0\")\n</pre> from llama_index.embeddings.huggingface import HuggingFaceEmbedding embed_model = HuggingFaceEmbedding(model_name=\"avsolatorio/GIST-Embedding-v0\") In\u00a0[11]: Copied! <pre>for node in nodes:\n    node_embedding = embed_model.get_text_embedding(\n    node.get_content(metadata_mode=\"all\")\n    )\n    node.embedding = node_embedding\n</pre> for node in nodes:     node_embedding = embed_model.get_text_embedding(     node.get_content(metadata_mode=\"all\")     )     node.embedding = node_embedding In\u00a0[12]: Copied! <pre>from llama_index.llms.ollama import Ollama\nllm = Ollama(model=\"llama2\", request_timeout=30.0)\n</pre> from llama_index.llms.ollama import Ollama llm = Ollama(model=\"llama2\", request_timeout=30.0) In\u00a0[13]: Copied! <pre>from llama_index.core import Settings\nfrom llama_index.core import ServiceContext, set_global_service_context\n\nservice_context = ServiceContext.from_defaults(\n  llm=llm, embed_model=embed_model\n)\n</pre> from llama_index.core import Settings from llama_index.core import ServiceContext, set_global_service_context  service_context = ServiceContext.from_defaults(   llm=llm, embed_model=embed_model ) <pre>/var/folders/d8/2pt8r3f50tq3863jc_l0zz0r0000gn/T/ipykernel_34872/183125863.py:4: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n  service_context = ServiceContext.from_defaults(\n</pre> In\u00a0[14]: Copied! <pre>import qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import VectorStoreIndex, StorageContext\nfrom qdrant_client import models\nclient = qdrant_client.QdrantClient(location=\":memory:\")\nclient.recreate_collection(\n    collection_name=\"my_collection\",\n    vectors_config={\n        \"text-dense\": models.VectorParams(\n            size=768,\n            distance=models.Distance.COSINE,\n        )\n    },\n    sparse_vectors_config={\n        \"text-sparse\": models.SparseVectorParams(\n            index=models.SparseIndexParams()\n        )\n    },\n)\n\n\nvector_store = QdrantVectorStore(\n    collection_name=\"my_collection\", client=client, enable_hybrid=True\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n</pre> import qdrant_client from llama_index.vector_stores.qdrant import QdrantVectorStore from llama_index.core import VectorStoreIndex, StorageContext from qdrant_client import models client = qdrant_client.QdrantClient(location=\":memory:\") client.recreate_collection(     collection_name=\"my_collection\",     vectors_config={         \"text-dense\": models.VectorParams(             size=768,             distance=models.Distance.COSINE,         )     },     sparse_vectors_config={         \"text-sparse\": models.SparseVectorParams(             index=models.SparseIndexParams()         )     }, )   vector_store = QdrantVectorStore(     collection_name=\"my_collection\", client=client, enable_hybrid=True ) storage_context = StorageContext.from_defaults(vector_store=vector_store) In\u00a0[15]: Copied! <pre>index = VectorStoreIndex.from_documents(\n documents, storage_context=storage_context, service_context=service_context\n)\n</pre> index = VectorStoreIndex.from_documents(  documents, storage_context=storage_context, service_context=service_context ) In\u00a0[16]: Copied! <pre>vector_store.add(nodes)\n</pre> vector_store.add(nodes) Out[16]: <pre>['b0052b8f-5349-4113-9341-dea28a8d639d',\n '9afd200b-904d-4e3b-bc61-5fb1a2a13604',\n '85bb9d30-39a5-4253-aad9-261788f57e6b',\n 'a078d100-e6ca-469f-83ab-eeaf3770dcb7',\n 'c978814f-6496-4012-a090-d0bcd125c670',\n 'c4d39fe1-753a-419d-a32d-8b3ae7bd9b70',\n '21a2f676-25c2-4e24-8cfe-0a2aaf4399dc',\n 'c2168d3c-ac69-492e-ac2f-790dc048a2d8',\n 'd8567608-3d53-4887-a026-362b3db112be',\n '60a8462a-3b24-4eeb-a03a-681ea00359c5',\n '89834f7f-d02f-4aa3-b178-f6038b33c556']</pre> In\u00a0[17]: Copied! <pre>query_str = \"how is the author of the article Building An \u201cAmazon.com\u201d For Your Data Products\"\nquery_embedding = embed_model.get_query_embedding(query_str)\n</pre> query_str = \"how is the author of the article Building An \u201cAmazon.com\u201d For Your Data Products\" query_embedding = embed_model.get_query_embedding(query_str) In\u00a0[18]: Copied! <pre>from llama_index.core.vector_stores import (\n    VectorStoreQuery,\n    VectorStoreQueryResult,\n)\nquery_mode = \"default\"\n# query_mode = \"sparse\"\n# query_mode = \"hybrid\"\nvector_store_query = VectorStoreQuery(\n query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n)\nquery_result = vector_store.query(vector_store_query)\nprint(query_result.nodes[0].get_content())\n</pre> from llama_index.core.vector_stores import (     VectorStoreQuery,     VectorStoreQueryResult, ) query_mode = \"default\" # query_mode = \"sparse\" # query_mode = \"hybrid\" vector_store_query = VectorStoreQuery(  query_embedding=query_embedding, similarity_top_k=2, mode=query_mode ) query_result = vector_store.query(vector_store_query) print(query_result.nodes[0].get_content()) <pre>Amazon provides an\nincredible amount of detail to help consumers purchase products from unknown\nthird-parties. Take the example of something as simple as a wrench:\n\n[\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n](https://www.amazon.com/Amazon-Brand-Denali-8-Inch-\nAdjustable/dp/B091BLK385/ref=sr_1_1_ffob_sspa?crid=39GIJHE50YBB1&amp;keywords=wrench&amp;qid=1681395714&amp;sprefix=wrench%2Caps%2C70&amp;sr=8-1-spons&amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUE1RDdMRDJXTFMxWEkmZW5jcnlwdGVkSWQ9QTAzMzY4NDQzT0NYSFNPR1A3OFZOJmVuY3J5cHRlZEFkSWQ9QTAxODYxODQxMVZDUzkyNlM4TFFRJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&amp;th=1)\n\nIt\u2019s not just a _wrench_ \u2014 it\u2019s an adjustable Denali, 7.7 inch, 4.4 ounce,\nrust resistant steel, adjustable wrench for repairs, maintenance and general\nuse, covered by a limited lifetime warranty. Oh, and here are similar products\nand reviews from users like yourself.\n\nData teams and data product owners need to be as capable of marketing data\nproducts as they are at building them. Otherwise, you\u2019re not going to see the\nadoption levels that justify the value of your data initiative.\n\nThe central \u201cstore\u201d for your data products needs to include not just\ninformation about the data, but information about the context of how it can be\nused. In other words, it needs to provide metrics such as uptime or data\nfreshness; these are commonly referred to as service level objectives (SLO)\n\n  \n  \n\nThoughtworks has helped create one of the more [advanced deployments of Monte\nCarlo \u2014 ](https://www.thoughtworks.com/en-th/insights/blog/data-strategy/dev-\nexperience-data-mesh-platform)a data observability platform that monitors the\nhealth and quality of data \u2014[ within a data mesh\nimplementation](https://www.thoughtworks.com/en-th/insights/blog/data-\nstrategy/dev-experience-data-mesh-platform).\n\nIn this post, we will explore the process of implementation and go further by\nexploring what else is possible.\n\n##  \n  \nWhere to start: Identifying reusable data products\n\nThe two best ways to fail at creating valuable, reusable data products are to\ndevelop them without any sense of who they are for and to make them more\ncomplicated than they need to be.\n\nOne of the best ways to succeed is by involving business and product\nleadership and identifying the most valuable and shared use cases.\nThoughtworks, for example, often identifies potential data products by working\nbackwards from the use case using the [Jobs to be done\n(JTBD)](https://jtbd.info/2-what-is-jobs-to-be-done-jtbd-796b82081cca)\nframework created by Clayton Christensen.\n\n![Example JTBD framework for a Customer 360 data product. Image courtesy of\nthe\nauthors.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_4.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nExample JTBD framework for a Customer 360 data product. Image courtesy of the\nauthors.\n\n![Example JTBD framework for a Customer 360 data product. Image courtesy of\nthe\nauthors.\n</pre> In\u00a0[19]: Copied! <pre>from llama_index.core.schema import NodeWithScore, TextNode\n</pre> from llama_index.core.schema import NodeWithScore, TextNode In\u00a0[20]: Copied! <pre>from typing import Optional\nnodes_with_scores = []\nfor index, node in enumerate(query_result.nodes):\n score: Optional[float] = None\n if query_result.similarities is not None:\n    score = query_result.similarities[index]\n    nodes_with_scores.append(NodeWithScore(node=node, score=score))\n</pre> from typing import Optional nodes_with_scores = [] for index, node in enumerate(query_result.nodes):  score: Optional[float] = None  if query_result.similarities is not None:     score = query_result.similarities[index]     nodes_with_scores.append(NodeWithScore(node=node, score=score)) In\u00a0[21]: Copied! <pre>from llama_index.core import QueryBundle\nfrom llama_index.core.retrievers import BaseRetriever\nfrom typing import Any, List\n\nclass VectorDBRetriever(BaseRetriever):\n    \"\"\"Retriever over a qdrant vector store.\"\"\"\n\n    def __init__(\n        self,\n        vector_store: 'QdrantVectorStore',  # Assuming QdrantVectorStore is defined elsewhere\n        embed_model: Any,\n        query_mode: str = \"default\",\n        similarity_top_k: int = 2,\n    ) -&gt; None:\n        \"\"\"Initialize parameters.\"\"\"\n        self._vector_store = vector_store\n        self._embed_model = embed_model\n        self._query_mode = query_mode\n        self._similarity_top_k = similarity_top_k\n        super().__init__()\n\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List['NodeWithScore']:\n        \"\"\"Retrieve documents based on the query.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(\n            query_bundle.query_str\n        )\n        vector_store_query = VectorStoreQuery(\n            query_embedding=query_embedding,\n            similarity_top_k=self._similarity_top_k,\n            mode=self._query_mode,\n        )\n        query_result = self._vector_store.query(vector_store_query)\n        nodes_with_scores = []\n        for index, node in enumerate(query_result.nodes):\n            score = query_result.similarities[index] if query_result.similarities is not None else None\n            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n        return nodes_with_scores\n</pre> from llama_index.core import QueryBundle from llama_index.core.retrievers import BaseRetriever from typing import Any, List  class VectorDBRetriever(BaseRetriever):     \"\"\"Retriever over a qdrant vector store.\"\"\"      def __init__(         self,         vector_store: 'QdrantVectorStore',  # Assuming QdrantVectorStore is defined elsewhere         embed_model: Any,         query_mode: str = \"default\",         similarity_top_k: int = 2,     ) -&gt; None:         \"\"\"Initialize parameters.\"\"\"         self._vector_store = vector_store         self._embed_model = embed_model         self._query_mode = query_mode         self._similarity_top_k = similarity_top_k         super().__init__()      def _retrieve(self, query_bundle: QueryBundle) -&gt; List['NodeWithScore']:         \"\"\"Retrieve documents based on the query.\"\"\"         query_embedding = self._embed_model.get_query_embedding(             query_bundle.query_str         )         vector_store_query = VectorStoreQuery(             query_embedding=query_embedding,             similarity_top_k=self._similarity_top_k,             mode=self._query_mode,         )         query_result = self._vector_store.query(vector_store_query)         nodes_with_scores = []         for index, node in enumerate(query_result.nodes):             score = query_result.similarities[index] if query_result.similarities is not None else None             nodes_with_scores.append(NodeWithScore(node=node, score=score))         return nodes_with_scores  In\u00a0[22]: Copied! <pre>hybrid_retriever = VectorDBRetriever(vector_store, embed_model, query_mode=\"hybrid\", similarity_top_k=2)\nsparse_retriever = VectorDBRetriever(vector_store, embed_model, query_mode=\"sparse\", similarity_top_k=2)\n</pre> hybrid_retriever = VectorDBRetriever(vector_store, embed_model, query_mode=\"hybrid\", similarity_top_k=2) sparse_retriever = VectorDBRetriever(vector_store, embed_model, query_mode=\"sparse\", similarity_top_k=2) In\u00a0[23]: Copied! <pre>def execute_and_compare(query_str: str):\n    hybrid_response = hybrid_retriever.retrieve(QueryBundle(query_str=query_str))\n    sparse_response = sparse_retriever.retrieve(QueryBundle(query_str=query_str))\n\n    print(\"Hybrid Results:\")\n    for result in hybrid_response:\n        print(f\"Text: {result.node.get_content()}, Score: {result.score}\")\n\n    print(\"\\nSparse Results:\")\n    for result in sparse_response:\n        print(f\"Text: {result.node.get_content()}, Score: {result.score}\")\n</pre> def execute_and_compare(query_str: str):     hybrid_response = hybrid_retriever.retrieve(QueryBundle(query_str=query_str))     sparse_response = sparse_retriever.retrieve(QueryBundle(query_str=query_str))      print(\"Hybrid Results:\")     for result in hybrid_response:         print(f\"Text: {result.node.get_content()}, Score: {result.score}\")      print(\"\\nSparse Results:\")     for result in sparse_response:         print(f\"Text: {result.node.get_content()}, Score: {result.score}\") In\u00a0[29]: Copied! <pre>query_str =  \"what is Data products?\"\nexecute_and_compare(query_str)\n</pre> query_str =  \"what is Data products?\" execute_and_compare(query_str) <pre>Hybrid Results:\nText: [Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\n![Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nHave you ever come across an internal [data\nproduct](https://www.thoughtworks.com/en-us/what-we-do/data-and-ai/modern-\ndata-engineering-playbook/data-as-a-product) and side-eyed it like it\u2019s your\nkid\u2019s prom date? While it _seems_ like it fits the requirements, you don\u2019t\nquite trust it \u2014 who knows where the data in this shifty table has been. Will\nit be reliable and safe even after you turn your focus elsewhere? Will the\nschema stay true?\n\nThis project is your baby; you just can\u2019t risk it. So, just to be safe you\ntake the extra time to recreate the dataset.\n\n## Data products and trustworthiness\n\nAccording to Zhamak Dehgahi, data products should be discoverable,\naddressable, trustworthy, self-describing, interoperable and secure. In our\nexperience, most data products only support one or two use cases. That\u2019s a\nlost opportunity experienced by too many data teams, especially those with\ndecentralized organizational structures or implementing [data\nmesh](https://www.montecarlodata.com/blog-what-is-a-data-mesh-and-how-not-to-\nmesh-it-up/).\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\nIn the focus on building data trust with business stakeholders, it\u2019s easy to\nlose sight of the importance of also building trust with data teams across\ndifferent domains. However, a data product must be trustworthy if it\u2019s to\nencourage the reuse of data products. This is what ultimately **separates data\nmesh from data silo.**\n\nThe data product is trustworthy if data consumers are confident in the\naccuracy and reliability of the data. Data products should be transparent with\nregards to information quality metrics and performance promises.\n\nCreating a central marketplace or catalog of internal data products is a great\nfirst step to raising awareness, but more is needed to convince skeptical data\nconsumers to actually start using them.\n\nFor this, we can take a page out of Amazon.com\u2019s playbook. Amazon provides an\nincredible amount of detail to help consumers purchase products from unknown\nthird-parties. Take the example of something as simple as a wrench:\n\n[\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench., Score: 0.8020849742832907\n\nSparse Results:\nText: [Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\n![Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nHave you ever come across an internal [data\nproduct](https://www.thoughtworks.com/en-us/what-we-do/data-and-ai/modern-\ndata-engineering-playbook/data-as-a-product) and side-eyed it like it\u2019s your\nkid\u2019s prom date? While it _seems_ like it fits the requirements, you don\u2019t\nquite trust it \u2014 who knows where the data in this shifty table has been. Will\nit be reliable and safe even after you turn your focus elsewhere? Will the\nschema stay true?\n\nThis project is your baby; you just can\u2019t risk it. So, just to be safe you\ntake the extra time to recreate the dataset.\n\n## Data products and trustworthiness\n\nAccording to Zhamak Dehgahi, data products should be discoverable,\naddressable, trustworthy, self-describing, interoperable and secure. In our\nexperience, most data products only support one or two use cases. That\u2019s a\nlost opportunity experienced by too many data teams, especially those with\ndecentralized organizational structures or implementing [data\nmesh](https://www.montecarlodata.com/blog-what-is-a-data-mesh-and-how-not-to-\nmesh-it-up/).\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\nIn the focus on building data trust with business stakeholders, it\u2019s easy to\nlose sight of the importance of also building trust with data teams across\ndifferent domains. However, a data product must be trustworthy if it\u2019s to\nencourage the reuse of data products. This is what ultimately **separates data\nmesh from data silo.**\n\nThe data product is trustworthy if data consumers are confident in the\naccuracy and reliability of the data. Data products should be transparent with\nregards to information quality metrics and performance promises.\n\nCreating a central marketplace or catalog of internal data products is a great\nfirst step to raising awareness, but more is needed to convince skeptical data\nconsumers to actually start using them.\n\nFor this, we can take a page out of Amazon.com\u2019s playbook. Amazon provides an\nincredible amount of detail to help consumers purchase products from unknown\nthird-parties. Take the example of something as simple as a wrench:\n\n[\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench., Score: 0.8020849742832907\n</pre> In\u00a0[34]: Copied! <pre>query_str =  \"How to Create data product SLOs?\"\nexecute_and_compare(query_str)\n</pre> query_str =  \"How to Create data product SLOs?\" execute_and_compare(query_str) <pre>Hybrid Results:\nText: This helps teams collectively brainstorm and understand usage, expectations,\ntrade-offs and business impact. The outcomes of the exercise are then used to\ndetermine the various SLOs that need to be set for individual products.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n## Third step: Implementing the SLOs\n\n##\n\nDuring the implementation phase of the data product, the data product team\nwill start by defining the metrics (SLIs) used to measure the SLO.\n\nOne common SLI for data products is freshness. In the example from the\nprevious section, the exercise may reveal the marketing team relies heavily on\na particular dashboard that supports the monitoring of daily campaign and\npurchasing behaviors, which means the data needs to be updated every day.\n\nThe customer service team, on the other hand, may require hourly updates to\nbetter engage with customers in real time. In this scenario, it is almost\ncertainly more efficient to build the data product to be updated hourly to\nserve both consumer groups rather than build two different data products. The\nmarketing team isn\u2019t going to complain about having data that is more\nfrequently updated than they requested after all!\n\nSLIs are typically expressed as a percentage over a period of time. In the\nexample presented earlier, 99% freshness over an hourly interval is the SLI in\nplace for the Customer 360 data product.\n\nIn our example, the team has decided to track data freshness checks based on\nthe processing timestamp attribute present in the dataset that is served by\nthe data product: processing_timestamp. To do this, they start by defining a\n[monitor as code](https://docs.getmontecarlo.com/docs/monitors-as-code) that\nwill become part of the data product which will support the implementation of\nthe freshness SLO:\n\n    \n    \n    namespace: customer-domain\n    montecarlo:\n      freshness:\n        - description: Customer 360 Data Product Freshness Monitor\n          name:\u00a0 Freshness - Customer 360 Data Product\n          table: analytics:prod.customer_360_dp.customers\n          freshness_threshold: 240\n          schedule:\n            type: fixed\n            interval_minutes: 240\n            start_time: \"2022-09-15T01:00:00\"\n    \n\nThe data team can then automate the deployment of this monitor via the CI/CD\npipeline using the Monte Carlo CLI:\n\n    \n    \n    montecarlo monitors apply --namespace customer-domain\n    \n\nThis ensures the monitor to support the SLO is implemented and deployed every\ntime there is a change via the CI/CD pipeline. The monitor as code\nfunctionality improves the experience of the data product developer in\nmaintaining and deploying these monitors at scale using version control  \n\nThe stakeholder exercise may also reveal that the Customer 360 data product\nshould not contain deleted rows in the final table as customers will be marked\nas active or inactive rather than removed entirely. To ensure this, a custom\nvolume SLI can be set to monitor and ensure the data product follows this\nbehavior.\n\nFinally, data product users need to be alerted whenever any changes are made\nto the schema of any tables within or upstream of the data product. This is\nbecause such changes could break processes downstream; there could be new\nfields that can enable new use cases. This can be covered by an automated\nschema monitor which sends alerts via the appropriate communication channel.\n\n## Going beyond basic SLOs\n\n##\n\nSo far we have covered three basic dimensions that can be used as SLOs. There\nare several other dimensions improving data product trust such as accuracy and\navailability. These and others are described in the [Implementing Service\nLevel Objectives book](https://www.oreilly.com/library/view/implementing-\nservice-level/9781492076803/).\n\nMore advanced SLOs can better validate data product quality and encourage\nwider use throughout the organization.\n\nFor example, let's imagine the data in our Customer 360 data product is not\ncomplete., Score: 0.7936968558435114\n\nSparse Results:\nText: This helps teams collectively brainstorm and understand usage, expectations,\ntrade-offs and business impact. The outcomes of the exercise are then used to\ndetermine the various SLOs that need to be set for individual products.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n## Third step: Implementing the SLOs\n\n##\n\nDuring the implementation phase of the data product, the data product team\nwill start by defining the metrics (SLIs) used to measure the SLO.\n\nOne common SLI for data products is freshness. In the example from the\nprevious section, the exercise may reveal the marketing team relies heavily on\na particular dashboard that supports the monitoring of daily campaign and\npurchasing behaviors, which means the data needs to be updated every day.\n\nThe customer service team, on the other hand, may require hourly updates to\nbetter engage with customers in real time. In this scenario, it is almost\ncertainly more efficient to build the data product to be updated hourly to\nserve both consumer groups rather than build two different data products. The\nmarketing team isn\u2019t going to complain about having data that is more\nfrequently updated than they requested after all!\n\nSLIs are typically expressed as a percentage over a period of time. In the\nexample presented earlier, 99% freshness over an hourly interval is the SLI in\nplace for the Customer 360 data product.\n\nIn our example, the team has decided to track data freshness checks based on\nthe processing timestamp attribute present in the dataset that is served by\nthe data product: processing_timestamp. To do this, they start by defining a\n[monitor as code](https://docs.getmontecarlo.com/docs/monitors-as-code) that\nwill become part of the data product which will support the implementation of\nthe freshness SLO:\n\n    \n    \n    namespace: customer-domain\n    montecarlo:\n      freshness:\n        - description: Customer 360 Data Product Freshness Monitor\n          name:\u00a0 Freshness - Customer 360 Data Product\n          table: analytics:prod.customer_360_dp.customers\n          freshness_threshold: 240\n          schedule:\n            type: fixed\n            interval_minutes: 240\n            start_time: \"2022-09-15T01:00:00\"\n    \n\nThe data team can then automate the deployment of this monitor via the CI/CD\npipeline using the Monte Carlo CLI:\n\n    \n    \n    montecarlo monitors apply --namespace customer-domain\n    \n\nThis ensures the monitor to support the SLO is implemented and deployed every\ntime there is a change via the CI/CD pipeline. The monitor as code\nfunctionality improves the experience of the data product developer in\nmaintaining and deploying these monitors at scale using version control  \n\nThe stakeholder exercise may also reveal that the Customer 360 data product\nshould not contain deleted rows in the final table as customers will be marked\nas active or inactive rather than removed entirely. To ensure this, a custom\nvolume SLI can be set to monitor and ensure the data product follows this\nbehavior.\n\nFinally, data product users need to be alerted whenever any changes are made\nto the schema of any tables within or upstream of the data product. This is\nbecause such changes could break processes downstream; there could be new\nfields that can enable new use cases. This can be covered by an automated\nschema monitor which sends alerts via the appropriate communication channel.\n\n## Going beyond basic SLOs\n\n##\n\nSo far we have covered three basic dimensions that can be used as SLOs. There\nare several other dimensions improving data product trust such as accuracy and\navailability. These and others are described in the [Implementing Service\nLevel Objectives book](https://www.oreilly.com/library/view/implementing-\nservice-level/9781492076803/).\n\nMore advanced SLOs can better validate data product quality and encourage\nwider use throughout the organization.\n\nFor example, let's imagine the data in our Customer 360 data product is not\ncomplete., Score: 0.7936968558435114\n</pre> In\u00a0[25]: Copied! <pre>from llama_index.core.query_engine import RetrieverQueryEngine\nquery_engine = RetrieverQueryEngine.from_args(\n sparse_retriever, service_context=service_context\n)\n</pre> from llama_index.core.query_engine import RetrieverQueryEngine query_engine = RetrieverQueryEngine.from_args(  sparse_retriever, service_context=service_context ) In\u00a0[28]: Copied! <pre>query_str =  \"what is Data products?\"\nresponse = query_engine.query(query_str)\nprint(str(response))\n</pre> query_str =  \"what is Data products?\" response = query_engine.query(query_str) print(str(response)) <pre>Based on the context information provided, a data product can be defined as a centralized marketplace or catalog of internal data assets that are discoverable, addressable, trustworthy, self-describing, interoperable, and secure. The data product is designed to raise awareness and convince skeptical data consumers to actually start using internal data products. By providing an incredible amount of detail, such as information quality metrics and performance promises, data products can help build trust with data consumers and encourage the reuse of data products.\n\nIn other words, a data product is a curated collection of internal data assets that are designed to be easily discoverable, accessible, and reusable across different domains and use cases. It provides a clear and consistent understanding of the data assets, their characteristics, and how they can be used, which helps build trust with data consumers and encourages them to adopt the data products for their own use cases.\n</pre> In\u00a0[32]: Copied! <pre>print(response.source_nodes[0].get_content())\n</pre> print(response.source_nodes[0].get_content()) <pre>[Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\n![Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nHave you ever come across an internal [data\nproduct](https://www.thoughtworks.com/en-us/what-we-do/data-and-ai/modern-\ndata-engineering-playbook/data-as-a-product) and side-eyed it like it\u2019s your\nkid\u2019s prom date? While it _seems_ like it fits the requirements, you don\u2019t\nquite trust it \u2014 who knows where the data in this shifty table has been. Will\nit be reliable and safe even after you turn your focus elsewhere? Will the\nschema stay true?\n\nThis project is your baby; you just can\u2019t risk it. So, just to be safe you\ntake the extra time to recreate the dataset.\n\n## Data products and trustworthiness\n\nAccording to Zhamak Dehgahi, data products should be discoverable,\naddressable, trustworthy, self-describing, interoperable and secure. In our\nexperience, most data products only support one or two use cases. That\u2019s a\nlost opportunity experienced by too many data teams, especially those with\ndecentralized organizational structures or implementing [data\nmesh](https://www.montecarlodata.com/blog-what-is-a-data-mesh-and-how-not-to-\nmesh-it-up/).\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\nIn the focus on building data trust with business stakeholders, it\u2019s easy to\nlose sight of the importance of also building trust with data teams across\ndifferent domains. However, a data product must be trustworthy if it\u2019s to\nencourage the reuse of data products. This is what ultimately **separates data\nmesh from data silo.**\n\nThe data product is trustworthy if data consumers are confident in the\naccuracy and reliability of the data. Data products should be transparent with\nregards to information quality metrics and performance promises.\n\nCreating a central marketplace or catalog of internal data products is a great\nfirst step to raising awareness, but more is needed to convince skeptical data\nconsumers to actually start using them.\n\nFor this, we can take a page out of Amazon.com\u2019s playbook. Amazon provides an\nincredible amount of detail to help consumers purchase products from unknown\nthird-parties. Take the example of something as simple as a wrench:\n\n[\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n</pre> In\u00a0[35]: Copied! <pre>response_1 = query_engine.query(\"How to Create data product SLOs?\")\nprint(str(response_1))\n</pre> response_1 = query_engine.query(\"How to Create data product SLOs?\") print(str(response_1)) <pre>To create SLOs for a data product, follow these steps:\n\n1. Identify the purpose of the data product: What is the main goal of the data product? What problems does it solve? Who are its target users?\n2. Determine the metrics that will be used to measure success: Based on the purpose of the data product, identify the key performance indicators (KPIs) that will be used to evaluate its success. For example, freshness, accuracy, availability, completeness, etc.\n3. Set specific and measurable targets for each metric: Define specific target values for each metric, such as 95% freshness rate or 99.9% accuracy rate.\n4. Establish a monitoring and alert system: Implement a system to continuously monitor the metrics and alert stakeholders when targets are not met.\n5. Review and adjust SLOs regularly: Regularly review the SLOs and adjust them as necessary based on changes in the business or technology landscape.\n6. Communicate SLOs to stakeholders: Share the SLOs with stakeholders, including developers, product owners, and executives, to ensure everyone is aligned and working towards the same goals.\n7. Use SLOs as a baseline for evaluating progress: Use the SLOs as a basis for evaluating the progress of the data product and identifying areas for improvement.\n8. Consider advanced SLOs: In addition to basic SLOs, consider implementing more advanced SLOs such as accuracy, availability, completeness, and other metrics that can help validate data product quality and encourage wider use throughout the organization.\n\nBy following these steps, you can create effective SLOs for your data product that will help ensure it meets the needs of its users and stakeholders, and continues to improve over time.\n</pre> In\u00a0[36]: Copied! <pre>print(response_1.source_nodes[0].get_content())\n</pre> print(response_1.source_nodes[0].get_content()) <pre>This helps teams collectively brainstorm and understand usage, expectations,\ntrade-offs and business impact. The outcomes of the exercise are then used to\ndetermine the various SLOs that need to be set for individual products.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n## Third step: Implementing the SLOs\n\n##\n\nDuring the implementation phase of the data product, the data product team\nwill start by defining the metrics (SLIs) used to measure the SLO.\n\nOne common SLI for data products is freshness. In the example from the\nprevious section, the exercise may reveal the marketing team relies heavily on\na particular dashboard that supports the monitoring of daily campaign and\npurchasing behaviors, which means the data needs to be updated every day.\n\nThe customer service team, on the other hand, may require hourly updates to\nbetter engage with customers in real time. In this scenario, it is almost\ncertainly more efficient to build the data product to be updated hourly to\nserve both consumer groups rather than build two different data products. The\nmarketing team isn\u2019t going to complain about having data that is more\nfrequently updated than they requested after all!\n\nSLIs are typically expressed as a percentage over a period of time. In the\nexample presented earlier, 99% freshness over an hourly interval is the SLI in\nplace for the Customer 360 data product.\n\nIn our example, the team has decided to track data freshness checks based on\nthe processing timestamp attribute present in the dataset that is served by\nthe data product: processing_timestamp. To do this, they start by defining a\n[monitor as code](https://docs.getmontecarlo.com/docs/monitors-as-code) that\nwill become part of the data product which will support the implementation of\nthe freshness SLO:\n\n    \n    \n    namespace: customer-domain\n    montecarlo:\n      freshness:\n        - description: Customer 360 Data Product Freshness Monitor\n          name:\u00a0 Freshness - Customer 360 Data Product\n          table: analytics:prod.customer_360_dp.customers\n          freshness_threshold: 240\n          schedule:\n            type: fixed\n            interval_minutes: 240\n            start_time: \"2022-09-15T01:00:00\"\n    \n\nThe data team can then automate the deployment of this monitor via the CI/CD\npipeline using the Monte Carlo CLI:\n\n    \n    \n    montecarlo monitors apply --namespace customer-domain\n    \n\nThis ensures the monitor to support the SLO is implemented and deployed every\ntime there is a change via the CI/CD pipeline. The monitor as code\nfunctionality improves the experience of the data product developer in\nmaintaining and deploying these monitors at scale using version control  \n\nThe stakeholder exercise may also reveal that the Customer 360 data product\nshould not contain deleted rows in the final table as customers will be marked\nas active or inactive rather than removed entirely. To ensure this, a custom\nvolume SLI can be set to monitor and ensure the data product follows this\nbehavior.\n\nFinally, data product users need to be alerted whenever any changes are made\nto the schema of any tables within or upstream of the data product. This is\nbecause such changes could break processes downstream; there could be new\nfields that can enable new use cases. This can be covered by an automated\nschema monitor which sends alerts via the appropriate communication channel.\n\n## Going beyond basic SLOs\n\n##\n\nSo far we have covered three basic dimensions that can be used as SLOs. There\nare several other dimensions improving data product trust such as accuracy and\navailability. These and others are described in the [Implementing Service\nLevel Objectives book](https://www.oreilly.com/library/view/implementing-\nservice-level/9781492076803/).\n\nMore advanced SLOs can better validate data product quality and encourage\nwider use throughout the organization.\n\nFor example, let's imagine the data in our Customer 360 data product is not\ncomplete.\n</pre> In\u00a0[39]: Copied! <pre>from llama_index.core.query_engine import RetrieverQueryEngine\nhybrid_query_engine = RetrieverQueryEngine.from_args(\n hybrid_retriever, service_context=service_context\n)\n</pre> from llama_index.core.query_engine import RetrieverQueryEngine hybrid_query_engine = RetrieverQueryEngine.from_args(  hybrid_retriever, service_context=service_context ) In\u00a0[40]: Copied! <pre>query_str = \"what is Data products?\"\nresponse =  hybrid_query_engine.query(query_str)\nprint(str(response))\n</pre> query_str = \"what is Data products?\" response =  hybrid_query_engine.query(query_str) print(str(response)) <pre>Based on the provided context, a data product can be defined as a centralized marketplace or catalog of internal data assets that are discoverable, addressable, trustworthy, self-describing, interoperable, and secure. It is important to create a data product that addresses the characteristics originally defined by Zhamak Dehghani, such as discoverability, addressability, trustworthiness, self-description, interoperability, and security. By doing so, data teams can build trust with business stakeholders and encourage the reuse of data products across different domains. Additionally, creating a central marketplace or catalog of internal data products can help raise awareness and convince skeptical data consumers to start using them.\n</pre> In\u00a0[41]: Copied! <pre>print(response.source_nodes[0].get_content())\n</pre> print(response.source_nodes[0].get_content()) <pre>[Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\n![Customer 360 Data\nProduct](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_1.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nHave you ever come across an internal [data\nproduct](https://www.thoughtworks.com/en-us/what-we-do/data-and-ai/modern-\ndata-engineering-playbook/data-as-a-product) and side-eyed it like it\u2019s your\nkid\u2019s prom date? While it _seems_ like it fits the requirements, you don\u2019t\nquite trust it \u2014 who knows where the data in this shifty table has been. Will\nit be reliable and safe even after you turn your focus elsewhere? Will the\nschema stay true?\n\nThis project is your baby; you just can\u2019t risk it. So, just to be safe you\ntake the extra time to recreate the dataset.\n\n## Data products and trustworthiness\n\nAccording to Zhamak Dehgahi, data products should be discoverable,\naddressable, trustworthy, self-describing, interoperable and secure. In our\nexperience, most data products only support one or two use cases. That\u2019s a\nlost opportunity experienced by too many data teams, especially those with\ndecentralized organizational structures or implementing [data\nmesh](https://www.montecarlodata.com/blog-what-is-a-data-mesh-and-how-not-to-\nmesh-it-up/).\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\n![Data product characteristics as originally defined by Zhamak Dehghani.\n](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_2.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nData product characteristics as originally defined by Zhamak Dehghani.\n\nIn the focus on building data trust with business stakeholders, it\u2019s easy to\nlose sight of the importance of also building trust with data teams across\ndifferent domains. However, a data product must be trustworthy if it\u2019s to\nencourage the reuse of data products. This is what ultimately **separates data\nmesh from data silo.**\n\nThe data product is trustworthy if data consumers are confident in the\naccuracy and reliability of the data. Data products should be transparent with\nregards to information quality metrics and performance promises.\n\nCreating a central marketplace or catalog of internal data products is a great\nfirst step to raising awareness, but more is needed to convince skeptical data\nconsumers to actually start using them.\n\nFor this, we can take a page out of Amazon.com\u2019s playbook. Amazon provides an\nincredible amount of detail to help consumers purchase products from unknown\nthird-parties. Take the example of something as simple as a wrench:\n\n[\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n![](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_3.png)\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg) I\u2019d buy this wrench.\n</pre> In\u00a0[42]: Copied! <pre>response_1 = hybrid_query_engine.query(\"How to Create data product SLOs?\")\nprint(str(response_1))\n</pre> response_1 = hybrid_query_engine.query(\"How to Create data product SLOs?\") print(str(response_1)) <pre>Creating SLOs (Service Level Objectives) for a data product involves several steps:\n\n1. Identify the usage patterns of the data product: Understand how the data product is being used and what are the key metrics that are important to measure. This can be done through surveys, interviews, or by analyzing usage patterns.\n2. Define the SLOs: Based on the usage patterns identified in step 1, define the SLOs that are relevant to the data product. These could include things like freshness, accuracy, completeness, and availability.\n3. Express the SLOs as percentages: Once the SLOs have been defined, express them as percentages over a period of time. For example, \"The data product will be 95% fresh within an hourly interval.\"\n4. Monitor and measure the SLOs: Use monitoring tools and techniques to track the SLOs and measure their compliance. This can be done through automated checks or manual audits.\n5. Adjust the SLOs as needed: Based on the measurements taken, adjust the SLOs as needed to ensure they are achievable and meaningful.\n6. Communicate the SLOs to stakeholders: Share the SLOs with stakeholders to ensure everyone is aware of what is expected from the data product.\n7. Continuously improve: Use the measurements taken to continuously improve the data product and increase its trustworthiness.\n\nIt's important to note that SLOs are not static and may need to be adjusted over time as the data product evolves and the needs of the stakeholders change.\n</pre> In\u00a0[43]: Copied! <pre>print(response_1.source_nodes[0].get_content())\n</pre> print(response_1.source_nodes[0].get_content()) <pre>This helps teams collectively brainstorm and understand usage, expectations,\ntrade-offs and business impact. The outcomes of the exercise are then used to\ndetermine the various SLOs that need to be set for individual products.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n![Product usage pattern exercise template. Courtesy of\nThoughtworks.](/content/dam/thoughtworks/images/infographic/Tw_illustration_blog_montecarlo_6.png)\n\n![Pause](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/pause-icon.svg)\n![Play](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\nsite/resources/images/play-icon.svg)\n\nProduct usage pattern exercise template. Courtesy of Thoughtworks.\n\n## Third step: Implementing the SLOs\n\n##\n\nDuring the implementation phase of the data product, the data product team\nwill start by defining the metrics (SLIs) used to measure the SLO.\n\nOne common SLI for data products is freshness. In the example from the\nprevious section, the exercise may reveal the marketing team relies heavily on\na particular dashboard that supports the monitoring of daily campaign and\npurchasing behaviors, which means the data needs to be updated every day.\n\nThe customer service team, on the other hand, may require hourly updates to\nbetter engage with customers in real time. In this scenario, it is almost\ncertainly more efficient to build the data product to be updated hourly to\nserve both consumer groups rather than build two different data products. The\nmarketing team isn\u2019t going to complain about having data that is more\nfrequently updated than they requested after all!\n\nSLIs are typically expressed as a percentage over a period of time. In the\nexample presented earlier, 99% freshness over an hourly interval is the SLI in\nplace for the Customer 360 data product.\n\nIn our example, the team has decided to track data freshness checks based on\nthe processing timestamp attribute present in the dataset that is served by\nthe data product: processing_timestamp. To do this, they start by defining a\n[monitor as code](https://docs.getmontecarlo.com/docs/monitors-as-code) that\nwill become part of the data product which will support the implementation of\nthe freshness SLO:\n\n    \n    \n    namespace: customer-domain\n    montecarlo:\n      freshness:\n        - description: Customer 360 Data Product Freshness Monitor\n          name:\u00a0 Freshness - Customer 360 Data Product\n          table: analytics:prod.customer_360_dp.customers\n          freshness_threshold: 240\n          schedule:\n            type: fixed\n            interval_minutes: 240\n            start_time: \"2022-09-15T01:00:00\"\n    \n\nThe data team can then automate the deployment of this monitor via the CI/CD\npipeline using the Monte Carlo CLI:\n\n    \n    \n    montecarlo monitors apply --namespace customer-domain\n    \n\nThis ensures the monitor to support the SLO is implemented and deployed every\ntime there is a change via the CI/CD pipeline. The monitor as code\nfunctionality improves the experience of the data product developer in\nmaintaining and deploying these monitors at scale using version control  \n\nThe stakeholder exercise may also reveal that the Customer 360 data product\nshould not contain deleted rows in the final table as customers will be marked\nas active or inactive rather than removed entirely. To ensure this, a custom\nvolume SLI can be set to monitor and ensure the data product follows this\nbehavior.\n\nFinally, data product users need to be alerted whenever any changes are made\nto the schema of any tables within or upstream of the data product. This is\nbecause such changes could break processes downstream; there could be new\nfields that can enable new use cases. This can be covered by an automated\nschema monitor which sends alerts via the appropriate communication channel.\n\n## Going beyond basic SLOs\n\n##\n\nSo far we have covered three basic dimensions that can be used as SLOs. There\nare several other dimensions improving data product trust such as accuracy and\navailability. These and others are described in the [Implementing Service\nLevel Objectives book](https://www.oreilly.com/library/view/implementing-\nservice-level/9781492076803/).\n\nMore advanced SLOs can better validate data product quality and encourage\nwider use throughout the organization.\n\nFor example, let's imagine the data in our Customer 360 data product is not\ncomplete.\n</pre>"},{"location":"RAG/03_Hybrid_RAG/_Qdrant_Hybrid_Search/#compare-result","title":"Compare Result\u00b6","text":""},{"location":"RAG/03_Hybrid_RAG/_Qdrant_Hybrid_Search/#sparse_retriever","title":"Sparse_retriever\u00b6","text":""},{"location":"RAG/03_Hybrid_RAG/_Qdrant_Hybrid_Search/#hybrid-retrievel","title":"Hybrid Retrievel\u00b6","text":""},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/","title":"Hybrid RAG(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install -U llama-index llama-index-vector-stores-qdrant fastembed\n</pre> %pip install -U llama-index llama-index-vector-stores-qdrant fastembed In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>!mkdir -p 'data/'\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\"\n</pre> !mkdir -p 'data/' !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\" In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data/\").load_data()\n</pre> from llama_index.core import SimpleDirectoryReader  documents = SimpleDirectoryReader(\"./data/\").load_data() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.core import Settings\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient, AsyncQdrantClient\n\n# creates a persistant index to disk\nclient = QdrantClient(host=\"localhost\", port=6333)\naclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n\n# create our vector store with hybrid indexing enabled\n# batch_size controls how many nodes are encoded with sparse vectors at once\nvector_store = QdrantVectorStore(\n    \"llama2_paper\",\n    client=client,\n    aclient=aclient,\n    enable_hybrid=True,\n    batch_size=20,\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nSettings.chunk_size = 512\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)\n</pre> from llama_index.core import VectorStoreIndex, StorageContext from llama_index.core import Settings from llama_index.vector_stores.qdrant import QdrantVectorStore from qdrant_client import QdrantClient, AsyncQdrantClient  # creates a persistant index to disk client = QdrantClient(host=\"localhost\", port=6333) aclient = AsyncQdrantClient(host=\"localhost\", port=6333)  # create our vector store with hybrid indexing enabled # batch_size controls how many nodes are encoded with sparse vectors at once vector_store = QdrantVectorStore(     \"llama2_paper\",     client=client,     aclient=aclient,     enable_hybrid=True,     batch_size=20, )  storage_context = StorageContext.from_defaults(vector_store=vector_store) Settings.chunk_size = 512  index = VectorStoreIndex.from_documents(     documents,     storage_context=storage_context, ) <pre>Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n</pre> <pre>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</pre> <pre>.gitattributes:   0%|          | 0.00/1.52k [00:00&lt;?, ?B/s]</pre> <pre>generation_config.json:   0%|          | 0.00/90.0 [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/712k [00:00&lt;?, ?B/s]</pre> <pre>config.json:   0%|          | 0.00/755 [00:00&lt;?, ?B/s]</pre> <pre>tokenizer_config.json:   0%|          | 0.00/1.38k [00:00&lt;?, ?B/s]</pre> <pre>README.md:   0%|          | 0.00/133 [00:00&lt;?, ?B/s]</pre> <pre>model.onnx:   0%|          | 0.00/532M [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>special_tokens_map.json:   0%|          | 0.00/695 [00:00&lt;?, ?B/s]</pre> <pre>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine(\n    similarity_top_k=2, sparse_top_k=12, vector_store_query_mode=\"hybrid\"\n)\n</pre> query_engine = index.as_query_engine(     similarity_top_k=2, sparse_top_k=12, vector_store_query_mode=\"hybrid\" ) In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, Markdown\n\nresponse = query_engine.query(\n    \"How was Llama2 specifically trained differently from Llama1?\"\n)\n\ndisplay(Markdown(str(response)))\n</pre> from IPython.display import display, Markdown  response = query_engine.query(     \"How was Llama2 specifically trained differently from Llama1?\" )  display(Markdown(str(response))) <p>Llama 2 was specifically trained differently from Llama 1 by making changes such as performing more robust data cleaning, updating data mixes, training on 40% more total tokens, doubling the context length, and using grouped-query attention (GQA) to improve inference scalability for larger models. Additionally, Llama 2 adopted most of the pretraining setting and model architecture from Llama 1 but included architectural enhancements like increased context length and grouped-query attention.</p> In\u00a0[\u00a0]: Copied! <pre>print(len(response.source_nodes))\n</pre> print(len(response.source_nodes)) <pre>2\n</pre> <p>Lets compare to not using hybrid search at all!</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, Markdown\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=2,\n    # sparse_top_k=10,\n    # vector_store_query_mode=\"hybrid\"\n)\n\nresponse = query_engine.query(\n    \"How was Llama2 specifically trained differently from Llama1?\"\n)\ndisplay(Markdown(str(response)))\n</pre> from IPython.display import display, Markdown  query_engine = index.as_query_engine(     similarity_top_k=2,     # sparse_top_k=10,     # vector_store_query_mode=\"hybrid\" )  response = query_engine.query(     \"How was Llama2 specifically trained differently from Llama1?\" ) display(Markdown(str(response))) <p>Llama 2 was specifically trained differently from Llama 1 by making changes to improve performance, such as performing more robust data cleaning, updating data mixes, training on 40% more total tokens, doubling the context length, and using grouped-query attention (GQA) to improve inference scalability for larger models.</p> In\u00a0[\u00a0]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.core import Settings\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n\n# create our vector store with hybrid indexing enabled\nvector_store = QdrantVectorStore(\n    collection_name=\"llama2_paper\",\n    client=client,\n    aclient=aclient,\n    enable_hybrid=True,\n    batch_size=20,\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nSettings.chunk_size = 512\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n    use_async=True,\n)\n\nquery_engine = index.as_query_engine(similarity_top_k=2, sparse_top_k=10)\n\nresponse = await query_engine.aquery(\n    \"What baseline models are measured against in the paper?\"\n)\n</pre> from llama_index.core import VectorStoreIndex, StorageContext from llama_index.core import Settings from llama_index.vector_stores.qdrant import QdrantVectorStore   # create our vector store with hybrid indexing enabled vector_store = QdrantVectorStore(     collection_name=\"llama2_paper\",     client=client,     aclient=aclient,     enable_hybrid=True,     batch_size=20, ) storage_context = StorageContext.from_defaults(vector_store=vector_store) Settings.chunk_size = 512  index = VectorStoreIndex.from_documents(     documents,     storage_context=storage_context,     use_async=True, )  query_engine = index.as_query_engine(similarity_top_k=2, sparse_top_k=10)  response = await query_engine.aquery(     \"What baseline models are measured against in the paper?\" ) In\u00a0[\u00a0]: Copied! <pre>from typing import Any, List, Tuple\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ndoc_tokenizer = AutoTokenizer.from_pretrained(\n    \"naver/efficient-splade-VI-BT-large-doc\"\n)\ndoc_model = AutoModelForMaskedLM.from_pretrained(\n    \"naver/efficient-splade-VI-BT-large-doc\"\n)\n\nquery_tokenizer = AutoTokenizer.from_pretrained(\n    \"naver/efficient-splade-VI-BT-large-query\"\n)\nquery_model = AutoModelForMaskedLM.from_pretrained(\n    \"naver/efficient-splade-VI-BT-large-query\"\n)\n\n\ndef sparse_doc_vectors(\n    texts: List[str],\n) -&gt; Tuple[List[List[int]], List[List[float]]]:\n    \"\"\"\n    Computes vectors from logits and attention mask using ReLU, log, and max operations.\n    \"\"\"\n    tokens = doc_tokenizer(\n        texts, truncation=True, padding=True, return_tensors=\"pt\"\n    )\n    if torch.cuda.is_available():\n        tokens = tokens.to(\"cuda\")\n\n    output = doc_model(**tokens)\n    logits, attention_mask = output.logits, tokens.attention_mask\n    relu_log = torch.log(1 + torch.relu(logits))\n    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n    tvecs, _ = torch.max(weighted_log, dim=1)\n\n    # extract the vectors that are non-zero and their indices\n    indices = []\n    vecs = []\n    for batch in tvecs:\n        indices.append(batch.nonzero(as_tuple=True)[0].tolist())\n        vecs.append(batch[indices[-1]].tolist())\n\n    return indices, vecs\n\n\ndef sparse_query_vectors(\n    texts: List[str],\n) -&gt; Tuple[List[List[int]], List[List[float]]]:\n    \"\"\"\n    Computes vectors from logits and attention mask using ReLU, log, and max operations.\n    \"\"\"\n    # TODO: compute sparse vectors in batches if max length is exceeded\n    tokens = query_tokenizer(\n        texts, truncation=True, padding=True, return_tensors=\"pt\"\n    )\n    if torch.cuda.is_available():\n        tokens = tokens.to(\"cuda\")\n\n    output = query_model(**tokens)\n    logits, attention_mask = output.logits, tokens.attention_mask\n    relu_log = torch.log(1 + torch.relu(logits))\n    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n    tvecs, _ = torch.max(weighted_log, dim=1)\n\n    # extract the vectors that are non-zero and their indices\n    indices = []\n    vecs = []\n    for batch in tvecs:\n        indices.append(batch.nonzero(as_tuple=True)[0].tolist())\n        vecs.append(batch[indices[-1]].tolist())\n\n    return indices, vecs\n</pre> from typing import Any, List, Tuple import torch from transformers import AutoTokenizer, AutoModelForMaskedLM  doc_tokenizer = AutoTokenizer.from_pretrained(     \"naver/efficient-splade-VI-BT-large-doc\" ) doc_model = AutoModelForMaskedLM.from_pretrained(     \"naver/efficient-splade-VI-BT-large-doc\" )  query_tokenizer = AutoTokenizer.from_pretrained(     \"naver/efficient-splade-VI-BT-large-query\" ) query_model = AutoModelForMaskedLM.from_pretrained(     \"naver/efficient-splade-VI-BT-large-query\" )   def sparse_doc_vectors(     texts: List[str], ) -&gt; Tuple[List[List[int]], List[List[float]]]:     \"\"\"     Computes vectors from logits and attention mask using ReLU, log, and max operations.     \"\"\"     tokens = doc_tokenizer(         texts, truncation=True, padding=True, return_tensors=\"pt\"     )     if torch.cuda.is_available():         tokens = tokens.to(\"cuda\")      output = doc_model(**tokens)     logits, attention_mask = output.logits, tokens.attention_mask     relu_log = torch.log(1 + torch.relu(logits))     weighted_log = relu_log * attention_mask.unsqueeze(-1)     tvecs, _ = torch.max(weighted_log, dim=1)      # extract the vectors that are non-zero and their indices     indices = []     vecs = []     for batch in tvecs:         indices.append(batch.nonzero(as_tuple=True)[0].tolist())         vecs.append(batch[indices[-1]].tolist())      return indices, vecs   def sparse_query_vectors(     texts: List[str], ) -&gt; Tuple[List[List[int]], List[List[float]]]:     \"\"\"     Computes vectors from logits and attention mask using ReLU, log, and max operations.     \"\"\"     # TODO: compute sparse vectors in batches if max length is exceeded     tokens = query_tokenizer(         texts, truncation=True, padding=True, return_tensors=\"pt\"     )     if torch.cuda.is_available():         tokens = tokens.to(\"cuda\")      output = query_model(**tokens)     logits, attention_mask = output.logits, tokens.attention_mask     relu_log = torch.log(1 + torch.relu(logits))     weighted_log = relu_log * attention_mask.unsqueeze(-1)     tvecs, _ = torch.max(weighted_log, dim=1)      # extract the vectors that are non-zero and their indices     indices = []     vecs = []     for batch in tvecs:         indices.append(batch.nonzero(as_tuple=True)[0].tolist())         vecs.append(batch[indices[-1]].tolist())      return indices, vecs In\u00a0[\u00a0]: Copied! <pre>vector_store = QdrantVectorStore(\n    \"llama2_paper\",\n    client=client,\n    enable_hybrid=True,\n    sparse_doc_fn=sparse_doc_vectors,\n    sparse_query_fn=sparse_query_vectors,\n)\n</pre> vector_store = QdrantVectorStore(     \"llama2_paper\",     client=client,     enable_hybrid=True,     sparse_doc_fn=sparse_doc_vectors,     sparse_query_fn=sparse_query_vectors, ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.vector_stores import VectorStoreQueryResult\n\n\ndef relative_score_fusion(\n    dense_result: VectorStoreQueryResult,\n    sparse_result: VectorStoreQueryResult,\n    alpha: float = 0.5,  # passed in from the query engine\n    top_k: int = 2,  # passed in from the query engine i.e. similarity_top_k\n) -&gt; VectorStoreQueryResult:\n    \"\"\"\n    Fuse dense and sparse results using relative score fusion.\n    \"\"\"\n    # sanity check\n    assert dense_result.nodes is not None\n    assert dense_result.similarities is not None\n    assert sparse_result.nodes is not None\n    assert sparse_result.similarities is not None\n\n    # deconstruct results\n    sparse_result_tuples = list(\n        zip(sparse_result.similarities, sparse_result.nodes)\n    )\n    sparse_result_tuples.sort(key=lambda x: x[0], reverse=True)\n\n    dense_result_tuples = list(\n        zip(dense_result.similarities, dense_result.nodes)\n    )\n    dense_result_tuples.sort(key=lambda x: x[0], reverse=True)\n\n    # track nodes in both results\n    all_nodes_dict = {x.node_id: x for x in dense_result.nodes}\n    for node in sparse_result.nodes:\n        if node.node_id not in all_nodes_dict:\n            all_nodes_dict[node.node_id] = node\n\n    # normalize sparse similarities from 0 to 1\n    sparse_similarities = [x[0] for x in sparse_result_tuples]\n    max_sparse_sim = max(sparse_similarities)\n    min_sparse_sim = min(sparse_similarities)\n    sparse_similarities = [\n        (x - min_sparse_sim) / (max_sparse_sim - min_sparse_sim)\n        for x in sparse_similarities\n    ]\n    sparse_per_node = {\n        sparse_result_tuples[i][1].node_id: x\n        for i, x in enumerate(sparse_similarities)\n    }\n\n    # normalize dense similarities from 0 to 1\n    dense_similarities = [x[0] for x in dense_result_tuples]\n    max_dense_sim = max(dense_similarities)\n    min_dense_sim = min(dense_similarities)\n    dense_similarities = [\n        (x - min_dense_sim) / (max_dense_sim - min_dense_sim)\n        for x in dense_similarities\n    ]\n    dense_per_node = {\n        dense_result_tuples[i][1].node_id: x\n        for i, x in enumerate(dense_similarities)\n    }\n\n    # fuse the scores\n    fused_similarities = []\n    for node_id in all_nodes_dict:\n        sparse_sim = sparse_per_node.get(node_id, 0)\n        dense_sim = dense_per_node.get(node_id, 0)\n        fused_sim = alpha * (sparse_sim + dense_sim)\n        fused_similarities.append((fused_sim, all_nodes_dict[node_id]))\n\n    fused_similarities.sort(key=lambda x: x[0], reverse=True)\n    fused_similarities = fused_similarities[:top_k]\n\n    # create final response object\n    return VectorStoreQueryResult(\n        nodes=[x[1] for x in fused_similarities],\n        similarities=[x[0] for x in fused_similarities],\n        ids=[x[1].node_id for x in fused_similarities],\n    )\n</pre> from llama_index.core.vector_stores import VectorStoreQueryResult   def relative_score_fusion(     dense_result: VectorStoreQueryResult,     sparse_result: VectorStoreQueryResult,     alpha: float = 0.5,  # passed in from the query engine     top_k: int = 2,  # passed in from the query engine i.e. similarity_top_k ) -&gt; VectorStoreQueryResult:     \"\"\"     Fuse dense and sparse results using relative score fusion.     \"\"\"     # sanity check     assert dense_result.nodes is not None     assert dense_result.similarities is not None     assert sparse_result.nodes is not None     assert sparse_result.similarities is not None      # deconstruct results     sparse_result_tuples = list(         zip(sparse_result.similarities, sparse_result.nodes)     )     sparse_result_tuples.sort(key=lambda x: x[0], reverse=True)      dense_result_tuples = list(         zip(dense_result.similarities, dense_result.nodes)     )     dense_result_tuples.sort(key=lambda x: x[0], reverse=True)      # track nodes in both results     all_nodes_dict = {x.node_id: x for x in dense_result.nodes}     for node in sparse_result.nodes:         if node.node_id not in all_nodes_dict:             all_nodes_dict[node.node_id] = node      # normalize sparse similarities from 0 to 1     sparse_similarities = [x[0] for x in sparse_result_tuples]     max_sparse_sim = max(sparse_similarities)     min_sparse_sim = min(sparse_similarities)     sparse_similarities = [         (x - min_sparse_sim) / (max_sparse_sim - min_sparse_sim)         for x in sparse_similarities     ]     sparse_per_node = {         sparse_result_tuples[i][1].node_id: x         for i, x in enumerate(sparse_similarities)     }      # normalize dense similarities from 0 to 1     dense_similarities = [x[0] for x in dense_result_tuples]     max_dense_sim = max(dense_similarities)     min_dense_sim = min(dense_similarities)     dense_similarities = [         (x - min_dense_sim) / (max_dense_sim - min_dense_sim)         for x in dense_similarities     ]     dense_per_node = {         dense_result_tuples[i][1].node_id: x         for i, x in enumerate(dense_similarities)     }      # fuse the scores     fused_similarities = []     for node_id in all_nodes_dict:         sparse_sim = sparse_per_node.get(node_id, 0)         dense_sim = dense_per_node.get(node_id, 0)         fused_sim = alpha * (sparse_sim + dense_sim)         fused_similarities.append((fused_sim, all_nodes_dict[node_id]))      fused_similarities.sort(key=lambda x: x[0], reverse=True)     fused_similarities = fused_similarities[:top_k]      # create final response object     return VectorStoreQueryResult(         nodes=[x[1] for x in fused_similarities],         similarities=[x[0] for x in fused_similarities],         ids=[x[1].node_id for x in fused_similarities],     ) In\u00a0[\u00a0]: Copied! <pre>vector_store = QdrantVectorStore(\n    \"llama2_paper\",\n    client=client,\n    enable_hybrid=True,\n    hybrid_fusion_fn=relative_score_fusion,\n)\n</pre> vector_store = QdrantVectorStore(     \"llama2_paper\",     client=client,     enable_hybrid=True,     hybrid_fusion_fn=relative_score_fusion, ) <p>You may have noticed the alpha parameter in the above function. This can be set directely in the <code>as_query_engine()</code> call, which will set it in the vector index retriever.</p> In\u00a0[\u00a0]: Copied! <pre>index.as_query_engine(alpha=0.5, similarity_top_k=2)\n</pre> index.as_query_engine(alpha=0.5, similarity_top_k=2) In\u00a0[\u00a0]: Copied! <pre>from qdrant_client import models\n\nclient.recreate_collection(\n    collection_name=\"llama2_paper\",\n    vectors_config={\n        \"text-dense\": models.VectorParams(\n            size=1536,  # openai vector size\n            distance=models.Distance.COSINE,\n        )\n    },\n    sparse_vectors_config={\n        \"text-sparse\": models.SparseVectorParams(\n            index=models.SparseIndexParams()\n        )\n    },\n)\n\n# enable hybrid since we created a sparse collection\nvector_store = QdrantVectorStore(\n    collection_name=\"llama2_paper\", client=client, enable_hybrid=True\n)\n</pre> from qdrant_client import models  client.recreate_collection(     collection_name=\"llama2_paper\",     vectors_config={         \"text-dense\": models.VectorParams(             size=1536,  # openai vector size             distance=models.Distance.COSINE,         )     },     sparse_vectors_config={         \"text-sparse\": models.SparseVectorParams(             index=models.SparseIndexParams()         )     }, )  # enable hybrid since we created a sparse collection vector_store = QdrantVectorStore(     collection_name=\"llama2_paper\", client=client, enable_hybrid=True )"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#qdrant-hybrid-search","title":"Qdrant Hybrid Search\u00b6","text":"<p>Qdrant supports hybrid search by combining search results from <code>sparse</code> and <code>dense</code> vectors.</p> <p><code>dense</code> vectors are the ones you have probably already been using -- embedding models from OpenAI, BGE, SentenceTransformers, etc. are typically <code>dense</code> embedding models. They create a numerical representation of a piece of text, represented as a long list of numbers. These <code>dense</code> vectors can capture rich semantics across the entire piece of text.</p> <p><code>sparse</code> vectors are slightly different. They use a specialized approach or model (TF-IDF, BM25, SPLADE, etc.) for generating vectors. These vectors are typically mostly zeros, making them <code>sparse</code> vectors. These <code>sparse</code> vectors are great at capturing specific keywords and similar small details.</p> <p>This notebook walks through setting up and customizing hybrid search with Qdrant and <code>\"prithvida/Splade_PP_en_v1\"</code> variants from Huggingface.</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#setup","title":"Setup\u00b6","text":"<p>First, we setup our env and load our data.</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#indexing-data","title":"Indexing Data\u00b6","text":"<p>Now, we can index our data.</p> <p>Hybrid search with Qdrant must be enabled from the beginning -- we can simply set <code>enable_hybrid=True</code>.</p> <p>This will run sparse vector generation locally using the <code>\"prithvida/Splade_PP_en_v1\"</code> using fastembed, in addition to generating dense vectors with OpenAI.</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#hybrid-queries","title":"Hybrid Queries\u00b6","text":"<p>When querying with hybrid mode, we can set <code>similarity_top_k</code> and <code>sparse_top_k</code> separately.</p> <p><code>sparse_top_k</code> represents how many nodes will be retrieved from each dense and sparse query. For example, if <code>sparse_top_k=5</code> is set, that means I will retrieve 5 nodes using sparse vectors and 5 nodes using dense vectors.</p> <p><code>similarity_top_k</code> controls the final number of returned nodes. In the above setting, we end up with 10 nodes. A fusion algorithm is applied to rank and order the nodes from different vector spaces (relative score fusion in this case). <code>similarity_top_k=2</code> means the top two nodes after fusion are returned.</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#async-support","title":"Async Support\u00b6","text":"<p>And of course, async queries are also supported (note that in-memory Qdrant data is not shared between async and sync clients!)</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#advanced-customizing-hybrid-search-with-qdrant","title":"[Advanced] Customizing Hybrid Search with Qdrant\u00b6","text":"<p>In this section, we walk through various settings that can be used to fully customize the hybrid search experience</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#customizing-sparse-vector-generation","title":"Customizing Sparse Vector Generation\u00b6","text":"<p>Sparse vector generation can be done using a single model, or sometimes distinct seperate models for queries and documents. Here we use two -- <code>\"naver/efficient-splade-VI-BT-large-doc\"</code> and <code>\"naver/efficient-splade-VI-BT-large-query\"</code></p> <p>Below is the sample code for generating the sparse vectors and how you can set the functionality in the constructor. You can use this and customize as needed.</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#customizing-hybrid_fusion_fn","title":"Customizing <code>hybrid_fusion_fn()</code>\u00b6","text":"<p>By default, when running hbyrid queries with Qdrant, Relative Score Fusion is used to combine the nodes retrieved from both sparse and dense queries.</p> <p>You can customize this function to be any other method (plain deduplication, Reciprocal Rank Fusion, etc.).</p> <p>Below is the default code for our relative score fusion approach and how you can pass it into the constructor.</p>"},{"location":"RAG/03_Hybrid_RAG/qdrant_hybrid/#customizing-hybrid-qdrant-collections","title":"Customizing Hybrid Qdrant Collections\u00b6","text":"<p>Instead of letting llama-index do it, you can also configure your Qdrant hybrid collections ahead of time.</p> <p>NOTE: The names of vector configs must be <code>text-dense</code> and <code>text-sparse</code> if creating a hybrid index.</p>"},{"location":"RAG/04_Sentence_Window_RAG/","title":"Sentence Window Retriever-Based RAG Approach","text":""},{"location":"RAG/04_Sentence_Window_RAG/#sentence-window-retriever-based-rag-approach","title":"Sentence Window Retriever-Based RAG Approach","text":"<pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Document] --&gt; B[Split into Sentences]\n        B --&gt; C[Group Sentences into Chunks]\n    end\n\n    subgraph \"2. Chunk Processing\"\n        C --&gt; D1[Chunk 1]\n        C --&gt; D2[Chunk 2]\n        C --&gt; D3[Chunk 3]\n        C --&gt; D4[...]\n        C --&gt; Dn[Chunk n]\n    end\n\n    subgraph \"3. Embedding\"\n        D1 &amp; D2 &amp; D3 &amp; D4 &amp; Dn --&gt; E{Embedding Model}\n        E --&gt; F1[Embedding 1]\n        E --&gt; F2[Embedding 2]\n        E --&gt; F3[Embedding 3]\n        E --&gt; F4[...]\n        E --&gt; Fn[Embedding n]\n    end\n\n    subgraph \"4. Indexing\"\n        F1 &amp; F2 &amp; F3 &amp; F4 &amp; Fn --&gt; G[(Vector Database)]\n        G --&gt;|Store| H[Chunk ID]\n        G --&gt;|Store| I[Chunk Text]\n        G --&gt;|Store| J[Embedding]\n    end\n\n    subgraph \"5. Document Structure Store\"\n        C --&gt; K[(Document Structure DB)]\n        K --&gt;|Store| L[Chunk ID]\n        K --&gt;|Store| M[Previous k Chunk IDs]\n        K --&gt;|Store| N[Next k Chunk IDs]\n    end\n\n    subgraph \"6. Retrieval\"\n        O[Query] --&gt; P{Embedding Model}\n        P --&gt; Q[Query Embedding]\n        Q --&gt; R{Similarity Search}\n        R --&gt; G\n        G --&gt; S[Retrieved Chunks]\n    end\n\n    subgraph \"7. Context Expansion\"\n        S --&gt; T{Expand Context}\n        T --&gt; K\n        K --&gt; U[Previous k Chunks]\n        K --&gt; V[Next k Chunks]\n        S &amp; U &amp; V --&gt; W[Expanded Context]\n    end\n\n    subgraph \"8. Generation\"\n        W --&gt; X[Query + Expanded Context]\n        X --&gt; Y[LLM]\n        Y --&gt; Z[Response]\n    end</code></pre>"},{"location":"RAG/04_Sentence_Window_RAG/#introduction","title":"Introduction","text":"<p>The Sentence Window Retriever-Based RAG (Retrieval-Augmented Generation) approach is an advanced implementation of the RAG framework, designed to enhance the context-awareness and coherence of AI-generated responses. This method combines the power of large language models with efficient information retrieval techniques, providing a robust solution for generating high-quality, context-rich responses.</p>"},{"location":"RAG/04_Sentence_Window_RAG/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with maintaining coherence across larger contexts or when dealing with information that spans multiple chunks of text. The Sentence Window Retriever-Based approach addresses this limitation by preserving the contextual relationships between chunks during the indexing process and leveraging this information during retrieval and generation.</p>"},{"location":"RAG/04_Sentence_Window_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/04_Sentence_Window_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Document Splitting: The input document is split into sentences.</li> <li>Chunk Creation: Sentences are grouped into manageable chunks.</li> <li>Embedding: Each chunk is processed through an embedding model to create vector representations.</li> <li>Vector Database Indexing: Chunk IDs, text, and embeddings are stored in a vector database for efficient similarity search.</li> <li>Document Structure Indexing: A separate database stores the relationships between chunks, including references to previous and next k chunks for each chunk.</li> </ol>"},{"location":"RAG/04_Sentence_Window_RAG/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Processing: The user query is embedded using the same embedding model used for chunks.</li> <li>Similarity Search: The query embedding is used to find the most relevant chunks in the vector database.</li> <li>Context Expansion: For each retrieved chunk, the system fetches the previous and next k chunks using the document structure database.</li> <li>Context Formation: The retrieved chunks and their expanded context are combined with the original query.</li> <li>Generation: The expanded context and query are passed to a large language model to generate a response.</li> </ol>"},{"location":"RAG/04_Sentence_Window_RAG/#key-features-of-rag","title":"Key Features of RAG","text":"<ul> <li>Efficient Retrieval: Utilizes vector similarity search for fast and accurate information retrieval.</li> <li>Context Preservation: Maintains document structure and chunk relationships during indexing.</li> <li>Flexible Context Window: Allows for adjustable context expansion at retrieval time.</li> <li>Scalability: Capable of handling large document collections and diverse query types.</li> </ul>"},{"location":"RAG/04_Sentence_Window_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Coherence: By including surrounding chunks, the system can generate more coherent and contextually accurate responses.</li> <li>Reduced Hallucination: Access to expanded context helps the model ground its responses in retrieved information, reducing the likelihood of generating false or irrelevant content.</li> <li>Efficient Storage: Only stores necessary information in the vector database, optimizing storage usage.</li> <li>Adaptable Context: The size of the context window can be adjusted based on the specific needs of different queries or applications.</li> <li>Preservation of Document Structure: Maintains the original structure and flow of the document, allowing for more nuanced understanding and generation.</li> </ol>"},{"location":"RAG/04_Sentence_Window_RAG/#conclusion","title":"Conclusion","text":"<p>The Sentence Window Retriever-Based RAG approach offers a powerful solution for enhancing the quality and contextual relevance of AI-generated responses. By preserving document structure and allowing for flexible context expansion, this method addresses key limitations of traditional RAG systems. It provides a robust framework for building advanced question-answering, document analysis, and content generation applications.</p>"},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/","title":"Sentence Window RAG(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') In\u00a0[\u00a0]: Copied! <pre>import utils\n\nimport os\nimport openai\nopenai.api_key = utils.get_openai_api_key()\n</pre> import utils  import os import openai openai.api_key = utils.get_openai_api_key() In\u00a0[\u00a0]: Copied! <pre>from llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\n    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n).load_data()\n</pre> from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(     input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"] ).load_data() In\u00a0[\u00a0]: Copied! <pre>print(type(documents), \"\\n\")\nprint(len(documents), \"\\n\")\nprint(type(documents[0]))\nprint(documents[0])\n</pre> print(type(documents), \"\\n\") print(len(documents), \"\\n\") print(type(documents[0])) print(documents[0]) In\u00a0[\u00a0]: Copied! <pre>from llama_index import Document\n\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n</pre> from llama_index import Document  document = Document(text=\"\\n\\n\".join([doc.text for doc in documents])) In\u00a0[\u00a0]: Copied! <pre>from llama_index.node_parser import SentenceWindowNodeParser\n\n# create the sentence window node parser w/ default settings\nnode_parser = SentenceWindowNodeParser.from_defaults(\n    window_size=3,\n    window_metadata_key=\"window\",\n    original_text_metadata_key=\"original_text\",\n)\n</pre> from llama_index.node_parser import SentenceWindowNodeParser  # create the sentence window node parser w/ default settings node_parser = SentenceWindowNodeParser.from_defaults(     window_size=3,     window_metadata_key=\"window\",     original_text_metadata_key=\"original_text\", ) In\u00a0[\u00a0]: Copied! <pre>text = \"hello. how are you? I am fine!  \"\n\nnodes = node_parser.get_nodes_from_documents([Document(text=text)])\n</pre> text = \"hello. how are you? I am fine!  \"  nodes = node_parser.get_nodes_from_documents([Document(text=text)]) In\u00a0[\u00a0]: Copied! <pre>print([x.text for x in nodes])\n</pre> print([x.text for x in nodes]) In\u00a0[\u00a0]: Copied! <pre>print(nodes[1].metadata[\"window\"])\n</pre> print(nodes[1].metadata[\"window\"]) In\u00a0[\u00a0]: Copied! <pre>text = \"hello. foo bar. cat dog. mouse\"\n\nnodes = node_parser.get_nodes_from_documents([Document(text=text)])\n</pre> text = \"hello. foo bar. cat dog. mouse\"  nodes = node_parser.get_nodes_from_documents([Document(text=text)]) In\u00a0[\u00a0]: Copied! <pre>print([x.text for x in nodes])\n</pre> print([x.text for x in nodes]) In\u00a0[\u00a0]: Copied! <pre>print(nodes[0].metadata[\"window\"])\n</pre> print(nodes[0].metadata[\"window\"]) In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n</pre> from llama_index.llms import OpenAI  llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1) In\u00a0[\u00a0]: Copied! <pre>from llama_index import ServiceContext\n\nsentence_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    # embed_model=\"local:BAAI/bge-large-en-v1.5\"\n    node_parser=node_parser,\n)\n</pre> from llama_index import ServiceContext  sentence_context = ServiceContext.from_defaults(     llm=llm,     embed_model=\"local:BAAI/bge-small-en-v1.5\",     # embed_model=\"local:BAAI/bge-large-en-v1.5\"     node_parser=node_parser, ) In\u00a0[\u00a0]: Copied! <pre>from llama_index import VectorStoreIndex\n\nsentence_index = VectorStoreIndex.from_documents(\n    [document], service_context=sentence_context\n)\n</pre> from llama_index import VectorStoreIndex  sentence_index = VectorStoreIndex.from_documents(     [document], service_context=sentence_context ) In\u00a0[\u00a0]: Copied! <pre>sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n</pre> sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")  In\u00a0[\u00a0]: Copied! <pre># This block of code is optional to check\n# if an index file exist, then it will load it\n# if not, it will rebuild it\n\nimport os\nfrom llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\nfrom llama_index import load_index_from_storage\n\nif not os.path.exists(\"./sentence_index\"):\n    sentence_index = VectorStoreIndex.from_documents(\n        [document], service_context=sentence_context\n    )\n\n    sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\nelse:\n    sentence_index = load_index_from_storage(\n        StorageContext.from_defaults(persist_dir=\"./sentence_index\"),\n        service_context=sentence_context\n    )\n</pre> # This block of code is optional to check # if an index file exist, then it will load it # if not, it will rebuild it  import os from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage from llama_index import load_index_from_storage  if not os.path.exists(\"./sentence_index\"):     sentence_index = VectorStoreIndex.from_documents(         [document], service_context=sentence_context     )      sentence_index.storage_context.persist(persist_dir=\"./sentence_index\") else:     sentence_index = load_index_from_storage(         StorageContext.from_defaults(persist_dir=\"./sentence_index\"),         service_context=sentence_context     ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n\npostproc = MetadataReplacementPostProcessor(\n    target_metadata_key=\"window\"\n)\n</pre> from llama_index.indices.postprocessor import MetadataReplacementPostProcessor  postproc = MetadataReplacementPostProcessor(     target_metadata_key=\"window\" ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.schema import NodeWithScore\nfrom copy import deepcopy\n\nscored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]\nnodes_old = [deepcopy(n) for n in nodes]\n</pre> from llama_index.schema import NodeWithScore from copy import deepcopy  scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes] nodes_old = [deepcopy(n) for n in nodes] In\u00a0[\u00a0]: Copied! <pre>nodes_old[1].text\n</pre> nodes_old[1].text In\u00a0[\u00a0]: Copied! <pre>replaced_nodes = postproc.postprocess_nodes(scored_nodes)\n</pre> replaced_nodes = postproc.postprocess_nodes(scored_nodes) In\u00a0[\u00a0]: Copied! <pre>print(replaced_nodes[1].text)\n</pre> print(replaced_nodes[1].text) In\u00a0[\u00a0]: Copied! <pre>from llama_index.indices.postprocessor import SentenceTransformerRerank\n\n# BAAI/bge-reranker-base\n# link: https://huggingface.co/BAAI/bge-reranker-base\nrerank = SentenceTransformerRerank(\n    top_n=2, model=\"BAAI/bge-reranker-base\"\n)\n</pre> from llama_index.indices.postprocessor import SentenceTransformerRerank  # BAAI/bge-reranker-base # link: https://huggingface.co/BAAI/bge-reranker-base rerank = SentenceTransformerRerank(     top_n=2, model=\"BAAI/bge-reranker-base\" ) In\u00a0[\u00a0]: Copied! <pre>from llama_index import QueryBundle\nfrom llama_index.schema import TextNode, NodeWithScore\n\nquery = QueryBundle(\"I want a dog.\")\n\nscored_nodes = [\n    NodeWithScore(node=TextNode(text=\"This is a cat\"), score=0.6),\n    NodeWithScore(node=TextNode(text=\"This is a dog\"), score=0.4),\n]\n</pre> from llama_index import QueryBundle from llama_index.schema import TextNode, NodeWithScore  query = QueryBundle(\"I want a dog.\")  scored_nodes = [     NodeWithScore(node=TextNode(text=\"This is a cat\"), score=0.6),     NodeWithScore(node=TextNode(text=\"This is a dog\"), score=0.4), ] In\u00a0[\u00a0]: Copied! <pre>reranked_nodes = rerank.postprocess_nodes(\n    scored_nodes, query_bundle=query\n)\n</pre> reranked_nodes = rerank.postprocess_nodes(     scored_nodes, query_bundle=query ) In\u00a0[\u00a0]: Copied! <pre>print([(x.text, x.score) for x in reranked_nodes])\n</pre> print([(x.text, x.score) for x in reranked_nodes]) In\u00a0[\u00a0]: Copied! <pre>sentence_window_engine = sentence_index.as_query_engine(\n    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n)\n</pre> sentence_window_engine = sentence_index.as_query_engine(     similarity_top_k=6, node_postprocessors=[postproc, rerank] ) In\u00a0[\u00a0]: Copied! <pre>window_response = sentence_window_engine.query(\n    \"What are the keys to building a career in AI?\"\n)\n</pre> window_response = sentence_window_engine.query(     \"What are the keys to building a career in AI?\" ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.response.notebook_utils import display_response\n\ndisplay_response(window_response)\n</pre> from llama_index.response.notebook_utils import display_response  display_response(window_response) In\u00a0[\u00a0]: Copied! <pre>import os\nfrom llama_index import ServiceContext, VectorStoreIndex, StorageContext\nfrom llama_index.node_parser import SentenceWindowNodeParser\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index import load_index_from_storage\n\n\ndef build_sentence_window_index(\n    documents,\n    llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    sentence_window_size=3,\n    save_dir=\"sentence_index\",\n):\n    # create the sentence window node parser w/ default settings\n    node_parser = SentenceWindowNodeParser.from_defaults(\n        window_size=sentence_window_size,\n        window_metadata_key=\"window\",\n        original_text_metadata_key=\"original_text\",\n    )\n    sentence_context = ServiceContext.from_defaults(\n        llm=llm,\n        embed_model=embed_model,\n        node_parser=node_parser,\n    )\n    if not os.path.exists(save_dir):\n        sentence_index = VectorStoreIndex.from_documents(\n            documents, service_context=sentence_context\n        )\n        sentence_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        sentence_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=sentence_context,\n        )\n\n    return sentence_index\n\n\ndef get_sentence_window_query_engine(\n    sentence_index, similarity_top_k=6, rerank_top_n=2\n):\n    # define postprocessors\n    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n\n    sentence_window_engine = sentence_index.as_query_engine(\n        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n    )\n    return sentence_window_engine\n</pre> import os from llama_index import ServiceContext, VectorStoreIndex, StorageContext from llama_index.node_parser import SentenceWindowNodeParser from llama_index.indices.postprocessor import MetadataReplacementPostProcessor from llama_index.indices.postprocessor import SentenceTransformerRerank from llama_index import load_index_from_storage   def build_sentence_window_index(     documents,     llm,     embed_model=\"local:BAAI/bge-small-en-v1.5\",     sentence_window_size=3,     save_dir=\"sentence_index\", ):     # create the sentence window node parser w/ default settings     node_parser = SentenceWindowNodeParser.from_defaults(         window_size=sentence_window_size,         window_metadata_key=\"window\",         original_text_metadata_key=\"original_text\",     )     sentence_context = ServiceContext.from_defaults(         llm=llm,         embed_model=embed_model,         node_parser=node_parser,     )     if not os.path.exists(save_dir):         sentence_index = VectorStoreIndex.from_documents(             documents, service_context=sentence_context         )         sentence_index.storage_context.persist(persist_dir=save_dir)     else:         sentence_index = load_index_from_storage(             StorageContext.from_defaults(persist_dir=save_dir),             service_context=sentence_context,         )      return sentence_index   def get_sentence_window_query_engine(     sentence_index, similarity_top_k=6, rerank_top_n=2 ):     # define postprocessors     postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")     rerank = SentenceTransformerRerank(         top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"     )      sentence_window_engine = sentence_index.as_query_engine(         similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]     )     return sentence_window_engine In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms import OpenAI\n\nindex = build_sentence_window_index(\n    [document],\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    save_dir=\"./sentence_index\",\n)\n</pre> from llama_index.llms import OpenAI  index = build_sentence_window_index(     [document],     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),     save_dir=\"./sentence_index\", )  In\u00a0[\u00a0]: Copied! <pre>query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)\n</pre> query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)  In\u00a0[\u00a0]: Copied! <pre>eval_questions = []\nwith open('generated_questions.text', 'r') as file:\n    for line in file:\n        # Remove newline character and convert to integer\n        item = line.strip()\n        eval_questions.append(item)\n</pre> eval_questions = [] with open('generated_questions.text', 'r') as file:     for line in file:         # Remove newline character and convert to integer         item = line.strip()         eval_questions.append(item) In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import Tru\n\ndef run_evals(eval_questions, tru_recorder, query_engine):\n    for question in eval_questions:\n        with tru_recorder as recording:\n            response = query_engine.query(question)\n</pre> from trulens_eval import Tru  def run_evals(eval_questions, tru_recorder, query_engine):     for question in eval_questions:         with tru_recorder as recording:             response = query_engine.query(question) In\u00a0[\u00a0]: Copied! <pre>from utils import get_prebuilt_trulens_recorder\n\nfrom trulens_eval import Tru\n\nTru().reset_database()\n</pre> from utils import get_prebuilt_trulens_recorder  from trulens_eval import Tru  Tru().reset_database() In\u00a0[\u00a0]: Copied! <pre>sentence_index_1 = build_sentence_window_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    sentence_window_size=1,\n    save_dir=\"sentence_index_1\",\n)\n</pre> sentence_index_1 = build_sentence_window_index(     documents,     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),     embed_model=\"local:BAAI/bge-small-en-v1.5\",     sentence_window_size=1,     save_dir=\"sentence_index_1\", ) In\u00a0[\u00a0]: Copied! <pre>sentence_window_engine_1 = get_sentence_window_query_engine(\n    sentence_index_1\n)\n</pre> sentence_window_engine_1 = get_sentence_window_query_engine(     sentence_index_1 ) In\u00a0[\u00a0]: Copied! <pre>tru_recorder_1 = get_prebuilt_trulens_recorder(\n    sentence_window_engine_1,\n    app_id='sentence window engine 1'\n)\n</pre> tru_recorder_1 = get_prebuilt_trulens_recorder(     sentence_window_engine_1,     app_id='sentence window engine 1' ) In\u00a0[\u00a0]: Copied! <pre>run_evals(eval_questions, tru_recorder_1, sentence_window_engine_1)\n</pre> run_evals(eval_questions, tru_recorder_1, sentence_window_engine_1) In\u00a0[\u00a0]: Copied! <pre>Tru().run_dashboard()\n</pre> Tru().run_dashboard() In\u00a0[\u00a0]: Copied! <pre>eval_questions = []\nwith open('generated_questions.text', 'r') as file:\n    for line in file:\n        # Remove newline character and convert to integer\n        item = line.strip()\n        eval_questions.append(item)\n</pre> eval_questions = [] with open('generated_questions.text', 'r') as file:     for line in file:         # Remove newline character and convert to integer         item = line.strip()         eval_questions.append(item) In\u00a0[\u00a0]: Copied! <pre>sentence_index_3 = build_sentence_window_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    sentence_window_size=3,\n    save_dir=\"sentence_index_3\",\n)\nsentence_window_engine_3 = get_sentence_window_query_engine(\n    sentence_index_3\n)\n\ntru_recorder_3 = get_prebuilt_trulens_recorder(\n    sentence_window_engine_3,\n    app_id='sentence window engine 3'\n)\n</pre> sentence_index_3 = build_sentence_window_index(     documents,     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),     embed_model=\"local:BAAI/bge-small-en-v1.5\",     sentence_window_size=3,     save_dir=\"sentence_index_3\", ) sentence_window_engine_3 = get_sentence_window_query_engine(     sentence_index_3 )  tru_recorder_3 = get_prebuilt_trulens_recorder(     sentence_window_engine_3,     app_id='sentence window engine 3' ) In\u00a0[\u00a0]: Copied! <pre>run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3)\n</pre> run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3) In\u00a0[\u00a0]: Copied! <pre>Tru().run_dashboard()\n</pre> Tru().run_dashboard()"},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#lesson-3-sentence-window-retrieval","title":"Lesson 3: Sentence Window Retrieval\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#window-sentence-retrieval-setup","title":"Window-sentence retrieval setup\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#building-the-index","title":"Building the index\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#building-the-postprocessor","title":"Building the postprocessor\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#adding-a-reranker","title":"Adding a reranker\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#runing-the-query-engine","title":"Runing the query engine\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#putting-it-all-together","title":"Putting it all Together\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#trulens-evaluation","title":"TruLens Evaluation\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#sentence-window-size-1","title":"Sentence window size = 1\u00b6","text":""},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#note-about-the-dataset-of-questions","title":"Note about the dataset of questions\u00b6","text":"<ul> <li>Since this evaluation process takes a long time to run, the following file <code>generated_questions.text</code> contains one question (the one mentioned in the lecture video).</li> <li>If you would like to explore other possible questions, feel free to explore the file directory by clicking on the \"Jupyter\" logo at the top right of this notebook. You'll see the following <code>.text</code> files:</li> </ul> <ul> <li><code>generated_questions_01_05.text</code></li> <li><code>generated_questions_06_10.text</code></li> <li><code>generated_questions_11_15.text</code></li> <li><code>generated_questions_16_20.text</code></li> <li><code>generated_questions_21_24.text</code></li> </ul> <p>Note that running an evaluation on more than one question can take some time, so we recommend choosing one of these files (with 5 questions each) to run and explore the results.</p> <ul> <li>For evaluating a personal project, an eval set of 20 is reasonable.</li> <li>For evaluating business applications, you may need a set of 100+ in order to cover all the use cases thoroughly.</li> <li>Note that since API calls can sometimes fail, you may occasionally see null responses, and would want to re-run your evaluations.  So running your evaluations in smaller batches can also help you save time and cost by only re-running the evaluation on the batches with issues.</li> </ul>"},{"location":"RAG/04_Sentence_Window_RAG/Sentence_window_retrieval/#sentence-window-size-3","title":"Sentence window size = 3\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/","title":"Auto Merging Retriever Approach","text":""},{"location":"RAG/05_Auto_Merging_RAG/#auto-merging-retriever-approach","title":"Auto Merging Retriever Approach","text":""},{"location":"RAG/05_Auto_Merging_RAG/#introduction","title":"Introduction","text":"<p>The Auto Merging Retriever is an advanced implementation of the Retrieval-Augmented Generation (RAG) framework. This approach is designed to enhance the context-awareness and coherence of AI-generated responses by consolidating potentially disparate, smaller contexts into larger, more comprehensive contexts that can aid in synthesis.</p> <pre><code>flowchart TD\n    subgraph \"1. Document Processing\"\n        A[Document] --&gt; B[HierarchicalNodeParser]\n        B --&gt; C1[Level 1 Nodes\\nChunk size 2048]\n        B --&gt; C2[Level 2 Nodes\\nChunk size 512]\n        B --&gt; C3[Level 3 Nodes\\nChunk size 128]\n    end\n\n    subgraph \"2. Storage\"\n        C1 &amp; C2 &amp; C3 --&gt; D[DocumentStore]\n        C3 --&gt; E[VectorStoreIndex]\n    end\n\n    subgraph \"3. Query Processing\"\n        F[User Query] --&gt; G{Embedding Model}\n        G --&gt; H[Query Embedding]\n    end\n\n    subgraph \"4. Base Retrieval\"\n        H --&gt; I{Similarity Search}\n        I --&gt; E\n        E --&gt; J[Retrieved Leaf Nodes]\n    end\n\n    subgraph \"5. Auto Merging\"\n        J --&gt; K{AutoMergingRetriever}\n        K --&gt; D\n        K --&gt; L[Merged Nodes]\n    end\n\n    subgraph \"6. Context Formation\"\n        L --&gt; M[Expanded Context]\n        F --&gt; M\n    end\n\n    subgraph \"7. Generation\"\n        M --&gt; N[Language Model]\n        N --&gt; O[Generated Response]\n    end</code></pre>"},{"location":"RAG/05_Auto_Merging_RAG/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with maintaining coherence across larger contexts or when dealing with information that spans multiple chunks of text. The Auto Merging Retriever addresses this limitation by recursively merging subsets of leaf nodes that reference a parent node beyond a given threshold, allowing for a more comprehensive and coherent context during retrieval and generation.</p>"},{"location":"RAG/05_Auto_Merging_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/05_Auto_Merging_RAG/#document-preprocessing-and-hierarchy-creation","title":"Document Preprocessing and Hierarchy Creation","text":"<ol> <li>Document Loading: The input document (e.g., a PDF) is loaded and processed.</li> <li>Hierarchical Parsing: The HierarchicalNodeParser is used to create a hierarchy of nodes from the document:</li> <li>1<sup>st</sup> level: chunk size 2048</li> <li>2<sup>nd</sup> level: chunk size 512</li> <li>3<sup>rd</sup> level: chunk size 128</li> <li>Node Storage: All nodes are stored in a document store, with leaf nodes also indexed in a vector store.</li> </ol>"},{"location":"RAG/05_Auto_Merging_RAG/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Processing: The user query is embedded using the same embedding model used for document chunks.</li> <li>Base Retrieval: A base retriever performs initial similarity search to find relevant leaf nodes.</li> <li>Auto Merging: The AutoMergingRetriever looks at the set of retrieved leaf nodes and recursively \"merges\" subsets of leaf nodes that reference a parent node beyond a given threshold.</li> <li>Context Formation: The merged nodes form an expanded context, which is combined with the original query.</li> <li>Generation: The expanded context and query are passed to a large language model to generate a response.</li> </ol>"},{"location":"RAG/05_Auto_Merging_RAG/#key-features-of-auto-merging-retriever","title":"Key Features of Auto Merging Retriever","text":"<ul> <li>Hierarchical Document Representation: Maintains a multi-level hierarchy of document chunks.</li> <li>Efficient Base Retrieval: Utilizes vector similarity search for fast and accurate initial information retrieval.</li> <li>Dynamic Context Expansion: Automatically merges relevant chunks into larger, more coherent contexts.</li> <li>Flexible Implementation: Can be used with various document types and language models.</li> </ul>"},{"location":"RAG/05_Auto_Merging_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Context Coherence: By merging related chunks, the system can provide more coherent and comprehensive contexts for the language model.</li> <li>Adaptable Retrieval: The merging process adapts to the specific query and retrieved chunks, providing context-appropriate information.</li> <li>Efficient Storage: Maintains a hierarchical structure while allowing for efficient base retrieval of leaf nodes.</li> <li>Potential for Improved Response Quality: The expanded context may lead to more accurate and detailed responses from the language model.</li> </ol>"},{"location":"RAG/05_Auto_Merging_RAG/#evaluation-results","title":"Evaluation Results","text":"<p>Evaluations comparing the Auto Merging Retriever to a base retriever showed: - Similar performance in correctness, relevancy, faithfulness, and semantic similarity metrics. - A slight preference (52.5%) for Auto Merging Retriever responses in pairwise comparisons.</p> <p>These results suggest that the Auto Merging Retriever can provide comparable or slightly better performance compared to traditional retrieval methods.</p>"},{"location":"RAG/05_Auto_Merging_RAG/#conclusion","title":"Conclusion","text":"<p>The Auto Merging Retriever offers a sophisticated approach to enhancing the retrieval process in RAG systems. By dynamically merging relevant chunks into larger contexts, it addresses some of the limitations of traditional chunk-based retrieval methods. While initial evaluations show promising results, further research and optimization may unlock more significant improvements in response quality and coherence.</p>"},{"location":"RAG/05_Auto_Merging_RAG/#prerequisites","title":"Prerequisites","text":"<p>To implement this system, you will need:</p> <ol> <li>A large language model capable of text generation (e.g., GPT-3.5-turbo, GPT-4).</li> <li>An embedding model for converting text chunks and queries into vector representations.</li> <li>A vector database for efficient similarity search (e.g., FAISS).</li> <li>A document store for maintaining the full hierarchy of nodes.</li> <li>The LlamaIndex library, which provides implementations for HierarchicalNodeParser and AutoMergingRetriever.</li> <li>Sufficient computational resources for processing and storing large document collections.</li> <li>Programming knowledge in Python for implementation and evaluation.</li> </ol>"},{"location":"RAG/05_Auto_Merging_RAG/#usage","title":"Usage","text":"<pre><code>from llama_index.core import StorageContext, VectorStoreIndex\nfrom llama_index.core.node_parser import HierarchicalNodeParser\nfrom llama_index.core.retrievers import AutoMergingRetriever\n\n# Parse document into a hierarchy of nodes\nnode_parser = HierarchicalNodeParser.from_defaults()\nnodes = node_parser.get_nodes_from_documents(docs)\n\n# Set up storage\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\n# Create base index and retriever\nleaf_nodes = get_leaf_nodes(nodes)\nbase_index = VectorStoreIndex(leaf_nodes, storage_context=storage_context)\nbase_retriever = base_index.as_retriever(similarity_top_k=6)\n\n# Create AutoMergingRetriever\nretriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n\n# Use the retriever in a query engine\nquery_engine = RetrieverQueryEngine.from_args(retriever)\nresponse = query_engine.query(query_str)\n</code></pre> <p>This README provides a comprehensive overview of the Auto Merging Retriever approach, its benefits, and basic usage instructions.</p>"},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/","title":"AutoMerging RAG(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') In\u00a0[\u00a0]: Copied! <pre>import utils\n\nimport os\nimport openai\nopenai.api_key = utils.get_openai_api_key()\n</pre> import utils  import os import openai openai.api_key = utils.get_openai_api_key() In\u00a0[\u00a0]: Copied! <pre>from llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\n    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n).load_data()\n</pre> from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(     input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"] ).load_data() In\u00a0[\u00a0]: Copied! <pre>print(type(documents), \"\\n\")\nprint(len(documents), \"\\n\")\nprint(type(documents[0]))\nprint(documents[0])\n</pre> print(type(documents), \"\\n\") print(len(documents), \"\\n\") print(type(documents[0])) print(documents[0]) In\u00a0[\u00a0]: Copied! <pre>from llama_index import Document\n\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n</pre> from llama_index import Document  document = Document(text=\"\\n\\n\".join([doc.text for doc in documents])) In\u00a0[\u00a0]: Copied! <pre>from llama_index.node_parser import HierarchicalNodeParser\n\n# create the hierarchical node parser w/ default settings\nnode_parser = HierarchicalNodeParser.from_defaults(\n    chunk_sizes=[2048, 512, 128]\n)\n</pre> from llama_index.node_parser import HierarchicalNodeParser  # create the hierarchical node parser w/ default settings node_parser = HierarchicalNodeParser.from_defaults(     chunk_sizes=[2048, 512, 128] ) In\u00a0[\u00a0]: Copied! <pre>nodes = node_parser.get_nodes_from_documents([document])\n</pre> nodes = node_parser.get_nodes_from_documents([document]) In\u00a0[\u00a0]: Copied! <pre>from llama_index.node_parser import get_leaf_nodes\n\nleaf_nodes = get_leaf_nodes(nodes)\nprint(leaf_nodes[30].text)\n</pre> from llama_index.node_parser import get_leaf_nodes  leaf_nodes = get_leaf_nodes(nodes) print(leaf_nodes[30].text) In\u00a0[\u00a0]: Copied! <pre>nodes_by_id = {node.node_id: node for node in nodes}\n\nparent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id]\nprint(parent_node.text)\n</pre> nodes_by_id = {node.node_id: node for node in nodes}  parent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id] print(parent_node.text) In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n</pre> from llama_index.llms import OpenAI  llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1) In\u00a0[\u00a0]: Copied! <pre>from llama_index import ServiceContext\n\nauto_merging_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    node_parser=node_parser,\n)\n</pre> from llama_index import ServiceContext  auto_merging_context = ServiceContext.from_defaults(     llm=llm,     embed_model=\"local:BAAI/bge-small-en-v1.5\",     node_parser=node_parser, ) In\u00a0[\u00a0]: Copied! <pre>from llama_index import VectorStoreIndex, StorageContext\n\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\nautomerging_index = VectorStoreIndex(\n    leaf_nodes, storage_context=storage_context, service_context=auto_merging_context\n)\n\nautomerging_index.storage_context.persist(persist_dir=\"./merging_index\")\n</pre> from llama_index import VectorStoreIndex, StorageContext  storage_context = StorageContext.from_defaults() storage_context.docstore.add_documents(nodes)  automerging_index = VectorStoreIndex(     leaf_nodes, storage_context=storage_context, service_context=auto_merging_context )  automerging_index.storage_context.persist(persist_dir=\"./merging_index\") In\u00a0[\u00a0]: Copied! <pre># This block of code is optional to check\n# if an index file exist, then it will load it\n# if not, it will rebuild it\n\nimport os\nfrom llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\nfrom llama_index import load_index_from_storage\n\nif not os.path.exists(\"./merging_index\"):\n    storage_context = StorageContext.from_defaults()\n    storage_context.docstore.add_documents(nodes)\n\n    automerging_index = VectorStoreIndex(\n            leaf_nodes,\n            storage_context=storage_context,\n            service_context=auto_merging_context\n        )\n\n    automerging_index.storage_context.persist(persist_dir=\"./merging_index\")\nelse:\n    automerging_index = load_index_from_storage(\n        StorageContext.from_defaults(persist_dir=\"./merging_index\"),\n        service_context=auto_merging_context\n    )\n</pre> # This block of code is optional to check # if an index file exist, then it will load it # if not, it will rebuild it  import os from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage from llama_index import load_index_from_storage  if not os.path.exists(\"./merging_index\"):     storage_context = StorageContext.from_defaults()     storage_context.docstore.add_documents(nodes)      automerging_index = VectorStoreIndex(             leaf_nodes,             storage_context=storage_context,             service_context=auto_merging_context         )      automerging_index.storage_context.persist(persist_dir=\"./merging_index\") else:     automerging_index = load_index_from_storage(         StorageContext.from_defaults(persist_dir=\"./merging_index\"),         service_context=auto_merging_context     )  In\u00a0[\u00a0]: Copied! <pre>from llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index.retrievers import AutoMergingRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nautomerging_retriever = automerging_index.as_retriever(\n    similarity_top_k=12\n)\n\nretriever = AutoMergingRetriever(\n    automerging_retriever, \n    automerging_index.storage_context, \n    verbose=True\n)\n\nrerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")\n\nauto_merging_engine = RetrieverQueryEngine.from_args(\n    automerging_retriever, node_postprocessors=[rerank]\n)\n</pre> from llama_index.indices.postprocessor import SentenceTransformerRerank from llama_index.retrievers import AutoMergingRetriever from llama_index.query_engine import RetrieverQueryEngine  automerging_retriever = automerging_index.as_retriever(     similarity_top_k=12 )  retriever = AutoMergingRetriever(     automerging_retriever,      automerging_index.storage_context,      verbose=True )  rerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")  auto_merging_engine = RetrieverQueryEngine.from_args(     automerging_retriever, node_postprocessors=[rerank] ) In\u00a0[\u00a0]: Copied! <pre>auto_merging_response = auto_merging_engine.query(\n    \"What is the importance of networking in AI?\"\n)\n</pre> auto_merging_response = auto_merging_engine.query(     \"What is the importance of networking in AI?\" ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.response.notebook_utils import display_response\n\ndisplay_response(auto_merging_response)\n</pre> from llama_index.response.notebook_utils import display_response  display_response(auto_merging_response) In\u00a0[\u00a0]: Copied! <pre>import os\n\nfrom llama_index import (\n    ServiceContext,\n    StorageContext,\n    VectorStoreIndex,\n    load_index_from_storage,\n)\nfrom llama_index.node_parser import HierarchicalNodeParser\nfrom llama_index.node_parser import get_leaf_nodes\nfrom llama_index import StorageContext, load_index_from_storage\nfrom llama_index.retrievers import AutoMergingRetriever\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index.query_engine import RetrieverQueryEngine\n\n\ndef build_automerging_index(\n    documents,\n    llm,\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index\",\n    chunk_sizes=None,\n):\n    chunk_sizes = chunk_sizes or [2048, 512, 128]\n    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n    nodes = node_parser.get_nodes_from_documents(documents)\n    leaf_nodes = get_leaf_nodes(nodes)\n    merging_context = ServiceContext.from_defaults(\n        llm=llm,\n        embed_model=embed_model,\n    )\n    storage_context = StorageContext.from_defaults()\n    storage_context.docstore.add_documents(nodes)\n\n    if not os.path.exists(save_dir):\n        automerging_index = VectorStoreIndex(\n            leaf_nodes, storage_context=storage_context, service_context=merging_context\n        )\n        automerging_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        automerging_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=merging_context,\n        )\n    return automerging_index\n\n\ndef get_automerging_query_engine(\n    automerging_index,\n    similarity_top_k=12,\n    rerank_top_n=6,\n):\n    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n    retriever = AutoMergingRetriever(\n        base_retriever, automerging_index.storage_context, verbose=True\n    )\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n    auto_merging_engine = RetrieverQueryEngine.from_args(\n        retriever, node_postprocessors=[rerank]\n    )\n    return auto_merging_engine\n</pre> import os  from llama_index import (     ServiceContext,     StorageContext,     VectorStoreIndex,     load_index_from_storage, ) from llama_index.node_parser import HierarchicalNodeParser from llama_index.node_parser import get_leaf_nodes from llama_index import StorageContext, load_index_from_storage from llama_index.retrievers import AutoMergingRetriever from llama_index.indices.postprocessor import SentenceTransformerRerank from llama_index.query_engine import RetrieverQueryEngine   def build_automerging_index(     documents,     llm,     embed_model=\"local:BAAI/bge-small-en-v1.5\",     save_dir=\"merging_index\",     chunk_sizes=None, ):     chunk_sizes = chunk_sizes or [2048, 512, 128]     node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)     nodes = node_parser.get_nodes_from_documents(documents)     leaf_nodes = get_leaf_nodes(nodes)     merging_context = ServiceContext.from_defaults(         llm=llm,         embed_model=embed_model,     )     storage_context = StorageContext.from_defaults()     storage_context.docstore.add_documents(nodes)      if not os.path.exists(save_dir):         automerging_index = VectorStoreIndex(             leaf_nodes, storage_context=storage_context, service_context=merging_context         )         automerging_index.storage_context.persist(persist_dir=save_dir)     else:         automerging_index = load_index_from_storage(             StorageContext.from_defaults(persist_dir=save_dir),             service_context=merging_context,         )     return automerging_index   def get_automerging_query_engine(     automerging_index,     similarity_top_k=12,     rerank_top_n=6, ):     base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)     retriever = AutoMergingRetriever(         base_retriever, automerging_index.storage_context, verbose=True     )     rerank = SentenceTransformerRerank(         top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"     )     auto_merging_engine = RetrieverQueryEngine.from_args(         retriever, node_postprocessors=[rerank]     )     return auto_merging_engine In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms import OpenAI\n\nindex = build_automerging_index(\n    [document],\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    save_dir=\"./merging_index\",\n)\n</pre> from llama_index.llms import OpenAI  index = build_automerging_index(     [document],     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),     save_dir=\"./merging_index\", )  In\u00a0[\u00a0]: Copied! <pre>query_engine = get_automerging_query_engine(index, similarity_top_k=6)\n</pre> query_engine = get_automerging_query_engine(index, similarity_top_k=6) In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import Tru\n\nTru().reset_database()\n</pre> from trulens_eval import Tru  Tru().reset_database() In\u00a0[\u00a0]: Copied! <pre>auto_merging_index_0 = build_automerging_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index_0\",\n    chunk_sizes=[2048,512],\n)\n</pre> auto_merging_index_0 = build_automerging_index(     documents,     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),     embed_model=\"local:BAAI/bge-small-en-v1.5\",     save_dir=\"merging_index_0\",     chunk_sizes=[2048,512], ) In\u00a0[\u00a0]: Copied! <pre>auto_merging_engine_0 = get_automerging_query_engine(\n    auto_merging_index_0,\n    similarity_top_k=12,\n    rerank_top_n=6,\n)\n</pre> auto_merging_engine_0 = get_automerging_query_engine(     auto_merging_index_0,     similarity_top_k=12,     rerank_top_n=6, ) In\u00a0[\u00a0]: Copied! <pre>from utils import get_prebuilt_trulens_recorder\n\ntru_recorder = get_prebuilt_trulens_recorder(\n    auto_merging_engine_0,\n    app_id ='app_0'\n)\n</pre> from utils import get_prebuilt_trulens_recorder  tru_recorder = get_prebuilt_trulens_recorder(     auto_merging_engine_0,     app_id ='app_0' ) In\u00a0[\u00a0]: Copied! <pre>eval_questions = []\nwith open('generated_questions.text', 'r') as file:\n    for line in file:\n        # Remove newline character and convert to integer\n        item = line.strip()\n        eval_questions.append(item)\n</pre> eval_questions = [] with open('generated_questions.text', 'r') as file:     for line in file:         # Remove newline character and convert to integer         item = line.strip()         eval_questions.append(item) In\u00a0[\u00a0]: Copied! <pre>def run_evals(eval_questions, tru_recorder, query_engine):\n    for question in eval_questions:\n        with tru_recorder as recording:\n            response = query_engine.query(question)\n</pre> def run_evals(eval_questions, tru_recorder, query_engine):     for question in eval_questions:         with tru_recorder as recording:             response = query_engine.query(question) In\u00a0[\u00a0]: Copied! <pre>run_evals(eval_questions, tru_recorder, auto_merging_engine_0)\n</pre> run_evals(eval_questions, tru_recorder, auto_merging_engine_0) In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import Tru\n\nTru().get_leaderboard(app_ids=[])\n</pre> from trulens_eval import Tru  Tru().get_leaderboard(app_ids=[]) In\u00a0[\u00a0]: Copied! <pre>Tru().run_dashboard()\n</pre> Tru().run_dashboard() In\u00a0[\u00a0]: Copied! <pre>auto_merging_index_1 = build_automerging_index(\n    documents,\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n    save_dir=\"merging_index_1\",\n    chunk_sizes=[2048,512,128],\n)\n</pre> auto_merging_index_1 = build_automerging_index(     documents,     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),     embed_model=\"local:BAAI/bge-small-en-v1.5\",     save_dir=\"merging_index_1\",     chunk_sizes=[2048,512,128], ) In\u00a0[\u00a0]: Copied! <pre>auto_merging_engine_1 = get_automerging_query_engine(\n    auto_merging_index_1,\n    similarity_top_k=12,\n    rerank_top_n=6,\n)\n</pre> auto_merging_engine_1 = get_automerging_query_engine(     auto_merging_index_1,     similarity_top_k=12,     rerank_top_n=6, )  In\u00a0[\u00a0]: Copied! <pre>tru_recorder = get_prebuilt_trulens_recorder(\n    auto_merging_engine_1,\n    app_id ='app_1'\n)\n</pre> tru_recorder = get_prebuilt_trulens_recorder(     auto_merging_engine_1,     app_id ='app_1' ) In\u00a0[\u00a0]: Copied! <pre>run_evals(eval_questions, tru_recorder, auto_merging_engine_1)\n</pre> run_evals(eval_questions, tru_recorder, auto_merging_engine_1) In\u00a0[\u00a0]: Copied! <pre>from trulens_eval import Tru\n\nTru().get_leaderboard(app_ids=[])\n</pre> from trulens_eval import Tru  Tru().get_leaderboard(app_ids=[]) In\u00a0[\u00a0]: Copied! <pre>Tru().run_dashboard()\n</pre> Tru().run_dashboard()"},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#lesson-4-auto-merging-retrieval","title":"Lesson 4: Auto-merging Retrieval\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#auto-merging-retrieval-setup","title":"Auto-merging retrieval setup\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#building-the-index","title":"Building the index\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#defining-the-retriever-and-running-the-query-engine","title":"Defining the retriever and running the query engine\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#putting-it-all-together","title":"Putting it all Together\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#trulens-evaluation","title":"TruLens Evaluation\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#two-layers","title":"Two layers\u00b6","text":""},{"location":"RAG/05_Auto_Merging_RAG/Auto-merging_Retrieval/#three-layers","title":"Three layers\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/","title":"Index","text":"<pre><code>graph TD\n    A[Query] --&gt; B[Generate Multiple Hypothetical Documents]\n    B --&gt; C1[Hypothetical Doc 1]\n    B --&gt; C2[Hypothetical Doc 2]\n    B --&gt; C3[Hypothetical Doc N]\n\n    C1 --&gt; D1[Embed Doc 1]\n    C2 --&gt; D2[Embed Doc 2]\n    C3 --&gt; D3[Embed Doc N]\n\n    D1 &amp; D2 &amp; D3 --&gt; E{Similarity Search}\n    E --&gt; F[Retrieve Top-K Documents]\n    F --&gt; G[Aggregate Results]\n    G --&gt; H[Form Context]\n    A --&gt; H\n    H --&gt; I[Generate Response]\n\n    subgraph \"Document Processing\"\n        J[Corpus Documents] --&gt; K[Split into Chunks]\n        K --&gt; L[Generate Embeddings]\n        L --&gt; M[(Vector Store)]\n    end\n\n    M -.-&gt; E\n</code></pre>"},{"location":"RAG/06_HyDE_RAG/#introduction","title":"Introduction","text":"<p>This project implements a Retrieval-Augmented Generation (RAG) system enhanced with Hypothetical Document Embeddings (HyDE), a novel approach to dense retrieval that improves the accuracy and relevance of retrieved information.</p>"},{"location":"RAG/06_HyDE_RAG/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with semantic understanding of complex queries. HyDE addresses this by generating a hypothetical answer, which serves as a more informative representation of the query intent, leading to more accurate document retrieval.</p>"},{"location":"RAG/06_HyDE_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/06_HyDE_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Split documents into manageable chunks</li> <li>Generate embeddings for each chunk using a suitable embedding model</li> <li>Store embeddings in a vector database for efficient similarity search</li> </ol>"},{"location":"RAG/06_HyDE_RAG/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Processing:</li> <li>Generate a hypothetical document/answer to the query using an LLM</li> <li>Document Embedding:</li> <li>Create an embedding of the hypothetical document</li> <li>Similarity Search:</li> <li>Compare the hypothetical document embedding against the corpus embeddings</li> <li>Retrieval:</li> <li>Fetch the top-K most similar real documents</li> <li>Context Formation:</li> <li>Combine the original query with the retrieved documents</li> <li>Generation:</li> <li>Use an LLM to generate the final response based on the formed context</li> </ol>"},{"location":"RAG/06_HyDE_RAG/#key-features-of-rag-with-hyde","title":"Key Features of RAG with HyDE","text":"<ul> <li>Hypothetical document generation for improved query understanding</li> <li>Dense retrieval using document-to-document similarity</li> <li>Integration with existing RAG pipelines</li> <li>Flexible architecture allowing for different embedding and language models</li> </ul>"},{"location":"RAG/06_HyDE_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ul> <li>Enhanced semantic understanding of complex queries</li> <li>Improved retrieval accuracy, especially for nuanced or abstract questions</li> <li>Reduced sensitivity to specific query phrasing</li> <li>Better handling of out-of-distribution queries</li> </ul>"},{"location":"RAG/06_HyDE_RAG/#conclusion","title":"Conclusion","text":"<p>The HyDE-enhanced RAG system represents a significant advancement in information retrieval and question-answering technologies. By leveraging hypothetical documents, it bridges the gap between user queries and relevant information, resulting in more accurate and contextually appropriate responses.</p>"},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/","title":"HyDE RAG(Llamaindex)","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n</pre> !pip install llama-index In\u00a0[\u00a0]: Copied! <pre>!mkdir -p 'data/paul_graham/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n</pre> !mkdir -p 'data/paul_graham/' !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' In\u00a0[\u00a0]: Copied! <pre>import logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.indices.query.query_transform import HyDEQueryTransform\nfrom llama_index.core.query_engine import TransformQueryEngine\nfrom IPython.display import Markdown, display\n</pre> import logging import sys  logging.basicConfig(stream=sys.stdout, level=logging.INFO) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))  from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core.indices.query.query_transform import HyDEQueryTransform from llama_index.core.query_engine import TransformQueryEngine from IPython.display import Markdown, display In\u00a0[\u00a0]: Copied! <pre># load documents\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n</pre> # load documents documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data() In\u00a0[\u00a0]: Copied! <pre>index = VectorStoreIndex.from_documents(documents)\n</pre> index = VectorStoreIndex.from_documents(documents) In\u00a0[\u00a0]: Copied! <pre>query_str = \"what did paul graham do after going to RISD\"\n</pre> query_str = \"what did paul graham do after going to RISD\" In\u00a0[\u00a0]: Copied! <pre>query_engine = index.as_query_engine()\nresponse = query_engine.query(query_str)\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> query_engine = index.as_query_engine() response = query_engine.query(query_str) display(Markdown(f\"{response}\")) <p>After going to RISD, Paul Graham continued to pursue his passion for painting and art. He took classes in the painting department at the Accademia di Belli Arti in Florence, and he also took the entrance exam for the school. He also continued to work on his book On Lisp, and he took on consulting work to make money. At the school, Paul Graham and the other students had an arrangement where the faculty wouldn't require the students to learn anything, and in return the students wouldn't require the faculty to teach anything. Paul Graham was one of the few students who actually painted the nude model that was provided, while the rest of the students spent their time chatting or occasionally trying to imitate things they'd seen in American art magazines. The model turned out to live just down the street from Paul Graham, and she made a living from a combination of modelling and making fakes for a local antique dealer.</p> In\u00a0[\u00a0]: Copied! <pre>hyde = HyDEQueryTransform(include_original=True)\nhyde_query_engine = TransformQueryEngine(query_engine, hyde)\nresponse = hyde_query_engine.query(query_str)\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> hyde = HyDEQueryTransform(include_original=True) hyde_query_engine = TransformQueryEngine(query_engine, hyde) response = hyde_query_engine.query(query_str) display(Markdown(f\"{response}\")) <p>After going to RISD, Paul Graham worked as a consultant for Interleaf and then co-founded Viaweb with Robert Morris. They created a software that allowed users to build websites via the web and received $10,000 in seed funding from Idelle's husband Julian. They gave Julian 10% of the company in return for the initial legal work and business advice. Paul Graham had a negative net worth due to taxes he owed, so the seed funding was necessary for him to live on. They opened for business in January 1996 with 6 stores.</p> <p>Paul Graham then left Yahoo after his options vested and went back to New York. He resumed his old life, but now he was rich. He tried to paint, but he didn't have much energy or ambition. He eventually moved back to Cambridge and started working on a web app for making web apps. He recruited Dan Giffin and two undergrads to help him, but he eventually realized he didn't want to run a company and decided to build a subset of the project as an open source project. He and Dan worked on a new dialect of Lisp, which he called Arc, in a house he bought in Cambridge. The subset he built as an open source project was the new Lisp, whose</p> In\u00a0[\u00a0]: Copied! <pre>query_bundle = hyde(query_str)\nhyde_doc = query_bundle.embedding_strs[0]\n</pre> query_bundle = hyde(query_str) hyde_doc = query_bundle.embedding_strs[0] In\u00a0[\u00a0]: Copied! <pre>hyde_doc\n</pre> hyde_doc <p>After graduating from the Rhode Island School of Design (RISD) in 1985, Paul Graham went on to pursue a career in computer programming. He worked as a software developer for several companies, including Viaweb, which he co-founded in 1995. Viaweb was eventually acquired by Yahoo in 1998, and Graham used the proceeds to become a venture capitalist. He founded Y Combinator in 2005, a startup accelerator that has helped launch over 2,000 companies, including Dropbox, Airbnb, and Reddit. Graham has also written several books on programming and startups, and he continues to be an active investor in the tech industry.</p> In\u00a0[\u00a0]: Copied! <pre>query_str = \"What is Bel?\"\n</pre> query_str = \"What is Bel?\" In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(query_str)\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> response = query_engine.query(query_str) display(Markdown(f\"{response}\")) <p>Bel is a programming language that was written in Arc by Paul Graham over the course of four years (March 26, 2015 to October 12, 2019). It is based on John McCarthy's original Lisp, but with additional features added. It is a spec expressed as code, and is meant to be a formal model of computation, an alternative to the Turing machine.</p> In\u00a0[\u00a0]: Copied! <pre>hyde = HyDEQueryTransform(include_original=True)\nhyde_query_engine = TransformQueryEngine(query_engine, hyde)\nresponse = hyde_query_engine.query(query_str)\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> hyde = HyDEQueryTransform(include_original=True) hyde_query_engine = TransformQueryEngine(query_engine, hyde) response = hyde_query_engine.query(query_str) display(Markdown(f\"{response}\")) <p>Bel is the pseudonym of Paul Graham, the author of the context information who was in need of seed funding to live on and was part of a deal that became the model for Y Combinator's.</p> In\u00a0[\u00a0]: Copied! <pre>query_bundle = hyde(query_str)\nhyde_doc = query_bundle.embedding_strs[0]\n</pre> query_bundle = hyde(query_str) hyde_doc = query_bundle.embedding_strs[0] In\u00a0[\u00a0]: Copied! <pre>hyde_doc\n</pre> hyde_doc <p>Bel is an ancient Semitic god, originating from the Middle East. He is often associated with the sun and is sometimes referred to as the \"Lord of Heaven\". Bel is also known as the god of fertility, abundance, and prosperity. He is often depicted as a bull or a man with a bull's head. In some cultures, Bel is seen as a creator god, responsible for the creation of the universe. He is also associated with the underworld and is sometimes seen as a god of death. Bel is also associated with justice and is often seen as a protector of the innocent. Bel is an important figure in many religions, including Judaism, Christianity, and Islam.</p> In\u00a0[\u00a0]: Copied! <pre>query_str = \"What would the author say about art vs. engineering?\"\n</pre> query_str = \"What would the author say about art vs. engineering?\" In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(query_str)\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> response = query_engine.query(query_str) display(Markdown(f\"{response}\")) <p>The author would likely say that art and engineering are two different disciplines that require different skills and approaches. Art is more focused on expression and creativity, while engineering is more focused on problem-solving and technical knowledge. The author also suggests that art school does not always provide the same level of rigor as engineering school, and that painting students are often encouraged to develop a signature style rather than learn the fundamentals of painting. Furthermore, the author would likely point out that engineering can provide more financial stability than art, as evidenced by the author's own experience of needing seed funding to live on while launching a company.</p> In\u00a0[\u00a0]: Copied! <pre>response = hyde_query_engine.query(query_str)\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n</pre> response = hyde_query_engine.query(query_str) display(Markdown(f\"{response}\")) <p>The author would likely say that art is a more lasting and independent form of work than engineering. They mention that software written today will be obsolete in a couple decades, and that systems work does not last. In contrast, they note that paintings can last hundreds of years and that it is possible to make a living as an artist. They also mention that as an artist, you can be truly independent and don't need to have a boss or research funding. Furthermore, they note that art can be a source of income for people who may not have access to traditional forms of employment, such as the model in the example who was able to make a living from modelling and making fakes for a local antique dealer.</p>"},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#hyde-query-transform","title":"HyDE Query Transform\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#download-data","title":"Download Data\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#load-documents-build-the-vectorstoreindex","title":"Load documents, build the VectorStoreIndex\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#example-hyde-improves-specific-temporal-queries","title":"Example: HyDE improves specific temporal queries\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#first-we-query-without-transformation-the-same-query-string-is-used-for-embedding-lookup-and-also-summarization","title":"First, we query without transformation: The same query string is used for embedding lookup and also summarization.\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#now-we-use-hydequerytransform-to-generate-a-hypothetical-document-and-use-it-for-embedding-lookup","title":"Now, we use <code>HyDEQueryTransform</code> to generate a hypothetical document and use it for embedding lookup.\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#in-this-example-hyde-improves-output-quality-significantly-by-hallucinating-accurately-what-paul-graham-did-after-risd-see-below-and-thus-improving-the-embedding-quality-and-final-output","title":"In this example, <code>HyDE</code> improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output.\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#failure-case-1-hyde-may-mislead-when-query-can-be-mis-interpreted-without-context","title":"Failure case 1: HyDE may mislead when query can be mis-interpreted without context.\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#querying-without-transformation-yields-reasonable-answer","title":"Querying without transformation yields reasonable answer\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#querying-with-hydequerytransform-results-in-nonsense","title":"Querying with <code>HyDEQueryTransform</code> results in nonsense\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#in-this-example-hyde-mis-interprets-bel-without-document-context-see-below-resulting-in-a-completely-unrelated-embedding-string-and-poor-retrieval-outcome","title":"In this example, <code>HyDE</code> mis-interprets Bel without document context (see below), resulting in a completely unrelated embedding string and poor retrieval outcome.\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#failure-case-2-hyde-may-bias-open-ended-queries","title":"Failure case 2: HyDE may bias open-ended queries\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#querying-without-transformation-yields-a-reasonable-answer","title":"Querying without transformation yields a reasonable answer\u00b6","text":""},{"location":"RAG/06_HyDE_RAG/HyDEQueryTransformDemo/#querying-with-hydequerytransform-results-in-a-more-biased-output","title":"Querying with <code>HyDEQueryTransform</code> results in a more biased output\u00b6","text":""},{"location":"RAG/06_Query_Transformation_RAG/","title":"Query Transform Cookbook","text":""},{"location":"RAG/06_Query_Transformation_RAG/#query-transform-cookbook","title":"Query Transform Cookbook","text":""},{"location":"RAG/06_Query_Transformation_RAG/#introduction","title":"Introduction","text":"<p>The Query Transform Cookbook demonstrates various techniques for transforming and decomposing user queries before execution in a Retrieval-Augmented Generation (RAG) query engine, agent, or other pipeline. These transformations can enhance the quality and relevance of responses in AI applications.</p>"},{"location":"RAG/06_Query_Transformation_RAG/#query-transformation-techniques","title":"Query Transformation Techniques","text":""},{"location":"RAG/06_Query_Transformation_RAG/#1-routing","title":"1. Routing","text":"<p>Routing involves identifying the relevant subset of tools that apply to a given query.</p> <p> flowchart LR     A[User Query] \u2192 B[Selector]     B \u2192 C[Tool 1]     B \u2192 D[Tool 2]     B \u2192 E[Tool N]     C &amp; D &amp; E \u2192 F[Selected Tools]</p>"},{"location":"RAG/06_Query_Transformation_RAG/#implementation","title":"Implementation:","text":"<ul> <li>Use <code>LLMSingleSelector</code> or <code>LLMMultiSelector</code> for LLM-based selection</li> <li>Use <code>PydanticSingleSelector</code> or <code>PydanticMultiSelector</code> for function calling-based selection</li> <li>Define tool choices using <code>ToolMetadata</code></li> </ul>"},{"location":"RAG/06_Query_Transformation_RAG/#2-query-rewriting","title":"2. Query Rewriting","text":"<p>Query rewriting involves generating multiple variations of the original query to improve retrieval results.</p> <pre><code>flowchart TD\n    A[Original Query] --&gt; B[Query Rewriter]\n    B --&gt; C[Rewritten Query 1]\n    B --&gt; D[Rewritten Query 2]\n    B --&gt; E[Rewritten Query N]\n    C &amp; D &amp; E --&gt; F[Retriever]\n    F --&gt; G[Enhanced Results]\n</code></pre>"},{"location":"RAG/06_Query_Transformation_RAG/#implementation_1","title":"Implementation:","text":"<ul> <li>Custom implementation using <code>PromptTemplate</code> and LLM</li> <li>Use <code>HyDEQueryTransform</code> for hypothetical document embeddings</li> </ul>"},{"location":"RAG/06_Query_Transformation_RAG/#3-sub-questions-generation","title":"3. Sub-Questions Generation","text":"<p>This technique decomposes a complex query into multiple sub-questions, each targeted at specific tools.</p> <pre><code>flowchart TD\n    A[Complex Query] --&gt; B[Question Generator]\n    B --&gt; C[Sub-Question 1]\n    B --&gt; D[Sub-Question 2]\n    B --&gt; E[Sub-Question N]\n    C --&gt; F[Tool 1]\n    D --&gt; G[Tool 2]\n    E --&gt; H[Tool N]\n    F &amp; G &amp; H --&gt; I[Aggregated Response]\n</code></pre>"},{"location":"RAG/06_Query_Transformation_RAG/#implementation_2","title":"Implementation:","text":"<ul> <li>Use <code>OpenAIQuestionGenerator</code> or <code>LLMQuestionGenerator</code></li> <li>Define tool choices using <code>ToolMetadata</code></li> </ul>"},{"location":"RAG/06_Query_Transformation_RAG/#4-react-agent-tool-picking","title":"4. ReAct Agent Tool Picking","text":"<p>This approach uses the ReAct framework to decide both the tool to use and the query to execute on that tool.</p> <pre><code>flowchart LR\n    A[User Query] --&gt; B[ReAct Agent]\n    B --&gt; C{Tool Selection}\n    C --&gt; D[Tool 1]\n    C --&gt; E[Tool 2]\n    C --&gt; F[Tool N]\n    D &amp; E &amp; F --&gt; G[Execute Query]\n    G --&gt; H[Response]\n    H --&gt; B\n</code></pre>"},{"location":"RAG/06_Query_Transformation_RAG/#implementation_3","title":"Implementation:","text":"<ul> <li>Use <code>ReActChatFormatter</code> for input formatting</li> <li>Use <code>ReActOutputParser</code> for parsing LLM output</li> <li>Define tools using <code>FunctionTool</code></li> </ul>"},{"location":"RAG/06_Query_Transformation_RAG/#usage","title":"Usage","text":"<p>Each query transformation technique can be used as a modular component in larger systems. Here's a basic usage example for query rewriting:</p> <pre><code>from llama_index.core import PromptTemplate\nfrom llama_index.llms.openai import OpenAI\n\nquery_gen_prompt = PromptTemplate(\"Your prompt template here\")\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\ndef generate_queries(query: str, llm, num_queries: int = 4):\n    response = llm.predict(query_gen_prompt, num_queries=num_queries, query=query)\n    queries = response.split(\"\\n\")\n    return queries\n\nqueries = generate_queries(\"Your query here\", llm)\n</code></pre>"},{"location":"RAG/06_Query_Transformation_RAG/#conclusion","title":"Conclusion","text":"<p>The Query Transform Cookbook provides a comprehensive set of techniques for enhancing query processing in AI applications. By leveraging these transformations, developers can create more robust and accurate information retrieval and question-answering systems.</p> <p>For more detailed implementations and integrations with specific query engines or retrievers, refer to the LlamaIndex documentation. </p> <p>This README provides an overview of the Query Transform Cookbook, including brief explanations and Mermaid diagrams for each of the four main query transformation techniques: Routing, Query Rewriting, Sub-Questions Generation, and ReAct Agent Tool Picking.</p> <p>Each section includes a diagram visualizing the process, a brief description of the technique, and basic implementation details. The README also includes a simple usage example for query rewriting to give users a starting point.</p> <p>Is there any specific part of this README you'd like me to expand on or modify?</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/","title":"Query Transformation(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-question-gen-openai\n%pip install llama-index-llms-openai\n</pre> %pip install llama-index-question-gen-openai %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Markdown, display\n\n\n# define prompt viewing function\ndef display_prompt_dict(prompts_dict):\n    for k, p in prompts_dict.items():\n        text_md = f\"**Prompt Key**: {k}&lt;br&gt;\" f\"**Text:** &lt;br&gt;\"\n        display(Markdown(text_md))\n        print(p.get_template())\n        display(Markdown(\"&lt;br&gt;&lt;br&gt;\"))\n</pre> from IPython.display import Markdown, display   # define prompt viewing function def display_prompt_dict(prompts_dict):     for k, p in prompts_dict.items():         text_md = f\"**Prompt Key**: {k}\" f\"**Text:** \"         display(Markdown(text_md))         print(p.get_template())         display(Markdown(\"\")) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\nfrom llama_index.core.selectors import (\n    PydanticMultiSelector,\n    PydanticSingleSelector,\n)\n</pre> from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector from llama_index.core.selectors import (     PydanticMultiSelector,     PydanticSingleSelector, ) In\u00a0[\u00a0]: Copied! <pre># pydantic selectors feed in pydantic objects to a function calling API\n# single selector (pydantic, function calling)\n# selector = PydanticSingleSelector.from_defaults()\n\n# multi selector (pydantic, function calling)\n# selector = PydanticMultiSelector.from_defaults()\n\n# LLM selectors use text completion endpoints\n# single selector (LLM)\n# selector = LLMSingleSelector.from_defaults()\n# multi selector (LLM)\nselector = LLMMultiSelector.from_defaults()\n</pre> # pydantic selectors feed in pydantic objects to a function calling API # single selector (pydantic, function calling) # selector = PydanticSingleSelector.from_defaults()  # multi selector (pydantic, function calling) # selector = PydanticMultiSelector.from_defaults()  # LLM selectors use text completion endpoints # single selector (LLM) # selector = LLMSingleSelector.from_defaults() # multi selector (LLM) selector = LLMMultiSelector.from_defaults() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.tools import ToolMetadata\n\ntool_choices = [\n    ToolMetadata(\n        name=\"covid_nyt\",\n        description=(\"This tool contains a NYT news article about COVID-19\"),\n    ),\n    ToolMetadata(\n        name=\"covid_wiki\",\n        description=(\"This tool contains the Wikipedia page about COVID-19\"),\n    ),\n    ToolMetadata(\n        name=\"covid_tesla\",\n        description=(\"This tool contains the Wikipedia page about apples\"),\n    ),\n]\n</pre> from llama_index.core.tools import ToolMetadata  tool_choices = [     ToolMetadata(         name=\"covid_nyt\",         description=(\"This tool contains a NYT news article about COVID-19\"),     ),     ToolMetadata(         name=\"covid_wiki\",         description=(\"This tool contains the Wikipedia page about COVID-19\"),     ),     ToolMetadata(         name=\"covid_tesla\",         description=(\"This tool contains the Wikipedia page about apples\"),     ), ] In\u00a0[\u00a0]: Copied! <pre>display_prompt_dict(selector.get_prompts())\n</pre> display_prompt_dict(selector.get_prompts()) <p>Prompt Key: promptText: </p> <pre>Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n---------------------\n{context_list}\n---------------------\nUsing only the choices above and not prior knowledge, return the top choices (no more than {max_outputs}, but only select what is needed) that are most relevant to the question: '{query_str}'\n\n\nThe output should be ONLY JSON formatted as a JSON instance.\n\nHere is an example:\n[\n    {{\n        choice: 1,\n        reason: \"&lt;insert reason for choice&gt;\"\n    }},\n    ...\n]\n\n</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>selector_result = selector.select(\n    tool_choices, query=\"Tell me more about COVID-19\"\n)\n</pre> selector_result = selector.select(     tool_choices, query=\"Tell me more about COVID-19\" ) In\u00a0[\u00a0]: Copied! <pre>selector_result.selections\n</pre> selector_result.selections Out[\u00a0]: <pre>[SingleSelection(index=0, reason='This tool contains a NYT news article about COVID-19'),\n SingleSelection(index=1, reason='This tool contains the Wikipedia page about COVID-19')]</pre> <p>Learn more about our routing abstractions in our dedicated Router page.</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import PromptTemplate\nfrom llama_index.llms.openai import OpenAI\n\nquery_gen_str = \"\"\"\\\nYou are a helpful assistant that generates multiple search queries based on a \\\nsingle input query. Generate {num_queries} search queries, one on each line, \\\nrelated to the following input query:\nQuery: {query}\nQueries:\n\"\"\"\nquery_gen_prompt = PromptTemplate(query_gen_str)\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\n\ndef generate_queries(query: str, llm, num_queries: int = 4):\n    response = llm.predict(\n        query_gen_prompt, num_queries=num_queries, query=query\n    )\n    # assume LLM proper put each query on a newline\n    queries = response.split(\"\\n\")\n    queries_str = \"\\n\".join(queries)\n    print(f\"Generated queries:\\n{queries_str}\")\n    return queries\n</pre> from llama_index.core import PromptTemplate from llama_index.llms.openai import OpenAI  query_gen_str = \"\"\"\\ You are a helpful assistant that generates multiple search queries based on a \\ single input query. Generate {num_queries} search queries, one on each line, \\ related to the following input query: Query: {query} Queries: \"\"\" query_gen_prompt = PromptTemplate(query_gen_str)  llm = OpenAI(model=\"gpt-3.5-turbo\")   def generate_queries(query: str, llm, num_queries: int = 4):     response = llm.predict(         query_gen_prompt, num_queries=num_queries, query=query     )     # assume LLM proper put each query on a newline     queries = response.split(\"\\n\")     queries_str = \"\\n\".join(queries)     print(f\"Generated queries:\\n{queries_str}\")     return queries In\u00a0[\u00a0]: Copied! <pre>queries = generate_queries(\"What happened at Interleaf and Viaweb?\", llm)\n</pre> queries = generate_queries(\"What happened at Interleaf and Viaweb?\", llm) <pre>Generated queries:\n1. What were the major events or milestones in the history of Interleaf and Viaweb?\n2. Who were the founders and key figures involved in the development of Interleaf and Viaweb?\n3. What were the products or services offered by Interleaf and Viaweb?\n4. Are there any notable success stories or failures associated with Interleaf and Viaweb?\n</pre> In\u00a0[\u00a0]: Copied! <pre>queries\n</pre> queries Out[\u00a0]: <pre>['1. What were the major events or milestones in the history of Interleaf and Viaweb?',\n '2. Who were the founders and key figures involved in the development of Interleaf and Viaweb?',\n '3. What were the products or services offered by Interleaf and Viaweb?',\n '4. Are there any notable success stories or failures associated with Interleaf and Viaweb?']</pre> <p>For more details about an e2e implementation with a retriever, check out our guides on our fusion retriever:</p> <ul> <li>Module Guide</li> <li>Build a Fusion Retriever from Scratch</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.indices.query.query_transform import HyDEQueryTransform\nfrom llama_index.llms.openai import OpenAI\n</pre> from llama_index.core.indices.query.query_transform import HyDEQueryTransform from llama_index.llms.openai import OpenAI In\u00a0[\u00a0]: Copied! <pre>hyde = HyDEQueryTransform(include_original=True)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\nquery_bundle = hyde.run(\"What is Bel?\")\n</pre> hyde = HyDEQueryTransform(include_original=True) llm = OpenAI(model=\"gpt-3.5-turbo\")  query_bundle = hyde.run(\"What is Bel?\") <p>This generates a query bundle that contains the original query, but also <code>custom_embedding_strs</code> representing the queries that should be embedded.</p> In\u00a0[\u00a0]: Copied! <pre>new_query.custom_embedding_strs\n</pre> new_query.custom_embedding_strs Out[\u00a0]: <pre>['Bel is a term that has multiple meanings and can be interpreted in various ways depending on the context. In ancient Mesopotamian mythology, Bel was a prominent deity and one of the chief gods of the Babylonian pantheon. He was often associated with the sky, storms, and fertility. Bel was considered to be the father of the gods and held great power and authority over the other deities.\\n\\nIn addition to its mythological significance, Bel is also a title that was used to address rulers and leaders in ancient Babylon. It was a term of respect and reverence, similar to the modern-day title of \"king\" or \"emperor.\" The title of Bel was bestowed upon those who held significant political and military power, and it symbolized their authority and dominion over their subjects.\\n\\nFurthermore, Bel is also a common given name in various cultures around the world. It can be found in different forms and variations, such as Belinda, Isabel, or Bella. As a personal name, Bel often carries connotations of beauty, grace, and strength.\\n\\nIn summary, Bel can refer to a powerful deity in ancient Mesopotamian mythology, a title of respect for rulers and leaders, or a personal name with positive attributes. The meaning of Bel can vary depending on the specific context in which it is used.',\n 'What is Bel?']</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.question_gen import LLMQuestionGenerator\nfrom llama_index.question_gen.openai import OpenAIQuestionGenerator\nfrom llama_index.llms.openai import OpenAI\n</pre> from llama_index.core.question_gen import LLMQuestionGenerator from llama_index.question_gen.openai import OpenAIQuestionGenerator from llama_index.llms.openai import OpenAI In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI()\nquestion_gen = OpenAIQuestionGenerator.from_defaults(llm=llm)\n</pre> llm = OpenAI() question_gen = OpenAIQuestionGenerator.from_defaults(llm=llm) In\u00a0[\u00a0]: Copied! <pre>display_prompt_dict(question_gen.get_prompts())\n</pre> display_prompt_dict(question_gen.get_prompts()) <p>Prompt Key: question_gen_promptText: </p> <pre>You are a world class state of the art agent.\n\nYou have access to multiple tools, each representing a different data source or API.\nEach of the tools has a name and a description, formatted as a JSON dictionary.\nThe keys of the dictionary are the names of the tools and the values are the descriptions.\nYour purpose is to help answer a complex user question by generating a list of sub questions that can be answered by the tools.\n\nThese are the guidelines you consider when completing your task:\n* Be as specific as possible\n* The sub questions should be relevant to the user question\n* The sub questions should be answerable by the tools provided\n* You can generate multiple sub questions for each tool\n* Tools must be specified by their name, not their description\n* You don't need to use a tool if you don't think it's relevant\n\nOutput the list of sub questions by calling the SubQuestionList function.\n\n## Tools\n```json\n{tools_str}\n```\n\n## User Question\n{query_str}\n\n</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.tools import ToolMetadata\n\ntool_choices = [\n    ToolMetadata(\n        name=\"uber_2021_10k\",\n        description=(\n            \"Provides information about Uber financials for year 2021\"\n        ),\n    ),\n    ToolMetadata(\n        name=\"lyft_2021_10k\",\n        description=(\n            \"Provides information about Lyft financials for year 2021\"\n        ),\n    ),\n]\n</pre> from llama_index.core.tools import ToolMetadata  tool_choices = [     ToolMetadata(         name=\"uber_2021_10k\",         description=(             \"Provides information about Uber financials for year 2021\"         ),     ),     ToolMetadata(         name=\"lyft_2021_10k\",         description=(             \"Provides information about Lyft financials for year 2021\"         ),     ), ] In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import QueryBundle\n\nquery_str = \"Compare and contrast Uber and Lyft\"\nchoices = question_gen.generate(tool_choices, QueryBundle(query_str=query_str))\n</pre> from llama_index.core import QueryBundle  query_str = \"Compare and contrast Uber and Lyft\" choices = question_gen.generate(tool_choices, QueryBundle(query_str=query_str)) <p>The outputs are <code>SubQuestion</code> Pydantic objects.</p> In\u00a0[\u00a0]: Copied! <pre>choices\n</pre> choices Out[\u00a0]: <pre>[SubQuestion(sub_question='What are the financials of Uber for the year 2021?', tool_name='uber_2021_10k'),\n SubQuestion(sub_question='What are the financials of Lyft for the year 2021?', tool_name='lyft_2021_10k')]</pre> <p>For details on how to plug this into your RAG pipeline in a more packaged fashion, check out our SubQuestionQueryEngine.</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.agent import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.core.llms import ChatMessage\n</pre> from llama_index.core.agent import ReActChatFormatter from llama_index.core.agent.react.output_parser import ReActOutputParser from llama_index.core.tools import FunctionTool from llama_index.core.llms import ChatMessage In\u00a0[\u00a0]: Copied! <pre>def execute_sql(sql: str) -&gt; str:\n    \"\"\"Given a SQL input string, execute it.\"\"\"\n    # NOTE: This is a mock function\n    return f\"Executed {sql}\"\n\n\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n\ntool1 = FunctionTool.from_defaults(fn=execute_sql)\ntool2 = FunctionTool.from_defaults(fn=add)\ntools = [tool1, tool2]\n</pre> def execute_sql(sql: str) -&gt; str:     \"\"\"Given a SQL input string, execute it.\"\"\"     # NOTE: This is a mock function     return f\"Executed {sql}\"   def add(a: int, b: int) -&gt; int:     \"\"\"Add two numbers.\"\"\"     return a + b   tool1 = FunctionTool.from_defaults(fn=execute_sql) tool2 = FunctionTool.from_defaults(fn=add) tools = [tool1, tool2] <p>Here we get the input prompt messages to pass to the LLM. Take a look!</p> In\u00a0[\u00a0]: Copied! <pre>chat_formatter = ReActChatFormatter()\noutput_parser = ReActOutputParser()\ninput_msgs = chat_formatter.format(\n    tools,\n    [\n        ChatMessage(\n            content=\"Can you find the top three rows from the table named `revenue_years`\",\n            role=\"user\",\n        )\n    ],\n)\ninput_msgs\n</pre> chat_formatter = ReActChatFormatter() output_parser = ReActOutputParser() input_msgs = chat_formatter.format(     tools,     [         ChatMessage(             content=\"Can you find the top three rows from the table named `revenue_years`\",             role=\"user\",         )     ], ) input_msgs Out[\u00a0]: <pre>[ChatMessage(role=&lt;MessageRole.SYSTEM: 'system'&gt;, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n&gt; Tool Name: execute_sql\\nTool Description: execute_sql(sql: str) -&gt; str\\nGiven a SQL input string, execute it.\\nTool Args: {\\'title\\': \\'execute_sql\\', \\'type\\': \\'object\\', \\'properties\\': {\\'sql\\': {\\'title\\': \\'Sql\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'sql\\']}\\n\\n&gt; Tool Name: add\\nTool Description: add(a: int, b: int) -&gt; int\\nAdd two numbers.\\nTool Args: {\\'title\\': \\'add\\', \\'type\\': \\'object\\', \\'properties\\': {\\'a\\': {\\'title\\': \\'A\\', \\'type\\': \\'integer\\'}, \\'b\\': {\\'title\\': \\'B\\', \\'type\\': \\'integer\\'}}, \\'required\\': [\\'a\\', \\'b\\']}\\n\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of execute_sql, add) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {\\'input\\': \\'hello world\\', \\'num_beams\\': 5}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n', additional_kwargs={}),\n ChatMessage(role=&lt;MessageRole.USER: 'user'&gt;, content='Can you find the top three rows from the table named `revenue_years`', additional_kwargs={})]</pre> <p>Next we get the output from the model.</p> In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI(model=\"gpt-4-1106-preview\")\n</pre> llm = OpenAI(model=\"gpt-4-1106-preview\") In\u00a0[\u00a0]: Copied! <pre>response = llm.chat(input_msgs)\n</pre> response = llm.chat(input_msgs) <p>Finally we use our ReActOutputParser to parse the content into a structured output, and analyze the action inputs.</p> In\u00a0[\u00a0]: Copied! <pre>reasoning_step = output_parser.parse(response.message.content)\n</pre> reasoning_step = output_parser.parse(response.message.content) In\u00a0[\u00a0]: Copied! <pre>reasoning_step.action_input\n</pre> reasoning_step.action_input Out[\u00a0]: <pre>{'sql': 'SELECT * FROM revenue_years ORDER BY revenue DESC LIMIT 3'}</pre>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#query-transform-cookbook","title":"Query Transform Cookbook\u00b6","text":"<p>A user query can be transformed and decomposed in many ways before being executed as part of a RAG query engine, agent, or any other pipeline.</p> <p>In this guide we show you different ways to transform, decompose queries, and find the set of relevant tools. Each technique might be applicable for different use cases!</p> <p>For naming purposes, we define the underlying pipeline as a \"tool\". Here are the different query transformations:</p> <ol> <li>Routing: Keep the query, but identify the relevant subset of tools that the query applies to. Output those tools as the relevant choices.</li> <li>Query-Rewriting: Keep the tools, but rewrite the query in a variety of different ways to execute against the same tools.</li> <li>Sub-Questions: Decompose queries into multiple sub-questions over different tools (identified by their metadata).</li> <li>ReAct Agent Tool Picking: Given the initial query, identify 1) the tool to pick, and 2) the query to execute on the tool.</li> </ol> <p>The goal of this guide is to show you how to use these query transforms as modular components. Of course, each of these components plug into a bigger system (e.g. the sub-question generator is a part of our <code>SubQuestionQueryEngine</code>) - and the guides for each of these are linked below.</p> <p>Take a look and let us know your thoughts!</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#routing","title":"Routing\u00b6","text":"<p>In this example, we show how a query can be used to select the set of relevant tool choices.</p> <p>We use our <code>selector</code> abstraction to pick the relevant tool(s) - it can be a single tool, or a multiple tool depending on the abstraction.</p> <p>We have four selectors: combination of (LLM or function calling) x (single selection or multi-selection)</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#query-rewriting","title":"Query Rewriting\u00b6","text":"<p>In this section, we show you how to rewrite queries into multiple queries. You can then execute all these queries against a retriever.</p> <p>This is a key step in advanced retrieval techniques. By doing query rewriting, you can generate multiple queries for [ensemble retrieval] and [fusion], leading to higher-quality retrieved results.</p> <p>Unlike the sub-question generator, this is just a prompt call, and exists independently of tools.</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#query-rewriting-custom","title":"Query Rewriting (Custom)\u00b6","text":"<p>Here we show you how to use a prompt to generate multiple queries, using our LLM and prompt abstractions.</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#query-rewriting-using-querytransform","title":"Query Rewriting (using QueryTransform)\u00b6","text":"<p>In this section we show you how to do query transformations using our QueryTransform class.</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#sub-questions","title":"Sub-Questions\u00b6","text":"<p>Given a set of tools and a user query, decide both the 1) set of sub-questions to generate, and 2) the tools that each sub-question should run over.</p> <p>We run through an example using the <code>OpenAIQuestionGenerator</code>, which depends on function calling, and also the <code>LLMQuestionGenerator</code>, which depends on prompting.</p>"},{"location":"RAG/06_Query_Transformation_RAG/query_transform_cookbook/#query-transformation-with-react-prompt","title":"Query Transformation with ReAct Prompt\u00b6","text":"<p>ReAct is a popular framework for agents, and here we show how the core ReAct prompt can be used to transform queries.</p> <p>We use the <code>ReActChatFormatter</code> to get the set of input messages for the LLM.</p>"},{"location":"RAG/07_Self_Query_RAG/","title":"Index","text":"<p>Certainly! Here's a README for the Self-Query RAG approach, which improves upon the base RAG by incorporating metadata extraction and intelligent query parsing:</p>"},{"location":"RAG/07_Self_Query_RAG/#self-query-rag-enhanced-retrieval-augmented-generation-with-metadata-filtering","title":"Self-Query RAG: Enhanced Retrieval-Augmented Generation with Metadata Filtering","text":""},{"location":"RAG/07_Self_Query_RAG/#introduction","title":"Introduction","text":"<p>Self-Query RAG is an advanced approach to Retrieval-Augmented Generation (RAG) that enhances the traditional RAG pipeline by incorporating metadata extraction during ingestion and intelligent query parsing during retrieval.</p>"},{"location":"RAG/07_Self_Query_RAG/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with complex queries that involve both semantic similarity and specific metadata constraints. Self-Query RAG addresses these challenges by leveraging metadata and using an LLM to parse and structure user queries intelligently.</p>"},{"location":"RAG/07_Self_Query_RAG/#method-details","title":"Method Details","text":"<pre><code>flowchart TB\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Metadata Extraction\"\n        C1 --&gt; D1[Extract Metadata]\n        C2 --&gt; D2[Extract Metadata]\n        C3 --&gt; D3[Extract Metadata]\n    end\n\n    subgraph \"3. Document Embedding\"\n        EM1{{Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; EM1\n        EM1 --&gt; E1[Embedding-1] &amp; E2[Embedding-2] &amp; E3[Embedding-3]\n    end\n\n    subgraph \"4. Indexing\"\n        E1 &amp; D1 --&gt; F1[Indexed Chunk-1]\n        E2 &amp; D2 --&gt; F2[Indexed Chunk-2]\n        E3 &amp; D3 --&gt; F3[Indexed Chunk-3]\n        F1 &amp; F2 &amp; F3 --&gt; G[(Vector DB + Metadata)]\n    end\n\n    subgraph \"5. Query Processing\"\n        H[User Query] --&gt; I[LLM for Query Understanding]\n        I --&gt; J[Structured Query]\n        J --&gt; K[Metadata Filters]\n        J --&gt; L[Semantic Query]\n    end\n\n    subgraph \"6. Retrieval\"\n        K --&gt; M{Apply Metadata Filters}\n        G --&gt; M\n        M --&gt; N[Filtered Subset]\n        N &amp; L --&gt; O{Semantic Search}\n        O --&gt; P[Relevant Chunks]\n    end\n\n    subgraph \"7. Context Formation\"\n        P --&gt; Q[Query + Relevant Chunks]\n    end\n\n    subgraph \"8. Generation\"\n        Q --&gt; R[LLM]\n        R --&gt; S[Response]\n    end\n\n    H --&gt; Q</code></pre>"},{"location":"RAG/07_Self_Query_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Documents are split into manageable chunks.</li> <li>Metadata is extracted from each chunk (e.g., date, author, category).</li> <li>Each chunk is embedded using a suitable embedding model.</li> <li>Chunks, their embeddings, and associated metadata are indexed in a vector database.</li> </ol>"},{"location":"RAG/07_Self_Query_RAG/#self-query-rag-workflow","title":"Self-Query RAG Workflow","text":"<ol> <li>The user submits a natural language query.</li> <li>An LLM parses the query to understand its intent and structure.</li> <li>The LLM generates:    a) Metadata filters based on the query.    b) A semantic search query for relevant content.</li> <li>Metadata filters are applied to narrow down the search space.</li> <li>Semantic search is performed on the filtered subset.</li> <li>Retrieved chunks are combined with the original query to form a context.</li> <li>This context is passed to a Large Language Model (LLM) to generate a response.</li> </ol>"},{"location":"RAG/07_Self_Query_RAG/#key-features-of-self-query-rag","title":"Key Features of Self-Query RAG","text":"<ul> <li>Metadata Extraction: Enhances document representation with structured information.</li> <li>Intelligent Query Parsing: Uses an LLM to understand complex user queries.</li> <li>Hybrid Retrieval: Combines metadata filtering with semantic search.</li> <li>Flexible Querying: Allows users to implicitly specify metadata constraints in natural language.</li> </ul>"},{"location":"RAG/07_Self_Query_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Retrieval Accuracy: Metadata filtering helps to narrow down the search space to more relevant documents.</li> <li>Handling Complex Queries: Can interpret and respond to queries that involve both content similarity and metadata constraints.</li> <li>Efficient Retrieval: Metadata filtering can significantly reduce the number of documents that need to be semantically searched.</li> <li>Enhanced Context: Metadata provides additional structured information to improve response generation.</li> </ol>"},{"location":"RAG/07_Self_Query_RAG/#conclusion","title":"Conclusion","text":"<p>Self-Query RAG enhances the traditional RAG pipeline by incorporating metadata extraction and intelligent query parsing. This approach allows for more precise and efficient retrieval, especially for complex queries that involve both semantic similarity and specific metadata constraints. By leveraging the power of LLMs for query understanding, Self-Query RAG can provide more accurate and contextually relevant responses in AI-powered question-answering systems.</p>"},{"location":"RAG/07_Self_Query_RAG/Self_Query_RAG/","title":"Self Query RAG(Llamaindex)","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-llms-openai\n%pip install llama-index-extractors-entity\n</pre> %pip install llama-index-llms-openai %pip install llama-index-extractors-entity In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n</pre> !pip install llama-index In\u00a0[\u00a0]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n\nimport os\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n</pre> import nest_asyncio  nest_asyncio.apply()  import os import openai  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\" In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.core.schema import MetadataMode\n</pre> from llama_index.llms.openai import OpenAI from llama_index.core.schema import MetadataMode In\u00a0[\u00a0]: Copied! <pre>llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\", max_tokens=512)\n</pre> llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\", max_tokens=512) <p>We create a node parser that extracts the document title and hypothetical question embeddings relevant to the document chunk.</p> <p>We also show how to instantiate the <code>SummaryExtractor</code> and <code>KeywordExtractor</code>, as well as how to create your own custom extractor based on the <code>BaseExtractor</code> base class</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.extractors import (\n    SummaryExtractor,\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n    KeywordExtractor,\n    BaseExtractor,\n)\nfrom llama_index.extractors.entity import EntityExtractor\nfrom llama_index.core.node_parser import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(\n    separator=\" \", chunk_size=512, chunk_overlap=128\n)\n\n\nclass CustomExtractor(BaseExtractor):\n    def extract(self, nodes):\n        metadata_list = [\n            {\n                \"custom\": (\n                    node.metadata[\"document_title\"]\n                    + \"\\n\"\n                    + node.metadata[\"excerpt_keywords\"]\n                )\n            }\n            for node in nodes\n        ]\n        return metadata_list\n\n\nextractors = [\n    TitleExtractor(nodes=5, llm=llm),\n    QuestionsAnsweredExtractor(questions=3, llm=llm),\n    # EntityExtractor(prediction_threshold=0.5),\n    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n    # KeywordExtractor(keywords=10, llm=llm),\n    # CustomExtractor()\n]\n\ntransformations = [text_splitter] + extractors\n</pre> from llama_index.core.extractors import (     SummaryExtractor,     QuestionsAnsweredExtractor,     TitleExtractor,     KeywordExtractor,     BaseExtractor, ) from llama_index.extractors.entity import EntityExtractor from llama_index.core.node_parser import TokenTextSplitter  text_splitter = TokenTextSplitter(     separator=\" \", chunk_size=512, chunk_overlap=128 )   class CustomExtractor(BaseExtractor):     def extract(self, nodes):         metadata_list = [             {                 \"custom\": (                     node.metadata[\"document_title\"]                     + \"\\n\"                     + node.metadata[\"excerpt_keywords\"]                 )             }             for node in nodes         ]         return metadata_list   extractors = [     TitleExtractor(nodes=5, llm=llm),     QuestionsAnsweredExtractor(questions=3, llm=llm),     # EntityExtractor(prediction_threshold=0.5),     # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),     # KeywordExtractor(keywords=10, llm=llm),     # CustomExtractor() ]  transformations = [text_splitter] + extractors In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n</pre> from llama_index.core import SimpleDirectoryReader <p>We first load the 10k annual SEC report for Uber and Lyft for the years 2019 and 2020 respectively.</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir -p data\n!wget -O \"data/10k-132.pdf\" \"https://www.dropbox.com/scl/fi/6dlqdk6e2k1mjhi8dee5j/uber.pdf?rlkey=2jyoe49bg2vwdlz30l76czq6g&amp;dl=1\"\n!wget -O \"data/10k-vFinal.pdf\" \"https://www.dropbox.com/scl/fi/qn7g3vrk5mqb18ko4e5in/lyft.pdf?rlkey=j6jxtjwo8zbstdo4wz3ns8zoj&amp;dl=1\"\n</pre> !mkdir -p data !wget -O \"data/10k-132.pdf\" \"https://www.dropbox.com/scl/fi/6dlqdk6e2k1mjhi8dee5j/uber.pdf?rlkey=2jyoe49bg2vwdlz30l76czq6g&amp;dl=1\" !wget -O \"data/10k-vFinal.pdf\" \"https://www.dropbox.com/scl/fi/qn7g3vrk5mqb18ko4e5in/lyft.pdf?rlkey=j6jxtjwo8zbstdo4wz3ns8zoj&amp;dl=1\" In\u00a0[\u00a0]: Copied! <pre># Note the uninformative document file name, which may be a common scenario in a production setting\nuber_docs = SimpleDirectoryReader(input_files=[\"data/10k-132.pdf\"]).load_data()\nuber_front_pages = uber_docs[0:3]\nuber_content = uber_docs[63:69]\nuber_docs = uber_front_pages + uber_content\n</pre> # Note the uninformative document file name, which may be a common scenario in a production setting uber_docs = SimpleDirectoryReader(input_files=[\"data/10k-132.pdf\"]).load_data() uber_front_pages = uber_docs[0:3] uber_content = uber_docs[63:69] uber_docs = uber_front_pages + uber_content In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(transformations=transformations)\n\nuber_nodes = pipeline.run(documents=uber_docs)\n</pre> from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(transformations=transformations)  uber_nodes = pipeline.run(documents=uber_docs) In\u00a0[\u00a0]: Copied! <pre>uber_nodes[1].metadata\n</pre> uber_nodes[1].metadata Out[\u00a0]: <pre>{'page_label': '2',\n 'file_name': '10k-132.pdf',\n 'document_title': 'Exploring the Diverse Landscape of 2019: A Comprehensive Annual Report on Uber Technologies, Inc.',\n 'questions_this_excerpt_can_answer': '1. How many countries does Uber operate in?\\n2. What is the total gross bookings of Uber in 2019?\\n3. How many trips did Uber facilitate in 2019?'}</pre> In\u00a0[\u00a0]: Copied! <pre># Note the uninformative document file name, which may be a common scenario in a production setting\nlyft_docs = SimpleDirectoryReader(\n    input_files=[\"data/10k-vFinal.pdf\"]\n).load_data()\nlyft_front_pages = lyft_docs[0:3]\nlyft_content = lyft_docs[68:73]\nlyft_docs = lyft_front_pages + lyft_content\n</pre> # Note the uninformative document file name, which may be a common scenario in a production setting lyft_docs = SimpleDirectoryReader(     input_files=[\"data/10k-vFinal.pdf\"] ).load_data() lyft_front_pages = lyft_docs[0:3] lyft_content = lyft_docs[68:73] lyft_docs = lyft_front_pages + lyft_content In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(transformations=transformations)\n\nlyft_nodes = pipeline.run(documents=lyft_docs)\n</pre> from llama_index.core.ingestion import IngestionPipeline  pipeline = IngestionPipeline(transformations=transformations)  lyft_nodes = pipeline.run(documents=lyft_docs) In\u00a0[\u00a0]: Copied! <pre>lyft_nodes[2].metadata\n</pre> lyft_nodes[2].metadata Out[\u00a0]: <pre>{'page_label': '2',\n 'file_name': '10k-vFinal.pdf',\n 'document_title': 'Lyft, Inc. Annual Report on Form 10-K for the Fiscal Year Ended December 31, 2020',\n 'questions_this_excerpt_can_answer': \"1. Has Lyft, Inc. filed a report on and attestation to its management's assessment of the effectiveness of its internal control over financial reporting under Section 404(b) of the Sarbanes-Oxley Act?\\n2. Is Lyft, Inc. considered a shell company according to Rule 12b-2 of the Exchange Act?\\n3. What was the aggregate market value of Lyft, Inc.'s common stock held by non-affiliates on June 30, 2020?\"}</pre> <p>Since we are asking fairly sophisticated questions, we utilize a subquestion query engine for all QnA pipelines below, and prompt it to pay more attention to the relevance of the retrieved sources.</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.question_gen import LLMQuestionGenerator\nfrom llama_index.core.question_gen.prompts import (\n    DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n)\n\n\nquestion_gen = LLMQuestionGenerator.from_defaults(\n    llm=llm,\n    prompt_template_str=\"\"\"\n        Follow the example, but instead of giving a question, always prefix the question\n        with: 'By first identifying and quoting the most relevant sources, '.\n        \"\"\"\n    + DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n)\n</pre> from llama_index.core.question_gen import LLMQuestionGenerator from llama_index.core.question_gen.prompts import (     DEFAULT_SUB_QUESTION_PROMPT_TMPL, )   question_gen = LLMQuestionGenerator.from_defaults(     llm=llm,     prompt_template_str=\"\"\"         Follow the example, but instead of giving a question, always prefix the question         with: 'By first identifying and quoting the most relevant sources, '.         \"\"\"     + DEFAULT_SUB_QUESTION_PROMPT_TMPL, ) In\u00a0[\u00a0]: Copied! <pre>from copy import deepcopy\n\nnodes_no_metadata = deepcopy(uber_nodes) + deepcopy(lyft_nodes)\nfor node in nodes_no_metadata:\n    node.metadata = {\n        k: node.metadata[k]\n        for k in node.metadata\n        if k in [\"page_label\", \"file_name\"]\n    }\nprint(\n    \"LLM sees:\\n\",\n    (nodes_no_metadata)[9].get_content(metadata_mode=MetadataMode.LLM),\n)\n</pre> from copy import deepcopy  nodes_no_metadata = deepcopy(uber_nodes) + deepcopy(lyft_nodes) for node in nodes_no_metadata:     node.metadata = {         k: node.metadata[k]         for k in node.metadata         if k in [\"page_label\", \"file_name\"]     } print(     \"LLM sees:\\n\",     (nodes_no_metadata)[9].get_content(metadata_mode=MetadataMode.LLM), ) <pre>LLM sees:\n [Excerpt from document]\npage_label: 65\nfile_name: 10k-132.pdf\nExcerpt:\n-----\nSee the section titled \u201cReconciliations of Non-GAAP Financial Measures\u201d for our definition and a \nreconciliation of net income (loss) attributable to  Uber Technologies, Inc. to Adjusted EBITDA. \n            \n  Year Ended December 31,   2017 to 2018   2018 to 2019   \n(In millions, exce pt percenta ges)  2017   2018   2019   % Chan ge  % Chan ge  \nAdjusted EBITDA ................................  $ (2,642) $ (1,847) $ (2,725)  30%  (48)%\n-----\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n</pre> from llama_index.core import VectorStoreIndex from llama_index.core.query_engine import SubQuestionQueryEngine from llama_index.core.tools import QueryEngineTool, ToolMetadata In\u00a0[\u00a0]: Copied! <pre>index_no_metadata = VectorStoreIndex(\n    nodes=nodes_no_metadata,\n)\nengine_no_metadata = index_no_metadata.as_query_engine(\n    similarity_top_k=10, llm=OpenAI(model=\"gpt-4\")\n)\n</pre> index_no_metadata = VectorStoreIndex(     nodes=nodes_no_metadata, ) engine_no_metadata = index_no_metadata.as_query_engine(     similarity_top_k=10, llm=OpenAI(model=\"gpt-4\") ) In\u00a0[\u00a0]: Copied! <pre>final_engine_no_metadata = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=[\n        QueryEngineTool(\n            query_engine=engine_no_metadata,\n            metadata=ToolMetadata(\n                name=\"sec_filing_documents\",\n                description=\"financial information on companies\",\n            ),\n        )\n    ],\n    question_gen=question_gen,\n    use_async=True,\n)\n</pre> final_engine_no_metadata = SubQuestionQueryEngine.from_defaults(     query_engine_tools=[         QueryEngineTool(             query_engine=engine_no_metadata,             metadata=ToolMetadata(                 name=\"sec_filing_documents\",                 description=\"financial information on companies\",             ),         )     ],     question_gen=question_gen,     use_async=True, ) In\u00a0[\u00a0]: Copied! <pre>response_no_metadata = final_engine_no_metadata.query(\n    \"\"\"\n    What was the cost due to research and development v.s. sales and marketing for uber and lyft in 2019 in millions of USD?\n    Give your answer as a JSON.\n    \"\"\"\n)\nprint(response_no_metadata.response)\n# Correct answer:\n# {\"Uber\": {\"Research and Development\": 4836, \"Sales and Marketing\": 4626},\n#  \"Lyft\": {\"Research and Development\": 1505.6, \"Sales and Marketing\": 814 }}\n</pre> response_no_metadata = final_engine_no_metadata.query(     \"\"\"     What was the cost due to research and development v.s. sales and marketing for uber and lyft in 2019 in millions of USD?     Give your answer as a JSON.     \"\"\" ) print(response_no_metadata.response) # Correct answer: # {\"Uber\": {\"Research and Development\": 4836, \"Sales and Marketing\": 4626}, #  \"Lyft\": {\"Research and Development\": 1505.6, \"Sales and Marketing\": 814 }} <pre>Generated 4 sub questions.\n[sec_filing_documents] Q: What was the cost due to research and development for Uber in 2019\n[sec_filing_documents] Q: What was the cost due to sales and marketing for Uber in 2019\n[sec_filing_documents] Q: What was the cost due to research and development for Lyft in 2019\n[sec_filing_documents] Q: What was the cost due to sales and marketing for Lyft in 2019\n[sec_filing_documents] A: The cost due to sales and marketing for Uber in 2019 was $814,122 in thousands.\n[sec_filing_documents] A: The cost due to research and development for Uber in 2019 was $1,505,640 in thousands.\n[sec_filing_documents] A: The cost of research and development for Lyft in 2019 was $1,505,640 in thousands.\n[sec_filing_documents] A: The cost due to sales and marketing for Lyft in 2019 was $814,122 in thousands.\n{\n  \"Uber\": {\n    \"Research and Development\": 1505.64,\n    \"Sales and Marketing\": 814.122\n  },\n  \"Lyft\": {\n    \"Research and Development\": 1505.64,\n    \"Sales and Marketing\": 814.122\n  }\n}\n</pre> <p>RESULT: As we can see, the QnA agent does not seem to know where to look for the right documents. As a result it gets the Lyft and Uber data completely mixed up.</p> In\u00a0[\u00a0]: Copied! <pre>print(\n    \"LLM sees:\\n\",\n    (uber_nodes + lyft_nodes)[9].get_content(metadata_mode=MetadataMode.LLM),\n)\n</pre> print(     \"LLM sees:\\n\",     (uber_nodes + lyft_nodes)[9].get_content(metadata_mode=MetadataMode.LLM), ) <pre>LLM sees:\n [Excerpt from document]\npage_label: 65\nfile_name: 10k-132.pdf\ndocument_title: Exploring the Diverse Landscape of 2019: A Comprehensive Annual Report on Uber Technologies, Inc.\nExcerpt:\n-----\nSee the section titled \u201cReconciliations of Non-GAAP Financial Measures\u201d for our definition and a \nreconciliation of net income (loss) attributable to  Uber Technologies, Inc. to Adjusted EBITDA. \n            \n  Year Ended December 31,   2017 to 2018   2018 to 2019   \n(In millions, exce pt percenta ges)  2017   2018   2019   % Chan ge  % Chan ge  \nAdjusted EBITDA ................................  $ (2,642) $ (1,847) $ (2,725)  30%  (48)%\n-----\n</pre> In\u00a0[\u00a0]: Copied! <pre>index = VectorStoreIndex(\n    nodes=uber_nodes + lyft_nodes,\n)\nengine = index.as_query_engine(similarity_top_k=10, llm=OpenAI(model=\"gpt-4\"))\n</pre> index = VectorStoreIndex(     nodes=uber_nodes + lyft_nodes, ) engine = index.as_query_engine(similarity_top_k=10, llm=OpenAI(model=\"gpt-4\")) In\u00a0[\u00a0]: Copied! <pre>final_engine = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=[\n        QueryEngineTool(\n            query_engine=engine,\n            metadata=ToolMetadata(\n                name=\"sec_filing_documents\",\n                description=\"financial information on companies.\",\n            ),\n        )\n    ],\n    question_gen=question_gen,\n    use_async=True,\n)\n</pre> final_engine = SubQuestionQueryEngine.from_defaults(     query_engine_tools=[         QueryEngineTool(             query_engine=engine,             metadata=ToolMetadata(                 name=\"sec_filing_documents\",                 description=\"financial information on companies.\",             ),         )     ],     question_gen=question_gen,     use_async=True, ) In\u00a0[\u00a0]: Copied! <pre>response = final_engine.query(\n    \"\"\"\n    What was the cost due to research and development v.s. sales and marketing for uber and lyft in 2019 in millions of USD?\n    Give your answer as a JSON.\n    \"\"\"\n)\nprint(response.response)\n# Correct answer:\n# {\"Uber\": {\"Research and Development\": 4836, \"Sales and Marketing\": 4626},\n#  \"Lyft\": {\"Research and Development\": 1505.6, \"Sales and Marketing\": 814 }}\n</pre> response = final_engine.query(     \"\"\"     What was the cost due to research and development v.s. sales and marketing for uber and lyft in 2019 in millions of USD?     Give your answer as a JSON.     \"\"\" ) print(response.response) # Correct answer: # {\"Uber\": {\"Research and Development\": 4836, \"Sales and Marketing\": 4626}, #  \"Lyft\": {\"Research and Development\": 1505.6, \"Sales and Marketing\": 814 }} <pre>Generated 4 sub questions.\n[sec_filing_documents] Q: What was the cost due to research and development for Uber in 2019\n[sec_filing_documents] Q: What was the cost due to sales and marketing for Uber in 2019\n[sec_filing_documents] Q: What was the cost due to research and development for Lyft in 2019\n[sec_filing_documents] Q: What was the cost due to sales and marketing for Lyft in 2019\n[sec_filing_documents] A: The cost due to sales and marketing for Uber in 2019 was $4,626 million.\n[sec_filing_documents] A: The cost due to research and development for Uber in 2019 was $4,836 million.\n[sec_filing_documents] A: The cost due to sales and marketing for Lyft in 2019 was $814,122 in thousands.\n[sec_filing_documents] A: The cost of research and development for Lyft in 2019 was $1,505,640 in thousands.\n{\n  \"Uber\": {\n    \"Research and Development\": 4836,\n    \"Sales and Marketing\": 4626\n  },\n  \"Lyft\": {\n    \"Research and Development\": 1505.64,\n    \"Sales and Marketing\": 814.122\n  }\n}\n</pre> <p>RESULT: As we can see, the LLM answers the questions correctly.</p>"},{"location":"RAG/07_Self_Query_RAG/Self_Query_RAG/#extracting-metadata-for-better-document-indexing-and-understanding","title":"Extracting Metadata for Better Document Indexing and Understanding\u00b6","text":"<p>In many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text. One method of addressing this is manually labelling each chunk in our dataset or knowledge base. However, this can be labour intensive and time consuming for a large number or continually updated set of documents.</p> <p>To combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.</p> <p>We do this through our brand-new <code>Metadata Extractor</code> modules.</p>"},{"location":"RAG/07_Self_Query_RAG/Self_Query_RAG/#querying-an-index-with-no-extra-metadata","title":"Querying an Index With No Extra Metadata\u00b6","text":""},{"location":"RAG/07_Self_Query_RAG/Self_Query_RAG/#querying-an-index-with-extracted-metadata","title":"Querying an Index With Extracted Metadata\u00b6","text":""},{"location":"RAG/07_Self_Query_RAG/Self_Query_RAG/#challenges-identified-in-the-problem-domain","title":"Challenges Identified in the Problem Domain\u00b6","text":"<p>In this example, we observed that the search quality as provided by vector embeddings was rather poor. This was likely due to highly dense financial documents that were likely not representative of the training set for the model.</p> <p>In order to improve the search quality, other methods of neural search that employ more keyword-based approaches may help, such as ColBERTv2/PLAID. In particular, this would help in matching on particular keywords to identify high-relevance chunks.</p> <p>Other valid steps may include utilizing models that are fine-tuned on financial datasets such as Bloomberg GPT.</p> <p>Finally, we can help to further enrich the metadata by providing more contextual information regarding the surrounding context that the chunk is located in.</p>"},{"location":"RAG/07_Self_Query_RAG/Self_Query_RAG/#improvements-to-this-example","title":"Improvements to this Example\u00b6","text":"<p>Generally, this example can be improved further with more rigorous evaluation of both the metadata extraction accuracy, and the accuracy and recall of the QnA pipeline. Further, incorporating a larger set of documents as well as the full length documents, which may provide more confounding passages that are difficult to disambiguate, could further stresss test the system we have built and suggest further improvements.</p>"},{"location":"RAG/08_RAG_Fusion/","title":"Index","text":"<pre><code>flowchart LR\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Document Embedding\"\n        EM1{{Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; EM1\n        EM1 --&gt; D1[Embedding-1] &amp; D2[Embedding-2] &amp; D3[Embedding-3]\n    end\n\n    subgraph \"3. Indexing\"\n        D1 &amp; D2 &amp; D3 --&gt; E[(VectorDB)]\n    end\n\n    subgraph \"4. Query Processing\"\n        F[Original Query] --&gt; LLM1{{LLM for Query Generation}}\n        LLM1 --&gt; G1[Generated Query 1]\n        LLM1 --&gt; G2[Generated Query 2]\n        LLM1 --&gt; G3[Generated Query n]\n        F &amp; G1 &amp; G2 &amp; G3 --&gt; EM2{{Embedding Model}}\n        EM2 --&gt; H1[Query Embedding 1]\n        EM2 --&gt; H2[Query Embedding 2]\n        EM2 --&gt; H3[Query Embedding 3]\n        EM2 --&gt; H4[Query Embedding n]\n    end\n\n    subgraph \"5. Multi-Query Retrieval\"\n        H1 &amp; H2 &amp; H3 &amp; H4 --&gt;|Similarity Search| E\n        E --&gt;|Top-K Retrieval| I1[Results Set 1]\n        E --&gt;|Top-K Retrieval| I2[Results Set 2]\n        E --&gt;|Top-K Retrieval| I3[Results Set 3]\n        E --&gt;|Top-K Retrieval| I4[Results Set n]\n    end\n\n    subgraph \"6. Reciprocal Rank Fusion\"\n        I1 &amp; I2 &amp; I3 &amp; I4 --&gt; J[RRF Algorithm]\n        J --&gt; K[Reranked Results]\n    end\n\n    subgraph \"7. Context Formation\"\n        K --&gt; L[Original Query + Generated Queries + Reranked Results]\n    end\n\n    subgraph \"8. Generation\"\n        L --&gt; M[LLM]\n        M --&gt; N[Final Response]\n    end\n\n    F --&gt; L</code></pre>"},{"location":"RAG/08_RAG_Fusion/#rag-fusion-enhanced-retrieval-augmented-generation","title":"RAG-Fusion: Enhanced Retrieval-Augmented Generation","text":""},{"location":"RAG/08_RAG_Fusion/#introduction","title":"Introduction","text":"<p>RAG-Fusion is an advanced approach to information retrieval and text generation that builds upon the foundation of Retrieval-Augmented Generation (RAG). This project implements RAG-Fusion to provide more accurate, contextually relevant, and comprehensive responses to user queries.</p>"},{"location":"RAG/08_RAG_Fusion/#motivation","title":"Motivation","text":"<p>Traditional RAG systems, while effective, often face limitations in capturing the full scope of user intent and retrieving the most relevant information. RAG-Fusion addresses these challenges by:</p> <ol> <li>Generating multiple queries to capture different aspects of the user's intent</li> <li>Utilizing advanced reranking techniques to improve retrieval accuracy</li> <li>Providing a more nuanced context for the language model to generate responses</li> </ol>"},{"location":"RAG/08_RAG_Fusion/#method-details","title":"Method Details","text":""},{"location":"RAG/08_RAG_Fusion/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Text Chunking: Documents are split into manageable chunks.</li> <li>Embedding Generation: Each chunk is converted into a vector representation using a pre-trained embedding model.</li> <li>Indexing: The embeddings are stored in a vector database for efficient retrieval.</li> </ol>"},{"location":"RAG/08_RAG_Fusion/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Expansion: The original user query is expanded into multiple related queries using a language model.</li> <li>Multi-Query Embedding: All queries (original and generated) are embedded.</li> <li>Vector Search: Each query embedding is used to retrieve relevant document chunks from the vector store.</li> <li>Reciprocal Rank Fusion (RRF): Results from multiple queries are combined and reranked using the RRF algorithm.</li> <li>Context Formation: The original query, generated queries, and reranked results form the context.</li> <li>Response Generation: A large language model generates the final response based on the enriched context.</li> </ol>"},{"location":"RAG/08_RAG_Fusion/#key-features-of-rag-fusion","title":"Key Features of RAG-Fusion","text":"<ul> <li>Multi-query generation for comprehensive intent capture</li> <li>Reciprocal Rank Fusion for improved result relevance</li> <li>Integration of multiple information retrieval techniques</li> <li>Flexible architecture supporting various embedding models and language models</li> </ul>"},{"location":"RAG/08_RAG_Fusion/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Enhanced Query Understanding: By generating multiple queries, RAG-Fusion captures a broader range of potential user intents.</li> <li>Improved Retrieval Accuracy: The use of RRF helps surface the most relevant information across multiple query results.</li> <li>Reduced Hallucination: By providing more comprehensive and accurate context, the chances of model hallucination are reduced.</li> <li>Versatility: The system can be applied to various domains and types of queries.</li> <li>Scalability: The architecture allows for easy scaling to handle large document collections.</li> </ol>"},{"location":"RAG/08_RAG_Fusion/#conclusion","title":"Conclusion","text":"<p>RAG-Fusion represents a significant advancement in the field of information retrieval and text generation. By addressing the limitations of traditional RAG systems, it offers a more robust, accurate, and versatile solution for a wide range of applications, from question-answering systems to document summarization tasks.</p>"},{"location":"RAG/08_RAG_Fusion/ragfusion/","title":"RAG Fusion(Llamaindex)","text":"In\u00a0[50]: Copied! <pre># Import necessary libraries\nimport os\nfrom typing import List, Dict\nfrom dotenv import load_dotenv\nfrom IPython.display import Markdown, display\n\n# OpenAI import\nfrom openai import OpenAI\n\n# LlamaIndex imports\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\nfrom llama_index.core import Settings\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\n\n# Qdrant client import\nimport qdrant_client\n</pre> # Import necessary libraries import os from typing import List, Dict from dotenv import load_dotenv from IPython.display import Markdown, display  # OpenAI import from openai import OpenAI  # LlamaIndex imports from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document from llama_index.core import Settings from llama_index.vector_stores.qdrant import QdrantVectorStore from llama_index.embeddings.fastembed import FastEmbedEmbedding from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline from llama_index.core.base.llms.types import ChatMessage, MessageRole  # Qdrant client import import qdrant_client In\u00a0[51]: Copied! <pre># Load environment variables\nload_dotenv()\n\n# Get OpenAI API key from environment variables\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif OPENAI_API_KEY is None:\n    raise Exception(\"No OpenAI API key found. Please set it as an environment variable.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\n# Set the embedding model\nSettings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n</pre> # Load environment variables load_dotenv()  # Get OpenAI API key from environment variables OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") if OPENAI_API_KEY is None:     raise Exception(\"No OpenAI API key found. Please set it as an environment variable.\")  # Initialize OpenAI client client = OpenAI(api_key=OPENAI_API_KEY)  # Set the embedding model Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\") In\u00a0[52]: Copied! <pre>def generate_queries_chatgpt(original_query: str) -&gt; List[str]:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates multiple search queries based on a single input query.\"},\n            {\"role\": \"user\", \"content\": f\"Generate multiple search queries related to: {original_query}\"},\n            {\"role\": \"user\", \"content\": \"OUTPUT (4 queries):\"}\n        ]\n    )\n    generated_queries = response.choices[0].message.content.strip().split(\"\\n\")\n    return generated_queries\n</pre> def generate_queries_chatgpt(original_query: str) -&gt; List[str]:     response = client.chat.completions.create(         model=\"gpt-3.5-turbo\",         messages=[             {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates multiple search queries based on a single input query.\"},             {\"role\": \"user\", \"content\": f\"Generate multiple search queries related to: {original_query}\"},             {\"role\": \"user\", \"content\": \"OUTPUT (4 queries):\"}         ]     )     generated_queries = response.choices[0].message.content.strip().split(\"\\n\")     return generated_queries In\u00a0[53]: Copied! <pre>def vector_search(query: str, index: VectorStoreIndex) -&gt; Dict[str, float]:\n    retriever = index.as_retriever(similarity_top_k=5)\n    nodes = retriever.retrieve(query)\n    return {node.node.get_content(): node.score for node in nodes}\n</pre> def vector_search(query: str, index: VectorStoreIndex) -&gt; Dict[str, float]:     retriever = index.as_retriever(similarity_top_k=5)     nodes = retriever.retrieve(query)     return {node.node.get_content(): node.score for node in nodes} In\u00a0[54]: Copied! <pre>def reciprocal_rank_fusion(search_results_dict: Dict[str, Dict[str, float]], k: int = 60) -&gt; Dict[str, float]:\n    fused_scores = {}\n    print(\"Initial individual search result ranks:\")\n    for query, doc_scores in search_results_dict.items():\n        print(f\"For query '{query}': {doc_scores}\")\n        \n    for query, doc_scores in search_results_dict.items():\n        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n            if doc not in fused_scores:\n                fused_scores[doc] = 0\n            previous_score = fused_scores[doc]\n            fused_scores[doc] += 1 / (rank + k)\n            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n\n    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n    print(\"Final reranked results:\", reranked_results)\n    return reranked_results\n</pre> def reciprocal_rank_fusion(search_results_dict: Dict[str, Dict[str, float]], k: int = 60) -&gt; Dict[str, float]:     fused_scores = {}     print(\"Initial individual search result ranks:\")     for query, doc_scores in search_results_dict.items():         print(f\"For query '{query}': {doc_scores}\")              for query, doc_scores in search_results_dict.items():         for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):             if doc not in fused_scores:                 fused_scores[doc] = 0             previous_score = fused_scores[doc]             fused_scores[doc] += 1 / (rank + k)             print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")      reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}     print(\"Final reranked results:\", reranked_results)     return reranked_results In\u00a0[55]: Copied! <pre>reader = SimpleDirectoryReader(\"data\", recursive=True)\ndocuments = reader.load_data(show_progress=True)\ndocuments = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n</pre> reader = SimpleDirectoryReader(\"data\", recursive=True) documents = reader.load_data(show_progress=True) documents = Document(text=\"\\n\\n\".join([doc.text for doc in documents])) <pre>Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  8.30file/s]\n</pre> In\u00a0[56]: Copied! <pre>qdrant_client = qdrant_client.QdrantClient(location=\":memory:\")\nvector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"RAG_Fusion\")\n</pre> qdrant_client = qdrant_client.QdrantClient(location=\":memory:\") vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"RAG_Fusion\") In\u00a0[58]: Copied! <pre>pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n        Settings.embed_model,\n    ],\n    vector_store=vector_store,\n)\n\nnodes = pipeline.run(documents=[documents], show_progress=True)\nprint(\"Number of chunks added to vector DB:\", len(nodes))\n</pre> pipeline = IngestionPipeline(     transformations=[         SentenceSplitter(chunk_size=1024, chunk_overlap=20),         Settings.embed_model,     ],     vector_store=vector_store, )  nodes = pipeline.run(documents=[documents], show_progress=True) print(\"Number of chunks added to vector DB:\", len(nodes)) <pre>Parsing nodes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 193.16it/s]\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  2.51it/s]</pre> <pre>Number of chunks added to vector DB: 2\n</pre> <pre>\n</pre> In\u00a0[59]: Copied! <pre># Create index\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n</pre> # Create index index = VectorStoreIndex.from_vector_store(vector_store=vector_store) In\u00a0[60]: Copied! <pre>class ChatEngineInterface:\n    def __init__(self, index: VectorStoreIndex):\n        self.index = index\n        self.chat_history: List[ChatMessage] = []\n\n    def display_message(self, role: str, content: str):\n        if role == \"USER\":\n            display(Markdown(f\"**Human:** {content}\"))\n        else:\n            display(Markdown(f\"**AI:** {content}\"))\n\n    def chat(self, message: str) -&gt; str:\n        user_message = ChatMessage(role=MessageRole.USER, content=message)\n        self.chat_history.append(user_message)\n        \n        # Generate multiple queries\n        generated_queries = generate_queries_chatgpt(message)\n        \n        # Perform vector search for each query\n        all_results = {}\n        for query in generated_queries:\n            search_results = vector_search(query, self.index)\n            all_results[query] = search_results\n        \n        # Apply Reciprocal Rank Fusion\n        reranked_results = reciprocal_rank_fusion(all_results)\n        \n        # Use reranked results to generate response\n        top_docs = list(reranked_results.keys())[:3]  # Get top 3 documents\n        context = \"\\n\".join(top_docs)\n        \n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the given context.\"},\n                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {message}\"}\n            ]\n        )\n        \n        ai_response = response.choices[0].message.content.strip()\n        ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=ai_response)\n        self.chat_history.append(ai_message)\n        \n        self.display_message(\"USER\", message)\n        self.display_message(\"ASSISTANT\", ai_response)\n        \n        print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability\n\n        return ai_response\n\n    def get_chat_history(self) -&gt; List[ChatMessage]:\n        return self.chat_history\n</pre> class ChatEngineInterface:     def __init__(self, index: VectorStoreIndex):         self.index = index         self.chat_history: List[ChatMessage] = []      def display_message(self, role: str, content: str):         if role == \"USER\":             display(Markdown(f\"**Human:** {content}\"))         else:             display(Markdown(f\"**AI:** {content}\"))      def chat(self, message: str) -&gt; str:         user_message = ChatMessage(role=MessageRole.USER, content=message)         self.chat_history.append(user_message)                  # Generate multiple queries         generated_queries = generate_queries_chatgpt(message)                  # Perform vector search for each query         all_results = {}         for query in generated_queries:             search_results = vector_search(query, self.index)             all_results[query] = search_results                  # Apply Reciprocal Rank Fusion         reranked_results = reciprocal_rank_fusion(all_results)                  # Use reranked results to generate response         top_docs = list(reranked_results.keys())[:3]  # Get top 3 documents         context = \"\\n\".join(top_docs)                  response = client.chat.completions.create(             model=\"gpt-3.5-turbo\",             messages=[                 {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the given context.\"},                 {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {message}\"}             ]         )                  ai_response = response.choices[0].message.content.strip()         ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=ai_response)         self.chat_history.append(ai_message)                  self.display_message(\"USER\", message)         self.display_message(\"ASSISTANT\", ai_response)                  print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability          return ai_response      def get_chat_history(self) -&gt; List[ChatMessage]:         return self.chat_history In\u00a0[61]: Copied! <pre># Usage\nchat_interface = ChatEngineInterface(index)\n</pre> # Usage chat_interface = ChatEngineInterface(index) In\u00a0[62]: Copied! <pre># Example usage\nchat_interface.chat(\"What is Samarth's CGPA?\")\nchat_interface.chat(\"What are all the AI projects done by samarth?\")\n</pre> # Example usage chat_interface.chat(\"What is Samarth's CGPA?\") chat_interface.chat(\"What are all the AI projects done by samarth?\") <pre>Initial individual search result ranks:\nFor query '1. Samarth CGPA transcript': {'PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern,': 0.7078029412854389, \"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.589905121981148, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.5303466536992336, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.5055852542659272, 'in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website.': 0.49850564846089873}\nFor query '2. How to calculate CGPA for Samarth': {'PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern,': 0.6847357902557305, \"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.5511591679417467, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.46491744774645705, \"| Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM\": 0.45725453656695775, 'in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website.': 0.45656594020465413}\nFor query '3. Samarth's college CGPA ranking': {'PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern,': 0.6937014902973797, \"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.5915607448451599, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.4915874323604704, 'Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.': 0.47546500691492116, \"| Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM\": 0.4713628999730835}\nFor query '4. CGPA conversion scale for Samarth's university': {'PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern,': 0.696238816770899, \"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.5796498471210573, 'in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website.': 0.4964184623584387, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.4943453584081807, \"| Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM\": 0.48648555242994007}\nUpdating score for PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, from 0 to 0.016666666666666666 based on rank 0 in query '1. Samarth CGPA transcript'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0 to 0.01639344262295082 based on rank 1 in query '1. Samarth CGPA transcript'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0 to 0.016129032258064516 based on rank 2 in query '1. Samarth CGPA transcript'\nUpdating score for Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter.in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. It includes a custom-built\nhardware vending machine for immediate medicine dispensing\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\nReciever\niverilog implementation includes a frequency scaler with adjustable\nscaling factor and PWM generation based on a specified Duty Cycle.\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\nvideo and cards.\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\ncombines IoT with AI to create an autonomous car equipped with a\nvirtual camera, allowing remote control and monitoring of surroundings\nwith scene descriptions generated through AI integration\nMANET Research (ongoing)\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0 to 0.015873015873015872 based on rank 3 in query '1. Samarth CGPA transcript'\nUpdating score for in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. from 0 to 0.015625 based on rank 4 in query '1. Samarth CGPA transcript'\nUpdating score for PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, from 0.016666666666666666 to 0.03333333333333333 based on rank 0 in query '2. How to calculate CGPA for Samarth'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0.01639344262295082 to 0.03278688524590164 based on rank 1 in query '2. How to calculate CGPA for Samarth'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0.016129032258064516 to 0.03225806451612903 based on rank 2 in query '2. How to calculate CGPA for Samarth'\nUpdating score for | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM from 0 to 0.015873015873015872 based on rank 3 in query '2. How to calculate CGPA for Samarth'\nUpdating score for in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. from 0.015625 to 0.03125 based on rank 4 in query '2. How to calculate CGPA for Samarth'\nUpdating score for PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, from 0.03333333333333333 to 0.05 based on rank 0 in query '3. Samarth's college CGPA ranking'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0.03278688524590164 to 0.04918032786885246 based on rank 1 in query '3. Samarth's college CGPA ranking'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0.03225806451612903 to 0.04838709677419355 based on rank 2 in query '3. Samarth's college CGPA ranking'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8. from 0 to 0.015873015873015872 based on rank 3 in query '3. Samarth's college CGPA ranking'\nUpdating score for | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM from 0.015873015873015872 to 0.03149801587301587 based on rank 4 in query '3. Samarth's college CGPA ranking'\nUpdating score for PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, from 0.05 to 0.06666666666666667 based on rank 0 in query '4. CGPA conversion scale for Samarth's university'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0.04918032786885246 to 0.06557377049180328 based on rank 1 in query '4. CGPA conversion scale for Samarth's university'\nUpdating score for in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. from 0.03125 to 0.047379032258064516 based on rank 2 in query '4. CGPA conversion scale for Samarth's university'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0.04838709677419355 to 0.06426011264720942 based on rank 3 in query '4. CGPA conversion scale for Samarth's university'\nUpdating score for | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM from 0.03149801587301587 to 0.04712301587301587 based on rank 4 in query '4. CGPA conversion scale for Samarth's university'\nFinal reranked results: {'PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern,': 0.06666666666666667, \"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.06557377049180328, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.06426011264720942, 'in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website.': 0.047379032258064516, \"| Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM\": 0.04712301587301587, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.015873015873015872, 'Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.': 0.015873015873015872}\n</pre> <p>Human: What is Samarth's CGPA?</p> <p>AI: Samarth's CGPA is 8.56 at PES University EC Campus where he is pursuing B. Tech CSE from 2022 to present.</p> <pre>\n--------------------------------------------------\n\nInitial individual search result ranks:\nFor query '1. \"List of AI projects by Samarth\"': {\"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.64229315400598, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.6300095332112591, 'ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.': 0.6282963988363128, 'DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6192984710699726, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6161929853493662}\nFor query '2. \"Samarth AI projects portfolio\"': {'DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6299981396061565, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.629866091094448, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6285683327767199, \"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.6264142302163573, 'ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.': 0.617870348586077}\nFor query '3. \"Samarth artificial intelligence projects\"': {\"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.6527829292498244, 'DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6331282865431569, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.6234571730754817, 'ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.': 0.6180030614680538, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6037578014381535}\nFor query '4. \"AI initiatives by Samarth\"': {\"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.6422919010884514, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.6260785080263631, 'DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.6239354107056428, 'ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.': 0.6132845275349096, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.5978226655407697}\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0 to 0.016666666666666666 based on rank 0 in query '1. \"List of AI projects by Samarth\"'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0 to 0.01639344262295082 based on rank 1 in query '1. \"List of AI projects by Samarth\"'\nUpdating score for ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter. from 0 to 0.016129032258064516 based on rank 2 in query '1. \"List of AI projects by Samarth\"'\nUpdating score for DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0 to 0.015873015873015872 based on rank 3 in query '1. \"List of AI projects by Samarth\"'\nUpdating score for Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter.in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. It includes a custom-built\nhardware vending machine for immediate medicine dispensing\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\nReciever\niverilog implementation includes a frequency scaler with adjustable\nscaling factor and PWM generation based on a specified Duty Cycle.\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\nvideo and cards.\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\ncombines IoT with AI to create an autonomous car equipped with a\nvirtual camera, allowing remote control and monitoring of surroundings\nwith scene descriptions generated through AI integration\nMANET Research (ongoing)\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0 to 0.015625 based on rank 4 in query '1. \"List of AI projects by Samarth\"'\nUpdating score for DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0.015873015873015872 to 0.032539682539682535 based on rank 0 in query '2. \"Samarth AI projects portfolio\"'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0.01639344262295082 to 0.03278688524590164 based on rank 1 in query '2. \"Samarth AI projects portfolio\"'\nUpdating score for Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter.in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. It includes a custom-built\nhardware vending machine for immediate medicine dispensing\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\nReciever\niverilog implementation includes a frequency scaler with adjustable\nscaling factor and PWM generation based on a specified Duty Cycle.\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\nvideo and cards.\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\ncombines IoT with AI to create an autonomous car equipped with a\nvirtual camera, allowing remote control and monitoring of surroundings\nwith scene descriptions generated through AI integration\nMANET Research (ongoing)\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0.015625 to 0.031754032258064516 based on rank 2 in query '2. \"Samarth AI projects portfolio\"'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0.016666666666666666 to 0.032539682539682535 based on rank 3 in query '2. \"Samarth AI projects portfolio\"'\nUpdating score for ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter. from 0.016129032258064516 to 0.031754032258064516 based on rank 4 in query '2. \"Samarth AI projects portfolio\"'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0.032539682539682535 to 0.0492063492063492 based on rank 0 in query '3. \"Samarth artificial intelligence projects\"'\nUpdating score for DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0.032539682539682535 to 0.04893312516263336 based on rank 1 in query '3. \"Samarth artificial intelligence projects\"'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0.03278688524590164 to 0.04891591750396616 based on rank 2 in query '3. \"Samarth artificial intelligence projects\"'\nUpdating score for ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter. from 0.031754032258064516 to 0.04762704813108039 based on rank 3 in query '3. \"Samarth artificial intelligence projects\"'\nUpdating score for Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter.in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. It includes a custom-built\nhardware vending machine for immediate medicine dispensing\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\nReciever\niverilog implementation includes a frequency scaler with adjustable\nscaling factor and PWM generation based on a specified Duty Cycle.\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\nvideo and cards.\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\ncombines IoT with AI to create an autonomous car equipped with a\nvirtual camera, allowing remote control and monitoring of surroundings\nwith scene descriptions generated through AI integration\nMANET Research (ongoing)\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0.031754032258064516 to 0.047379032258064516 based on rank 4 in query '3. \"Samarth artificial intelligence projects\"'\nUpdating score for Certifications/CoursesSA M A RTH  P\n3rd Year, B. Tech CSE at PES University, EC Campus\nSkills\nProficient in Python, C, C++, JavaScript\nIn depth knowledge of Machine\nlearning Algorithms, GenAI, LLMs,\nRAG, Agentic AI Systems, Diffusion\nModels\nFull stack deveopment with Flutter,\nHTML, CSS, JavaScript, React Node.js,\nNext.js, MongoDB, Firebase, Flask,\nDjango\nExperience in IoT and robotics, deep\nunderstanding of embedded systems\nnotably Arduino\nFoundational understanding of\nQuantum Physics principles and\npractical experience using Qiskit for\nQuantum Computing \nSkilled in fine arts especially acrylic\nand watercolor paintings\nEducation Background\nB Tech CSE, PES University EC Campus\n2022 - Present  | 8.56 CGPA\nGEAR Innovative International School\nChinmaya Vidyalaya, Koramangala\n2010 - 2020 | Grade X ICSE - 93%Profile\nsamarthprakash8@gmail.com\nhttps://samarth.arthttps://github.com/samarth777\n+91 7337610771\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\n2nd Sem  | 8.82 SGPA\n3rd Sem   | 8.63 SGPA\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\nShunya Math Club + IEEE PESU \nUnimate'22 | Winner \nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\nPESU ECC \nBinary Battles | 3rd Place\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\nQiskit Quantum Challenge Spring 2023\nCompleted all labs in this IBM run event to test Qiskit knowledge\nLabyrinth Speed Coding Contest | 2nd Place\nTeam won 2nd place in the speed coding challenge called Labyrinth\norganised by Codechef PESU ECC\nCodeventure | Winner\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\nPESUIBM Quantum Challenge 2024\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\nCTF-Workshop and Hackathon, PESU C-ISFCR\nCertificate of Recognigition -HPE CodeWars 2022\nCertificate of Recognigition -HPE CodeWars 2021\nThe Complete 2023 Web Development Bootcamp\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\nQiskit Global Summer School 2023\nDive into Deep Learning -d2l.ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar. from 0.0492063492063492 to 0.06587301587301586 based on rank 0 in query '4. \"AI initiatives by Samarth\"'\nUpdating score for com/in/samarth-\np-3964721b3/Experience\nSummer Intern, IIIT Bangalore (June 2024 - Present)\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\nproject by implementing a pipeline to easily search information from a\ncorpus in colloquial indic languages.\nBosch University Connect Program (March 2024 - Present)\nDeveloping a  Generative AI platform Bosch through the University\nConnect Program to help Automobile Technicians with Vehicular\nTroubleshoot Assistance\nAchievements\nOverride'22 | Winner\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\nby GDSC \u2013 PESU ECC\nArithemania'22 | Winner RAS Track\nTeam won 1st place at from 0.04891591750396616 to 0.06530936012691697 based on rank 1 in query '4. \"AI initiatives by Samarth\"'\nUpdating score for DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0.04893312516263336 to 0.06506215742069787 based on rank 2 in query '4. \"AI initiatives by Samarth\"'\nUpdating score for ai (currently doing)\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\nguided autonomous navigaing robot\n\nClubs\nQuantumania PESU ECC -Club\nSecretary Built website for QuaNaD Lab,\nConducted a workshop \u201cBeyond Bits\u201d\nexplaining the fundamentals of\nQuantum Computing, Quantum\nresearch with Prof. Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter. from 0.04762704813108039 to 0.06350006400409626 based on rank 3 in query '4. \"AI initiatives by Samarth\"'\nUpdating score for Gajanan Honnavar.\nGave a lab tour and lecture for the\nQuaNaD Lab on the occasion of world\nQuantum Day\nCodechef PESU ECC -CP Mentor                                      \nSpeaker at a workshop that introduced\n1st and 2nd year students to the C++ STL\nlibrary\nEquinox PESU ECC -Technical Team\nIEEE RAS PESU ECC -Technical TeamProjects\nHandwriter https://handwriter.in\nBuilt a website that converts any typed piece of text into your own\nhandwriting easily\nGita Daily https://gitadaily.in\nBuilt a WhatsApp bot that sends subscribed users a verse from the\nBhagavad Gita everyday (1000+ users)\nWebsited for PESU Research Centres: QuaNaD, CONECT  \nhttps://quanad.pes.edu\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\nbuilding a website for Center for Networking and Evolving\nCommunication Technologies CONECT) and a website for PESU to use for\nassignment submissions of various lab courses\nMedMaster www.github.com/samarth777/MedMaster\nAn automated micro-pharmacy facilitates quick medicine purchases\nthrough QR scanning on our website. It includes a custom-built\nhardware vending machine for immediate medicine dispensing\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\nReciever\niverilog implementation includes a frequency scaler with adjustable\nscaling factor and PWM generation based on a specified Duty Cycle.\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\nvideo and cards.\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\ncombines IoT with AI to create an autonomous car equipped with a\nvirtual camera, allowing remote control and monitoring of surroundings\nwith scene descriptions generated through AI integration\nMANET Research (ongoing)\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\nBuilding a GenAI project for Bosch to help technicians with vehicular\ntroublehoot assistance by creating a platform powered by RAG and\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\nAn AI-powered fashion e-commerce platform that revolutionizes\nclothing discovery and purchasing. It uses RAG and vector search for\nsemantic queries, enabling users to find products using natural\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\nA voice call-based AI agent assistant designed to empower farmers by\nproviding them with vital information on agricultural subsidies, weather\nupdates, and more. By addressing key challenges faced by farmers\nKissanDial aims to enhance their access to essential resources. from 0.047379032258064516 to 0.06300403225806452 based on rank 4 in query '4. \"AI initiatives by Samarth\"'\nFinal reranked results: {\"Certifications/CoursesSA M A RTH  P\\n3rd Year, B. Tech CSE at PES University, EC Campus\\nSkills\\nProficient in Python, C, C++, JavaScript\\nIn depth knowledge of Machine\\nlearning Algorithms, GenAI, LLMs,\\nRAG, Agentic AI Systems, Diffusion\\nModels\\nFull stack deveopment with Flutter,\\nHTML, CSS, JavaScript, React Node.js,\\nNext.js, MongoDB, Firebase, Flask,\\nDjango\\nExperience in IoT and robotics, deep\\nunderstanding of embedded systems\\nnotably Arduino\\nFoundational understanding of\\nQuantum Physics principles and\\npractical experience using Qiskit for\\nQuantum Computing \\nSkilled in fine arts especially acrylic\\nand watercolor paintings\\nEducation Background\\nB Tech CSE, PES University EC Campus\\n2022 - Present  | 8.56 CGPA\\nGEAR Innovative International School\\nChinmaya Vidyalaya, Koramangala\\n2010 - 2020 | Grade X ICSE - 93%Profile\\nsamarthprakash8@gmail.com\\nhttps://samarth.arthttps://github.com/samarth777\\n+91 7337610771\\n2020 - 2022 | Grade XII CBSE - 87%1st Sem     | 8.23 SGPA\\n2nd Sem  | 8.82 SGPA\\n3rd Sem   | 8.63 SGPA\\nAwarded CNR Scholarship in Semester 3https://www.linkedin.com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at Arithemania\u201922 a 36-hr hackathon organised by\\nShunya Math Club + IEEE PESU \\nUnimate'22 | Winner \\nTeam won 1st place at Unimate\u201922 a 24-hr hackathon organised by IEEE RAS\\nPESU ECC \\nBinary Battles | 3rd Place\\nWon 3rd prize in this competitive coding event by Codechef PESU ECC\\nQiskit Quantum Challenge Spring 2023\\nCompleted all labs in this IBM run event to test Qiskit knowledge\\nLabyrinth Speed Coding Contest | 2nd Place\\nTeam won 2nd place in the speed coding challenge called Labyrinth\\norganised by Codechef PESU ECC\\nCodeventure | Winner\\nTeam won 1st place in the Codeventure Digital Quest organised by ACM-W\\nPESUIBM Quantum Challenge 2024\\nCompleted all labs in this IBM event working with the new Qiskit 1.0 SDKMachine Learning Specialization - deeplearning.ai\\nCTF-Workshop and Hackathon, PESU C-ISFCR\\nCertificate of Recognigition -HPE CodeWars 2022\\nCertificate of Recognigition -HPE CodeWars 2021\\nThe Complete 2023 Web Development Bootcamp\\nWorkshop on \u201cVarious Aspects of Quantum Computing\u201d -IIITB COMET\\nQiskit Global Summer School 2023\\nDive into Deep Learning -d2l.ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\": 0.06587301587301586, \"com/in/samarth-\\np-3964721b3/Experience\\nSummer Intern, IIIT Bangalore (June 2024 - Present)\\nWorking as a summer intern at the Web Science Lab (WSL), IIIT Bangalore\\nunder Prof. Srinath Srinivasa on the IndicNLP project. Contributing to the\\nproject by implementing a pipeline to easily search information from a\\ncorpus in colloquial indic languages.\\nBosch University Connect Program (March 2024 - Present)\\nDeveloping a  Generative AI platform Bosch through the University\\nConnect Program to help Automobile Technicians with Vehicular\\nTroubleshoot Assistance\\nAchievements\\nOverride'22 | Winner\\nTeam won \u2018Best Overall App\u2019 at Override \u201922 a 48-hr hackathon organised\\nby GDSC \u2013 PESU ECC\\nArithemania'22 | Winner RAS Track\\nTeam won 1st place at\": 0.06530936012691697, 'DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.06506215742069787, 'ai (currently doing)\\nAMD Pervasive AI Hackathon | Hardware Winner Robotics AI Category\\nIdea awarded the AMD Kria KR260 FPGA for the development of an AI vision\\nguided autonomous navigaing robot\\n\\nClubs\\nQuantumania PESU ECC -Club\\nSecretary Built website for QuaNaD Lab,\\nConducted a workshop \u201cBeyond Bits\u201d\\nexplaining the fundamentals of\\nQuantum Computing, Quantum\\nresearch with Prof. Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.': 0.06350006400409626, 'Gajanan Honnavar.\\nGave a lab tour and lecture for the\\nQuaNaD Lab on the occasion of world\\nQuantum Day\\nCodechef PESU ECC -CP Mentor                                      \\nSpeaker at a workshop that introduced\\n1st and 2nd year students to the C++ STL\\nlibrary\\nEquinox PESU ECC -Technical Team\\nIEEE RAS PESU ECC -Technical TeamProjects\\nHandwriter https://handwriter.in\\nBuilt a website that converts any typed piece of text into your own\\nhandwriting easily\\nGita Daily https://gitadaily.in\\nBuilt a WhatsApp bot that sends subscribed users a verse from the\\nBhagavad Gita everyday (1000+ users)\\nWebsited for PESU Research Centres: QuaNaD, CONECT  \\nhttps://quanad.pes.edu\\nBuilt a website for the Quantum and Nano Devices Laboratory of PESU\\nECC using Next.js 13 and Tailwind CSS hosted on Vercel. Currently\\nbuilding a website for Center for Networking and Evolving\\nCommunication Technologies CONECT) and a website for PESU to use for\\nassignment submissions of various lab courses\\nMedMaster www.github.com/samarth777/MedMaster\\nAn automated micro-pharmacy facilitates quick medicine purchases\\nthrough QR scanning on our website. It includes a custom-built\\nhardware vending machine for immediate medicine dispensing\\nFrequency Scaling + PWM Generator  and 8 Bit UART Transmitter and\\nReciever\\niverilog implementation includes a frequency scaler with adjustable\\nscaling factor and PWM generation based on a specified Duty Cycle.\\nAdditionally, an 8-bit UART transmitter and receiver are integrated.Generative AI Projects\\nBuilt a couple of Gen AI projects with a senior using preexisting diffusion\\nmodels like StableDiffusionXL, InstantID, etc. for editing personalized\\nvideo and cards.\\nSmartGuardian  www.gith ub.com/samarth777/smartguardian\\ncombines IoT with AI to create an autonomous car equipped with a\\nvirtual camera, allowing remote control and monitoring of surroundings\\nwith scene descriptions generated through AI integration\\nMANET Research (ongoing)\\nResearching  Mobile Ad-hoc Networks under Dr. DP Chavan using tools\\nlike NS3Bosch University Connect Program -VTAGen AI Project (ongoing)\\nBuilding a GenAI project for Bosch to help technicians with vehicular\\ntroublehoot assistance by creating a platform powered by RAG and\\nfinetuned LLM\u2019sAlienWear https://alienwear.vercel.app\\nAn AI-powered fashion e-commerce platform that revolutionizes\\nclothing discovery and purchasing. It uses RAG and vector search for\\nsemantic queries, enabling users to find products using natural\\nlanguageKissanDial https://github.com/shadow-penguins/-KissanDial\\nA voice call-based AI agent assistant designed to empower farmers by\\nproviding them with vital information on agricultural subsidies, weather\\nupdates, and more. By addressing key challenges faced by farmers\\nKissanDial aims to enhance their access to essential resources.': 0.06300403225806452}\n</pre> <p>Human: What are all the AI projects done by samarth?</p> <p>AI: The AI projects done by Samarth are:</p> <ol> <li>Gen AI Project for Bosch under the University Connect Program</li> <li>AlienWear - AI-powered fashion e-commerce platform</li> <li>KissanDial - Voice call-based AI agent assistant</li> </ol> <pre>\n--------------------------------------------------\n\n</pre> Out[62]: <pre>'The AI projects done by Samarth are:\\n\\n1. Gen AI Project for Bosch under the University Connect Program\\n2. AlienWear - AI-powered fashion e-commerce platform\\n3. KissanDial - Voice call-based AI agent assistant'</pre> In\u00a0[63]: Copied! <pre># To view chat history:\nhistory = chat_interface.get_chat_history()\nfor message in history:\n    print(f\"{message.role}: {message.content}\")\n</pre> # To view chat history: history = chat_interface.get_chat_history() for message in history:     print(f\"{message.role}: {message.content}\") <pre>MessageRole.USER: What is Samarth's CGPA?\nMessageRole.ASSISTANT: Samarth's CGPA is 8.56 at PES University EC Campus where he is pursuing B. Tech CSE from 2022 to present.\nMessageRole.USER: What are all the AI projects done by samarth?\nMessageRole.ASSISTANT: The AI projects done by Samarth are:\n\n1. Gen AI Project for Bosch under the University Connect Program\n2. AlienWear - AI-powered fashion e-commerce platform\n3. KissanDial - Voice call-based AI agent assistant\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#rag-fusion-implementation","title":"RAG Fusion Implementation:\u00b6","text":"<p>A Guide to Advanced Retrieval-Augmented Generation</p> <p>Welcome to the RAG Fusion Implementation guide! This notebook is designed to introduce you to the concept of RAG Fusion, an advanced technique that enhances the traditional Retrieval-Augmented Generation (RAG) approach. We'll provide a step-by-step walkthrough of implementing a RAG Fusion system.</p>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#introduction","title":"Introduction\u00b6","text":"<p>RAG Fusion is an advanced technique that builds upon the foundations of Retrieval-Augmented Generation (RAG). It combines the power of large language models with sophisticated information retrieval methods to produce more accurate, diverse, and contextually rich responses. RAG Fusion enhances the basic RAG approach by employing query expansion, multiple retrievals, and intelligent result combination techniques.</p> <p>This notebook aims to provide a clear and comprehensive introduction to RAG Fusion, suitable for those who are familiar with basic RAG and want to explore more advanced implementations.</p>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#getting-started","title":"Getting Started\u00b6","text":"<p>To get the most out of this notebook, you should have a good understanding of Python and be familiar with basic RAG concepts. Don't worry if some advanced ideas are new to you \u2013 we'll guide you through each step of the RAG Fusion process!</p>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Python 3.9+</li> <li>Jupyter Notebook or JupyterLab</li> <li>Familiarity with basic RAG concepts</li> <li>Understanding of vector databases and embeddings</li> <li>Basic knowledge of natural language processing (NLP) concepts</li> </ul>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#notebook-contents","title":"Notebook Contents\u00b6","text":"<p>Our notebook is structured into the following main sections:</p> <ol> <li><p>Environment Set Up: We'll guide you through setting up your Python environment with all the necessary libraries and dependencies for RAG Fusion.</p> </li> <li><p>Query Expansion: Learn how to generate multiple queries from a single user input to capture different aspects of the user's intent.</p> </li> <li><p>Multiple Retrievals: Understand how to perform and manage multiple retrieval operations using various queries.</p> </li> <li><p>Reciprocal Rank Fusion (RRF): Dive into the RRF algorithm and how it's used to combine results from multiple retrievals effectively.</p> </li> <li><p>Context Selection and Reranking: Explore techniques for selecting diverse and relevant contexts from the fused retrieval results.</p> </li> <li><p>Enhanced Prompting: Learn how to craft effective prompts that leverage the diverse contexts obtained through RAG Fusion.</p> </li> <li><p>RAG Fusion Pipeline: We'll walk you through the process of setting up a complete RAG Fusion pipeline, integrating all the components.</p> </li> <li><p>Advanced Topics and Optimizations: Explore advanced concepts like dynamic query expansion, adaptive fusion techniques, and performance optimizations.</p> </li> </ol> <p>By the end of this notebook, you'll have a solid understanding of RAG Fusion and be able to implement this advanced technique in your own projects. Let's dive in and explore the cutting edge of Retrieval-Augmented Generation!</p>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#set-up-environment","title":"Set up environment\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#function-to-generate-queries-using-openais-chatgpt","title":"Function to generate queries using OpenAI's ChatGPT\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#function-to-perform-vector-search","title":"Function to perform vector search\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#reciprocal-rank-fusion-algorithm","title":"Reciprocal Rank Fusion algorithm\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#load-the-data","title":"Load the Data\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#setting-up-vector-database","title":"Setting up Vector Database\u00b6","text":"<p>We will be using qDrant as the Vector database There are 4 ways to initialize qdrant</p> <ol> <li>Inmemory</li> </ol> <pre>client = qdrant_client.QdrantClient(location=\":memory:\")\n</pre> <ol> <li>Disk</li> </ol> <pre>client = qdrant_client.QdrantClient(path=\"./data\")\n</pre> <ol> <li>Self hosted or Docker</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    # url=\"http://&lt;host&gt;:&lt;port&gt;\"\n    host=\"localhost\",port=6333\n)\n</pre> <ol> <li>Qdrant cloud</li> </ol> <pre>client = qdrant_client.QdrantClient(\n    url=QDRANT_CLOUD_ENDPOINT,\n    api_key=QDRANT_API_KEY,\n)\n</pre> <p>for this notebook we will be using qdrant cloud</p>"},{"location":"RAG/08_RAG_Fusion/ragfusion/#ingest-data-into-vector-db","title":"Ingest Data into vector DB\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#setting-up-retriever","title":"Setting Up Retriever\u00b6","text":""},{"location":"RAG/08_RAG_Fusion/ragfusion/#chatengineinterface-with-rag-fusion","title":"ChatEngineInterface with RAG Fusion\u00b6","text":""},{"location":"RAG/09_RAPTOR/","title":"RAPTOR: Recursive Abstractive Processing for Tree Organized Retrieval","text":""},{"location":"RAG/09_RAPTOR/#raptor-recursive-abstractive-processing-for-tree-organized-retrieval","title":"RAPTOR: Recursive Abstractive Processing for Tree Organized Retrieval","text":""},{"location":"RAG/09_RAPTOR/#introduction","title":"Introduction","text":"<p>RAPTOR (Recursive Abstractive Processing for Tree Organized Retrieval) is an advanced approach to Retrieval-Augmented Generation (RAG) that enhances the traditional RAG pipeline by incorporating hierarchical document structuring and summarization.</p>"},{"location":"RAG/09_RAPTOR/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with large document sets and complex queries. RAPTOR addresses these challenges by creating a hierarchical representation of the document corpus, allowing for more nuanced and efficient retrieval.</p>"},{"location":"RAG/09_RAPTOR/#method-details","title":"Method Details","text":"<pre><code>flowchart TB\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split Text into Chunks]\n        B --&gt; C1[Chunk-1]\n        B --&gt; C2[Chunk-2]\n        B --&gt; C3[Chunk-n]\n    end\n\n    subgraph \"2. Document Embedding\"\n        EM1{{Embedding Model}}\n        C1 &amp; C2 &amp; C3 --&gt; EM1\n        EM1 --&gt; D1[Embedding-1] &amp; D2[Embedding-2] &amp; D3[Embedding-3]\n    end\n\n    subgraph \"3. Clustering\"\n        D1 &amp; D2 &amp; D3 --&gt; E[Clustering Algorithm]\n        E --&gt; F1[Cluster-1] &amp; F2[Cluster-2] &amp; F3[Cluster-m]\n    end\n\n    subgraph \"4. Summarization\"\n        F1 --&gt; G1[Summary-1]\n        F2 --&gt; G2[Summary-2]\n        F3 --&gt; G3[Summary-m]\n    end\n\n    subgraph \"5. Tree Construction\"\n        G1 &amp; G2 &amp; G3 --&gt; H[Build Hierarchical Tree]\n        H --&gt; I[RAPTOR Tree]\n    end\n\n    subgraph \"6. Query Processing\"\n        J[Query] --&gt; EM2{{Embedding Model}}\n        EM2 --&gt; K[Query Embedding]\n    end\n\n    subgraph \"7. Tree Traversal\"\n        K --&gt; L[Traverse RAPTOR Tree]\n        I --&gt; L\n        L --&gt; M[Relevant Nodes]\n    end\n\n    subgraph \"8. Context Formation\"\n        M --&gt; N[Query + Relevant Summaries/Chunks]\n    end\n\n    subgraph \"9. Generation\"\n        N --&gt; O[LLM]\n        O --&gt; P[Response]\n    end\n\n    J --&gt; N</code></pre>"},{"location":"RAG/09_RAPTOR/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Documents are split into manageable chunks.</li> <li>Each chunk is embedded using a suitable embedding model.</li> <li>Embeddings are clustered to group similar content.</li> <li>Clusters are summarized to create higher-level abstractions.</li> <li>A hierarchical tree structure (RAPTOR Tree) is built using these summaries and original chunks.</li> </ol>"},{"location":"RAG/09_RAPTOR/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>The user query is embedded using the same embedding model.</li> <li>The RAPTOR Tree is traversed to find relevant nodes (summaries or chunks).</li> <li>Relevant content is combined with the original query to form a context.</li> <li>This context is passed to a Large Language Model (LLM) to generate a response.</li> </ol>"},{"location":"RAG/09_RAPTOR/#key-features-of-raptor","title":"Key Features of RAPTOR","text":"<ul> <li>Hierarchical Document Representation: Creates a tree structure of document content.</li> <li>Multi-level Summarization: Provides abstractions at various levels of detail.</li> <li>Efficient Retrieval: Utilizes tree traversal for faster and more relevant information retrieval.</li> <li>Scalability: Better handles large document sets compared to flat vector stores.</li> </ul>"},{"location":"RAG/09_RAPTOR/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Context Relevance: The hierarchical structure allows for more nuanced matching of queries to relevant content.</li> <li>Efficient Retrieval: Tree traversal can be more efficient than exhaustive search in large vector spaces.</li> <li>Handling Complex Queries: The multi-level structure is better equipped to handle queries that require information from multiple parts of the corpus.</li> <li>Scalability: Can handle larger document sets more effectively than traditional RAG approaches.</li> </ol>"},{"location":"RAG/09_RAPTOR/#conclusion","title":"Conclusion","text":"<p>RAPTOR enhances the RAG pipeline by introducing a hierarchical, summary-based approach to document representation and retrieval. This method promises to improve the quality and efficiency of information retrieval, especially for large and complex document sets, leading to more accurate and contextually relevant responses in AI-powered question-answering systems.</p>"},{"location":"RAG/09_RAPTOR/raptor/","title":"RAPTOR(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index llama-index-packs-raptor llama-index-vector-stores-qdrant\n</pre> !pip install llama-index llama-index-packs-raptor llama-index-vector-stores-qdrant In\u00a0[\u00a0]: Copied! <pre>from llama_index.packs.raptor import RaptorPack\n\n# optionally download the pack to inspect/modify it yourself!\n# from llama_index.core.llama_pack import download_llama_pack\n# RaptorPack = download_llama_pack(\"RaptorPack\", \"./raptor_pack\")\n</pre> from llama_index.packs.raptor import RaptorPack  # optionally download the pack to inspect/modify it yourself! # from llama_index.core.llama_pack import download_llama_pack # RaptorPack = download_llama_pack(\"RaptorPack\", \"./raptor_pack\") In\u00a0[\u00a0]: Copied! <pre>!wget https://arxiv.org/pdf/2401.18059.pdf -O ./raptor_paper.pdf\n</pre> !wget https://arxiv.org/pdf/2401.18059.pdf -O ./raptor_paper.pdf <pre>Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\nERROR: could not open HSTS store at '/home/loganm/.wget-hsts'. HSTS will be disabled.\n--2024-02-29 22:16:11--  https://arxiv.org/pdf/2401.18059.pdf\nResolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.131.42, ...\nConnecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2547113 (2.4M) [application/pdf]\nSaving to: \u2018./raptor_paper.pdf\u2019\n\n./raptor_paper.pdf  100%[===================&gt;]   2.43M  12.5MB/s    in 0.2s    \n\n2024-02-29 22:16:12 (12.5 MB/s) - \u2018./raptor_paper.pdf\u2019 saved [2547113/2547113]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(input_files=[\"./raptor_paper.pdf\"]).load_data()\n</pre> from llama_index.core import SimpleDirectoryReader  documents = SimpleDirectoryReader(input_files=[\"./raptor_paper.pdf\"]).load_data() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nimport chromadb\n\nclient = chromadb.PersistentClient(path=\"./raptor_paper_db\")\ncollection = client.get_or_create_collection(\"raptor\")\n\nvector_store = ChromaVectorStore(chroma_collection=collection)\n\nraptor_pack = RaptorPack(\n    documents,\n    embed_model=OpenAIEmbedding(\n        model=\"text-embedding-3-small\"\n    ),  # used for embedding clusters\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),  # used for generating summaries\n    vector_store=vector_store,  # used for storage\n    similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed\n    mode=\"collapsed\",  # sets default mode\n    transformations=[\n        SentenceSplitter(chunk_size=400, chunk_overlap=50)\n    ],  # transformations applied for ingestion\n)\n</pre> from llama_index.core.node_parser import SentenceSplitter from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.vector_stores.chroma import ChromaVectorStore import chromadb  client = chromadb.PersistentClient(path=\"./raptor_paper_db\") collection = client.get_or_create_collection(\"raptor\")  vector_store = ChromaVectorStore(chroma_collection=collection)  raptor_pack = RaptorPack(     documents,     embed_model=OpenAIEmbedding(         model=\"text-embedding-3-small\"     ),  # used for embedding clusters     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),  # used for generating summaries     vector_store=vector_store,  # used for storage     similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed     mode=\"collapsed\",  # sets default mode     transformations=[         SentenceSplitter(chunk_size=400, chunk_overlap=50)     ],  # transformations applied for ingestion ) <pre>Generating embeddings for level 0.\nPerforming clustering for level 0.\nGenerating summaries for level 0 with 10 clusters.\nLevel 0 created summaries/clusters: 10\nGenerating embeddings for level 1.\nPerforming clustering for level 1.\nGenerating summaries for level 1 with 1 clusters.\nLevel 1 created summaries/clusters: 1\nGenerating embeddings for level 2.\nPerforming clustering for level 2.\nGenerating summaries for level 2 with 1 clusters.\nLevel 2 created summaries/clusters: 1\n</pre> In\u00a0[\u00a0]: Copied! <pre>nodes = raptor_pack.run(\"What baselines is raptor compared against?\", mode=\"collapsed\")\nprint(len(nodes))\nprint(nodes[0].text)\n</pre> nodes = raptor_pack.run(\"What baselines is raptor compared against?\", mode=\"collapsed\") print(len(nodes)) print(nodes[0].text) <pre>2\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\nRetriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\nTitle + Abstract 25.2 22.2 17.5\nBM25 46.6 50.2 26.4\nDPR 51.3 53.0 32.1\nRAPTOR 53.1 55.7 36.6\nTable 4: Comparison of accuracies on the QuAL-\nITY dev dataset for two different language mod-\nels (GPT-3, UnifiedQA 3B) using various retrieval\nmethods. RAPTOR outperforms the baselines of\nBM25 and DPR by at least 2.0% in accuracy.\nModel GPT-3 Acc. UnifiedQA Acc.\nBM25 57.3 49.9\nDPR 60.4 53.9\nRAPTOR 62.4 56.6\nTable 5: Results on F-1 Match scores of various\nmodels on the QASPER dataset.\nModel F-1 Match\nLongT5 XL (Guo et al., 2022) 53.1\nCoLT5 XL (Ainslie et al., 2023) 53.9\nRAPTOR + GPT-4 55.7Comparison to State-of-the-art Systems\nBuilding upon our controlled comparisons,\nwe examine RAPTOR\u2019s performance relative\nto other state-of-the-art models.\n</pre> In\u00a0[\u00a0]: Copied! <pre>nodes = raptor_pack.run(\n    \"What baselines is raptor compared against?\", mode=\"tree_traversal\"\n)\nprint(len(nodes))\nprint(nodes[0].text)\n</pre> nodes = raptor_pack.run(     \"What baselines is raptor compared against?\", mode=\"tree_traversal\" ) print(len(nodes)) print(nodes[0].text) <pre>Retrieved parent IDs from level 2: ['cc3b3f41-f4ca-4020-b11f-be7e0ce04c4f']\nRetrieved 1 from parents at level 2.\nRetrieved parent IDs from level 1: ['a4ca9426-a312-4a01-813a-c9b02aefc7e8']\nRetrieved 2 from parents at level 1.\nRetrieved parent IDs from level 0: ['63126782-2778-449f-99c0-1e8fd90caa36', 'd8f68d31-d878-41f1-aeb6-a7dde8ed5143']\nRetrieved 4 from parents at level 0.\n4\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\nRetriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\nTitle + Abstract 25.2 22.2 17.5\nBM25 46.6 50.2 26.4\nDPR 51.3 53.0 32.1\nRAPTOR 53.1 55.7 36.6\nTable 4: Comparison of accuracies on the QuAL-\nITY dev dataset for two different language mod-\nels (GPT-3, UnifiedQA 3B) using various retrieval\nmethods. RAPTOR outperforms the baselines of\nBM25 and DPR by at least 2.0% in accuracy.\nModel GPT-3 Acc. UnifiedQA Acc.\nBM25 57.3 49.9\nDPR 60.4 53.9\nRAPTOR 62.4 56.6\nTable 5: Results on F-1 Match scores of various\nmodels on the QASPER dataset.\nModel F-1 Match\nLongT5 XL (Guo et al., 2022) 53.1\nCoLT5 XL (Ainslie et al., 2023) 53.9\nRAPTOR + GPT-4 55.7Comparison to State-of-the-art Systems\nBuilding upon our controlled comparisons,\nwe examine RAPTOR\u2019s performance relative\nto other state-of-the-art models.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.packs.raptor import RaptorRetriever\n\nretriever = RaptorRetriever(\n    [],\n    embed_model=OpenAIEmbedding(\n        model=\"text-embedding-3-small\"\n    ),  # used for embedding clusters\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),  # used for generating summaries\n    vector_store=vector_store,  # used for storage\n    similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed\n    mode=\"tree_traversal\",  # sets default mode\n)\n</pre> from llama_index.packs.raptor import RaptorRetriever  retriever = RaptorRetriever(     [],     embed_model=OpenAIEmbedding(         model=\"text-embedding-3-small\"     ),  # used for embedding clusters     llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),  # used for generating summaries     vector_store=vector_store,  # used for storage     similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed     mode=\"tree_traversal\",  # sets default mode ) In\u00a0[\u00a0]: Copied! <pre># if using a default vector store\n# retriever.persist(\"./persist\")\n# retriever = RaptorRetriever.from_persist_dir(\"./persist\", ...)\n</pre> # if using a default vector store # retriever.persist(\"./persist\") # retriever = RaptorRetriever.from_persist_dir(\"./persist\", ...) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_engine import RetrieverQueryEngine\n\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever, llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n)\n</pre> from llama_index.core.query_engine import RetrieverQueryEngine  query_engine = RetrieverQueryEngine.from_args(     retriever, llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1) ) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What baselines was RAPTOR compared against?\")\n</pre> response = query_engine.query(\"What baselines was RAPTOR compared against?\") In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response)) <pre>BM25 and DPR\n</pre>"},{"location":"RAG/09_RAPTOR/raptor/#raptor-recursive-abstractive-processing-for-tree-organized-retrieval","title":"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\u00b6","text":"<p>This notebook shows how to use an implementation of RAPTOR with llama-index, leveraging the RAPTOR llama-pack.</p> <p>RAPTOR works by recursively clustering and summarizing clusters in layers for retrieval.</p> <p>There two retrieval modes:</p> <ul> <li>tree_traversal -- traversing the tree of clusters, performing top-k at each level in the tree.</li> <li>collapsed -- treat the entire tree as a giant pile of nodes, perform simple top-k.</li> </ul> <p>See the paper for full algorithm details.</p>"},{"location":"RAG/09_RAPTOR/raptor/#setup","title":"Setup\u00b6","text":""},{"location":"RAG/09_RAPTOR/raptor/#constructing-the-clustershierarchy-tree","title":"Constructing the Clusters/Hierarchy Tree\u00b6","text":""},{"location":"RAG/09_RAPTOR/raptor/#retrieval","title":"Retrieval\u00b6","text":""},{"location":"RAG/09_RAPTOR/raptor/#loading","title":"Loading\u00b6","text":"<p>Since we saved to a vector store, we can also use it again! (For local vector stores, there is a <code>persist</code> and <code>from_persist_dir</code> method on the retriever)</p>"},{"location":"RAG/09_RAPTOR/raptor/#query-engine","title":"Query Engine\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/","title":"Index","text":"<p>ColBERT (Contextualized Late Interaction over BERT) is indeed different from traditional dense embedding models. Here's a brief explanation of how ColBERT works:</p> <ol> <li> <p>Token-level embeddings: Instead of creating a single vector for an entire document or query, ColBERT creates embeddings for each token.</p> </li> <li> <p>Late interaction: The similarity between a query and a document is computed by comparing each query token embedding with each document token embedding, rather than comparing single vectors.</p> </li> <li> <p>MaxSim operation: For each query token, ColBERT finds the maximum similarity with any document token. These maximum similarities are then summed to get the final relevance score.</p> </li> </ol> <p>Now, let me create diagrams to illustrate this process within a RAG pipeline.</p> <pre><code>flowchart TB\n    subgraph \"1. Document Processing\"\n        A[Documents] --&gt; B[Split into Chunks]\n        B --&gt; C[ColBERT Document Encoder]\n        C --&gt; D[Token-level Embeddings]\n        D --&gt; E[(Vector Index)]\n    end\n\n    subgraph \"2. Query Processing\"\n        F[Query] --&gt; G[ColBERT Query Encoder]\n        G --&gt; H[Query Token Embeddings]\n    end\n\n    subgraph \"3. Retrieval\"\n        H --&gt; I{Vector Similarity Search}\n        E --&gt; I\n        I --&gt; J[Top-K Chunks]\n    end\n\n    subgraph \"4. Late Interaction\"\n        H --&gt; K{MaxSim + Sum}\n        J --&gt; K\n        K --&gt; L[Relevance Scores]\n    end\n\n    subgraph \"5. Context Formation\"\n        L --&gt; M[Re-rank and Select Top Chunks]\n        F --&gt; N[Query + Selected Chunks]\n        M --&gt; N\n    end\n\n    subgraph \"6. Generation\"\n        N --&gt; O[LLM]\n        O --&gt; P[Response]\n    end\n</code></pre> <p>This diagram shows the overall ColBERT-based RAG pipeline, emphasizing the token-level processing and late interaction that are key to ColBERT's approach.</p> <p>Now, let's create a more detailed diagram focusing on ColBERT's token-level embedding and late interaction mechanism:</p> <pre><code>flowchart TB\n    subgraph \"Document Processing\"\n        A[Document] --&gt; B[BERT]\n        B --&gt; C[Linear Layer]\n        C --&gt; D[Document Token Embeddings]\n        D --&gt; |D1| E((D1))\n        D --&gt; |D2| F((D2))\n        D --&gt; |...| G((...))\n        D --&gt; |Dn| H((Dn))\n    end\n\n    subgraph \"Query Processing\"\n        I[Query] --&gt; J[BERT]\n        J --&gt; K[Linear Layer]\n        K --&gt; L[Query Token Embeddings]\n        L --&gt; |Q1| M((Q1))\n        L --&gt; |Q2| N((Q2))\n        L --&gt; |...| O((...))\n        L --&gt; |Qm| P((Qm))\n    end\n\n    subgraph \"Late Interaction\"\n        M &amp; N &amp; O &amp; P --&gt; Q{MaxSim}\n        E &amp; F &amp; G &amp; H --&gt; Q\n        Q --&gt; R[Sum]\n        R --&gt; S[Final Score]\n    end\n</code></pre> <p>This diagram illustrates: 1. How documents and queries are processed into token-level embeddings using BERT and a linear layer. 2. The late interaction mechanism where each query token is compared with each document token. 3. The MaxSim operation followed by summation to produce the final relevance score.</p> <p>These diagrams more accurately represent how ColBERT works within a RAG pipeline, emphasizing its token-level approach and late interaction mechanism. This approach allows ColBERT to maintain more fine-grained information from both queries and documents, enabling more nuanced matching and potentially better retrieval performance compared to traditional dense embedding models.</p>"},{"location":"RAG/10_ColBERT_RAG/ColBert_RAG/","title":"ColBERT(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q -U ragatouille\n!pip install -q langchain\n!pip install -q langchain-openai\n!pip install -q langchain-core\n!pip install -q langchain-community\n!pip install -q pypdf\n</pre> !pip install -q -U ragatouille !pip install -q langchain !pip install -q langchain-openai !pip install -q langchain-core !pip install -q langchain-community !pip install -q pypdf In\u00a0[\u00a0]: Copied! <pre>from ragatouille import RAGPretrainedModel\n\nRAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n</pre> from ragatouille import RAGPretrainedModel  RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") In\u00a0[\u00a0]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"Orca_paper.pdf\")\npages = loader.load_and_split()\n</pre> from langchain_community.document_loaders import PyPDFLoader loader = PyPDFLoader(\"Orca_paper.pdf\") pages = loader.load_and_split()   In\u00a0[\u00a0]: Copied! <pre>len(pages)\n</pre> len(pages) In\u00a0[\u00a0]: Copied! <pre>full_document = \"\"\n\nfor page in pages:\n  full_document += page.page_content\n</pre> full_document = \"\"  for page in pages:   full_document += page.page_content In\u00a0[\u00a0]: Copied! <pre>print(full_document)\n</pre> print(full_document) In\u00a0[\u00a0]: Copied! <pre>type(full_document)\n</pre> type(full_document) In\u00a0[\u00a0]: Copied! <pre>RAG.index(\n    collection=[full_document],\n    index_name=\"orca_paper\",\n    max_document_length=512,\n    split_documents=True,\n)\n</pre> RAG.index(     collection=[full_document],     index_name=\"orca_paper\",     max_document_length=512,     split_documents=True, ) In\u00a0[\u00a0]: Copied! <pre>results = RAG.search(query=\"What is instruction tuning?\", k=3)\n</pre> results = RAG.search(query=\"What is instruction tuning?\", k=3)  In\u00a0[\u00a0]: Copied! <pre>results\n</pre> results In\u00a0[\u00a0]: Copied! <pre>retriever = RAG.as_langchain_retriever(k=3)\n</pre> retriever = RAG.as_langchain_retriever(k=3) In\u00a0[\u00a0]: Copied! <pre>retriever.invoke(\"What is instruction tuning?\")\n</pre> retriever.invoke(\"What is instruction tuning?\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom google.colab import userdata\nos.environ[\"OPENAI_API_KEY\"] = userdata.get('openai')\n</pre> import os from google.colab import userdata os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai') In\u00a0[\u00a0]: Copied! <pre>from langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_template(\n    \"\"\"Answer the following question based only on the provided context:\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\nQuestion: {input}\"\"\"\n)\n\nllm = ChatOpenAI()\n\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\n\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\n</pre> from langchain.chains import create_retrieval_chain from langchain.chains.combine_documents import create_stuff_documents_chain from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI  prompt = ChatPromptTemplate.from_template(     \"\"\"Answer the following question based only on the provided context:   {context}   Question: {input}\"\"\" )  llm = ChatOpenAI()  document_chain = create_stuff_documents_chain(llm, prompt)   retrieval_chain = create_retrieval_chain(retriever, document_chain) In\u00a0[\u00a0]: Copied! <pre>retrieval_chain.invoke({\"input\": \"What is instruction tuning?\"})\n</pre> retrieval_chain.invoke({\"input\": \"What is instruction tuning?\"}) In\u00a0[\u00a0]: Copied! <pre>response = retrieval_chain.invoke({\"input\": \"What is instruction tuning?\"})\n</pre> response = retrieval_chain.invoke({\"input\": \"What is instruction tuning?\"}) In\u00a0[\u00a0]: Copied! <pre>response[\"answer\"]\n</pre> response[\"answer\"] In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q llama-index\n!pip install -q llama-hub\n!pip install -q llama-index-core\n!pip install -q llama-index-llms-openai\n</pre> !pip install -q llama-index !pip install -q llama-hub !pip install -q llama-index-core !pip install -q llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\nreader = SimpleDirectoryReader(input_files=[\"Orca_paper.pdf\"])\ndocs = reader.load_data()\n</pre> from llama_index.core import VectorStoreIndex, SimpleDirectoryReader  reader = SimpleDirectoryReader(input_files=[\"Orca_paper.pdf\"]) docs = reader.load_data() In\u00a0[\u00a0]: Copied! <pre># docs\n</pre> # docs In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.llama_pack import download_llama_pack\n\n# download and install dependencies\nRAGatouilleRetrieverPack = download_llama_pack(\n    \"RAGatouilleRetrieverPack\", \"./ragatouille_pack\"\n)\n</pre> from llama_index.core.llama_pack import download_llama_pack  # download and install dependencies RAGatouilleRetrieverPack = download_llama_pack(     \"RAGatouilleRetrieverPack\", \"./ragatouille_pack\" ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\n</pre> from llama_index.llms.openai import OpenAI In\u00a0[\u00a0]: Copied! <pre># create the pack\nragatouille_pack = RAGatouilleRetrieverPack(\n    docs,  # List[Document]\n    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n    index_name=\"orca_paper\",\n    top_k=5,\n)\n</pre> # create the pack ragatouille_pack = RAGatouilleRetrieverPack(     docs,  # List[Document]     llm=OpenAI(model=\"gpt-3.5-turbo\"),     index_name=\"orca_paper\",     top_k=5, ) In\u00a0[\u00a0]: Copied! <pre>response = ragatouille_pack.run(\"What is instruction tuning? \")\n</pre> response = ragatouille_pack.run(\"What is instruction tuning? \")  In\u00a0[\u00a0]: Copied! <pre>response\n</pre> response In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"RAG/10_ColBERT_RAG/ColBert_RAG/#langchain","title":"LangChain\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/ColBert_RAG/#do-retrieval","title":"Do Retrieval\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/ColBert_RAG/#use-as-langchain-retriever","title":"Use as LangChain Retriever\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/ColBert_RAG/#create-a-chain","title":"Create a Chain\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/ColBert_RAG/#llama-index","title":"Llama-Index\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/ragatouille_retriever/","title":"Colbert(Ragatouille Retriever)","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-llms-openai\n%pip install llama-index-packs-ragatouille-retriever\n</pre> %pip install llama-index-llms-openai %pip install llama-index-packs-ragatouille-retriever In\u00a0[\u00a0]: Copied! <pre># Option: if developing with the llama_hub package\nfrom llama_index.packs.ragatouille_retriever import RAGatouilleRetrieverPack\n\n# Option: download_llama_pack\nfrom llama_index.core.llama_pack import download_llama_pack\n\n# RAGatouilleRetrieverPack = download_llama_pack(\n#     \"RAGatouilleRetrieverPack\",\n#     \"./ragatouille_pack\",\n#     skip_load=True,\n#     # leave the below line commented out if using the notebook on main\n#     # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llm_compiler_pack/llama_hub\"\n# )\n</pre> # Option: if developing with the llama_hub package from llama_index.packs.ragatouille_retriever import RAGatouilleRetrieverPack  # Option: download_llama_pack from llama_index.core.llama_pack import download_llama_pack  # RAGatouilleRetrieverPack = download_llama_pack( #     \"RAGatouilleRetrieverPack\", #     \"./ragatouille_pack\", #     skip_load=True, #     # leave the below line commented out if using the notebook on main #     # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llm_compiler_pack/llama_hub\" # ) In\u00a0[\u00a0]: Copied! <pre>!wget \"https://arxiv.org/pdf/2004.12832.pdf\" -O colbertv1.pdf\n</pre> !wget \"https://arxiv.org/pdf/2004.12832.pdf\" -O colbertv1.pdf <pre>--2024-01-04 16:02:16--  https://arxiv.org/pdf/2004.12832.pdf\nResolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\nConnecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4918165 (4.7M) [application/pdf]\nSaving to: \u2018colbertv1.pdf\u2019\n\ncolbertv1.pdf       100%[===================&gt;]   4.69M  --.-KB/s    in 0.1s    \n\n2024-01-04 16:02:16 (34.6 MB/s) - \u2018colbertv1.pdf\u2019 saved [4918165/4918165]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import SimpleDirectoryReader\nfrom llama_index.llms.openai import OpenAI\n\nreader = SimpleDirectoryReader(input_files=[\"colbertv1.pdf\"])\ndocs = reader.load_data()\n</pre> from llama_index.core import SimpleDirectoryReader from llama_index.llms.openai import OpenAI  reader = SimpleDirectoryReader(input_files=[\"colbertv1.pdf\"]) docs = reader.load_data() In\u00a0[\u00a0]: Copied! <pre>index_name = \"my_index\"\nragatouille_pack = RAGatouilleRetrieverPack(\n    docs, llm=OpenAI(model=\"gpt-3.5-turbo\"), index_name=index_name, top_k=5\n)\n</pre> index_name = \"my_index\" ragatouille_pack = RAGatouilleRetrieverPack(     docs, llm=OpenAI(model=\"gpt-3.5-turbo\"), index_name=index_name, top_k=5 ) <pre>/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>\n\n[Jan 04, 16:02:19] #&gt; Note: Output directory .ragatouille/colbert/indexes/my_index already exists\n\n\n[Jan 04, 16:02:19] #&gt; Will delete 10 files already at .ragatouille/colbert/indexes/my_index in 20 seconds...\n#&gt; Starting...\n[Jan 04, 16:02:42] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n</pre> <pre>/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n  0%|          | 0/2 [00:00&lt;?, ?it/s]/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n</pre> <pre>[Jan 04, 16:02:43] [0] \t\t #&gt; Encoding 90 passages..\n</pre> <pre> 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:03&lt;00:03,  3.87s/it]/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:05&lt;00:00,  2.64s/it]\nWARNING clustering 14894 points to 1024 centroids: please provide at least 39936 training points\n</pre> <pre>[Jan 04, 16:02:48] [0] \t\t avg_doclen_est = 174.1888885498047 \t len(local_sample) = 90\n[Jan 04, 16:02:48] [0] \t\t Creating 1,024 partitions.\n[Jan 04, 16:02:48] [0] \t\t *Estimated* 15,676 embeddings.\n[Jan 04, 16:02:48] [0] \t\t #&gt; Saving the indexing plan to .ragatouille/colbert/indexes/my_index/plan.json ..\nClustering 14894 points in 128D to 1024 clusters, redo 1 times, 20 iterations\n  Preprocessing in 0.00 s\n[0.037, 0.037, 0.033, 0.033, 0.033, 0.035, 0.035, 0.035, 0.032, 0.036, 0.032, 0.031, 0.035, 0.036, 0.035, 0.036, 0.034, 0.037, 0.033, 0.034, 0.036, 0.036, 0.035, 0.035, 0.033, 0.036, 0.036, 0.033, 0.037, 0.035, 0.035, 0.037, 0.036, 0.033, 0.037, 0.031, 0.035, 0.036, 0.035, 0.042, 0.037, 0.037, 0.037, 0.036, 0.036, 0.033, 0.034, 0.037, 0.036, 0.032, 0.034, 0.036, 0.038, 0.038, 0.035, 0.034, 0.039, 0.035, 0.036, 0.034, 0.035, 0.038, 0.035, 0.037, 0.035, 0.036, 0.04, 0.033, 0.034, 0.034, 0.038, 0.034, 0.038, 0.036, 0.038, 0.035, 0.037, 0.04, 0.036, 0.04, 0.037, 0.037, 0.037, 0.037, 0.034, 0.036, 0.034, 0.037, 0.032, 0.039, 0.037, 0.036, 0.034, 0.038, 0.035, 0.033, 0.039, 0.036, 0.035, 0.035, 0.039, 0.038, 0.034, 0.035, 0.037, 0.033, 0.033, 0.031, 0.035, 0.035, 0.035, 0.038, 0.036, 0.033, 0.035, 0.035, 0.038, 0.035, 0.035, 0.036, 0.036, 0.039, 0.036, 0.039, 0.034, 0.038, 0.038, 0.034]\n[Jan 04, 16:02:48] [0] \t\t #&gt; Encoding 90 passages..\n</pre> <pre>0it [00:00, ?it/s]\n  0%|          | 0/2 [00:00&lt;?, ?it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:03&lt;00:03,  3.32s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04&lt;00:00,  2.34s/it]\n1it [00:04,  4.72s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 5322.72it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1024/1024 [00:00&lt;00:00, 331171.82it/s]\n</pre> <pre>[Jan 04, 16:02:53] #&gt; Optimizing IVF to store map from centroids to list of pids..\n[Jan 04, 16:02:53] #&gt; Building the emb2pid mapping..\n[Jan 04, 16:02:53] len(emb2pid) = 15677\n[Jan 04, 16:02:53] #&gt; Saved optimized IVF to .ragatouille/colbert/indexes/my_index/ivf.pid.pt\n\n#&gt; Joined...\nDone indexing!\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.response.notebook_utils import display_source_node\n\nretriever = ragatouille_pack.get_modules()[\"retriever\"]\nnodes = retriever.retrieve(\"How does ColBERTv2 compare with other BERT models?\")\n\nfor node in nodes:\n    display_source_node(node)\n</pre> from llama_index.core.response.notebook_utils import display_source_node  retriever = ragatouille_pack.get_modules()[\"retriever\"] nodes = retriever.retrieve(\"How does ColBERTv2 compare with other BERT models?\")  for node in nodes:     display_source_node(node) <pre>New index_name received! Updating current index_name (my_index) to my_index\nLoading searcher for index my_index for the first time... This may take a few seconds\n[Jan 04, 16:02:55] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n[Jan 04, 16:02:56] #&gt; Loading codec...\n[Jan 04, 16:02:56] #&gt; Loading IVF...\n[Jan 04, 16:02:56] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n</pre> <pre>/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n</pre> <pre>[Jan 04, 16:02:56] #&gt; Loading doclens...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 5555.37it/s]</pre> <pre>[Jan 04, 16:02:56] #&gt; Loading codes and residuals...\n</pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 521.10it/s]</pre> <pre>[Jan 04, 16:02:56] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n</pre> <pre>\n</pre> <pre>[Jan 04, 16:02:56] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\nSearcher loaded!\n\n#&gt; QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n#&gt; Input: . How does ColBERTv2 compare with SPLADEv2?, \t\t True, \t\t None\n#&gt; Output IDs: torch.Size([32]), tensor([  101,     1,  2129,  2515, 23928,  2615,  2475, 12826,  2007, 11867,\n        27266,  6777,  2475,  1029,   102,   103,   103,   103,   103,   103,\n          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n          103,   103])\n#&gt; Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n\n</pre> <pre>/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n</pre> <p>Node ID: 5e4028f7-fbb5-4440-abd0-0d8270cc8979Similarity: 17.003997802734375Text: While highly competitive in e\ufb00ec- tiveness, ColBERT is orders of magnitude cheaper than BERT base...</p> <p>Node ID: d6240a29-0a5e-458f-86f1-abe570e13200Similarity: 16.764663696289062Text: Note that any BERT-based model must incur the computational cost of processing each document at l...</p> <p>Node ID: d19c0fe7-bdb7-4a51-ae89-00cd746b2d3aSimilarity: 16.70589828491211Text: For instance, its Recall@50 actually exceeds the o\ufb03cial BM25\u2019s Recall@1000 and even all but docTT...</p> <p>Node ID: 38e84e5b-4345-4b08-a7fd-de2de4fa645aSimilarity: 16.577777862548828Text: /T_his layer serves to control the dimension of ColBERT\u2019s embeddings, producing m-dimensional emb...</p> <p>Node ID: c82df506-412a-40c2-baf3-df51ab43e434Similarity: 16.252092361450195Text: For instance, at k=10, BERT requires nearly 180\u0002more FLOPs than ColBERT; at k=1000, BERT\u2019s overhe...</p> In\u00a0[\u00a0]: Copied! <pre># try out the RAG module directly\nRAG = ragatouille_pack.get_modules()[\"RAG\"]\nresults = RAG.search(\n    \"How does ColBERTv2 compare with other BERT models?\", index_name=index_name, k=4\n)\nresults\n</pre> # try out the RAG module directly RAG = ragatouille_pack.get_modules()[\"RAG\"] results = RAG.search(     \"How does ColBERTv2 compare with other BERT models?\", index_name=index_name, k=4 ) results <pre>/Users/jerryliu/Programming/llama-hub/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n</pre> Out[\u00a0]: <pre>[{'content': 'While highly competitive in e\ufb00ec-\\ntiveness, ColBERT is orders of magnitude cheaper than BERT base,\\nin particular, by over 170 \\x02in latency and 13,900 \\x02in FLOPs. /T_his\\nhighlights the expressiveness of our proposed late interaction mech-\\nanism, particularly when coupled with a powerful pre-trained LM\\nlike BERT. While ColBERT\u2019s re-ranking latency is slightly higher\\nthan the non-BERT re-ranking models shown (i.e., by 10s of mil-\\nliseconds), this di\ufb00erence is explained by the time it takes to gather,\\nstack, and transfer the document embeddings to the GPU. In partic-\\nular, the query encoding and interaction in ColBERT consume only\\n13 milliseconds of its total execution time. We note that ColBERT\u2019s\\nlatency and FLOPs can be considerably reduced by padding queries\\nto a shorter length, using smaller vector dimensions (the MRR@10\\nof which is tested in \u00a74.5), employing quantization of the document\\n6h/t_tps://github.com/mit-han-lab/torchpro/f_ile',\n  'score': 17.003997802734375,\n  'rank': 1},\n {'content': 'Note that any BERT-based model\\nmust incur the computational cost of processing each document\\nat least once. While ColBERT encodes each document with BERT\\nexactly once, existing BERT-based rankers would repeat similar\\ncomputations on possibly hundreds of documents for each query.\\nSe/t_ting Dimension( m) Bytes/Dim Space(GiBs) MRR@10\\nRe-rank Cosine 128 4 286 34.9\\nEnd-to-end L2 128 2 154 36.0\\nRe-rank L2 128 2 143 34.8\\nRe-rank Cosine 48 4 54 34.4\\nRe-rank Cosine 24 2 27 33.9\\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\\nTable 4 reports the space footprint of ColBERT under various\\nse/t_tings as we reduce the embeddings dimension and/or the bytes\\nper dimension.',\n  'score': 16.764663696289062,\n  'rank': 2},\n {'content': 'For instance,\\nits Recall@50 actually exceeds the o\ufb03cial BM25\u2019s Recall@1000 and\\neven all but docTTTTTquery\u2019s Recall@200, emphasizing the value\\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\\n4.4 Ablation Studies\\n0.220.240.260.280.300.320.340.36\\nMRR@10BERT [CLS]-based dot-product (5-layer)  [A]\\nColBERT via average similarity (5-layer)  [B]\\nColBERT without query augmentation (5-layer)  [C]\\nColBERT (5-layer)  [D]\\nColBERT (12-layer)  [E]\\nColBERT + e2e retrieval (12-layer)  [F]\\nFigure 5: Ablation results on MS MARCO (Dev). Between\\nbrackets is the number of BERT layers used in each model.\\n/T_he results from \u00a74.2 indicate that ColBERT is highly e\ufb00ective\\ndespite the low cost and simplicity of its late interaction mechanism.',\n  'score': 16.70589828491211,\n  'rank': 3},\n {'content': '/T_his layer serves to control the dimension\\nof ColBERT\u2019s embeddings, producing m-dimensional embeddings\\nfor the layer\u2019s output size m. As we discuss later in more detail,\\nwe typically /f_ix mto be much smaller than BERT\u2019s /f_ixed hidden\\ndimension.\\nWhile ColBERT\u2019s embedding dimension has limited impact on\\nthe e\ufb03ciency of query encoding, this step is crucial for controlling\\nthe space footprint of documents, as we show in \u00a74.5. In addition, it\\ncan have a signi/f_icant impact on query execution time, particularly\\nthe time taken for transferring the document representations onto\\nthe GPU from system memory (where they reside before processing\\na query). In fact, as we show in \u00a74.2, gathering, stacking, and\\ntransferring the embeddings from CPU to GPU can be the most\\nexpensive step in re-ranking with ColBERT. Finally, the output\\nembeddings are normalized so each has L2 norm equal to one.\\n/T_he result is that the dot-product of any two embeddings becomes\\nequivalent to their cosine similarity, falling in the \u00bb\\x001;1\u00bcrange.\\nDocument Encoder.',\n  'score': 16.577777862548828,\n  'rank': 4}]</pre> In\u00a0[\u00a0]: Copied! <pre># run pack e2e, which includes the full query engine with OpenAI LLMs\nresponse = ragatouille_pack.run(\"How does ColBERTv2 compare with other BERT models?\")\nprint(str(response))\n</pre> # run pack e2e, which includes the full query engine with OpenAI LLMs response = ragatouille_pack.run(\"How does ColBERTv2 compare with other BERT models?\") print(str(response)) <pre>ColBERTv2, which employs late interaction over BERT base, performs no worse than the original adaptation of BERT base for ranking. It is only marginally less effective than BERT large and our training of BERT base. While highly competitive in effectiveness, ColBERTv2 is orders of magnitude cheaper than BERT base, particularly in terms of latency and FLOPs.\n</pre>"},{"location":"RAG/10_ColBERT_RAG/ragatouille_retriever/#ragatouille-retriever-llama-pack","title":"RAGatouille Retriever Llama Pack\u00b6","text":"<p>RAGatouille is a cool library that lets you use e.g. ColBERT and other SOTA retrieval models in your RAG pipeline. You can use it to either run inference on ColBERT, or use it to train/fine-tune models.</p> <p>This LlamaPack shows you an easy way to bundle RAGatouille into your RAG pipeline. We use RAGatouille to index a corpus of documents (by default using colbertv2.0), and then we combine it with LlamaIndex query modules to synthesize an answer with an LLM.</p>"},{"location":"RAG/10_ColBERT_RAG/ragatouille_retriever/#load-documents","title":"Load Documents\u00b6","text":"<p>Here we load the ColBERTv2 paper: https://arxiv.org/pdf/2112.01488.pdf.</p>"},{"location":"RAG/10_ColBERT_RAG/ragatouille_retriever/#create-pack","title":"Create Pack\u00b6","text":""},{"location":"RAG/10_ColBERT_RAG/ragatouille_retriever/#try-out-pack","title":"Try out Pack\u00b6","text":"<p>We try out both the individual modules in the pack as well as running it e2e!</p>"},{"location":"RAG/11_Graph_RAG/","title":"Index","text":"<pre><code>graph TD\n    subgraph \"Document Preprocessing\"\n        A[Source Documents] --&gt; B[Document Chunking]\n        B --&gt; C[Entity and Relationship Extraction]\n        C --&gt; D[Element Summarization]\n    end\n\n    subgraph \"Graph Construction\"\n        D --&gt; E[Graph Creation]\n        E --&gt; F[Community Detection]\n        F --&gt; G[Community Summarization]\n    end\n\n    subgraph \"Embedding and Indexing\"\n        G --&gt; H[Generate Embeddings]\n        H --&gt; I[Populate Vector Store]\n    end\n\n    subgraph \"Query Processing\"\n        J[User Query] --&gt; K[Query Analysis]\n        K --&gt; L[Query Embedding]\n    end\n\n    subgraph \"Multi-level Retrieval\"\n        L --&gt; M[Community Retrieval]\n        M --&gt; N[In-Community Retrieval]\n        N --&gt; O[Chunk/Entity/Relationship Retrieval]\n    end\n\n    subgraph \"Context Formation and Generation\"\n        O --&gt; P[Context Assembly]\n        J --&gt; P\n        P --&gt; Q[LLM-based Answer Generation]\n    end\n\n    subgraph \"Iterative Refinement\"\n        Q --&gt; R{Satisfactory Answer?}\n        R --&gt;|No| S[Refine Retrieval]\n        S --&gt; M\n        R --&gt;|Yes| T[Final Answer]\n    end\n\n    I -.-&gt; M\n    I -.-&gt; N\n    I -.-&gt; O</code></pre>"},{"location":"RAG/11_Graph_RAG/#graphrag-enhancing-retrieval-augmented-generation-with-graph-based-approaches","title":"GraphRAG: Enhancing Retrieval Augmented Generation with Graph-based Approaches","text":""},{"location":"RAG/11_Graph_RAG/#introduction","title":"Introduction","text":"<p>GraphRAG (Graph-based Retrieval Augmented Generation) is an advanced approach to information retrieval and generation that combines the strengths of graph-based data structures with the power of large language models (LLMs). This method aims to overcome limitations of traditional RAG systems by incorporating relational information and hierarchical structures into the retrieval and generation process.</p>"},{"location":"RAG/11_Graph_RAG/#motivation","title":"Motivation","text":"<p>While traditional RAG systems excel at retrieving relevant information for specific queries, they often struggle with: 1. Understanding complex relationships between different pieces of information 2. Handling queries that require a broader context or thematic understanding 3. Efficiently processing and retrieving information from large, diverse datasets</p> <p>GraphRAG addresses these challenges by leveraging graph structures to represent and navigate information, enabling more nuanced and context-aware responses to user queries.</p>"},{"location":"RAG/11_Graph_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/11_Graph_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Document Ingestion: Source documents are processed and divided into smaller, manageable chunks.</li> <li>Entity and Relationship Extraction: Each chunk is analyzed to identify entities and relationships between them.</li> <li>Element Summarization: Extracted entities and relationships are summarized into descriptive text blocks.</li> <li>Graph Construction: A graph is created using the entities as nodes and relationships as edges.</li> <li>Community Detection: The graph is partitioned into communities using algorithms like Hierarchical Leiden.</li> <li>Community Summarization: Each community is summarized to capture its main themes and content.</li> <li>Embedding Generation: Embeddings are created for chunks, entities, relationships, and community summaries.</li> <li>Vector Store Population: All embeddings are stored in a vector database for efficient retrieval.</li> </ol>"},{"location":"RAG/11_Graph_RAG/#retrieval-augmented-generation-workflow","title":"Retrieval-Augmented Generation Workflow","text":"<ol> <li>Query Analysis: The user's query is analyzed to identify key entities and themes.</li> <li>Multi-level Retrieval:     a. Relevant communities are retrieved based on the query.    b. Within these communities, specific chunks, entities, and relationships are retrieved.</li> <li>Context Formation: Retrieved information is assembled into a coherent context.</li> <li>Answer Generation: The LLM generates an answer using the formed context and the original query.</li> <li>Iterative Refinement: If needed, the system can perform multiple retrieval-generation cycles to refine the answer.</li> </ol>"},{"location":"RAG/11_Graph_RAG/#key-features-of-graphrag","title":"Key Features of GraphRAG","text":"<ol> <li>Hierarchical Information Representation: Allows for both broad and specific information retrieval.</li> <li>Relational Context: Captures and utilizes relationships between different pieces of information.</li> <li>Scalability: Efficiently handles large and diverse datasets through community-based organization.</li> <li>Flexible Querying: Supports both specific fact-finding and broader thematic inquiries.</li> <li>Explainable Retrieval: The graph structure provides a clear path of how information was retrieved and connected.</li> </ol>"},{"location":"RAG/11_Graph_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Context Understanding: By leveraging graph structures, GraphRAG can provide more contextually relevant answers.</li> <li>Enhanced Thematic Awareness: Community-based organization allows for better handling of broad, theme-based queries.</li> <li>Reduced Hallucination: The structured approach to information retrieval helps in grounding the LLM's responses in factual data.</li> <li>Scalability: Can efficiently handle larger and more diverse datasets compared to traditional RAG systems.</li> <li>Flexibility: Adapts well to various types of queries, from specific fact-checking to open-ended exploration.</li> </ol>"},{"location":"RAG/11_Graph_RAG/#conclusion","title":"Conclusion","text":"<p>GraphRAG represents a significant advancement in the field of retrieval-augmented generation. By integrating graph-based approaches with traditional RAG techniques, it offers a more nuanced, context-aware, and scalable solution for information retrieval and generation. This approach opens up new possibilities for building more intelligent and responsive AI systems capable of handling complex queries across large and diverse knowledge bases.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/","title":"Graph RAG(Microsoft)","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index graspologic numpy==1.24.4 scipy==1.12.0\n</pre> !pip install llama-index graspologic numpy==1.24.4 scipy==1.12.0 In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom llama_index.core import Document\n\nnews = pd.read_csv(\n    \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\"\n)[:50]\n\nnews.head()\n</pre> import pandas as pd from llama_index.core import Document  news = pd.read_csv(     \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\" )[:50]  news.head() Out[\u00a0]: title date text 0 Chevron: Best Of Breed 2031-04-06T01:36:32.000000000+00:00 JHVEPhoto Like many companies in the O&amp;G secto... 1 FirstEnergy (NYSE:FE) Posts Earnings Results 2030-04-29T06:55:28.000000000+00:00 FirstEnergy (NYSE:FE \u2013 Get Rating) posted its ... 2 D\u00e1il almost suspended after Sinn F\u00e9in TD put p... 2023-06-15T14:32:11.000000000+00:00 The D\u00e1il was almost suspended on Thursday afte... 3 Epic\u2019s latest tool can animate hyperrealistic ... 2023-06-15T14:00:00.000000000+00:00 Today, Epic is releasing a new tool designed t... 4 EU to Ban Huawei, ZTE from Internal Commission... 2023-06-15T13:50:00.000000000+00:00 The European Commission is planning to ban equ... <p>Prepare documents as required by LlamaIndex</p> In\u00a0[\u00a0]: Copied! <pre>documents = [\n    Document(text=f\"{row['title']}: {row['text']}\")\n    for i, row in news.iterrows()\n]\n</pre> documents = [     Document(text=f\"{row['title']}: {row['text']}\")     for i, row in news.iterrows() ] In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-4\")\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  from llama_index.llms.openai import OpenAI  llm = OpenAI(model=\"gpt-4\") In\u00a0[\u00a0]: Copied! <pre>import asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom typing import Any, List, Callable, Optional, Union, Dict\nfrom IPython.display import Markdown, display\n\nfrom llama_index.core.async_utils import run_jobs\nfrom llama_index.core.indices.property_graph.utils import (\n    default_parse_triplets_fn,\n)\nfrom llama_index.core.graph_stores.types import (\n    EntityNode,\n    KG_NODES_KEY,\n    KG_RELATIONS_KEY,\n    Relation,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n)\nfrom llama_index.core.schema import TransformComponent, BaseNode\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\n\n\nclass GraphRAGExtractor(TransformComponent):\n    \"\"\"Extract triples from a graph.\n\n    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n\n    Args:\n        llm (LLM):\n            The language model to use.\n        extract_prompt (Union[str, PromptTemplate]):\n            The prompt to use for extracting triples.\n        parse_fn (callable):\n            A function to parse the output of the language model.\n        num_workers (int):\n            The number of workers to use for parallel processing.\n        max_paths_per_chunk (int):\n            The maximum number of paths to extract per chunk.\n    \"\"\"\n\n    llm: LLM\n    extract_prompt: PromptTemplate\n    parse_fn: Callable\n    num_workers: int\n    max_paths_per_chunk: int\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n        parse_fn: Callable = default_parse_triplets_fn,\n        max_paths_per_chunk: int = 10,\n        num_workers: int = 4,\n    ) -&gt; None:\n        \"\"\"Init params.\"\"\"\n        from llama_index.core import Settings\n\n        if isinstance(extract_prompt, str):\n            extract_prompt = PromptTemplate(extract_prompt)\n\n        super().__init__(\n            llm=llm or Settings.llm,\n            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n            parse_fn=parse_fn,\n            num_workers=num_workers,\n            max_paths_per_chunk=max_paths_per_chunk,\n        )\n\n    @classmethod\n    def class_name(cls) -&gt; str:\n        return \"GraphExtractor\"\n\n    def __call__(\n        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -&gt; List[BaseNode]:\n        \"\"\"Extract triples from nodes.\"\"\"\n        return asyncio.run(\n            self.acall(nodes, show_progress=show_progress, **kwargs)\n        )\n\n    async def _aextract(self, node: BaseNode) -&gt; BaseNode:\n        \"\"\"Extract triples from a node.\"\"\"\n        assert hasattr(node, \"text\")\n\n        text = node.get_content(metadata_mode=\"llm\")\n        try:\n            llm_response = await self.llm.apredict(\n                self.extract_prompt,\n                text=text,\n                max_knowledge_triplets=self.max_paths_per_chunk,\n            )\n            entities, entities_relationship = self.parse_fn(llm_response)\n        except ValueError:\n            entities = []\n            entities_relationship = []\n\n        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n        metadata = node.metadata.copy()\n        for entity, entity_type, description in entities:\n            metadata[\n                \"entity_description\"\n            ] = description  # Not used in the current implementation. But will be useful in future work.\n            entity_node = EntityNode(\n                name=entity, label=entity_type, properties=metadata\n            )\n            existing_nodes.append(entity_node)\n\n        metadata = node.metadata.copy()\n        for triple in entities_relationship:\n            subj, rel, obj, description = triple\n            subj_node = EntityNode(name=subj, properties=metadata)\n            obj_node = EntityNode(name=obj, properties=metadata)\n            metadata[\"relationship_description\"] = description\n            rel_node = Relation(\n                label=rel,\n                source_id=subj_node.id,\n                target_id=obj_node.id,\n                properties=metadata,\n            )\n\n            existing_nodes.extend([subj_node, obj_node])\n            existing_relations.append(rel_node)\n\n        node.metadata[KG_NODES_KEY] = existing_nodes\n        node.metadata[KG_RELATIONS_KEY] = existing_relations\n        return node\n\n    async def acall(\n        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -&gt; List[BaseNode]:\n        \"\"\"Extract triples from nodes async.\"\"\"\n        jobs = []\n        for node in nodes:\n            jobs.append(self._aextract(node))\n\n        return await run_jobs(\n            jobs,\n            workers=self.num_workers,\n            show_progress=show_progress,\n            desc=\"Extracting paths from text\",\n        )\n</pre> import asyncio import nest_asyncio  nest_asyncio.apply()  from typing import Any, List, Callable, Optional, Union, Dict from IPython.display import Markdown, display  from llama_index.core.async_utils import run_jobs from llama_index.core.indices.property_graph.utils import (     default_parse_triplets_fn, ) from llama_index.core.graph_stores.types import (     EntityNode,     KG_NODES_KEY,     KG_RELATIONS_KEY,     Relation, ) from llama_index.core.llms.llm import LLM from llama_index.core.prompts import PromptTemplate from llama_index.core.prompts.default_prompts import (     DEFAULT_KG_TRIPLET_EXTRACT_PROMPT, ) from llama_index.core.schema import TransformComponent, BaseNode from llama_index.core.bridge.pydantic import BaseModel, Field   class GraphRAGExtractor(TransformComponent):     \"\"\"Extract triples from a graph.      Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.      Args:         llm (LLM):             The language model to use.         extract_prompt (Union[str, PromptTemplate]):             The prompt to use for extracting triples.         parse_fn (callable):             A function to parse the output of the language model.         num_workers (int):             The number of workers to use for parallel processing.         max_paths_per_chunk (int):             The maximum number of paths to extract per chunk.     \"\"\"      llm: LLM     extract_prompt: PromptTemplate     parse_fn: Callable     num_workers: int     max_paths_per_chunk: int      def __init__(         self,         llm: Optional[LLM] = None,         extract_prompt: Optional[Union[str, PromptTemplate]] = None,         parse_fn: Callable = default_parse_triplets_fn,         max_paths_per_chunk: int = 10,         num_workers: int = 4,     ) -&gt; None:         \"\"\"Init params.\"\"\"         from llama_index.core import Settings          if isinstance(extract_prompt, str):             extract_prompt = PromptTemplate(extract_prompt)          super().__init__(             llm=llm or Settings.llm,             extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,             parse_fn=parse_fn,             num_workers=num_workers,             max_paths_per_chunk=max_paths_per_chunk,         )      @classmethod     def class_name(cls) -&gt; str:         return \"GraphExtractor\"      def __call__(         self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any     ) -&gt; List[BaseNode]:         \"\"\"Extract triples from nodes.\"\"\"         return asyncio.run(             self.acall(nodes, show_progress=show_progress, **kwargs)         )      async def _aextract(self, node: BaseNode) -&gt; BaseNode:         \"\"\"Extract triples from a node.\"\"\"         assert hasattr(node, \"text\")          text = node.get_content(metadata_mode=\"llm\")         try:             llm_response = await self.llm.apredict(                 self.extract_prompt,                 text=text,                 max_knowledge_triplets=self.max_paths_per_chunk,             )             entities, entities_relationship = self.parse_fn(llm_response)         except ValueError:             entities = []             entities_relationship = []          existing_nodes = node.metadata.pop(KG_NODES_KEY, [])         existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])         metadata = node.metadata.copy()         for entity, entity_type, description in entities:             metadata[                 \"entity_description\"             ] = description  # Not used in the current implementation. But will be useful in future work.             entity_node = EntityNode(                 name=entity, label=entity_type, properties=metadata             )             existing_nodes.append(entity_node)          metadata = node.metadata.copy()         for triple in entities_relationship:             subj, rel, obj, description = triple             subj_node = EntityNode(name=subj, properties=metadata)             obj_node = EntityNode(name=obj, properties=metadata)             metadata[\"relationship_description\"] = description             rel_node = Relation(                 label=rel,                 source_id=subj_node.id,                 target_id=obj_node.id,                 properties=metadata,             )              existing_nodes.extend([subj_node, obj_node])             existing_relations.append(rel_node)          node.metadata[KG_NODES_KEY] = existing_nodes         node.metadata[KG_RELATIONS_KEY] = existing_relations         return node      async def acall(         self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any     ) -&gt; List[BaseNode]:         \"\"\"Extract triples from nodes async.\"\"\"         jobs = []         for node in nodes:             jobs.append(self._aextract(node))          return await run_jobs(             jobs,             workers=self.num_workers,             show_progress=show_progress,             desc=\"Extracting paths from text\",         ) In\u00a0[\u00a0]: Copied! <pre>import re\nfrom llama_index.core.graph_stores import SimplePropertyGraphStore\nimport networkx as nx\nfrom graspologic.partition import hierarchical_leiden\n\nfrom llama_index.core.llms import ChatMessage\n\n\nclass GraphRAGStore(SimplePropertyGraphStore):\n    community_summary = {}\n    max_cluster_size = 5\n\n    def generate_community_summary(self, text):\n        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n        messages = [\n            ChatMessage(\n                role=\"system\",\n                content=(\n                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n                    \"entity1-&gt;entity2-&gt;relation-&gt;relationship_description. Your task is to create a summary of these \"\n                    \"relationships. The summary should include the names of the entities involved and a concise synthesis \"\n                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n                ),\n            ),\n            ChatMessage(role=\"user\", content=text),\n        ]\n        response = OpenAI().chat(messages)\n        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n        return clean_response\n\n    def build_communities(self):\n        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n        nx_graph = self._create_nx_graph()\n        community_hierarchical_clusters = hierarchical_leiden(\n            nx_graph, max_cluster_size=self.max_cluster_size\n        )\n        community_info = self._collect_community_info(\n            nx_graph, community_hierarchical_clusters\n        )\n        self._summarize_communities(community_info)\n\n    def _create_nx_graph(self):\n        \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"\n        nx_graph = nx.Graph()\n        for node in self.graph.nodes.values():\n            nx_graph.add_node(str(node))\n        for relation in self.graph.relations.values():\n            nx_graph.add_edge(\n                relation.source_id,\n                relation.target_id,\n                relationship=relation.label,\n                description=relation.properties[\"relationship_description\"],\n            )\n        return nx_graph\n\n    def _collect_community_info(self, nx_graph, clusters):\n        \"\"\"Collect detailed information for each node based on their community.\"\"\"\n        community_mapping = {item.node: item.cluster for item in clusters}\n        community_info = {}\n        for item in clusters:\n            cluster_id = item.cluster\n            node = item.node\n            if cluster_id not in community_info:\n                community_info[cluster_id] = []\n\n            for neighbor in nx_graph.neighbors(node):\n                if community_mapping[neighbor] == cluster_id:\n                    edge_data = nx_graph.get_edge_data(node, neighbor)\n                    if edge_data:\n                        detail = f\"{node} -&gt; {neighbor} -&gt; {edge_data['relationship']} -&gt; {edge_data['description']}\"\n                        community_info[cluster_id].append(detail)\n        return community_info\n\n    def _summarize_communities(self, community_info):\n        \"\"\"Generate and store summaries for each community.\"\"\"\n        for community_id, details in community_info.items():\n            details_text = (\n                \"\\n\".join(details) + \".\"\n            )  # Ensure it ends with a period\n            self.community_summary[\n                community_id\n            ] = self.generate_community_summary(details_text)\n\n    def get_community_summaries(self):\n        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n        if not self.community_summary:\n            self.build_communities()\n        return self.community_summary\n</pre> import re from llama_index.core.graph_stores import SimplePropertyGraphStore import networkx as nx from graspologic.partition import hierarchical_leiden  from llama_index.core.llms import ChatMessage   class GraphRAGStore(SimplePropertyGraphStore):     community_summary = {}     max_cluster_size = 5      def generate_community_summary(self, text):         \"\"\"Generate summary for a given text using an LLM.\"\"\"         messages = [             ChatMessage(                 role=\"system\",                 content=(                     \"You are provided with a set of relationships from a knowledge graph, each represented as \"                     \"entity1-&gt;entity2-&gt;relation-&gt;relationship_description. Your task is to create a summary of these \"                     \"relationships. The summary should include the names of the entities involved and a concise synthesis \"                     \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"                     \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"                     \"integrates the information in a way that emphasizes the key aspects of the relationships.\"                 ),             ),             ChatMessage(role=\"user\", content=text),         ]         response = OpenAI().chat(messages)         clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()         return clean_response      def build_communities(self):         \"\"\"Builds communities from the graph and summarizes them.\"\"\"         nx_graph = self._create_nx_graph()         community_hierarchical_clusters = hierarchical_leiden(             nx_graph, max_cluster_size=self.max_cluster_size         )         community_info = self._collect_community_info(             nx_graph, community_hierarchical_clusters         )         self._summarize_communities(community_info)      def _create_nx_graph(self):         \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"         nx_graph = nx.Graph()         for node in self.graph.nodes.values():             nx_graph.add_node(str(node))         for relation in self.graph.relations.values():             nx_graph.add_edge(                 relation.source_id,                 relation.target_id,                 relationship=relation.label,                 description=relation.properties[\"relationship_description\"],             )         return nx_graph      def _collect_community_info(self, nx_graph, clusters):         \"\"\"Collect detailed information for each node based on their community.\"\"\"         community_mapping = {item.node: item.cluster for item in clusters}         community_info = {}         for item in clusters:             cluster_id = item.cluster             node = item.node             if cluster_id not in community_info:                 community_info[cluster_id] = []              for neighbor in nx_graph.neighbors(node):                 if community_mapping[neighbor] == cluster_id:                     edge_data = nx_graph.get_edge_data(node, neighbor)                     if edge_data:                         detail = f\"{node} -&gt; {neighbor} -&gt; {edge_data['relationship']} -&gt; {edge_data['description']}\"                         community_info[cluster_id].append(detail)         return community_info      def _summarize_communities(self, community_info):         \"\"\"Generate and store summaries for each community.\"\"\"         for community_id, details in community_info.items():             details_text = (                 \"\\n\".join(details) + \".\"             )  # Ensure it ends with a period             self.community_summary[                 community_id             ] = self.generate_community_summary(details_text)      def get_community_summaries(self):         \"\"\"Returns the community summaries, building them if not already done.\"\"\"         if not self.community_summary:             self.build_communities()         return self.community_summary <pre>/usr/local/lib/python3.10/dist-packages/graspologic/models/edge_swaps.py:215: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  _edge_swap_numba = nb.jit(_edge_swap, nopython=False)\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.query_engine import CustomQueryEngine\nfrom llama_index.core.llms import LLM\n\n\nclass GraphRAGQueryEngine(CustomQueryEngine):\n    graph_store: GraphRAGStore\n    llm: LLM\n\n    def custom_query(self, query_str: str) -&gt; str:\n        \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"\n        community_summaries = self.graph_store.get_community_summaries()\n        community_answers = [\n            self.generate_answer_from_summary(community_summary, query_str)\n            for _, community_summary in community_summaries.items()\n        ]\n\n        final_answer = self.aggregate_answers(community_answers)\n        return final_answer\n\n    def generate_answer_from_summary(self, community_summary, query):\n        \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"\n        prompt = (\n            f\"Given the community summary: {community_summary}, \"\n            f\"how would you answer the following query? Query: {query}\"\n        )\n        messages = [\n            ChatMessage(role=\"system\", content=prompt),\n            ChatMessage(\n                role=\"user\",\n                content=\"I need an answer based on the above information.\",\n            ),\n        ]\n        response = self.llm.chat(messages)\n        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n        return cleaned_response\n\n    def aggregate_answers(self, community_answers):\n        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n        # intermediate_text = \" \".join(community_answers)\n        prompt = \"Combine the following intermediate answers into a final, concise response.\"\n        messages = [\n            ChatMessage(role=\"system\", content=prompt),\n            ChatMessage(\n                role=\"user\",\n                content=f\"Intermediate answers: {community_answers}\",\n            ),\n        ]\n        final_response = self.llm.chat(messages)\n        cleaned_final_response = re.sub(\n            r\"^assistant:\\s*\", \"\", str(final_response)\n        ).strip()\n        return cleaned_final_response\n</pre> from llama_index.core.query_engine import CustomQueryEngine from llama_index.core.llms import LLM   class GraphRAGQueryEngine(CustomQueryEngine):     graph_store: GraphRAGStore     llm: LLM      def custom_query(self, query_str: str) -&gt; str:         \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"         community_summaries = self.graph_store.get_community_summaries()         community_answers = [             self.generate_answer_from_summary(community_summary, query_str)             for _, community_summary in community_summaries.items()         ]          final_answer = self.aggregate_answers(community_answers)         return final_answer      def generate_answer_from_summary(self, community_summary, query):         \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"         prompt = (             f\"Given the community summary: {community_summary}, \"             f\"how would you answer the following query? Query: {query}\"         )         messages = [             ChatMessage(role=\"system\", content=prompt),             ChatMessage(                 role=\"user\",                 content=\"I need an answer based on the above information.\",             ),         ]         response = self.llm.chat(messages)         cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()         return cleaned_response      def aggregate_answers(self, community_answers):         \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"         # intermediate_text = \" \".join(community_answers)         prompt = \"Combine the following intermediate answers into a final, concise response.\"         messages = [             ChatMessage(role=\"system\", content=prompt),             ChatMessage(                 role=\"user\",                 content=f\"Intermediate answers: {community_answers}\",             ),         ]         final_response = self.llm.chat(messages)         cleaned_final_response = re.sub(             r\"^assistant:\\s*\", \"\", str(final_response)         ).strip()         return cleaned_final_response In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(\n    chunk_size=1024,\n    chunk_overlap=20,\n)\nnodes = splitter.get_nodes_from_documents(documents)\n</pre> from llama_index.core.node_parser import SentenceSplitter  splitter = SentenceSplitter(     chunk_size=1024,     chunk_overlap=20, ) nodes = splitter.get_nodes_from_documents(documents) In\u00a0[\u00a0]: Copied! <pre>len(nodes)\n</pre> len(nodes) Out[\u00a0]: <pre>50</pre> In\u00a0[\u00a0]: Copied! <pre>KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n-Goal-\nGiven a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\nGiven the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Type of the entity\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"$$$$&lt;entity_name&gt;$$$$&lt;entity_type&gt;$$$$&lt;entity_description&gt;)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relation: relationship between source_entity and target_entity\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n\nFormat each relationship as (\"relationship\"$$$$&lt;source_entity&gt;$$$$&lt;target_entity&gt;$$$$&lt;relation&gt;$$$$&lt;relationship_description&gt;)\n\n3. When finished, output.\n\n-Real Data-\n######################\ntext: {text}\n######################\noutput:\"\"\"\n</pre> KG_TRIPLET_EXTRACT_TMPL = \"\"\" -Goal- Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities. Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.  -Steps- 1. Identify all entities. For each identified entity, extract the following information: - entity_name: Name of the entity, capitalized - entity_type: Type of the entity - entity_description: Comprehensive description of the entity's attributes and activities Format each entity as (\"entity\"$$$$$$$$$$$$)  2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other. For each pair of related entities, extract the following information: - source_entity: name of the source entity, as identified in step 1 - target_entity: name of the target entity, as identified in step 1 - relation: relationship between source_entity and target_entity - relationship_description: explanation as to why you think the source entity and the target entity are related to each other  Format each relationship as (\"relationship\"$$$$$$$$$$$$$$$$)  3. When finished, output.  -Real Data- ###################### text: {text} ###################### output:\"\"\" In\u00a0[\u00a0]: Copied! <pre>entity_pattern = r'\\(\"entity\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'\nrelationship_pattern = r'\\(\"relationship\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'\n\n\ndef parse_fn(response_str: str) -&gt; Any:\n    entities = re.findall(entity_pattern, response_str)\n    relationships = re.findall(relationship_pattern, response_str)\n    return entities, relationships\n\n\nkg_extractor = GraphRAGExtractor(\n    llm=llm,\n    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n    max_paths_per_chunk=2,\n    parse_fn=parse_fn,\n)\n</pre> entity_pattern = r'\\(\"entity\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)' relationship_pattern = r'\\(\"relationship\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'   def parse_fn(response_str: str) -&gt; Any:     entities = re.findall(entity_pattern, response_str)     relationships = re.findall(relationship_pattern, response_str)     return entities, relationships   kg_extractor = GraphRAGExtractor(     llm=llm,     extract_prompt=KG_TRIPLET_EXTRACT_TMPL,     max_paths_per_chunk=2,     parse_fn=parse_fn, ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import PropertyGraphIndex\n\nindex = PropertyGraphIndex(\n    nodes=nodes,\n    property_graph_store=GraphRAGStore(),\n    kg_extractors=[kg_extractor],\n    show_progress=True,\n)\n</pre> from llama_index.core import PropertyGraphIndex  index = PropertyGraphIndex(     nodes=nodes,     property_graph_store=GraphRAGStore(),     kg_extractors=[kg_extractor],     show_progress=True, ) <pre>Extracting paths from text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [04:30&lt;00:00,  5.41s/it]\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.24s/it]\nGenerating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00,  4.22it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>list(index.property_graph_store.graph.nodes.values())[-1]\n</pre> list(index.property_graph_store.graph.nodes.values())[-1] Out[\u00a0]: <pre>EntityNode(label='entity', embedding=None, properties={'relationship_description': 'Gett Taxi is a competitor of Uber in the Israeli taxi market.', 'triplet_source_id': 'e4f765e3-fdfd-48d0-92a9-36f75b5865aa'}, name='Competition')</pre> In\u00a0[\u00a0]: Copied! <pre>list(index.property_graph_store.graph.relations.values())[0]\n</pre> list(index.property_graph_store.graph.relations.values())[0] Out[\u00a0]: <pre>Relation(label='O&amp;G sector', source_id='Chevron', target_id='Operates in', properties={'relationship_description': 'Chevron operates in the O&amp;G sector, as evidenced by the text mentioning that it is a company in this industry.', 'triplet_source_id': '6a28dc67-0dc0-486f-8dd6-70a3502f1c8e'})</pre> In\u00a0[\u00a0]: Copied! <pre>list(index.property_graph_store.graph.relations.values())[0].properties[\n    \"relationship_description\"\n]\n</pre> list(index.property_graph_store.graph.relations.values())[0].properties[     \"relationship_description\" ] Out[\u00a0]: <pre>'Chevron operates in the O&amp;G sector, as evidenced by the text mentioning that it is a company in this industry.'</pre> In\u00a0[\u00a0]: Copied! <pre>index.property_graph_store.build_communities()\n</pre> index.property_graph_store.build_communities() In\u00a0[\u00a0]: Copied! <pre>query_engine = GraphRAGQueryEngine(\n    graph_store=index.property_graph_store, llm=llm\n)\n</pre> query_engine = GraphRAGQueryEngine(     graph_store=index.property_graph_store, llm=llm ) In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\n    \"What are the main news discussed in the document?\"\n)\ndisplay(Markdown(f\"{response.response}\"))\n</pre> response = query_engine.query(     \"What are the main news discussed in the document?\" ) display(Markdown(f\"{response.response}\")) <p>The document discusses various news topics across different sectors. In the business sector, it mentions FirstEnergy being a publicly traded company on the New York Stock Exchange and State Street Corporation being listed on the NYSE. It also discusses Coinbase Global Inc.'s repurchase of $64.5 million worth of 0.50% convertible senior notes and the closure of the startup Protonn. In the political sphere, it highlights a theatrical act performed by Sinn F\u00e9in TD John Brady during a debate on retained firefighters. In the tech industry, it discusses the European Commission's actions against ZTE Corp. and TikTok Inc. due to security concerns. In the sports sector, it mentions Manchester United's interest in Harry Kane, the transfer of Jude Bellingham from Borussia Dortmund to Real Madrid, and the negotiation process for Maliek Collins' contract extension with the Houston Texans. In the music industry, it discusses the acquisition of The Hollies' recording catalog by BMG and the distribution pact between ADA Worldwide and Rostrum Records. In the hospitality sector, it mentions the partnership between Supplier.io and Hyatt Hotels. In the energy sector, it discusses the partnership between GE Vernova and Amplus Solar. In the gaming industry, it discusses the creation of the unannounced game \"Star Ocean: The Second Story R\" by Square Enix. In the automotive industry, it mentions the upcoming launch of the Hyundai Exter in India and Stellantis' plans to shut down the Belvidere Assembly Plant. In the airline industry, it discusses Deutsche Bank's decision to upgrade Allegiant Travel's status from Hold to Buy. In the football sector, it discusses the rejected bids made by Arsenal for Rice and the rejected bid received by Chelsea for Mason Mount. In the space industry, it mentions MDA Ltd.'s participation in the Jefferies Virtual Space Summit. In the transportation industry, it discusses Uber's strategic decision to exit the Israeli market and the emergence of Yango as a key player in the Israeli taxi market.</p> In\u00a0[\u00a0]: Copied! <pre>response = query_engine.query(\"What are news related to financial sector?\")\ndisplay(Markdown(f\"{response.response}\"))\n</pre> response = query_engine.query(\"What are news related to financial sector?\") display(Markdown(f\"{response.response}\")) <p>The recent news related to the financial sector includes Morgan Stanley hiring Thomas Christl to co-head its coverage of consumer and retail clients in Europe. KeyBank has expanded its presence in the Western U.S. by opening a new branch in American Fork and donated $10,000 to the Five.12 Foundation. BMG has acquired the recording catalog of The Hollies, and Matt Pincus led a $15 million pre-growth round of investment for Soundtrack Your Brand. Hyatt Hotels and Supplier.io have been honored with the Supply &amp; Demand Chain Executive 2023 Top Supply Chain Projects award. Bank of America Corp. reported a decline in uninsured deposits, while JPMorgan Chase &amp; Co. reported a 1.9% increase in uninsured deposits. Coinbase Global Inc. repurchased $64.5 million worth of 0.50% convertible senior notes and also decided to repurchase its 0.50% Convertible Senior Notes due 2026 for approximately $45.5 million. Deutsche Bank upgraded Allegiant Travel's status from Hold to Buy and increased the price target to $145. Lastly, Tesla Inc.'s stock performance was analyzed by Ihor Dusaniwsky, a managing director at S3 Partners, and the company formed a significant partnership with General Motors Co. in the electric vehicle industry.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#graphrag-implementation-with-llamaindex","title":"GraphRAG Implementation with LlamaIndex\u00b6","text":"<p>GraphRAG (Graphs + Retrieval Augmented Generation) combines the strengths of Retrieval Augmented Generation (RAG) and Query-Focused Summarization (QFS) to effectively handle complex queries over large text datasets. While RAG excels in fetching precise information, it struggles with broader queries that require thematic understanding, a challenge that QFS addresses but cannot scale well. GraphRAG integrates these approaches to offer responsive and thorough querying capabilities across extensive, diverse text corpora.</p> <p>This notebook provides guidance on constructing the GraphRAG pipeline using the LlamaIndex PropertyGraph abstractions.</p> <p>NOTE: This is an approximate implementation of GraphRAG. We are currently developing a series of cookbooks that will detail the exact implementation of GraphRAG.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#graphrag-aproach","title":"GraphRAG Aproach\u00b6","text":"<p>The GraphRAG involves two steps:</p> <ol> <li>Graph Generation - Creates Graph, builds communities and its summaries over the given document.</li> <li>Answer to the Query - Use summaries of the communities created from step-1 to answer the query.</li> </ol> <p>Graph Generation:</p> <ol> <li><p>Source Documents to Text Chunks: Source documents are divided into smaller text chunks for easier processing.</p> </li> <li><p>Text Chunks to Element Instances: Each text chunk is analyzed to identify and extract entities and relationships, resulting in a list of tuples that represent these elements.</p> </li> <li><p>Element Instances to Element Summaries: The extracted entities and relationships are summarized into descriptive text blocks for each element using the LLM.</p> </li> <li><p>Element Summaries to Graph Communities: These entities, relationships and summaries form a graph, which is subsequently partitioned into communities using algorithms using Heirarchical Leiden to establish a hierarchical structure.</p> </li> <li><p>Graph Communities to Community Summaries: The LLM generates summaries for each community, providing insights into the dataset\u2019s overall topical structure and semantics.</p> </li> </ol> <p>Answering the Query:</p> <p>Community Summaries to Global Answers: The summaries of the communities are utilized to respond to user queries. This involves generating intermediate answers, which are then consolidated into a comprehensive global answer.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#graphrag-pipeline-components","title":"GraphRAG Pipeline Components\u00b6","text":"<p>Here are the different components we implemented to build all of the processes mentioned above.</p> <ol> <li><p>Source Documents to Text Chunks: Implemented using <code>SentenceSplitter</code> with a chunk size of 1024 and chunk overlap of 20 tokens.</p> </li> <li><p>Text Chunks to Element Instances AND Element Instances to Element Summaries: Implemented using <code>GraphRAGExtractor</code>.</p> </li> <li><p>Element Summaries to Graph Communities AND Graph Communities to Community Summaries: Implemented using <code>GraphRAGStore</code>.</p> </li> <li><p>Community Summaries to Global Answers: Implemented using <code>GraphQueryEngine</code>.</p> </li> </ol> <p>Let's check into each of these components and build GraphRAG pipeline.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#installation","title":"Installation\u00b6","text":"<p><code>graspologic</code> is used to use hierarchical_leiden for building communities.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#load-data","title":"Load Data\u00b6","text":"<p>We will use a sample news article dataset retrieved from Diffbot, which Tomaz has conveniently made available on GitHub for easy access.</p> <p>The dataset contains 2,500 samples; for ease of experimentation, we will use 50 of these samples, which include the <code>title</code> and <code>text</code> of news articles.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#setup-api-key-and-llm","title":"Setup API Key and LLM\u00b6","text":""},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#graphragextractor","title":"GraphRAGExtractor\u00b6","text":"<p>The GraphRAGExtractor class is designed to extract triples (subject-relation-object) from text and enrich them by adding descriptions for entities and relationships to their properties using an LLM.</p> <p>This functionality is similar to that of the <code>SimpleLLMPathExtractor</code>, but includes additional enhancements to handle entity, relationship descriptions. For guidance on implementation, you may look at similar existing extractors.</p> <p>Here's a breakdown of its functionality:</p> <p>Key Components:</p> <ol> <li><code>llm:</code> The language model used for extraction.</li> <li><code>extract_prompt:</code> A prompt template used to guide the LLM in extracting information.</li> <li><code>parse_fn:</code> A function to parse the LLM's output into structured data.</li> <li><code>max_paths_per_chunk:</code> Limits the number of triples extracted per text chunk.</li> <li><code>num_workers:</code> For parallel processing of multiple text nodes.</li> </ol> <p>Main Methods:</p> <ol> <li><code>__call__:</code> The entry point for processing a list of text nodes.</li> <li><code>acall:</code> An asynchronous version of call for improved performance.</li> <li><code>_aextract:</code> The core method that processes each individual node.</li> </ol> <p>Extraction Process:</p> <p>For each input node (chunk of text):</p> <ol> <li>It sends the text to the LLM along with the extraction prompt.</li> <li>The LLM's response is parsed to extract entities, relationships, descriptions for entities and relations.</li> <li>Entities are converted into EntityNode objects. Entity description is stored in metadata</li> <li>Relationships are converted into Relation objects. Relationship description is stored in metadata.</li> <li>These are added to the node's metadata under KG_NODES_KEY and KG_RELATIONS_KEY.</li> </ol> <p>NOTE: In the current implementation, we are using only relationship descriptions. In the next implementation, we will utilize entity descriptions during the retrieval stage.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#graphragstore","title":"GraphRAGStore\u00b6","text":"<p>The <code>GraphRAGStore</code> class is an extension of the <code>SimplePropertyGraphStore </code>class, designed to implement GraphRAG pipeline. Here's a breakdown of its key components and functions:</p> <p>The class uses community detection algorithms to group related nodes in the graph and then it generates summaries for each community using an LLM.</p> <p>Key Methods:</p> <p><code>build_communities():</code></p> <ol> <li><p>Converts the internal graph representation to a NetworkX graph.</p> </li> <li><p>Applies the hierarchical Leiden algorithm for community detection.</p> </li> <li><p>Collects detailed information about each community.</p> </li> <li><p>Generates summaries for each community.</p> </li> </ol> <p><code>generate_community_summary(text):</code></p> <ol> <li>Uses LLM to generate a summary of the relationships in a community.</li> <li>The summary includes entity names and a synthesis of relationship descriptions.</li> </ol> <p><code>_create_nx_graph():</code></p> <ol> <li>Converts the internal graph representation to a NetworkX graph for community detection.</li> </ol> <p><code>_collect_community_info(nx_graph, clusters):</code></p> <ol> <li>Collects detailed information about each node based on its community.</li> <li>Creates a string representation of each relationship within a community.</li> </ol> <p><code>_summarize_communities(community_info):</code></p> <ol> <li>Generates and stores summaries for each community using LLM.</li> </ol> <p><code>get_community_summaries():</code></p> <ol> <li>Returns the community summaries by building them if not already done.</li> </ol>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#graphragqueryengine","title":"GraphRAGQueryEngine\u00b6","text":"<p>The GraphRAGQueryEngine class is a custom query engine designed to process queries using the GraphRAG approach. It leverages the community summaries generated by the GraphRAGStore to answer user queries. Here's a breakdown of its functionality:</p> <p>Main Components:</p> <p><code>graph_store:</code> An instance of GraphRAGStore, which contains the community summaries. <code>llm:</code> A Language Model (LLM) used for generating and aggregating answers.</p> <p>Key Methods:</p> <p><code>custom_query(query_str: str)</code></p> <ol> <li>This is the main entry point for processing a query. It retrieves community summaries, generates answers from each summary, and then aggregates these answers into a final response.</li> </ol> <p><code>generate_answer_from_summary(community_summary, query):</code></p> <ol> <li>Generates an answer for the query based on a single community summary. Uses the LLM to interpret the community summary in the context of the query.</li> </ol> <p><code>aggregate_answers(community_answers):</code></p> <ol> <li>Combines individual answers from different communities into a coherent final response.</li> <li>Uses the LLM to synthesize multiple perspectives into a single, concise answer.</li> </ol> <p>Query Processing Flow:</p> <ol> <li>Retrieve community summaries from the graph store.</li> <li>For each community summary, generate a specific answer to the query.</li> <li>Aggregate all community-specific answers into a final, coherent response.</li> </ol> <p>Example usage:</p> <pre><code>query_engine = GraphRAGQueryEngine(graph_store=graph_store, llm=llm)\n\nresponse = query_engine.query(\"query\")\n</code></pre>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#build-end-to-end-graphrag-pipeline","title":"Build End to End GraphRAG Pipeline\u00b6","text":"<p>Now that we have defined all the necessary components, let\u2019s construct the GraphRAG pipeline:</p> <ol> <li>Create nodes/chunks from the text.</li> <li>Build a PropertyGraphIndex using <code>GraphRAGExtractor</code> and <code>GraphRAGStore</code>.</li> <li>Construct communities and generate a summary for each community using the graph built above.</li> <li>Create a <code>GraphRAGQueryEngine</code> and begin querying.</li> </ol>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#create-nodes-chunks-from-the-text","title":"Create nodes/ chunks from the text.\u00b6","text":""},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#build-propergraphindex-using-graphragextractor-and-graphragstore","title":"Build ProperGraphIndex using <code>GraphRAGExtractor</code> and <code>GraphRAGStore</code>\u00b6","text":""},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#build-communities","title":"Build communities\u00b6","text":"<p>This will create communities and summary for each community.</p>"},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#create-queryengine","title":"Create QueryEngine\u00b6","text":""},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#querying","title":"Querying\u00b6","text":""},{"location":"RAG/11_Graph_RAG/GraphRAG_v1/#future-work","title":"Future Work:\u00b6","text":"<p>This cookbook is an approximate implementation of GraphRAG. In future cookbooks, we plan to extend it as follows:</p> <ol> <li>Implement retrieval using entity description embeddings.</li> <li>Integrate with Neo4JPropertyGraphStore.</li> <li>Calculate a helpfulness score for each answer generated from the community summaries and filter out answers where the helpfulness score is zero.</li> <li>Perform entity disambiguation to remove duplicate entities.</li> <li>Implement claims or covariate information extraction, Local Search and Global Search techniques.</li> </ol>"},{"location":"RAG/12_Agnetic_RAG/","title":"Index","text":"<pre><code>graph TD\n    subgraph \"Document Preprocessing\"\n        A[Source Documents] --&gt; B[Document Chunking]\n        B --&gt; C[Embedding Generation]\n        C --&gt; D[Vector Store Population]\n    end\n\n    subgraph \"Index Creation\"\n        D --&gt; E[Vector Index Creation]\n        D --&gt; F[Summary Index Creation]\n    end\n\n    subgraph \"Agent Creation\"\n        E &amp; F --&gt; G[Document Agent 1]\n        E &amp; F --&gt; H[Document Agent 2]\n        E &amp; F --&gt; I[Document Agent n]\n        G &amp; H &amp; I --&gt; J[Top-Level Agent]\n    end\n\n    subgraph \"Query Processing\"\n        K[User Query] --&gt; L[Query Analysis]\n        L --&gt; M[Relevant Agent Selection]\n    end\n\n    subgraph \"Multi-Agent Retrieval\"\n        M --&gt; N[Activate Document Agents]\n        N --&gt; O[Vector Search]\n        N --&gt; P[Summarization]\n    end\n\n    subgraph \"Information Synthesis\"\n        O &amp; P --&gt; Q[Collect Agent Outputs]\n        Q --&gt; R[Synthesize Information]\n    end\n\n    subgraph \"Answer Generation\"\n        R --&gt; S[LLM-based Answer Generation]\n        K --&gt; S\n    end\n\n    subgraph \"Iterative Refinement\"\n        S --&gt; T{Satisfactory Answer?}\n        T --&gt;|No| U[Refine Query]\n        U --&gt; L\n        T --&gt;|Yes| V[Final Answer]\n    end</code></pre>"},{"location":"RAG/12_Agnetic_RAG/#multi-document-agentic-rag-enhancing-retrieval-augmented-generation-with-agent-based-approaches","title":"Multi-Document Agentic RAG: Enhancing Retrieval Augmented Generation with Agent-Based Approaches","text":""},{"location":"RAG/12_Agnetic_RAG/#introduction","title":"Introduction","text":"<p>Multi-Document Agentic RAG (Retrieval Augmented Generation) is an advanced approach to information retrieval and generation that combines the strengths of multi-document processing, agent-based systems, and large language models (LLMs). This method aims to overcome limitations of traditional RAG systems by incorporating intelligent agents to handle complex queries across multiple documents.</p>"},{"location":"RAG/12_Agnetic_RAG/#motivation","title":"Motivation","text":"<p>While traditional RAG systems excel at retrieving relevant information from a single document, they often struggle with: 1. Handling queries that span multiple documents 2. Comparing and contrasting information from different sources 3. Providing context-aware responses that consider the relationships between documents 4. Efficiently processing and retrieving information from large, diverse datasets</p> <p>Multi-Document Agentic RAG addresses these challenges by leveraging specialized document agents and a top-level orchestrating agent to provide more comprehensive and nuanced responses to user queries.</p>"},{"location":"RAG/12_Agnetic_RAG/#method-details","title":"Method Details","text":""},{"location":"RAG/12_Agnetic_RAG/#document-preprocessing-and-vector-store-creation","title":"Document Preprocessing and Vector Store Creation","text":"<ol> <li>Document Ingestion: Source documents are processed and divided into smaller, manageable chunks.</li> <li>Embedding Generation: Embeddings are created for each chunk of text.</li> <li>Vector Store Population: Embeddings are stored in a vector database for efficient retrieval.</li> <li>Index Creation: Both vector and summary indexes are created for each document.</li> </ol>"},{"location":"RAG/12_Agnetic_RAG/#multi-document-agentic-rag-workflow","title":"Multi-Document Agentic RAG Workflow","text":"<ol> <li>Document Agent Creation: For each document, create a specialized agent with access to:    a. A vector query engine for semantic search within the document    b. A summary query engine for generating document summaries</li> <li>Top-Level Agent Setup: Create a master agent that can access and coordinate all document agents.</li> <li>Query Processing: The top-level agent analyzes the user's query to determine which document agents to involve.</li> <li>Multi-Agent Retrieval:     a. Relevant document agents are activated based on the query.    b. Each activated agent performs retrieval or summarization as needed.</li> <li>Information Synthesis: The top-level agent collects and synthesizes information from the document agents.</li> <li>Answer Generation: The LLM generates a comprehensive answer using the synthesized information and the original query.</li> <li>Iterative Refinement: If needed, the system can perform multiple retrieval-generation cycles to refine the answer.</li> </ol>"},{"location":"RAG/12_Agnetic_RAG/#key-features-of-multi-document-agentic-rag","title":"Key Features of Multi-Document Agentic RAG","text":"<ol> <li>Specialized Document Agents: Each document has its own agent, allowing for focused and efficient retrieval.</li> <li>Hierarchical Agent Structure: A top-level agent orchestrates the process, ensuring coherent multi-document reasoning.</li> <li>Flexible Querying: Supports both specific fact-finding and broader thematic inquiries across multiple documents.</li> <li>Dynamic Tool Selection: The top-level agent can choose the most appropriate tools (vector search or summarization) for each sub-query.</li> <li>Cross-Document Analysis: Enables comparison and synthesis of information from multiple sources.</li> </ol>"},{"location":"RAG/12_Agnetic_RAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li>Improved Context Understanding: By leveraging multiple document agents, the system can provide more contextually relevant answers that span multiple sources.</li> <li>Enhanced Comparative Analysis: Enables easy comparison of information between different documents or topics.</li> <li>Scalability: Can efficiently handle larger and more diverse datasets by distributing the workload across multiple agents.</li> <li>Flexibility: Adapts well to various types of queries, from specific fact-checking to open-ended exploration across multiple documents.</li> <li>Reduced Hallucination: The structured, multi-agent approach helps in grounding the LLM's responses in factual data from multiple sources.</li> </ol>"},{"location":"RAG/12_Agnetic_RAG/#conclusion","title":"Conclusion","text":"<p>Multi-Document Agentic RAG represents a significant advancement in the field of retrieval-augmented generation. By integrating agent-based approaches with traditional RAG techniques, it offers a more nuanced, context-aware, and scalable solution for information retrieval and generation across multiple documents. This approach opens up new possibilities for building more intelligent and responsive AI systems capable of handling complex queries that span diverse knowledge bases.</p>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/","title":"Agentic RAG(Llamaindex)","text":"<p>If you're opening this Notebook on colab, you will probably need to install LlamaIndex \ud83e\udd99.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-agent-openai\n%pip install llama-index-embeddings-openai\n%pip install llama-index-llms-openai\n</pre> %pip install llama-index-agent-openai %pip install llama-index-embeddings-openai %pip install llama-index-llms-openai In\u00a0[\u00a0]: Copied! <pre>!pip install llama-index\n</pre> !pip install llama-index In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import (\n    VectorStoreIndex,\n    SimpleKeywordTableIndex,\n    SimpleDirectoryReader,\n)\nfrom llama_index.core import SummaryIndex\nfrom llama_index.core.schema import IndexNode\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.callbacks import CallbackManager\n</pre> from llama_index.core import (     VectorStoreIndex,     SimpleKeywordTableIndex,     SimpleDirectoryReader, ) from llama_index.core import SummaryIndex from llama_index.core.schema import IndexNode from llama_index.core.tools import QueryEngineTool, ToolMetadata from llama_index.llms.openai import OpenAI from llama_index.core.callbacks import CallbackManager In\u00a0[\u00a0]: Copied! <pre>wiki_titles = [\n    \"Toronto\",\n    \"Seattle\",\n    \"Chicago\",\n    \"Boston\",\n    \"Houston\",\n    \"Tokyo\",\n    \"Berlin\",\n    \"Lisbon\",\n    \"Paris\",\n    \"London\",\n    \"Atlanta\",\n    \"Munich\",\n    \"Shanghai\",\n    \"Beijing\",\n    \"Copenhagen\",\n    \"Moscow\",\n    \"Cairo\",\n    \"Karachi\",\n]\n</pre> wiki_titles = [     \"Toronto\",     \"Seattle\",     \"Chicago\",     \"Boston\",     \"Houston\",     \"Tokyo\",     \"Berlin\",     \"Lisbon\",     \"Paris\",     \"London\",     \"Atlanta\",     \"Munich\",     \"Shanghai\",     \"Beijing\",     \"Copenhagen\",     \"Moscow\",     \"Cairo\",     \"Karachi\", ] In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\nimport requests\n\nfor title in wiki_titles:\n    response = requests.get(\n        \"https://en.wikipedia.org/w/api.php\",\n        params={\n            \"action\": \"query\",\n            \"format\": \"json\",\n            \"titles\": title,\n            \"prop\": \"extracts\",\n            # 'exintro': True,\n            \"explaintext\": True,\n        },\n    ).json()\n    page = next(iter(response[\"query\"][\"pages\"].values()))\n    wiki_text = page[\"extract\"]\n\n    data_path = Path(\"data\")\n    if not data_path.exists():\n        Path.mkdir(data_path)\n\n    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n        fp.write(wiki_text)\n</pre> from pathlib import Path  import requests  for title in wiki_titles:     response = requests.get(         \"https://en.wikipedia.org/w/api.php\",         params={             \"action\": \"query\",             \"format\": \"json\",             \"titles\": title,             \"prop\": \"extracts\",             # 'exintro': True,             \"explaintext\": True,         },     ).json()     page = next(iter(response[\"query\"][\"pages\"].values()))     wiki_text = page[\"extract\"]      data_path = Path(\"data\")     if not data_path.exists():         Path.mkdir(data_path)      with open(data_path / f\"{title}.txt\", \"w\") as fp:         fp.write(wiki_text) In\u00a0[\u00a0]: Copied! <pre># Load all wiki documents\ncity_docs = {}\nfor wiki_title in wiki_titles:\n    city_docs[wiki_title] = SimpleDirectoryReader(\n        input_files=[f\"data/{wiki_title}.txt\"]\n    ).load_data()\n</pre> # Load all wiki documents city_docs = {} for wiki_title in wiki_titles:     city_docs[wiki_title] = SimpleDirectoryReader(         input_files=[f\"data/{wiki_title}.txt\"]     ).load_data() <p>Define Global LLM and Embeddings</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\nSettings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n</pre> from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding from llama_index.core import Settings  Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\") Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\") In\u00a0[\u00a0]: Copied! <pre>from llama_index.agent.openai import OpenAIAgent\nfrom llama_index.core import load_index_from_storage, StorageContext\nfrom llama_index.core.node_parser import SentenceSplitter\nimport os\n\nnode_parser = SentenceSplitter()\n\n# Build agents dictionary\nagents = {}\nquery_engines = {}\n\n# this is for the baseline\nall_nodes = []\n\nfor idx, wiki_title in enumerate(wiki_titles):\n    nodes = node_parser.get_nodes_from_documents(city_docs[wiki_title])\n    all_nodes.extend(nodes)\n\n    if not os.path.exists(f\"./data/{wiki_title}\"):\n        # build vector index\n        vector_index = VectorStoreIndex(nodes)\n        vector_index.storage_context.persist(\n            persist_dir=f\"./data/{wiki_title}\"\n        )\n    else:\n        vector_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=f\"./data/{wiki_title}\"),\n        )\n\n    # build summary index\n    summary_index = SummaryIndex(nodes)\n    # define query engines\n    vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)\n    summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)\n\n    # define tools\n    query_engine_tools = [\n        QueryEngineTool(\n            query_engine=vector_query_engine,\n            metadata=ToolMetadata(\n                name=\"vector_tool\",\n                description=(\n                    \"Useful for questions related to specific aspects of\"\n                    f\" {wiki_title} (e.g. the history, arts and culture,\"\n                    \" sports, demographics, or more).\"\n                ),\n            ),\n        ),\n        QueryEngineTool(\n            query_engine=summary_query_engine,\n            metadata=ToolMetadata(\n                name=\"summary_tool\",\n                description=(\n                    \"Useful for any requests that require a holistic summary\"\n                    f\" of EVERYTHING about {wiki_title}. For questions about\"\n                    \" more specific sections, please use the vector_tool.\"\n                ),\n            ),\n        ),\n    ]\n\n    # build agent\n    function_llm = OpenAI(model=\"gpt-4\")\n    agent = OpenAIAgent.from_tools(\n        query_engine_tools,\n        llm=function_llm,\n        verbose=True,\n        system_prompt=f\"\"\"\\\nYou are a specialized agent designed to answer queries about {wiki_title}.\nYou must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n\"\"\",\n    )\n\n    agents[wiki_title] = agent\n    query_engines[wiki_title] = vector_index.as_query_engine(\n        similarity_top_k=2\n    )\n</pre> from llama_index.agent.openai import OpenAIAgent from llama_index.core import load_index_from_storage, StorageContext from llama_index.core.node_parser import SentenceSplitter import os  node_parser = SentenceSplitter()  # Build agents dictionary agents = {} query_engines = {}  # this is for the baseline all_nodes = []  for idx, wiki_title in enumerate(wiki_titles):     nodes = node_parser.get_nodes_from_documents(city_docs[wiki_title])     all_nodes.extend(nodes)      if not os.path.exists(f\"./data/{wiki_title}\"):         # build vector index         vector_index = VectorStoreIndex(nodes)         vector_index.storage_context.persist(             persist_dir=f\"./data/{wiki_title}\"         )     else:         vector_index = load_index_from_storage(             StorageContext.from_defaults(persist_dir=f\"./data/{wiki_title}\"),         )      # build summary index     summary_index = SummaryIndex(nodes)     # define query engines     vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)     summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)      # define tools     query_engine_tools = [         QueryEngineTool(             query_engine=vector_query_engine,             metadata=ToolMetadata(                 name=\"vector_tool\",                 description=(                     \"Useful for questions related to specific aspects of\"                     f\" {wiki_title} (e.g. the history, arts and culture,\"                     \" sports, demographics, or more).\"                 ),             ),         ),         QueryEngineTool(             query_engine=summary_query_engine,             metadata=ToolMetadata(                 name=\"summary_tool\",                 description=(                     \"Useful for any requests that require a holistic summary\"                     f\" of EVERYTHING about {wiki_title}. For questions about\"                     \" more specific sections, please use the vector_tool.\"                 ),             ),         ),     ]      # build agent     function_llm = OpenAI(model=\"gpt-4\")     agent = OpenAIAgent.from_tools(         query_engine_tools,         llm=function_llm,         verbose=True,         system_prompt=f\"\"\"\\ You are a specialized agent designed to answer queries about {wiki_title}. You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\ \"\"\",     )      agents[wiki_title] = agent     query_engines[wiki_title] = vector_index.as_query_engine(         similarity_top_k=2     ) In\u00a0[\u00a0]: Copied! <pre># define tool for each document agent\nall_tools = []\nfor wiki_title in wiki_titles:\n    wiki_summary = (\n        f\"This content contains Wikipedia articles about {wiki_title}. Use\"\n        f\" this tool if you want to answer any questions about {wiki_title}.\\n\"\n    )\n    doc_tool = QueryEngineTool(\n        query_engine=agents[wiki_title],\n        metadata=ToolMetadata(\n            name=f\"tool_{wiki_title}\",\n            description=wiki_summary,\n        ),\n    )\n    all_tools.append(doc_tool)\n</pre> # define tool for each document agent all_tools = [] for wiki_title in wiki_titles:     wiki_summary = (         f\"This content contains Wikipedia articles about {wiki_title}. Use\"         f\" this tool if you want to answer any questions about {wiki_title}.\\n\"     )     doc_tool = QueryEngineTool(         query_engine=agents[wiki_title],         metadata=ToolMetadata(             name=f\"tool_{wiki_title}\",             description=wiki_summary,         ),     )     all_tools.append(doc_tool) In\u00a0[\u00a0]: Copied! <pre># define an \"object\" index and retriever over these tools\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.objects import ObjectIndex\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n)\n</pre> # define an \"object\" index and retriever over these tools from llama_index.core import VectorStoreIndex from llama_index.core.objects import ObjectIndex  obj_index = ObjectIndex.from_objects(     all_tools,     index_cls=VectorStoreIndex, ) In\u00a0[\u00a0]: Copied! <pre>from llama_index.agent.openai import OpenAIAgent\n\ntop_agent = OpenAIAgent.from_tools(\n    tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n    system_prompt=\"\"\" \\\nYou are an agent designed to answer queries about a set of given cities.\nPlease always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n\n\"\"\",\n    verbose=True,\n)\n</pre> from llama_index.agent.openai import OpenAIAgent  top_agent = OpenAIAgent.from_tools(     tool_retriever=obj_index.as_retriever(similarity_top_k=3),     system_prompt=\"\"\" \\ You are an agent designed to answer queries about a set of given cities. Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\  \"\"\",     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>base_index = VectorStoreIndex(all_nodes)\nbase_query_engine = base_index.as_query_engine(similarity_top_k=4)\n</pre> base_index = VectorStoreIndex(all_nodes) base_query_engine = base_index.as_query_engine(similarity_top_k=4) In\u00a0[\u00a0]: Copied! <pre># should use Boston agent -&gt; vector tool\nresponse = top_agent.query(\"Tell me about the arts and culture in Boston\")\n</pre> # should use Boston agent -&gt; vector tool response = top_agent.query(\"Tell me about the arts and culture in Boston\") <pre>=== Calling Function ===\nCalling function: tool_Boston with args: {\n  \"input\": \"arts and culture\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"arts and culture\"\n}\nGot output: Boston is known for its vibrant arts and culture scene. The city is home to a number of performing arts organizations, including the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. There are also several theaters in or near the Theater District, such as the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre. Boston is a center for contemporary classical music, with groups like the Boston Modern Orchestra Project and Boston Musica Viva. The city also hosts major annual events, such as First Night, the Boston Early Music Festival, and the Boston Arts Festival. In addition, Boston has several art museums and galleries, including the Museum of Fine Arts, the Isabella Stewart Gardner Museum, and the Institute of Contemporary Art.\n========================\nGot output: Boston is renowned for its vibrant arts and culture scene. It is home to numerous performing arts organizations, including the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. The city's Theater District houses several theaters, such as the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre.\n\nBoston is also a hub for contemporary classical music, with groups like the Boston Modern Orchestra Project and Boston Musica Viva. The city hosts major annual events, such as First Night, the Boston Early Music Festival, and the Boston Arts Festival, which contribute to its cultural richness.\n\nIn terms of visual arts, Boston boasts several art museums and galleries. The Museum of Fine Arts, the Isabella Stewart Gardner Museum, and the Institute of Contemporary Art are among the most notable. These institutions offer a wide range of art collections, from ancient to contemporary, attracting art enthusiasts from around the world.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) <pre>Boston has a rich arts and culture scene, with a variety of performing arts organizations and venues. The city is home to renowned institutions such as the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. The Theater District in Boston is a hub for theatrical performances, with theaters like the Cutler Majestic Theatre, Citi Performing Arts Center, Colonial Theater, and Orpheum Theatre.\n\nIn addition to performing arts, Boston also has a thriving contemporary classical music scene, with groups like the Boston Modern Orchestra Project and Boston Musica Viva. The city hosts several annual events that celebrate the arts, including First Night, the Boston Early Music Festival, and the Boston Arts Festival.\n\nBoston is also known for its visual arts scene, with a number of art museums and galleries. The Museum of Fine Arts, the Isabella Stewart Gardner Museum, and the Institute of Contemporary Art are among the notable institutions in the city. These museums offer a diverse range of art collections, spanning from ancient to contemporary art, and attract art enthusiasts from around the world.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Tell me about the arts and culture in Boston\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Tell me about the arts and culture in Boston\" ) print(str(response)) <pre>Boston has a rich arts and culture scene. The city is home to a variety of performing arts organizations, such as the Boston Ballet, Boston Lyric Opera Company, Opera Boston, Boston Baroque, and the Handel and Haydn Society. Additionally, there are numerous contemporary classical music groups associated with the city's conservatories and universities, like the Boston Modern Orchestra Project and Boston Musica Viva. The Theater District in Boston is a hub for theater, with notable venues including the Cutler Majestic Theatre, Citi Performing Arts Center, the Colonial Theater, and the Orpheum Theatre. Boston also hosts several significant annual events, including First Night, the Boston Early Music Festival, the Boston Arts Festival, and the Boston gay pride parade and festival. The city is renowned for its historic sites connected to the American Revolution, as well as its art museums and galleries, such as the Museum of Fine Arts, Isabella Stewart Gardner Museum, and the Institute of Contemporary Art.\n</pre> In\u00a0[\u00a0]: Copied! <pre># should use Houston agent -&gt; vector tool\nresponse = top_agent.query(\n    \"Give me a summary of all the positive aspects of Houston\"\n)\n</pre> # should use Houston agent -&gt; vector tool response = top_agent.query(     \"Give me a summary of all the positive aspects of Houston\" ) <pre>=== Calling Function ===\nCalling function: tool_Houston with args: {\n  \"input\": \"positive aspects\"\n}\n=== Calling Function ===\nCalling function: summary_tool with args: {\n  \"input\": \"positive aspects\"\n}\nGot output: Houston has many positive aspects that make it an attractive place to live and visit. The city's diverse population, with people from different ethnic and religious backgrounds, adds to its cultural richness and inclusiveness. Additionally, Houston is home to the Texas Medical Center, which is the largest concentration of healthcare and research institutions in the world. The presence of NASA's Johnson Space Center also highlights Houston's importance in the fields of medicine and space exploration. The city's strong economy, supported by industries such as energy, manufacturing, aeronautics, and transportation, provides numerous economic opportunities for residents and visitors alike. Furthermore, Houston has a thriving visual and performing arts scene, including a theater district and a variety of museums and galleries. Overall, Houston's diverse community, cultural attractions, and economic prospects make it an exceptionally appealing city.\n========================\nGot output: Houston has numerous positive aspects that make it a desirable place to live and visit. Some of these include:\n\n1. **Diversity**: Houston is known for its diverse population, with people from different ethnic and religious backgrounds. This diversity adds to the city's cultural richness and inclusiveness.\n\n2. **Healthcare and Research Institutions**: The city is home to the Texas Medical Center, the largest concentration of healthcare and research institutions in the world. This makes Houston a hub for medical innovation and healthcare services.\n\n3. **Space Exploration**: Houston is also known for NASA's Johnson Space Center, highlighting the city's significant role in space exploration.\n\n4. **Strong Economy**: Houston's economy is robust and diverse, supported by industries such as energy, manufacturing, aeronautics, and transportation. This provides numerous economic opportunities for its residents.\n\n5. **Arts and Culture**: The city has a thriving visual and performing arts scene, with a theater district and a variety of museums and galleries. This makes Houston a vibrant place for art lovers and creatives.\n\nOverall, these aspects contribute to making Houston an appealing and dynamic city.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) <pre>Houston has numerous positive aspects that make it a desirable place to live and visit. Some of these include:\n\n1. Diversity: Houston is known for its diverse population, with people from different ethnic and religious backgrounds. This diversity adds to the city's cultural richness and inclusiveness.\n\n2. Healthcare and Research Institutions: The city is home to the Texas Medical Center, the largest concentration of healthcare and research institutions in the world. This makes Houston a hub for medical innovation and healthcare services.\n\n3. Space Exploration: Houston is also known for NASA's Johnson Space Center, highlighting the city's significant role in space exploration.\n\n4. Strong Economy: Houston's economy is robust and diverse, supported by industries such as energy, manufacturing, aeronautics, and transportation. This provides numerous economic opportunities for its residents.\n\n5. Arts and Culture: The city has a thriving visual and performing arts scene, with a theater district and a variety of museums and galleries. This makes Houston a vibrant place for art lovers and creatives.\n\nOverall, these aspects contribute to making Houston an appealing and dynamic city.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Give me a summary of all the positive aspects of Houston\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Give me a summary of all the positive aspects of Houston\" ) print(str(response)) <pre>Houston has several positive aspects that contribute to its reputation as a thriving city. It is home to a diverse and growing international community, with a large number of foreign banks and consular offices representing 92 countries. The city has received numerous accolades, including being ranked as one of the best cities for employment, college graduates, and homebuyers. Houston has a strong economy, with a broad industrial base in sectors such as energy, manufacturing, aeronautics, and healthcare. It is also a major center for the oil and gas industry and has the second-most Fortune 500 headquarters in the United States. The city's cultural scene is vibrant, with a variety of annual events celebrating different cultures, as well as a reputation for diverse and excellent food. Houston is known for its world-class museums and performing arts scene. Additionally, the city has made significant investments in renewable energy sources like wind and solar. Overall, Houston offers a high quality of life, reasonable living costs, and abundant employment opportunities.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline: the response doesn't quite match the sources...\nresponse.source_nodes[1].get_content()\n</pre> # baseline: the response doesn't quite match the sources... response.source_nodes[1].get_content() In\u00a0[\u00a0]: Copied! <pre>response = top_agent.query(\n    \"Tell the demographics of Houston, and then compare that with the\"\n    \" demographics of Chicago\"\n)\n</pre> response = top_agent.query(     \"Tell the demographics of Houston, and then compare that with the\"     \" demographics of Chicago\" ) <pre>=== Calling Function ===\nCalling function: tool_Houston with args: {\n  \"input\": \"demographics\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"demographics\"\n}\nGot output: Houston is a majority-minority city with a diverse population. According to the U.S. Census Bureau, in 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in the city is Mexican Americans, followed by Puerto Ricans and Cuban Americans. Houston is also home to the largest African American community west of the Mississippi River. Additionally, Houston has a growing Muslim population, with Muslims estimated to make up 1.2% of the city's population. The city is known for its LGBT community and is home to one of the largest pride parades in the United States. The Hindu, Sikh, and Buddhist communities are also growing in Houston. Overall, Houston is considered one of the most ethnically and culturally diverse metropolitan areas in the country.\n========================\nGot output: Houston is a majority-minority city with a diverse population. According to the U.S. Census Bureau, in 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in the city is Mexican Americans, followed by Puerto Ricans and Cuban Americans. \n\nHouston is also home to the largest African American community west of the Mississippi River. Additionally, Houston has a growing Muslim population, with Muslims estimated to make up 1.2% of the city's population. The city is known for its LGBT community and is home to one of the largest pride parades in the United States. The Hindu, Sikh, and Buddhist communities are also growing in Houston. \n\nOverall, Houston is considered one of the most ethnically and culturally diverse metropolitan areas in the country.\n========================\n=== Calling Function ===\nCalling function: tool_Chicago with args: {\n  \"input\": \"demographics\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"demographics\"\n}\nGot output: Chicago has a diverse demographic makeup. It experienced rapid population growth during its early years, becoming one of the fastest-growing cities in the world. Waves of immigrants from various European countries, as well as African Americans from the American South, contributed to the city's population growth. Over time, Chicago's population has fluctuated, with a decline in the latter half of the 20th century followed by a rise in recent years. As of the latest census estimates, the largest racial or ethnic groups in Chicago are non-Hispanic White, Black, and Hispanic. Additionally, Chicago has a significant LGBT population and is known for its cultural diversity.\n========================\nGot output: Chicago is known for its diverse demographic makeup. The city experienced rapid population growth during its early years, with immigrants from various European countries and African Americans from the American South contributing significantly to this growth. Over time, the population has fluctuated, with a decline in the latter half of the 20th century followed by a rise in recent years. \n\nAs per the latest census estimates, the largest racial or ethnic groups in Chicago are non-Hispanic White, Black, and Hispanic. The city also has a significant LGBT population and is celebrated for its cultural diversity.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(response)\n</pre> print(response) <pre>Houston has a diverse population with a demographic makeup that includes non-Hispanic whites (23.3%), Hispanics and Latino Americans (45.8%), Blacks or African Americans (22.4%), and Asian Americans (6.5%). The largest Hispanic or Latino American ethnic group in Houston is Mexican Americans. Houston is also home to the largest African American community west of the Mississippi River and has a growing Muslim population.\n\nOn the other hand, Chicago is also known for its diverse demographics. The city has a significant non-Hispanic White population, along with a substantial Black population and Hispanic population. Chicago is celebrated for its cultural diversity and has a significant LGBT population.\n\nBoth Houston and Chicago have diverse populations, with a mix of different racial and ethnic groups contributing to their vibrant communities.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Tell the demographics of Houston, and then compare that with the\"\n    \" demographics of Chicago\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Tell the demographics of Houston, and then compare that with the\"     \" demographics of Chicago\" ) print(str(response)) <pre>Houston is the most populous city in Texas and the fourth-most populous city in the United States. It has a population of 2,304,580 as of the 2020 U.S. census. The city is known for its diversity, with a significant proportion of minorities. In 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in Houston is Mexican Americans, comprising 31.6% of the population.\n\nIn comparison, Chicago is the third-most populous city in the United States. According to the 2020 U.S. census, Chicago has a population of 2,746,388. The demographics of Chicago are different from Houston, with non-Hispanic whites making up 32.7% of the population, Hispanics and Latino Americans 29.9%, Blacks or African Americans 29.8%, and Asian Americans 7.6%. The largest Hispanic or Latino American ethnic group in Chicago is Mexican Americans, comprising 21.6% of the population.\n\nOverall, both Houston and Chicago have diverse populations, but the specific demographic composition differs between the two cities.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline: the response tells you nothing about Chicago...\nresponse.source_nodes[3].get_content()\n</pre> # baseline: the response tells you nothing about Chicago... response.source_nodes[3].get_content() In\u00a0[\u00a0]: Copied! <pre>response = top_agent.query(\n    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n    \" and current economy\"\n)\n</pre> response = top_agent.query(     \"Tell me the differences between Shanghai and Beijing in terms of history\"     \" and current economy\" ) <pre>=== Calling Function ===\nCalling function: tool_Shanghai with args: {\n  \"input\": \"history\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"history\"\n}\nGot output: Shanghai has a rich history that dates back to ancient times. However, in the context provided, the history of Shanghai is mainly discussed in relation to its modern development. After the war, Shanghai's economy experienced significant growth, with increased agricultural and industrial output. The city's administrative divisions were rearranged, and it became a center for radical leftism during the 1950s and 1960s. The Cultural Revolution had a severe impact on Shanghai's society, but the city maintained economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government. Economic reforms were initiated in Shanghai in 1990, leading to the development of the Pudong district and its classification as an Alpha+ city.\n========================\nGot output: Shanghai's history is rich and complex, dating back to ancient times. However, its modern development is particularly noteworthy. After the war, Shanghai experienced significant economic growth, with a boost in both agricultural and industrial output. The city's administrative divisions were restructured, and it became a hub for radical leftism during the 1950s and 1960s.\n\nThe Cultural Revolution had a profound impact on Shanghai's society, but despite this, the city managed to maintain economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government.\n\nIn 1990, economic reforms were initiated in Shanghai, leading to the development of the Pudong district. This has helped Shanghai to be classified as an Alpha+ city, indicating its influence on the global economic stage.\n========================\n=== Calling Function ===\nCalling function: tool_Beijing with args: {\n  \"input\": \"history\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"history\"\n}\nGot output: Beijing has a rich history that spans several dynasties. It was the capital of the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged. However, the city faced challenges during the Second Opium War and the Boxer Rebellion, resulting in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, which led to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911.\n========================\nGot output: Beijing has a rich and complex history that spans several dynasties. It served as the capital during the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged.\n\nHowever, the city faced significant challenges during the Second Opium War and the Boxer Rebellion, which resulted in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, leading to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911. Despite these tumultuous events, Beijing has managed to preserve its historical heritage while also evolving into a modern metropolis.\n========================\n=== Calling Function ===\nCalling function: tool_Shanghai with args: {\n  \"input\": \"current economy\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"current economy\"\n}\nGot output: The current economy of Shanghai is strong and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. Shanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CN\u00a54.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. It is also the most expensive city in mainland China to live in. Shanghai is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. It is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. The city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n========================\nGot output: The current economy of Shanghai is robust and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. \n\nShanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CN\u00a54.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. \n\nShanghai is also the most expensive city in mainland China to live in. It is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. The city is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. \n\nThe city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n========================\n=== Calling Function ===\nCalling function: tool_Beijing with args: {\n  \"input\": \"current economy\"\n}\n=== Calling Function ===\nCalling function: vector_tool with args: {\n  \"input\": \"current economy\"\n}\nGot output: The current economy of Beijing is dominated by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%. The city has also identified six high-end economic output zones that are driving local economic growth, including Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n========================\nGot output: The current economy of Beijing is primarily driven by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%.\n\nBeijing has also identified six high-end economic output zones that are driving local economic growth. These include Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n========================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response)) <pre>In terms of history, both Shanghai and Beijing have rich and complex pasts. Shanghai's history dates back to ancient times, but its modern development is particularly noteworthy. It experienced significant economic growth after the war and played a major role in China's economic reforms. Beijing, on the other hand, has a history that spans several dynasties and served as the capital during the Ming and Qing dynasties. It has preserved its historical heritage while evolving into a modern metropolis.\n\nIn terms of current economy, Shanghai is a global center for finance and innovation. It has a diverse economy and has experienced rapid development, with a high GDP and significant foreign investment. It is a major player in the global financial industry and is home to the Shanghai Stock Exchange. Beijing's economy is primarily driven by the tertiary sector, with a focus on services such as professional services, information technology, and commercial real estate. It has identified high-end economic output zones that are driving local economic growth.\n\nOverall, both cities have thriving economies, but Shanghai has a stronger focus on finance and global influence, while Beijing has a diverse economy with a focus on services and high-end economic zones.\n</pre> In\u00a0[\u00a0]: Copied! <pre># baseline\nresponse = base_query_engine.query(\n    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n    \" and current economy\"\n)\nprint(str(response))\n</pre> # baseline response = base_query_engine.query(     \"Tell me the differences between Shanghai and Beijing in terms of history\"     \" and current economy\" ) print(str(response)) <pre>Shanghai and Beijing have distinct differences in terms of history and current economy. Historically, Shanghai was the largest and most prosperous city in East Asia during the 1930s, while Beijing served as the capital of the Republic of China and later the People's Republic of China. Shanghai experienced significant growth and redevelopment in the 1990s, while Beijing expanded its urban area and underwent rapid development in the last two decades.\n\nIn terms of the current economy, Shanghai is considered the \"showpiece\" of China's booming economy. It is a global center for finance and innovation, with a strong focus on industries such as retail, finance, IT, real estate, machine manufacturing, and automotive manufacturing. Shanghai is also home to the world's busiest container port, the Port of Shanghai. The city has a high GDP and is classified as an Alpha+ city by the Globalization and World Cities Research Network.\n\nOn the other hand, Beijing is a global financial center and ranks third globally in the Global Financial Centres Index. It is also a hub for the Chinese and global technology industry, with a large startup ecosystem. Beijing has a strong presence in industries such as finance, technology, and pharmaceuticals. The city is home to the headquarters of large state banks and insurance companies, as well as the country's financial regulatory agencies.\n\nOverall, while both Shanghai and Beijing are important economic centers in China, Shanghai has a stronger focus on industries such as finance, retail, and manufacturing, while Beijing has a strong presence in finance, technology, and pharmaceuticals.\n</pre>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#multi-document-agents","title":"Multi-Document Agents\u00b6","text":"<p>In this guide, you learn towards setting up an agent that can effectively answer different types of questions over a larger set of documents.</p> <p>These questions include the following</p> <ul> <li>QA over a specific doc</li> <li>QA comparing different docs</li> <li>Summaries over a specific doc</li> <li>Comparing summaries between different docs</li> </ul> <p>We do this with the following architecture:</p> <ul> <li>setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc</li> <li>setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question.</li> </ul>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#setup-and-download-data","title":"Setup and Download Data\u00b6","text":"<p>In this section, we'll define imports and then download Wikipedia articles about different cities. Each article is stored separately.</p> <p>We load in 18 cities - this is not quite at the level of \"hundreds\" of documents but its still large enough to warrant some top-level document retrieval!</p>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#building-multi-document-agents","title":"Building Multi-Document Agents\u00b6","text":"<p>In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index.</p>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#build-document-agent-for-each-document","title":"Build Document Agent for each Document\u00b6","text":"<p>In this section we define \"document agents\" for each document.</p> <p>We define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.</p> <p>This document agent can dynamically choose to perform semantic search or summarization within a given document.</p> <p>We create a separate document agent for each city.</p>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#build-retriever-enabled-openai-agent","title":"Build Retriever-Enabled OpenAI Agent\u00b6","text":"<p>We build a top-level agent that can orchestrate across the different document agents to answer any user query.</p> <p>This agent takes in all document agents as tools. This specific agent <code>RetrieverOpenAIAgent</code> performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).</p> <p>Here we use a top-k retriever, but we encourage you to customize the tool retriever method!</p>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#define-baseline-vector-store-index","title":"Define Baseline Vector Store Index\u00b6","text":"<p>As a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.</p> <p>We set the top_k = 4</p>"},{"location":"RAG/12_Agnetic_RAG/multi_document_agents/#running-example-queries","title":"Running Example Queries\u00b6","text":"<p>Let's run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents.</p>"},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/","title":"Visison RAG(Llamaindex)","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install llama-index-multi-modal-llms-openai\n%pip install llama-index-vector-stores-qdrant\n</pre> %pip install llama-index-multi-modal-llms-openai %pip install llama-index-vector-stores-qdrant In\u00a0[\u00a0]: Copied! <pre>%pip install llama_index ftfy regex tqdm\n%pip install git+https://github.com/openai/CLIP.git\n%pip install torch torchvision\n%pip install matplotlib scikit-image\n%pip install -U qdrant_client\n</pre> %pip install llama_index ftfy regex tqdm %pip install git+https://github.com/openai/CLIP.git %pip install torch torchvision %pip install matplotlib scikit-image %pip install -U qdrant_client In\u00a0[\u00a0]: Copied! <pre>import os\n\nOPENAI_API_KEY = \"\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n</pre> import os  OPENAI_API_KEY = \"\" os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\ninput_image_path = Path(\"input_images\")\nif not input_image_path.exists():\n    Path.mkdir(input_image_path)\n</pre> from pathlib import Path  input_image_path = Path(\"input_images\") if not input_image_path.exists():     Path.mkdir(input_image_path) In\u00a0[\u00a0]: Copied! <pre>!wget \"https://docs.google.com/uc?export=download&amp;id=1nUhsBRiSWxcVQv8t8Cvvro8HJZ88LCzj\" -O ./input_images/long_range_spec.png\n!wget \"https://docs.google.com/uc?export=download&amp;id=19pLwx0nVqsop7lo0ubUSYTzQfMtKJJtJ\" -O ./input_images/model_y.png\n!wget \"https://docs.google.com/uc?export=download&amp;id=1utu3iD9XEgR5Sb7PrbtMf1qw8T1WdNmF\" -O ./input_images/performance_spec.png\n!wget \"https://docs.google.com/uc?export=download&amp;id=1dpUakWMqaXR4Jjn1kHuZfB0pAXvjn2-i\" -O ./input_images/price.png\n!wget \"https://docs.google.com/uc?export=download&amp;id=1qNeT201QAesnAP5va1ty0Ky5Q_jKkguV\" -O ./input_images/real_wheel_spec.png\n</pre> !wget \"https://docs.google.com/uc?export=download&amp;id=1nUhsBRiSWxcVQv8t8Cvvro8HJZ88LCzj\" -O ./input_images/long_range_spec.png !wget \"https://docs.google.com/uc?export=download&amp;id=19pLwx0nVqsop7lo0ubUSYTzQfMtKJJtJ\" -O ./input_images/model_y.png !wget \"https://docs.google.com/uc?export=download&amp;id=1utu3iD9XEgR5Sb7PrbtMf1qw8T1WdNmF\" -O ./input_images/performance_spec.png !wget \"https://docs.google.com/uc?export=download&amp;id=1dpUakWMqaXR4Jjn1kHuZfB0pAXvjn2-i\" -O ./input_images/price.png !wget \"https://docs.google.com/uc?export=download&amp;id=1qNeT201QAesnAP5va1ty0Ky5Q_jKkguV\" -O ./input_images/real_wheel_spec.png In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\nimage_paths = []\nfor img_path in os.listdir(\"./input_images\"):\n    image_paths.append(str(os.path.join(\"./input_images\", img_path)))\n\n\ndef plot_images(image_paths):\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths:\n        if os.path.isfile(img_path):\n            image = Image.open(img_path)\n\n            plt.subplot(2, 3, images_shown + 1)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown += 1\n            if images_shown &gt;= 9:\n                break\n\n\nplot_images(image_paths)\n</pre> from PIL import Image import matplotlib.pyplot as plt import os  image_paths = [] for img_path in os.listdir(\"./input_images\"):     image_paths.append(str(os.path.join(\"./input_images\", img_path)))   def plot_images(image_paths):     images_shown = 0     plt.figure(figsize=(16, 9))     for img_path in image_paths:         if os.path.isfile(img_path):             image = Image.open(img_path)              plt.subplot(2, 3, images_shown + 1)             plt.imshow(image)             plt.xticks([])             plt.yticks([])              images_shown += 1             if images_shown &gt;= 9:                 break   plot_images(image_paths) In\u00a0[\u00a0]: Copied! <pre>from llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index.core import SimpleDirectoryReader\n\n# put your local directore here\nimage_documents = SimpleDirectoryReader(\"./input_images\").load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4o\", api_key=OPENAI_API_KEY, max_new_tokens=1500\n)\n\nresponse_1 = openai_mm_llm.complete(\n    prompt=\"Describe the images as an alternative text\",\n    image_documents=image_documents,\n)\n\nprint(response_1)\n</pre> from llama_index.multi_modal_llms.openai import OpenAIMultiModal from llama_index.core import SimpleDirectoryReader  # put your local directore here image_documents = SimpleDirectoryReader(\"./input_images\").load_data()  openai_mm_llm = OpenAIMultiModal(     model=\"gpt-4o\", api_key=OPENAI_API_KEY, max_new_tokens=1500 )  response_1 = openai_mm_llm.complete(     prompt=\"Describe the images as an alternative text\",     image_documents=image_documents, )  print(response_1) <pre>The images depict information and specifications about electric vehicles, presumably from a car manufacturer's website.\n\nImage 1:\nThis image contains text that lists specifications for two different car models, one with Rear-Wheel Drive and the other with Long Range AWD (All-Wheel Drive). Categories covered include Battery, Weight, Acceleration, Range, Top Speed, Drive, Seating, Wheels, and Warranty.\n\nImage 2:\nThis image shows a cutaway illustration of an electric vehicle highlighting its structural components. The car is rendered to show its internal features such as rigid structure and impact protection zones.\n\nImage 3:\nSimilar to the first image, this image contains text showing specifications for two variants of what appears to be the same model of electric vehicle, with one being a performance model and the other Long Range AWD. The specs include Battery, Acceleration, Range, Drive, Seating, Wheels, Display, Tire Type, Supercharging Max/Power, and Warranty.\n\nImage 4:\nThe image presents pricing and potential savings information for different variants of an electric vehicle model. It includes a federal incentive notice, an area to enter a delivery postal code, purchase price for different versions (Model Y Rear-Wheel Drive, Model Y Long Range, Model Y Performance), and additional feature details. There's also a note about potential savings over gas at the bottom.\n\nImage 5:\nThis image lists specifications for electric vehicles, focused on two categories: Performance and Long Range AWD. Specs listed include Battery, Acceleration, Range, Top Speed, Drive, Seating, Wheels, Display, Tire Type, Supercharging Max/Power, and Warranty.\n\nEach of these images would be used to provide customers with information regarding electric car models, their features, capabilities, pricing, and potential savings.\n</pre> In\u00a0[\u00a0]: Copied! <pre>response_2 = openai_mm_llm.complete(\n    prompt=\"Can you tell me what is the price with each spec?\",\n    image_documents=image_documents,\n)\n\nprint(response_2)\n</pre> response_2 = openai_mm_llm.complete(     prompt=\"Can you tell me what is the price with each spec?\",     image_documents=image_documents, )  print(response_2) <pre>The images you've provided appear to be from a car manufacturer's website, showing different specifications for an electric vehicle and the associated prices for different trim levels or configurations of the vehicle. However, since the actual text content for the price per specification is not fully legible in the images provided, I can't give you precise pricing information. Generally, these types of websites often list the following trims with increasing features and therefore increasing prices:\n\n1. Rear-Wheel Drive (Standard Range or Long Range)\n2. Dual Motor All-Wheel Drive (often dubbed Long Range AWD)\n3. Performance (typically comes with the most features and fastest acceleration)\n\nFeatures like acceleration times, range, top speed, curb weight, cargo volume, seating capacity, display type, drive type, wheels size, warranty, and others can vary by trim level. The images show that there are different specs for the \"Performance\" and \"Long Range AWD\" trims such as acceleration, range, top speed, and potentially others related to power and luxury features.\n\nThe final image provided shows some pricing details:\n- Model 3 Rear-Wheel Drive: $57,990\n- Model 3 Dual Motor All-Wheel Drive: $67,990\n- Model 3 Performance: $74,990\n\nThese prices might be eligible for certain incentives, as indicated by a \"$5,000 Federal Incentive\" notice, which would effectively reduce the purchase price, though this depends on individual eligibility and local laws.\n\nPlease proactively check the manufacturer\u2019s website or reach out to an official dealership for the most accurate and up-to-date information regarding pricing and specifications for these vehicle trims.\n</pre> In\u00a0[\u00a0]: Copied! <pre>import requests\n\n\ndef get_wikipedia_images(title):\n    response = requests.get(\n        \"https://en.wikipedia.org/w/api.php\",\n        params={\n            \"action\": \"query\",\n            \"format\": \"json\",\n            \"titles\": title,\n            \"prop\": \"imageinfo\",\n            \"iiprop\": \"url|dimensions|mime\",\n            \"generator\": \"images\",\n            \"gimlimit\": \"50\",\n        },\n    ).json()\n    image_urls = []\n    for page in response[\"query\"][\"pages\"].values():\n        if page[\"imageinfo\"][0][\"url\"].endswith(\".jpg\") or page[\"imageinfo\"][\n            0\n        ][\"url\"].endswith(\".png\"):\n            image_urls.append(page[\"imageinfo\"][0][\"url\"])\n    return image_urls\n</pre> import requests   def get_wikipedia_images(title):     response = requests.get(         \"https://en.wikipedia.org/w/api.php\",         params={             \"action\": \"query\",             \"format\": \"json\",             \"titles\": title,             \"prop\": \"imageinfo\",             \"iiprop\": \"url|dimensions|mime\",             \"generator\": \"images\",             \"gimlimit\": \"50\",         },     ).json()     image_urls = []     for page in response[\"query\"][\"pages\"].values():         if page[\"imageinfo\"][0][\"url\"].endswith(\".jpg\") or page[\"imageinfo\"][             0         ][\"url\"].endswith(\".png\"):             image_urls.append(page[\"imageinfo\"][0][\"url\"])     return image_urls In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nimport requests\nimport urllib.request\n\nimage_uuid = 0\n# image_metadata_dict stores images metadata including image uuid, filename and path\nimage_metadata_dict = {}\nMAX_IMAGES_PER_WIKI = 20\n\nwiki_titles = {\n    \"Tesla Model Y\",\n    \"Tesla Model X\",\n    \"Tesla Model 3\",\n    \"Tesla Model S\",\n    \"Kia EV6\",\n    \"BMW i3\",\n    \"Audi e-tron\",\n    \"Ford Mustang\",\n    \"Porsche Taycan\",\n    \"Rivian\",\n    \"Polestar\",\n}\n\n\ndata_path = Path(\"mixed_wiki\")\nif not data_path.exists():\n    Path.mkdir(data_path)\n\nfor title in wiki_titles:\n    response = requests.get(\n        \"https://en.wikipedia.org/w/api.php\",\n        params={\n            \"action\": \"query\",\n            \"format\": \"json\",\n            \"titles\": title,\n            \"prop\": \"extracts\",\n            \"explaintext\": True,\n        },\n    ).json()\n    page = next(iter(response[\"query\"][\"pages\"].values()))\n    wiki_text = page[\"extract\"]\n\n    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n        fp.write(wiki_text)\n\n    images_per_wiki = 0\n    try:\n        # page_py = wikipedia.page(title)\n        list_img_urls = get_wikipedia_images(title)\n        # print(list_img_urls)\n\n        for url in list_img_urls:\n            if (\n                url.endswith(\".jpg\")\n                or url.endswith(\".png\")\n                or url.endswith(\".svg\")\n            ):\n                image_uuid += 1\n                # image_file_name = title + \"_\" + url.split(\"/\")[-1]\n\n                urllib.request.urlretrieve(\n                    url, data_path / f\"{image_uuid}.jpg\"\n                )\n                images_per_wiki += 1\n                # Limit the number of images downloaded per wiki page to 15\n                if images_per_wiki &gt; MAX_IMAGES_PER_WIKI:\n                    break\n    except:\n        print(str(Exception(\"No images found for Wikipedia page: \")) + title)\n        continue\n</pre> from pathlib import Path import requests import urllib.request  image_uuid = 0 # image_metadata_dict stores images metadata including image uuid, filename and path image_metadata_dict = {} MAX_IMAGES_PER_WIKI = 20  wiki_titles = {     \"Tesla Model Y\",     \"Tesla Model X\",     \"Tesla Model 3\",     \"Tesla Model S\",     \"Kia EV6\",     \"BMW i3\",     \"Audi e-tron\",     \"Ford Mustang\",     \"Porsche Taycan\",     \"Rivian\",     \"Polestar\", }   data_path = Path(\"mixed_wiki\") if not data_path.exists():     Path.mkdir(data_path)  for title in wiki_titles:     response = requests.get(         \"https://en.wikipedia.org/w/api.php\",         params={             \"action\": \"query\",             \"format\": \"json\",             \"titles\": title,             \"prop\": \"extracts\",             \"explaintext\": True,         },     ).json()     page = next(iter(response[\"query\"][\"pages\"].values()))     wiki_text = page[\"extract\"]      with open(data_path / f\"{title}.txt\", \"w\") as fp:         fp.write(wiki_text)      images_per_wiki = 0     try:         # page_py = wikipedia.page(title)         list_img_urls = get_wikipedia_images(title)         # print(list_img_urls)          for url in list_img_urls:             if (                 url.endswith(\".jpg\")                 or url.endswith(\".png\")                 or url.endswith(\".svg\")             ):                 image_uuid += 1                 # image_file_name = title + \"_\" + url.split(\"/\")[-1]                  urllib.request.urlretrieve(                     url, data_path / f\"{image_uuid}.jpg\"                 )                 images_per_wiki += 1                 # Limit the number of images downloaded per wiki page to 15                 if images_per_wiki &gt; MAX_IMAGES_PER_WIKI:                     break     except:         print(str(Exception(\"No images found for Wikipedia page: \")) + title)         continue In\u00a0[\u00a0]: Copied! <pre>!wget \"https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&amp;dl=1\" -O ./mixed_wiki/tesla_2021_10k.htm\n</pre> !wget \"https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&amp;dl=1\" -O ./mixed_wiki/tesla_2021_10k.htm In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.indices import MultiModalVectorStoreIndex\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\nimport qdrant_client\nfrom llama_index.core import SimpleDirectoryReader\n\n\n# Create a local Qdrant vector store\nclient = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n\ntext_store = QdrantVectorStore(\n    client=client, collection_name=\"text_collection\"\n)\nimage_store = QdrantVectorStore(\n    client=client, collection_name=\"image_collection\"\n)\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n# Create the MultiModal index\ndocuments = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)\n\n# Save it\n# index.storage_context.persist(persist_dir=\"./storage\")\n\n# # Load it\nfrom llama_index.core import load_index_from_storage\n\n# storage_context = StorageContext.from_defaults(\n#     vector_store=text_store, persist_dir=\"./storage\"\n# )\n# index = load_index_from_storage(storage_context, image_store=image_store)\n</pre> from llama_index.core.indices import MultiModalVectorStoreIndex from llama_index.vector_stores.qdrant import QdrantVectorStore from llama_index.core import SimpleDirectoryReader, StorageContext  import qdrant_client from llama_index.core import SimpleDirectoryReader   # Create a local Qdrant vector store client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")  text_store = QdrantVectorStore(     client=client, collection_name=\"text_collection\" ) image_store = QdrantVectorStore(     client=client, collection_name=\"image_collection\" ) storage_context = StorageContext.from_defaults(     vector_store=text_store, image_store=image_store )  # Create the MultiModal index documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data() index = MultiModalVectorStoreIndex.from_documents(     documents,     storage_context=storage_context, )  # Save it # index.storage_context.persist(persist_dir=\"./storage\")  # # Load it from llama_index.core import load_index_from_storage  # storage_context = StorageContext.from_defaults( #     vector_store=text_store, persist_dir=\"./storage\" # ) # index = load_index_from_storage(storage_context, image_store=image_store) In\u00a0[\u00a0]: Copied! <pre>print(response_2.text)\n</pre> print(response_2.text) In\u00a0[\u00a0]: Copied! <pre># generate Text retrieval results\nMAX_TOKENS = 50\nretriever_engine = index.as_retriever(\n    similarity_top_k=3, image_similarity_top_k=3\n)\n# retrieve more information from the GPT4V response\nretrieval_results = retriever_engine.retrieve(response_2.text[:MAX_TOKENS])\n</pre> # generate Text retrieval results MAX_TOKENS = 50 retriever_engine = index.as_retriever(     similarity_top_k=3, image_similarity_top_k=3 ) # retrieve more information from the GPT4V response retrieval_results = retriever_engine.retrieve(response_2.text[:MAX_TOKENS]) In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.response.notebook_utils import display_source_node\nfrom llama_index.core.schema import ImageNode\n\nretrieved_image = []\nfor res_node in retrieval_results:\n    if isinstance(res_node.node, ImageNode):\n        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n    else:\n        display_source_node(res_node, source_length=200)\n\nplot_images(retrieved_image)\n</pre> from llama_index.core.response.notebook_utils import display_source_node from llama_index.core.schema import ImageNode  retrieved_image = [] for res_node in retrieval_results:     if isinstance(res_node.node, ImageNode):         retrieved_image.append(res_node.node.metadata[\"file_path\"])     else:         display_source_node(res_node, source_length=200)  plot_images(retrieved_image) <p>Node ID: 8a67ab30-545c-46ee-a25f-64c95a4571beSimilarity: 0.7758026357212682Text: == Reception == Consumer Reports wrote that the all-wheel-drive Model X 90D largely disappoints, as rear doors are prone to pausing and stopping, the second-row seats that cannot be folded, and the...</p> <p>Node ID: 5db1e928-197d-41d4-b1c1-34d2bcf1cc4dSimilarity: 0.7712850768830459Text: == Design and technology ==</p> <p>=== Body and chassis === The i3 was the first mass production car with most of its internal structure and body made of carbon-fiber reinforced plastic (CFRP). BMW took...</p> <p>Node ID: 89e533c6-3e25-4933-b58a-7d42ac67e957Similarity: 0.768609543932987Text: === Autoshift === Introduced in mid-2021, the Plaid and Long Range versions of the Model S feature no steering column-mounted shift stalk; instead, the Model S uses cameras to infer whether to shif...</p> In\u00a0[\u00a0]: Copied! <pre>response_3 = openai_mm_llm.complete(\n    prompt=\"what are other similar cars?\",\n    image_documents=image_documents,\n)\n\nprint(response_3)\n</pre> response_3 = openai_mm_llm.complete(     prompt=\"what are other similar cars?\",     image_documents=image_documents, )  print(response_3) <pre>The images provided show information about electric vehicles, specifically the Model Y. This is a compact crossover SUV from a prominent electric vehicle manufacturer. When considering similar vehicles in the electric automobile market, you might want to look at the following models that offer comparable characteristics, in terms of performance, size, and luxury:\n\n1. Tesla Model 3 - A smaller sedan from the same manufacturer with similar technology and performance capabilities.\n2. Chevrolet Bolt EUV - A compact electric SUV with semi-autonomous driving capabilities.\n3. Ford Mustang Mach-E - An all-electric SUV that offers performance and technology options.\n4. Volkswagen ID.4 - An electric SUV with a focus on interior space and comfort.\n5. Hyundai Kona Electric - A compact electric SUV with a competitive range and features.\n6. Kia EV6 - An electric crossover with a sporty design and good performance metrics.\n7. Audi Q4 e-tron - A luxury compact electric SUV with a focus on performance and high-end features.\n8. Volvo XC40 Recharge - An electric version of Volvo's popular compact SUV with an emphasis on safety and Scandinavian design.\n\nEach of these vehicles offers a different mix of range, performance, interior space, technology, and price. When comparing them to the Model Y specifications seen in the images, factors such as acceleration, range, weight, cargo volume, and top speed can be used to evaluate their similarities and differences. Keep in mind that new electric vehicle models are continuously being released, so it's always good to check the latest offerings for the most current comparisons.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import PromptTemplate\nfrom llama_index.core.query_engine import SimpleMultiModalQueryEngine\n\nqa_tmpl_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_tmpl = PromptTemplate(qa_tmpl_str)\n\nquery_engine = index.as_query_engine(\n    llm=openai_mm_llm, text_qa_template=qa_tmpl\n)\n\nquery_str = \"Tell me more about the Porsche\"\nresponse = query_engine.query(query_str)\n</pre> from llama_index.core import PromptTemplate from llama_index.core.query_engine import SimpleMultiModalQueryEngine  qa_tmpl_str = (     \"Context information is below.\\n\"     \"---------------------\\n\"     \"{context_str}\\n\"     \"---------------------\\n\"     \"Given the context information and not prior knowledge, \"     \"answer the query.\\n\"     \"Query: {query_str}\\n\"     \"Answer: \" ) qa_tmpl = PromptTemplate(qa_tmpl_str)  query_engine = index.as_query_engine(     llm=openai_mm_llm, text_qa_template=qa_tmpl )  query_str = \"Tell me more about the Porsche\" response = query_engine.query(query_str) In\u00a0[\u00a0]: Copied! <pre>print(str(response))\n</pre> print(str(response)) <pre>The Porsche Taycan represents a significant step for Porsche as their first series production electric car. The Taycan model line includes a diverse range of variants: from the more affordable base rear-wheel-drive (RWD) model to the high-performance all-wheel-drive (AWD) Turbo and Turbo S models. The Taycan is not limited to just the 4-door saloon format but has expanded to include estate variations such as the Taycan Cross Turismo and the Taycan Sport Turismo.\n\nThe interior of the Taycan is a showcase of Porsche's commitment to modernity and technology, with up to four digital displays for instrumentation and infotainment, while still retaining iconic features like the classic Porsche clock. The exterior design is a tribute to Porsche's heritage with contemporary touches, maintaining the brand's visual language.\n\nPerformance-wise, the Taycan offers different power options, with the most powerful Turbo and Turbo S variants reaching 460 kW (617 hp) under specific conditions like overboost power with launch control mode. The Taycan's design incorporates advanced features like a retractable rear spoiler and door handles, and it utilizes a regenerative braking system to optimize efficiency.\n\nThe Taycan has not only impressed customers and the automotive market but has also earned accolades from prestigious entities, with the 4S model being named Performance Car of the Year by What Car? magazine, and the Taycan Cross Turismo gaining recognition as Best Estate in the Top Gear Electric Awards.\n\nMoreover, the concept cars that previewed the Taycan, specifically the Porsche Mission E and the Mission E Cross Turismo, pointed toward Porsche's electric future and set a benchmark in the electric vehicle market for design and performance expectations. The Mission E concept set ambitious goals for range and charging time, leveraging an 800 V DC system voltage for rapid charging capabilities.\n\nOverall, the Porsche Taycan is a blend of traditional Porsche DNA and forward-looking electric vehicle technology, epitomizing high performance, luxury, and sustainability in a package that appeals to both loyal customers and a new generation seeking electric alternatives.\n</pre> In\u00a0[\u00a0]: Copied! <pre># show sources\nfrom llama_index.core.response.notebook_utils import display_source_node\n\nfor text_node in response.metadata[\"text_nodes\"]:\n    display_source_node(text_node, source_length=200)\nplot_images(\n    [n.metadata[\"file_path\"] for n in response.metadata[\"image_nodes\"]]\n)\n</pre> # show sources from llama_index.core.response.notebook_utils import display_source_node  for text_node in response.metadata[\"text_nodes\"]:     display_source_node(text_node, source_length=200) plot_images(     [n.metadata[\"file_path\"] for n in response.metadata[\"image_nodes\"]] ) <p>Node ID: c9dac736-51ce-429a-9b77-96c95a00d91fSimilarity: 0.8241315758378377Text: == Models == The Taycan is currently offered as a 4-door saloon model and a 4-door estate model, the Taycan Cross Turismo. Other planned variants include a two-door coupe and convertible models, wh...</p> <p>Node ID: 531c87f5-fcc4-453e-a013-fa6c9a3a7d24Similarity: 0.822575963523647Text: The Porsche Taycan is a battery electric saloon and shooting brake produced by German automobile manufacturer Porsche. The concept version of the Taycan, named the Porsche Mission E, debuted at the...</p>"},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#advanced-multi-modal-retrieval-using-gpt4v-and-multi-modal-indexretriever","title":"Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index/Retriever\u00b6","text":"<p>In this notebook, we show how to build a Multi-Modal retrieval system using LlamaIndex with GPT4-V and CLIP.</p> <p>LlamaIndex Multi-Modal Retrieval</p> <ul> <li>Text embedding index: Generate GPT text embeddings</li> <li>Images embedding index: CLIP embeddings from OpenAI for images</li> </ul> <p>Encoding queries:</p> <ul> <li>Encode query text for text index using ada</li> <li>Encode query text for image index using CLIP</li> </ul> <p>Framework: LlamaIndex</p> <p>Steps:</p> <ol> <li>Using Multi-Modal LLM GPT4V class to undertand multiple images</li> <li>Download texts, images, pdf raw files from related Wikipedia articles and SEC 10K report</li> <li>Build Multi-Modal index and vetor store for both texts and images</li> <li>Retrieve relevant text and image simultaneously using Multi-Modal Retriver according to the image reasoning from Step 1</li> </ol>"},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#download-images-from-tesla-website-for-gpt4v-image-reasoning","title":"Download images from Tesla website for GPT4V image reasoning\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#generate-image-reasoning-from-gpt4v-multi-modal-llm","title":"Generate image reasoning from GPT4V Multi-Modal LLM\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#plot-input-images","title":"Plot input images\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#using-gpt4v-to-understand-those-input-images","title":"Using GPT4V to understand those input images\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#generating-text-pdf-images-data-from-raw-files-wikipedia-sec-files-for-multi-modal-indexretrieval","title":"Generating text, pdf, images data from raw files [Wikipedia, SEC files] for Multi Modal Index/Retrieval\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#build-multi-modal-index-and-vector-store-to-index-both-text-and-images","title":"Build Multi-modal index and vector store to index both text and images\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#retrieve-and-query-texts-and-images-from-our-multi-modal-index","title":"Retrieve and query texts and images from our Multi-Modal Index\u00b6","text":"<p>We show two examples leveraging multi-modal retrieval.</p> <ol> <li>Retrieval-Augmented Captioning: In the first example, we perform multi-modal retrieval based on an existing image caption, to return more relevant context. We can then continue to query the LLM for related vehicles.</li> <li>Multi-modal RAG Querying: In the second example, given a user-query, we first retrieve a mix of both text and images, and feed it to an LLM for synthesis.</li> </ol>"},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#1-retrieval-augmented-captioning","title":"1. Retrieval-Augmented Captioning\u00b6","text":""},{"location":"RAG/13_Vision_RAG/gpt4v_multi_modal_retrieval/#2-multi-modal-rag-querying","title":"2. Multi-Modal RAG Querying\u00b6","text":""},{"location":"RAG/14_CAG/CAG/","title":"Cache Augmented Generation(CAG)","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/hhhuang/CAG.git\n%cd CAG\n%pwd\n</pre> !git clone https://github.com/hhhuang/CAG.git %cd CAG %pwd In\u00a0[\u00a0]: Copied! <pre>!pip install -r ./requirements.txt\n</pre> !pip install -r ./requirements.txt In\u00a0[\u00a0]: Copied! <pre>!pip uninstall torch torchvision torchaudio --yes\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n</pre> !pip uninstall torch torchvision torchaudio --yes !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 In\u00a0[\u00a0]: Copied! <pre>!sh ./downloads.sh\n</pre> !sh ./downloads.sh <p>Datasets used:</p> <ul> <li>SQuAD: Dataset which focuses on precise, context-aware answers withing single passages</li> <li>HotPotQA: Dataset which focuses on multihop reasoning questions across multiple documents.</li> </ul> <p>To add your HF_TOKEN key to Colab secrets:</p> <ol> <li>Click on the \"\ud83d\udd11 Secrets\" tab in the left sidebar of your Colab notebook.</li> <li>Click the \"+\" button to add a new secret.</li> <li>In the \"Name\" field, enter <code>HF_TOKEN</code>.</li> <li>In the \"Value\" field, paste your HF Token.</li> <li>Make sure the \"Notebook access\" toggle is enabled for this secret.</li> <li>Click \"Done\".</li> </ol> In\u00a0[11]: Copied! <pre>import os\nfrom google.colab import userdata\nos.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n</pre> import os from google.colab import userdata os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN') <p>Create a HF_TOKEN with Write Access, create a new token at https://huggingface.co/settings/tokens</p> In\u00a0[\u00a0]: Copied! <pre>!python ./kvcache.py --kvcache file --dataset \"squad-train\" --similarity bertscore \\\n    --maxKnowledge 1 --maxParagraph 1 --maxQuestion 1 \\\n    --modelname \"meta-llama/Llama-3.2-8B-Instruct\" --randomSeed 0 \\\n    --output \"./result_kvcache.txt\"\n</pre> !python ./kvcache.py --kvcache file --dataset \"squad-train\" --similarity bertscore \\     --maxKnowledge 1 --maxParagraph 1 --maxQuestion 1 \\     --modelname \"meta-llama/Llama-3.2-8B-Instruct\" --randomSeed 0 \\     --output \"./result_kvcache.txt\" In\u00a0[\u00a0]: Copied! <pre>!python ./rag.py --index \"bm25\" --dataset \"hotpotqa-train\" --similarity bertscore \\\n    --maxKnowledge 80 --maxParagraph 100 --maxQuestion 80 --topk 3 \\\n    --modelname \"meta-llama/Llama-3.2-8B-Instruct\" --randomSeed 0 \\\n    --output \"./rag_results.txt\"\n</pre> !python ./rag.py --index \"bm25\" --dataset \"hotpotqa-train\" --similarity bertscore \\     --maxKnowledge 80 --maxParagraph 100 --maxQuestion 80 --topk 3 \\     --modelname \"meta-llama/Llama-3.2-8B-Instruct\" --randomSeed 0 \\     --output \"./rag_results.txt\""},{"location":"RAG/14_CAG/CAG/#cache-augmented-generation-cag-explained","title":"Cache-Augmented Generation (CAG) Explained\u00b6","text":"<p>This notebook provides an explanation of Cache-Augmented Generation (CAG), its advantages, limitations, and a guide on how to set up and run the provided code for testing CAG.</p>"},{"location":"RAG/14_CAG/CAG/#what-is-cache-augmented-generation-cag","title":"What is Cache-Augmented Generation (CAG)?\u00b6","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful approach that enhances language models by integrating external knowledge. However, RAG faces challenges like retrieval latency, retrieval errors, and system complexity.</p> <p>Cache-Augmented Generation (CAG) is an alternative paradigm designed to bypass real-time retrieval. It leverages the extended context windows of modern Large Language Models (LLMs) by preloading all relevant resources into the model's context and caching its runtime parameters (specifically, the Key-Value (KV) cache). During inference, the preloaded KV-cache allows the model to generate responses directly, eliminating the need for dynamic retrieval.</p>"},{"location":"RAG/14_CAG/CAG/#advantages-of-cag","title":"Advantages of CAG\u00b6","text":"<ul> <li>Reduced Latency: Eliminates real-time retrieval, leading to faster inference.</li> <li>Improved Reliability: Minimizes retrieval errors while maintaining context relevance.</li> <li>Simplified Design: Offers a streamlined, retrieval-free alternative to RAG, capable of achieving comparable or superior results with lower complexity.</li> </ul>"},{"location":"RAG/14_CAG/CAG/#limitations-of-cag","title":"Limitations of CAG\u00b6","text":"<ul> <li>Limited Context Size Window of LLM: CAG requires the entire knowledge source to fit within the context window, making it less suitable for tasks involving extremely large datasets.</li> <li>Context being too large: The performance of LLMs may degrade with very long contexts.</li> </ul>"},{"location":"RAG/14_CAG/CAG/#architectural-overview-of-cag-and-rag","title":"Architectural Overview of CAG and RAG\u00b6","text":"<p>In CAG, the query is appended to the KV Cache and then LLM generates the answer and after that KV Cache is reset to original by truncating query. So, that the KV Cache size does not keep on increasing, if it would have happened then it would have consumed the whole LLM Context Window.</p>"},{"location":"RAG/14_CAG/CAG/#installation","title":"Installation\u00b6","text":"<p>To get started, first ensure you have the necessary dependencies installed. You can do this by running the following command in your terminal:</p>"},{"location":"RAG/14_CAG/CAG/#preparation","title":"Preparation\u00b6","text":"<p>Before running the CAG experiments, you need to perform a couple of preparation steps:</p>"},{"location":"RAG/14_CAG/CAG/#1-download-datasets","title":"1. Download Datasets\u00b6","text":"<p>Download the required <code>squad</code> and <code>hotpotqa</code> datasets by running the provided curl script:</p>"},{"location":"RAG/14_CAG/CAG/#2-configure-environment-variables","title":"2. Configure Environment Variables\u00b6","text":"<p>Insert your <code>HF_TOKEN</code> for Hugging Face authentication. If <code>HF_TOKEN</code> is not found, the script will raise an error.</p>"},{"location":"RAG/14_CAG/CAG/#setup","title":"Setup\u00b6","text":"<p>Model used here for testing CAG and RAG is Llama 3.2-1B Model which has a context size of 128k tokens</p> <p>In RAG, we comapre the results of CAG with 2 metrics:</p> <ul> <li>BM25 (Sparse Retrieval):  BM25,a sparse retrieval algorithm, ranks documents based on term frequency inverse document frequency (TF-IDF) and document length normalization</li> <li>Dense Retrieval System (OpenAI Indexes):  For a query q, dense retrieval selects the top-k passages p that semantically align with the query, offering improved contextual understanding compared to sparse methods.</li> </ul>"},{"location":"RAG/14_CAG/CAG/#running-cag-experiments-with-kvcachepy","title":"Running CAG Experiments with <code>kvcache.py</code>\u00b6","text":"<p>The <code>kvcache.py</code> script is designed for running CAG experiments. It preprocesses knowledge into a KV cache and then uses this cache for generating responses to questions, evaluating performance based on semantic similarity and generation time.</p>"},{"location":"RAG/14_CAG/CAG/#key-concepts-from-kvcachepy","title":"Key Concepts from <code>kvcache.py</code>\u00b6","text":"<ul> <li>KV Cache (Key-Value Cache): In transformer models, the KV cache stores the 'keys' and 'values' from previous token computations. This prevents re-computation for each new token during sequential decoding, significantly speeding up inference.</li> <li><code>preprocess_knowledge</code> function: This function takes a knowledge prompt, tokenizes it, and runs it through the model to generate the initial KV cache (lines 80-99 in <code>kvcache.py</code>). This precomputed cache represents the 'augmented knowledge' in CAG.</li> <li><code>write_kv_cache</code> and <code>read_kv_cache</code> functions: These functions handle saving and loading the precomputed KV cache to/from disk (lines 102-126 in <code>kvcache.py</code>). This allows for persistent storage and reuse of the knowledge cache.</li> <li><code>generate</code> function: This function performs greedy decoding using the pre-existing <code>past_key_values</code> (KV cache) to generate new tokens (lines 40-77 in <code>kvcache.py</code>). In the CAG context, this <code>past_key_values</code> is the knowledge cache.</li> <li><code>clean_up</code> function: This function truncates the KV cache to its original length, which is important when reusing the same knowledge cache for multiple queries to prevent it from growing indefinitely (lines 114-118 in <code>kvcache.py</code>).</li> </ul>"},{"location":"RAG/14_CAG/CAG/#kvcachepy-parameters","title":"<code>kvcache.py</code> Parameters\u00b6","text":"<p>Here are the important parameters you can use with <code>kvcache.py</code>:</p> <ul> <li><code>--kvcache</code>: Specifies the KV cache method. Use <code>\"file\"</code> to read/write cache from/to a file.</li> <li><code>--dataset</code>: The dataset to use for experiments. Options include <code>\"hotpotqa-train\"</code>, <code>\"squad-train\"</code>, <code>\"kis\"</code>, <code>\"kis_sample\"</code>, <code>\"squad-dev\"</code>, <code>\"hotpotqa-dev\"</code>, <code>\"hotpotqa-test\"</code></li> <li><code>--similarity</code>: The similarity metric for evaluation. Currently, <code>\"bertscore\"</code> is supported.</li> <li><code>--modelname</code>: The name of the Hugging Face model to use (e.g., <code>\"meta-llama/Llama-3.1-8B-Instruct\"</code>).</li> <li><code>--maxKnowledge</code>:  Integer. Selects how many documents from the dataset to use as knowledge.</li> <li><code>--maxParagraph</code>:  Integer. Limits the number of paragraphs per knowledge document (default: 100).</li> <li><code>--maxQuestion</code>:  Integer. Specifies the maximum number of questions to test.</li> <li><code>--randomSeed</code>:  Integer. A random seed number for reproducibility.</li> <li><code>--output</code>: String. The filepath to save the results of the experiment.</li> <li><code>--usePrompt</code>: (Flag) Add this parameter if you do NOT want to use the CAG knowledge cache acceleration and instead include the knowledge directly in the prompt (disables caching).</li> </ul>"},{"location":"RAG/14_CAG/CAG/#notes-on-parameters","title":"Notes on Parameters\u00b6","text":""},{"location":"RAG/14_CAG/CAG/#-maxknowledge-parameter-notice","title":"<code>--maxKnowledge</code> parameter notice:\u00b6","text":"<p>Approximate Tokens count corresponding to knowledge document size of <code>\"squad-train\"</code> and <code>\"hotpotqa-train\"</code> dataset.</p> <ul> <li><p><code>datasets=(\"squad-train\")</code></p> <ul> <li>when k = 3, tokens = 21,000</li> <li>when k = 4, tokens = 32,000</li> <li>when k = 7, tokens = 50,000</li> </ul> </li> <li><p><code>datasets=(\"hotpotqa-train\")</code></p> <ul> <li>all k = 7405 article, tokens = 10,038,084</li> <li>when k = 1, tokens = 1,400</li> <li>when k = 16, tokens = 22,400</li> <li>when k = 24, tokens = 33,667</li> <li>when k = 32, tokens = 44,800</li> <li>when k = 48, tokens = 64,000</li> <li>when k = 64, tokens = 85,000</li> <li>when k = 80, tokens = 106,000</li> </ul> </li> </ul>"},{"location":"RAG/14_CAG/CAG/#-maxquestion-parameter-notice","title":"<code>--maxQuestion</code> parameter notice:\u00b6","text":"<ul> <li>When using <code>\"squad-train\"</code> dataset, 1 knowledge has an average of 150 questions.</li> <li>When using <code>\"hotpotqa-train\"</code> dataset, 1 knowledge has 1 question.</li> </ul> <p>TIP: Since 1 document in <code>\"hotpoqa-train\"</code> dataset has only 1 question, it may not satisfy large-scale evaluation. Multiple evaluations could be a relatively better approach.</p>"},{"location":"RAG/14_CAG/CAG/#example-usage-cag","title":"Example Usage - CAG\u00b6","text":"<p>To run a CAG experiment, you can use a command similar to this:</p>"},{"location":"RAG/14_CAG/CAG/#running-rag-experiments-with-ragpy","title":"Running RAG Experiments with <code>rag.py</code>\u00b6","text":"<p>For comparison purposes, this repository also includes a traditional RAG implementation in <code>rag.py</code>. This allows you to benchmark CAG performance against conventional retrieval-augmented generation approaches.</p>"},{"location":"RAG/14_CAG/CAG/#key-differences-rag-vs-cag","title":"Key Differences: RAG vs CAG\u00b6","text":"Aspect RAG CAG Retrieval Dynamic, on-demand Pre-loaded, cached Latency Higher (retrieval overhead) Lower (no retrieval step) Knowledge Size Can handle large datasets Limited by context window Complexity Higher system complexity Simpler, streamlined Memory Usage Lower during inference Higher (cached knowledge) Retrieval Errors Possible Eliminated"},{"location":"RAG/14_CAG/CAG/#ragpy-parameters","title":"<code>rag.py</code> Parameters\u00b6","text":"<p>Here are the important parameters you can use with <code>rag.py</code>:</p> <ul> <li><code>--index</code>: The indexing method for retrieval. Options: <code>\"openai\"</code> or <code>\"bm25\"</code></li> <li><code>--dataset</code>: The dataset to use for experiments. Options: <code>\"hotpotqa-train\"</code> or <code>\"squad-train\"</code></li> <li><code>--similarity</code>: The similarity metric for evaluation. Currently, <code>\"bertscore\"</code> is supported.</li> <li><code>--maxKnowledge</code>: (Optional) Integer. Selects how many documents from the dataset to use as knowledge.</li> <li><code>--maxParagraph</code>: (Optional) Integer. Limits the number of paragraphs per knowledge document (default: 100).</li> <li><code>--maxQuestion</code>: (Optional) Integer. Specifies the maximum number of questions to test.</li> <li><code>--topk</code>: Integer. The number of top similar documents to retrieve for each query.</li> <li><code>--modelname</code>: The name of the Hugging Face model to use (e.g., <code>\"meta-llama/Llama-3.1-8B-Instruct\"</code>).</li> <li><code>--randomSeed</code>: (Optional) Integer. A random seed number for reproducibility.</li> <li><code>--output</code>: String. The filepath to save the results of the experiment.</li> </ul>"},{"location":"RAG/14_CAG/CAG/#example-usage-rag","title":"Example Usage - RAG\u00b6","text":"<p>To run a RAG experiment for comparison with CAG, you can use a command similar to this:</p>"},{"location":"RAG/14_CAG/CAG/#note-you-can-download-the-ouptut-results-from-files-tab-in-the-left-sidebar-of-your-colab-notebook","title":"NOTE: You can download the ouptut results from  \"Files\" tab in the left sidebar of your Colab notebook.\u00b6","text":""},{"location":"RAG/14_CAG/CAG/#mind-map-of-cag-process","title":"Mind Map of CAG Process\u00b6","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/","title":"Introducing AI Engineering Academy","text":""},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#welcome-to-ai-engineering-academy-shaping-the-future-of-applied-ai","title":"Welcome to AI Engineering Academy: Shaping the Future of Applied AI","text":"<p>The world of Artificial Intelligence (AI) is evolving rapidly, with breakthroughs happening almost daily. While this is exciting, it also presents a challenge: navigating the overwhelming number of resources, tools, and methodologies available. That\u2019s where AI Engineering Academy comes in.</p> <p>We\u2019re proud to introduce AI Engineering Academy, an open-source learning platform designed to provide a structured and comprehensive learning path for aspiring AI engineers. Whether you\u2019re a student, a professional transitioning into AI, or a seasoned developer deepening your expertise, our platform offers clear guidance and practical tools.</p>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#our-mission","title":"Our Mission","text":"<p>AI Engineering Academy is built to address three core goals:</p> <ol> <li>Organizing Knowledge: Offering a well-curated roadmap to help learners progress systematically, from foundational concepts to advanced applications.</li> <li>Focusing on Application: Bridging the gap between theoretical understanding and real-world implementation through practical projects and case studies.</li> <li>Fostering Collaboration: Building a space for learners, educators, and industry experts to connect, share knowledge, and grow together.</li> </ol>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#what-makes-us-different","title":"What Makes Us Different","text":"<p>In a crowded field of online courses and tutorials, AI Engineering Academy focuses on providing depth and structure:</p>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#1-open-source-principles","title":"1. Open-Source Principles","text":"<p>Our resources are freely available and designed to be accessible to anyone. We believe that sharing knowledge openly accelerates learning and innovation for all.</p>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#2-structured-learning-paths","title":"2. Structured Learning Paths","text":"<p>Rather than offering isolated courses, we provide a guided curriculum that evolves with your skills. Our topics include:</p> <ul> <li>Foundations: Linear algebra, probability, statistics, and Python programming.</li> <li>Core AI: Machine learning, deep learning, natural language processing, and computer vision.</li> <li>Advanced AI: Generative models, reinforcement learning, and edge AI.</li> <li>Practical Skills: Deployment strategies, MLOps, and integrating AI into scalable applications.</li> </ul>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#3-project-based-learning","title":"3. Project-Based Learning","text":"<p>We emphasize hands-on learning through real-world projects. Each module includes tasks designed to help you apply your skills and build a portfolio of work that demonstrates your abilities.</p>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#4-community-engagement","title":"4. Community Engagement","text":"<p>Our platform connects learners and experts, making it easy to exchange ideas, solve problems, and find mentorship opportunities. Collaboration is at the heart of progress.</p>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#who-should-join","title":"Who Should Join?","text":"<p>AI Engineering Academy is for:</p> <ul> <li>Beginners: Start with foundational courses and build your confidence.</li> <li>Intermediate Learners: Expand your knowledge and tackle advanced topics.</li> <li>Professionals: Acquire specialized skills or transition into AI-focused roles.</li> </ul>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#whats-next","title":"What\u2019s Next?","text":"<p>We\u2019re continually expanding our offerings to ensure a comprehensive learning experience. Here\u2019s what\u2019s on the horizon:</p> <ul> <li>Interactive Workshops: Learn directly from AI practitioners through live sessions.</li> <li>Hackathons: Collaborate on solving real-world challenges.</li> <li>Career Resources: Get guidance on building your portfolio and navigating AI job opportunities.</li> <li>Platform Enhancements: Regular updates based on feedback to improve usability and content.</li> </ul>"},{"location":"blog/2025/01/01/introducing-ai-engineering-academy/#start-learning-today","title":"Start Learning Today","text":"<p>AI is transforming industries and creating opportunities worldwide. At AI Engineering Academy, we\u2019re committed to helping you acquire the skills needed to thrive in this dynamic field.</p> <p>Visit AI Engineering Academy to explore our resources and join a growing network of learners and professionals. Let\u2019s get to work building the future of applied AI.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/general/","title":"General","text":""}]}