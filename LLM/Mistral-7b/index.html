<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/LLM/Mistral-7b/ rel=canonical><link href=../LLama2/Llama_2_Fine_Tuning_using_QLora/ rel=prev><link href=Mistral_finetuning_notebook/ rel=next><link rel=icon href=../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.0"><title>A Beginner’s Guide to Fine-Tuning Mistral 7B Instruct Model - AI Engineering Academy</title><link rel=stylesheet href=../../assets/stylesheets/main.618322db.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
  posthog.init('phc_OL7nUCVeKtVJe8eHSKGs8zPTQAyr0hm8opAPFdFlkBz', {
      api_host: 'https://us.i.posthog.com',
      person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
  })
</script><meta property=og:type content=website><meta property=og:title content="A Beginner’s Guide to Fine-Tuning Mistral 7B Instruct Model - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/LLM/Mistral-7b/index.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/LLM/Mistral-7b/ property=og:url><meta property=twitter:card content=summary_large_image><meta property=twitter:title content="A Beginner’s Guide to Fine-Tuning Mistral 7B Instruct Model - AI Engineering Academy"><meta property=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta property=twitter:image content=https://aiengineering.academy/assets/images/social/LLM/Mistral-7b/index.png></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#meet-mistral-7b-instruct class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> A Beginner’s Guide to Fine-Tuning Mistral 7B Instruct Model </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../RAG/ class=md-tabs__link> RAG </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> LLM </a> </li> <li class=md-tabs__item> <a href=../../Deployment/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../AIBreakDown/TRM/ class=md-tabs__link> AI BreakDown </a> </li> <li class=md-tabs__item> <a href=../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> LLM </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> Finetuning Techniques </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Finetuning Techniques </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/PreTrain/ class=md-nav__link> <span class=md-ellipsis> PreTraining LLMs </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../TheoryBehindFinetuning/SFT/ class=md-nav__link> <span class=md-ellipsis> SFT </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/PPO/ class=md-nav__link> <span class=md-ellipsis> PPO(Proximal Policy Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/DPO/ class=md-nav__link> <span class=md-ellipsis> DPO(Direct Preference Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/ORPO/ class=md-nav__link> <span class=md-ellipsis> ORPO(Odds Ratio Preference Optimization) </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../TheoryBehindFinetuning/GRPO/ class=md-nav__link> <span class=md-ellipsis> GRPO(Group Relative Policy Optimization) </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3 checked> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> LLM Finetuning Hands on </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=true> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> LLM Finetuning Hands on </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../Gemma/ class=md-nav__link> <span class=md-ellipsis> Gemma </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../LLama2/Llama2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Llama2 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../Llama3_finetuning_notebook.ipynb class=md-nav__link> <span class=md-ellipsis> Llama3 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_4 checked> <div class="md-nav__link md-nav__container"> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Mistral </span> </a> <label class="md-nav__link md-nav__link--active" for=__nav_4_3_4 id=__nav_4_3_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4_3_4> <span class="md-nav__icon md-icon"></span> Mistral </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=Mistral_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Finetune Mistral </span> </a> </li> <li class=md-nav__item> <a href=LLM_evaluation_harness_for_Arc_Easy_and_SST/ class=md-nav__link> <span class=md-ellipsis> Evaluation Mistral </span> </a> </li> <li class=md-nav__item> <a href=notebooks_DPO_fine_tuning/ class=md-nav__link> <span class=md-ellipsis> DPO Fine-tuning Mistral </span> </a> </li> <li class=md-nav__item> <a href=notebooks_SFTTrainer%20TRL/ class=md-nav__link> <span class=md-ellipsis> SFT Trainer Mistral </span> </a> </li> <li class=md-nav__item> <a href=notebooks_chatml_inference/ class=md-nav__link> <span class=md-ellipsis> ChatML Inference </span> </a> </li> <li class=md-nav__item> <a href=../Mixtral/Mixtral_fine_tuning/ class=md-nav__link> <span class=md-ellipsis> Mixtral </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> VLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> VLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../VLM/Florence2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Florence2 </span> </a> </li> <li class=md-nav__item> <a href=../VLM/PaliGemma_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> PaliGemma </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_5> <div class="md-nav__link md-nav__container"> <a href=../ServerLessFinetuning/ class="md-nav__link "> <span class=md-ellipsis> Serverless Finetuning with Modal </span> </a> <label class="md-nav__link " for=__nav_4_5 id=__nav_4_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> Serverless Finetuning with Modal </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ServerLessFinetuning/TrainNanoGPTModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Training NanoGPT on Modal </span> </a> </li> <li class=md-nav__item> <a href=../ServerLessFinetuning/TrainNanochatModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Training Nanochat on Modal </span> </a> </li> <li class=md-nav__item> <a href=../ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Fine-tuning Gemma with Unsloth </span> </a> </li> <li class=md-nav__item> <a href=../ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Multi-GPU Training with Axolotl </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_6> <div class="md-nav__link md-nav__container"> <a href=../LLMArchitecture/ParameterCount/ class="md-nav__link "> <span class=md-ellipsis> LLM Architecture </span> </a> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_6> <span class="md-nav__icon md-icon"></span> LLM Architecture </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../AIBreakDown/TRM/ class=md-nav__link> <span class=md-ellipsis> AI BreakDown </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#meet-mistral-7b-instruct class=md-nav__link> <span class=md-ellipsis> Meet Mistral 7B Instruct </span> </a> </li> <li class=md-nav__item> <a href=#colab-notebook-to-finetuning-mistral-7b-instruct class=md-nav__link> <span class=md-ellipsis> Colab Notebook to Finetuning Mistral-7b-Instruct </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#getting-started class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> </li> <li class=md-nav__item> <a href=#loading-required-libraries class=md-nav__link> <span class=md-ellipsis> Loading Required Libraries </span> </a> </li> <li class=md-nav__item> <a href=#logging-into-hugging-face-hub class=md-nav__link> <span class=md-ellipsis> Logging into Hugging Face Hub </span> </a> </li> <li class=md-nav__item> <a href=#loading-the-dataset class=md-nav__link> <span class=md-ellipsis> Loading the Dataset </span> </a> </li> <li class=md-nav__item> <a href=#formatting-the-dataset class=md-nav__link> <span class=md-ellipsis> Formatting the Dataset </span> </a> </li> <li class=md-nav__item> <a href=#after-formatting class=md-nav__link> <span class=md-ellipsis> After Formatting </span> </a> </li> <li class=md-nav__item> <a href=#loading-the-training-dataset class=md-nav__link> <span class=md-ellipsis> Loading the Training Dataset </span> </a> </li> <li class=md-nav__item> <a href=#setting-model-parameters class=md-nav__link> <span class=md-ellipsis> Setting Model Parameters </span> </a> <nav class=md-nav aria-label="Setting Model Parameters"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> ########################################################################## </span> </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> ########################################################################## </span> </a> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> </span> </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> ########################################################################## </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#loading-the-base-model class=md-nav__link> <span class=md-ellipsis> Loading the Base Model </span> </a> </li> <li class=md-nav__item> <a href=#base-model-inference class=md-nav__link> <span class=md-ellipsis> Base model Inference </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning-with-qlora-and-supervised-finetuning class=md-nav__link> <span class=md-ellipsis> Fine-Tuning with qLora and Supervised Finetuning </span> </a> </li> <li class=md-nav__item> <a href=#lets-start-the-training-process class=md-nav__link> <span class=md-ellipsis> Lets start the training process </span> </a> </li> <li class=md-nav__item> <a href=#inference-with-fine-tuned-model class=md-nav__link> <span class=md-ellipsis> Inference with Fine-Tuned Model </span> </a> </li> <li class=md-nav__item> <a href=#merge-and-share class=md-nav__link> <span class=md-ellipsis> Merge and Share </span> </a> </li> <li class=md-nav__item> <a href=#test-the-merged-model class=md-nav__link> <span class=md-ellipsis> Test the merged model </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/LLM/Mistral-7b/README.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/LLM/Mistral-7b/README.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1>Finetune Mistral</h1> <div><p>URL Source: <a href=https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe>https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe</a></p> <p>Published Time: 2023-10-06T18:30:13.121Z</p> <p>Markdown Content: Fine-Tuning for Code Generation Using a Single Google Colab Notebook</p> <hr> <p><a href="https://adithyask.medium.com/?source=post_page-----0f39647b20fe--------------------------------"><img alt="Image 1: Adithya S K" src=https://miro.medium.com/v2/resize:fill:88:88/1*w1_VSVDg5oqt19oTB4MAMg.jpeg></a></p> <blockquote> <p><em>Updated : 10<sup>th</sup> December 2023</em></p> </blockquote> <p>Fine-tuning a state-of-the-art language model like Mistral 7B Instruct can be an exciting journey. This guide will walk you through the process step by step, from setting up your environment to fine-tuning the model for your specific task. Whether you’re a seasoned machine learning practitioner or a newcomer to the field, this beginner-friendly tutorial will help you harness the power of Mistral 7B for your projects.</p> <h2 id=meet-mistral-7b-instruct>Meet Mistral 7B Instruct<a class=headerlink href=#meet-mistral-7b-instruct title="Permanent link">¶</a></h2> <p>The team at <a href=https://mistral.ai/news/announcing-mistral->MistralAI</a> has created an exceptional language model called Mistral 7B Instruct. It has consistently delivered outstanding results in a range of benchmarks, which positions it as an ideal option for natural language generation and understanding. This guide will concentrate on how to fine-tune the model for coding purposes, but the methodology can effectively be applied to other tasks.</p> <h2 id=colab-notebook-to-finetuning-mistral-7b-instruct>Colab Notebook to Finetuning Mistral-7b-Instruct<a class=headerlink href=#colab-notebook-to-finetuning-mistral-7b-instruct title="Permanent link">¶</a></h2> <p>Code has been updated on December 10<sup>th</sup> , 2023</p> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">¶</a></h2> <p>Before diving into the fine-tuning process, make sure you have the following prerequisites in place:</p> <ol> <li><strong>GPU</strong>: While this tutorial can run on a free Google Colab notebook with a GPU, it’s recommended to use more powerful GPUs like V100 or A100 for better performance.</li> <li><strong>Python Packages</strong>: Ensure you have the required Python packages installed. You can run the following commands to install them:</li> </ol> <p>!pip install -q torch<br> !pip install -q git+<a href=https://github.com/huggingface/transformers>https://github.com/huggingface/transformers</a> #huggingface transformers for downloading models weights<br> !pip install -q datasets #huggingface datasets to download and manipulate datasets<br> !pip install -q peft #Parameter efficient finetuning - for qLora Finetuning<br> !pip install -q bitsandbytes #For Model weights quantisation<br> !pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning<br> !pip install -q wandb -U #Used to monitor the model score during training</p> <ol> <li><strong>Hugging Face Hub Account</strong>: You’ll need an account on the Hugging Face Model Hub. You can sign up <a href=https://huggingface.co/join>here</a>.</li> </ol> <h2 id=getting-started>Getting Started<a class=headerlink href=#getting-started title="Permanent link">¶</a></h2> <p>Let’s start by checking if your GPU is correctly detected:</p> <p>!nvidia-smi</p> <p>If your GPU is not recognized or you encounter CUDA out-of-memory errors during fine-tuning, consider using a more powerful GPU.</p> <h2 id=loading-required-libraries>Loading Required Libraries<a class=headerlink href=#loading-required-libraries title="Permanent link">¶</a></h2> <p>We’ll load the necessary Python libraries for our fine-tuning process:</p> <p>import json<br> import pandas as pd<br> import torch<br> from datasets import Dataset, load_dataset<br> from huggingface_hub import notebook_login<br> from peft import LoraConfig, PeftModel<br> from transformers import (<br> AutoModelForCausalLM,<br> AutoTokenizer,<br> BitsAndBytesConfig,<br> TrainingArguments,<br> pipeline,<br> logging,<br> )<br> from trl import SFTTrainer</p> <h2 id=logging-into-hugging-face-hub>Logging into Hugging Face Hub<a class=headerlink href=#logging-into-hugging-face-hub title="Permanent link">¶</a></h2> <p>Log in to the Hugging Face Model Hub using your credentials:</p> <p>notebook_login()</p> <h2 id=loading-the-dataset>Loading the Dataset<a class=headerlink href=#loading-the-dataset title="Permanent link">¶</a></h2> <p>For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.</p> <p>we will be using this <a href=https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style>dataset</a> which is curated by <a href=https://twitter.com/4evaBehindSOTA>TokenBender (e/xperiments)</a> which is a awesome data for finetuning model for code generation. It follows the alpaca style of instructions which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <p>{<br> "instruction": "Create a function to calculate the sum of a sequence of integers.",<br> "input":"[1, 2, 3, 4, 5]",<br> "output": "# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum"<br> }</p> <p>now lets load the dataset using huggingfaces datasets library</p> <p># Load your dataset (replace 'your_dataset_name' and 'split_name' with your actual dataset information)<br> # dataset = load_dataset("your_dataset_name", split="split_name")<br> dataset = load_dataset("TokenBender/code_instructions_122k_alpaca_style", split="train")</p> <h2 id=formatting-the-dataset>Formatting the Dataset<a class=headerlink href=#formatting-the-dataset title="Permanent link">¶</a></h2> <p>Now, let’s format the dataset in the required <a href=https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1>Mistral-7B-Instruct-v0.1 format</a>.</p> <blockquote> <p><em>Many tutorial and blogs skip over this part but i feel this is a really important step.</em></p> </blockquote> <p>We’ll put each instruction and input pair between <code>[INST]</code> and <code>[/INST]</code> output after that, like this:</p> <p>&lt;s&gt;[INST] What is your favourite condiment? [/INST]<br> Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!&lt;/s&gt;</p> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p> <p># this function is used to output the right formate for each row in the dataset<br> def create_text_row(instruction, output, input):<br> text_row = f"""&lt;s&gt;[INST] {instruction} here are the inputs {input} [/INST] \\n {output} &lt;/s&gt;"""<br> return text_row# interate over all the rows formate the dataset and store it in a jsonl file<br> def process_jsonl_file(output_file_path):<br> with open(output_file_path, "w") as output_jsonl_file:<br> for item in dataset:<br> json_object = {<br> "text": create_text_row(item["instruction"], item["input"] ,item["output"]),<br> "instruction": item["instruction"],<br> "input": item["input"],<br> "output": item["output"]<br> }<br> output_jsonl_file.write(json.dumps(json_object) + "\\n")</p> <p># Provide the path where you want to save the formatted dataset<br> process_jsonl_file("./training_dataset.jsonl")</p> <h2 id=after-formatting>After Formatting<a class=headerlink href=#after-formatting title="Permanent link">¶</a></h2> <p>{<br> "text":"&lt;s&gt;[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]<br> # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum&lt;/s&gt;",<br> "instruction":"Create a function to calculate the sum of a sequence of integers",<br> "input":"[1, 2, 3, 4, 5]",<br> "output":"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum"<br> }</p> <p>While using SFT(<a href=https://huggingface.co/docs/trl/main/en/sft_trainer>Supervised Fine-tuning Trainer</a>) to finetune we will be only passing in the “text” column of the dataset for finetuning.</p> <h2 id=loading-the-training-dataset>Loading the Training Dataset<a class=headerlink href=#loading-the-training-dataset title="Permanent link">¶</a></h2> <p>Now, let’s load the training dataset from the JSONL file we created:</p> <p>train_dataset = load_dataset('json', data_files='./training_dataset.jsonl' , split='train')</p> <h2 id=setting-model-parameters>Setting Model Parameters<a class=headerlink href=#setting-model-parameters title="Permanent link">¶</a></h2> <p>We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:</p> <p>new_model = "mistralai-Code-Instruct" #set the name of the new model################################################################################<br> # QLoRA parameters </p> <h6 id=_1><a class=headerlink href=#_1 title="Permanent link">¶</a></h6> <p># LoRA attention dimension<br> lora_r = 64</p> <p># Alpha parameter for LoRA scaling<br> lora_alpha = 16</p> <p># Dropout probability for LoRA layers<br> lora_dropout = 0.1</p> <h6 id=_2>##########################################################################<a class=headerlink href=#_2 title="Permanent link">¶</a></h6> <p># bitsandbytes parameters </p> <h6 id=_3><a class=headerlink href=#_3 title="Permanent link">¶</a></h6> <p># Activate 4-bit precision base model loading<br> use_4bit = True</p> <p># Compute dtype for 4-bit base models<br> bnb_4bit_compute_dtype = "float16"</p> <p># Quantization type (fp4 or nf4)<br> bnb_4bit_quant_type = "nf4"</p> <p># Activate nested quantization for 4-bit base models (double quantization)<br> use_nested_quant = False</p> <h6 id=_4>##########################################################################<a class=headerlink href=#_4 title="Permanent link">¶</a></h6> <p># TrainingArguments parameters </p> <h6 id=_5><a class=headerlink href=#_5 title="Permanent link">¶</a></h6> <p># Output directory where the model predictions and checkpoints will be stored<br> output_dir = "./results"</p> <p># Number of training epochs<br> num_train_epochs = 1</p> <p># Enable fp16/bf16 training (set bf16 to True with an A100)<br> fp16 = False<br> bf16 = False</p> <p># Batch size per GPU for training<br> per_device_train_batch_size = 4</p> <p># Batch size per GPU for evaluation<br> per_device_eval_batch_size = 4</p> <p># Number of update steps to accumulate the gradients for<br> gradient_accumulation_steps = 1</p> <p># Enable gradient checkpointing<br> gradient_checkpointing = True</p> <p># Maximum gradient normal (gradient clipping)<br> max_grad_norm = 0.3</p> <p># Initial learning rate (AdamW optimizer)<br> learning_rate = 2e-4</p> <p># Weight decay to apply to all layers except bias/LayerNorm weights<br> weight_decay = 0.001</p> <p># Optimizer to use<br> optim = "paged_adamw_32bit"</p> <p># Learning rate schedule (constant a bit better than cosine)<br> lr_scheduler_type = "constant"</p> <p># Number of training steps (overrides num_train_epochs)<br> max_steps = -1</p> <p># Ratio of steps for a linear warmup (from 0 to learning rate)<br> warmup_ratio = 0.03</p> <p># Group sequences into batches with same length<br> # Saves memory and speeds up training considerably<br> group_by_length = True</p> <p># Save checkpoint every X updates steps<br> save_steps = 25</p> <p># Log every X updates steps<br> logging_steps = 25</p> <h6 id=_6>##########################################################################<a class=headerlink href=#_6 title="Permanent link">¶</a></h6> <p># SFT parameters </p> <h6 id=_7><a class=headerlink href=#_7 title="Permanent link">¶</a></h6> <p># Maximum sequence length to use<br> max_seq_length = None</p> <p># Pack multiple short examples in the same input sequence to increase efficiency<br> packing = False</p> <p># Load the entire model on the GPU 0<br> device_map = {"": 0}</p> <h2 id=loading-the-base-model>Loading the Base Model<a class=headerlink href=#loading-the-base-model title="Permanent link">¶</a></h2> <p>Let’s load the Mistral 7B Instruct base model:</p> <p>model_name = "mistralai/Mistral-7B-Instruct-v0.1"# Load the base model with QLoRA configuration<br> compute_dtype = getattr(torch, bnb_4bit_compute_dtype)</p> <p>bnb_config = BitsAndBytesConfig(<br> load_in_4bit=use_4bit,<br> bnb_4bit_quant_type=bnb_4bit_quant_type,<br> bnb_4bit_compute_dtype=compute_dtype,<br> bnb_4bit_use_double_quant=use_nested_quant,<br> )</p> <p>base_model = AutoModelForCausalLM.from_pretrained(<br> model_name,<br> quantization_config=bnb_config,<br> device_map={"": 0}<br> )</p> <p>base_model.config.use_cache = False<br> base_model.config.pretraining_tp = 1</p> <p># Load MitsralAi tokenizer<br> tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)<br> tokenizer.pad_token = tokenizer.eos_token<br> tokenizer.padding_side = "right"pyt</p> <h2 id=base-model-inference>Base model Inference<a class=headerlink href=#base-model-inference title="Permanent link">¶</a></h2> <p>eval_prompt = """Print hello world in python c and c++"""# import random<br> model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")</p> <p>model.eval()<br> with torch.no_grad():<br> print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))Fine-Tuning with qLora</p> <p>The results from the base model tend to be of poor quality and doesn’t always generate sytactically correct code</p> <h2 id=fine-tuning-with-qlora-and-supervised-finetuning>Fine-Tuning with qLora and Supervised Finetuning<a class=headerlink href=#fine-tuning-with-qlora-and-supervised-finetuning title="Permanent link">¶</a></h2> <p>We’re ready to fine-tune our model using qLora. For this tutorial, we’ll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p> <p># Set LoRA configuration<br> peft_config = LoraConfig(<br> lora_alpha=lora_alpha,<br> lora_dropout=lora_dropout,<br> r=lora_r,<br> target_modules=[<br> "q_proj",<br> "k_proj",<br> "v_proj",<br> "o_proj",<br> "gate_proj",<br> "up_proj",<br> "down_proj",<br> "lm_head",<br> ],<br> bias="none",<br> task_type="CAUSAL_LM",<br> )# Set training parameters<br> training_arguments = TrainingArguments(<br> output_dir=output_dir,<br> num_train_epochs=num_train_epochs,<br> per_device_train_batch_size=per_device_train_batch_size,<br> gradient_accumulation_steps=gradient_accumulation_steps,<br> optim=optim,<br> save_steps=save_steps,<br> logging_steps=logging_steps,<br> learning_rate=learning_rate,<br> weight_decay=weight_decay,<br> fp16=fp16,<br> bf16=bf16,<br> max_grad_norm=max_grad_norm,<br> max_steps=100, # the total number of training steps to perform<br> warmup_ratio=warmup_ratio,<br> group_by_length=group_by_length,<br> lr_scheduler_type=lr_scheduler_type,<br> report_to="tensorboard"<br> )</p> <p># Initialize the SFTTrainer for fine-tuning<br> trainer = SFTTrainer(<br> model=base_model,<br> train_dataset=train_dataset,<br> peft_config=peft_config,<br> dataset_text_field="text",<br> max_seq_length=max_seq_length, # You can specify the maximum sequence length here<br> tokenizer=tokenizer,<br> args=training_arguments,<br> packing=packing,<br> )</p> <h2 id=lets-start-the-training-process>Lets start the training process<a class=headerlink href=#lets-start-the-training-process title="Permanent link">¶</a></h2> <p># Start the training process<br> trainer.train()# Save the fine-tuned model<br> trainer.model.save_pretrained(new_model)</p> <h2 id=inference-with-fine-tuned-model>Inference with Fine-Tuned Model<a class=headerlink href=#inference-with-fine-tuned-model title="Permanent link">¶</a></h2> <p>Now that we have fine-tuned our model, let’s test its performance with some code generation tasks. Replace <code>eval_prompt</code> with your code generation prompt:</p> <p>eval_prompt = """Print hello world in python c and c++"""model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")<br> model.eval()<br> with torch.no_grad():<br> generated_code = tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True)<br> print(generated_code)</p> <h2 id=merge-and-share>Merge and Share<a class=headerlink href=#merge-and-share title="Permanent link">¶</a></h2> <p>After fine-tuning, if you want to merge the model with LoRA weights or share it with the Hugging Face Model Hub, you can do so. This step is optional and depends on your specific use case.</p> <p># Merge the model with LoRA weights<br> base_model = AutoModelForCausalLM.from_pretrained(<br> model_name,<br> low_cpu_mem_usage=True,<br> return_dict=True,<br> torch_dtype=torch.float16,<br> device_map={"": 0},<br> )<br> merged_model= PeftModel.from_pretrained(base_model, new_model)<br> merged_model= model.merge_and_unload()# Save the merged model<br> merged_model.save_pretrained("merged_model",safe_serialization=True)<br> tokenizer.save_pretrained("merged_model")</p> <p># Merge the model with LoRA weights<br> base_model = AutoModelForCausalLM.from_pretrained(<br> model_name,<br> low_cpu_mem_usage=True,<br> return_dict=True,<br> torch_dtype=torch.float16,<br> device_map={"": 0},<br> )<br> merged_model= PeftModel.from_pretrained(base_model, new_model)<br> merged_model= merged_model.merge_and_unload()</p> <p># Save the merged model<br> merged_model.save_pretrained("merged_model",safe_serialization=True)<br> tokenizer.save_pretrained("merged_model")</p> <h2 id=test-the-merged-model>Test the merged model<a class=headerlink href=#test-the-merged-model title="Permanent link">¶</a></h2> <p>from random import randrange<br> sample = train_dataset [randrange(len(train_dataset ))]prompt = f"""&lt;s&gt; <br> {sample['instruction']}<br> {sample['input']}<br> [INST]</p> <p>"""</p> <p>input_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.cuda()<br> # with torch.inference_mode():<br> outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)</p> <p>print(f"Prompt:\n{prompt}\n")<br> print(f"\nGenerated instruction:\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}")<br> print(f"\nGround truth:\n{sample['output']}")</p> <p>And that’s it! You’ve successfully fine-tuned Mistral 7B Instruct for code generation. You can adapt this process for various natural language understanding and generation tasks. Keep exploring and experimenting with Mistral 7B to unlock its full potential for your projects.</p> <p>All the code will be available on my github. Do drop by and give a follow and a star</p> <p>I also post content about Generative AI | LLMs | Stable Diffusion and what i have been working on twitter — <a href=https://twitter.com/adithya_s_k>AdithyaSK (@adithya_s_k) / X</a></p></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 27, 2025 17:29:26 UTC">November 27, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 27, 2025 17:29:26 UTC">November 27, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../LLama2/Llama_2_Fine_Tuning_using_QLora/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Finetune LLama 2(QLoRA)"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Finetune LLama 2(QLoRA) </div> </div> </a> <a href=Mistral_finetuning_notebook/ class="md-footer__link md-footer__link--next" aria-label="Next: Finetune Mistral"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Finetune Mistral </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.e71a0d61.min.js></script> <script src=../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>