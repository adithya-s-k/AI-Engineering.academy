<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/ rel=canonical><link href=../FinetuneGemmaUnslothModalTutorial/ rel=prev><link href=../../LLMArchitecture/ParameterCount/ rel=next><link rel=icon href=../../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.21"><title>Multi-GPU Training with Axolotl - AI Engineering Academy</title><link rel=stylesheet href=../../../assets/stylesheets/main.2a3383ac.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Multi-GPU Training with Axolotl - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Multi-GPU Training with Axolotl - AI Engineering Academy"><meta name=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta name=twitter:image content=https://aiengineering.academy/assets/images/social/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial.png><link rel=stylesheet href=../../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
  posthog.init('phc_OL7nUCVeKtVJe8eHSKGs8zPTQAyr0hm8opAPFdFlkBz', {
      api_host: 'https://us.i.posthog.com',
      person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
  })
</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#fine-tuning-llama-8-70b-with-axolotl-on-modal-multi-gpu-training-made-simple class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Multi-GPU Training with Axolotl </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../../RAG/ class=md-tabs__link> RAG </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> LLM </a> </li> <li class=md-tabs__item> <a href=../../../Deployment/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> LLM </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> Finetuning Techniques </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Finetuning Techniques </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../TheoryBehindFinetuning/PreTrain/ class=md-nav__link> <span class=md-ellipsis> PreTraining LLMs </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../TheoryBehindFinetuning/SFT/ class=md-nav__link> <span class=md-ellipsis> SFT </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../../TheoryBehindFinetuning/PPO/ class=md-nav__link> <span class=md-ellipsis> PPO(Proximal Policy Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../../TheoryBehindFinetuning/DPO/ class=md-nav__link> <span class=md-ellipsis> DPO(Direct Preference Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../../TheoryBehindFinetuning/ORPO/ class=md-nav__link> <span class=md-ellipsis> ORPO(Odds Ratio Preference Optimization) </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../TheoryBehindFinetuning/GRPO/ class=md-nav__link> <span class=md-ellipsis> GRPO(Group Relative Policy Optimization) </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> LLM Finetuning Hands on </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> LLM Finetuning Hands on </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Gemma/ class=md-nav__link> <span class=md-ellipsis> Gemma </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../LLama2/Llama2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Llama2 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Llama3_finetuning_notebook.ipynb class=md-nav__link> <span class=md-ellipsis> Llama3 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Mistral-7b/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> VLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> VLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../VLM/Florence2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Florence2 </span> </a> </li> <li class=md-nav__item> <a href=../../VLM/PaliGemma_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> PaliGemma </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_5 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> Serverless Finetuning with Modal </span> </a> <label class="md-nav__link " for=__nav_4_5 id=__nav_4_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=true> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> Serverless Finetuning with Modal </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../TrainNanoGPTModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Training NanoGPT on Modal </span> </a> </li> <li class=md-nav__item> <a href=../TrainNanochatModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Training Nanochat on Modal </span> </a> </li> <li class=md-nav__item> <a href=../FinetuneGemmaUnslothModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Fine-tuning Gemma with Unsloth </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Multi-GPU Training with Axolotl </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Multi-GPU Training with Axolotl </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#why-axolotl class=md-nav__link> <span class=md-ellipsis> Why Axolotl? </span> </a> </li> <li class=md-nav__item> <a href=#what-were-building class=md-nav__link> <span class=md-ellipsis> What We're Building </span> </a> </li> <li class=md-nav__item> <a href=#getting-started class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> <nav class=md-nav aria-label="Getting Started"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#install-modal class=md-nav__link> <span class=md-ellipsis> Install Modal </span> </a> </li> <li class=md-nav__item> <a href=#authenticate class=md-nav__link> <span class=md-ellipsis> Authenticate </span> </a> </li> <li class=md-nav__item> <a href=#set-up-your-secrets class=md-nav__link> <span class=md-ellipsis> Set Up Your Secrets </span> </a> </li> <li class=md-nav__item> <a href=#project-structure class=md-nav__link> <span class=md-ellipsis> Project Structure </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#understanding-multi-gpu-training class=md-nav__link> <span class=md-ellipsis> Understanding Multi-GPU Training </span> </a> <nav class=md-nav aria-label="Understanding Multi-GPU Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-memory-problem class=md-nav__link> <span class=md-ellipsis> The Memory Problem </span> </a> </li> <li class=md-nav__item> <a href=#the-multi-gpu-solution class=md-nav__link> <span class=md-ellipsis> The Multi-GPU Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#configuration-overview class=md-nav__link> <span class=md-ellipsis> Configuration Overview </span> </a> <nav class=md-nav aria-label="Configuration Overview"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#configuration-constants class=md-nav__link> <span class=md-ellipsis> Configuration Constants </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#building-the-axolotl-image class=md-nav__link> <span class=md-ellipsis> Building the Axolotl Image </span> </a> <nav class=md-nav aria-label="Building the Axolotl Image"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cuda-base-configuration class=md-nav__link> <span class=md-ellipsis> CUDA Base Configuration </span> </a> </li> <li class=md-nav__item> <a href=#complete-image-definition class=md-nav__link> <span class=md-ellipsis> Complete Image Definition </span> </a> </li> <li class=md-nav__item> <a href=#alternative-official-axolotl-image class=md-nav__link> <span class=md-ellipsis> Alternative: Official Axolotl Image </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#training-configuration-with-yaml class=md-nav__link> <span class=md-ellipsis> Training Configuration with YAML </span> </a> <nav class=md-nav aria-label="Training Configuration with YAML"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#complete-training-config class=md-nav__link> <span class=md-ellipsis> Complete Training Config </span> </a> </li> <li class=md-nav__item> <a href=#model-settings class=md-nav__link> <span class=md-ellipsis> Model Settings </span> </a> </li> <li class=md-nav__item> <a href=#quantization-settings class=md-nav__link> <span class=md-ellipsis> Quantization Settings </span> </a> </li> <li class=md-nav__item> <a href=#dataset-configuration class=md-nav__link> <span class=md-ellipsis> Dataset Configuration </span> </a> </li> <li class=md-nav__item> <a href=#lora-parameters class=md-nav__link> <span class=md-ellipsis> LoRA Parameters </span> </a> </li> <li class=md-nav__item> <a href=#training-hyperparameters class=md-nav__link> <span class=md-ellipsis> Training Hyperparameters </span> </a> </li> <li class=md-nav__item> <a href=#memory-optimizations class=md-nav__link> <span class=md-ellipsis> Memory Optimizations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#helper-function-write-config-to-volume class=md-nav__link> <span class=md-ellipsis> Helper Function: Write Config to Volume </span> </a> </li> <li class=md-nav__item> <a href=#stage-1-dataset-preprocessing class=md-nav__link> <span class=md-ellipsis> Stage 1: Dataset Preprocessing </span> </a> </li> <li class=md-nav__item> <a href=#stage-2-multi-gpu-training class=md-nav__link> <span class=md-ellipsis> Stage 2: Multi-GPU Training </span> </a> <nav class=md-nav aria-label="Stage 2: Multi-GPU Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gpu-configuration class=md-nav__link> <span class=md-ellipsis> GPU Configuration </span> </a> </li> <li class=md-nav__item> <a href=#training-function class=md-nav__link> <span class=md-ellipsis> Training Function </span> </a> </li> <li class=md-nav__item> <a href=#understanding-the-accelerate-command class=md-nav__link> <span class=md-ellipsis> Understanding the Accelerate Command </span> </a> </li> <li class=md-nav__item> <a href=#run-training class=md-nav__link> <span class=md-ellipsis> Run Training </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#stage-3-merge-lora-adapters class=md-nav__link> <span class=md-ellipsis> Stage 3: Merge LoRA Adapters </span> </a> </li> <li class=md-nav__item> <a href=#stage-4-inference class=md-nav__link> <span class=md-ellipsis> Stage 4: Inference </span> </a> </li> <li class=md-nav__item> <a href=#complete-workflow-example class=md-nav__link> <span class=md-ellipsis> Complete Workflow Example </span> </a> <nav class=md-nav aria-label="Complete Workflow Example"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-customize-configuration class=md-nav__link> <span class=md-ellipsis> 1. Customize Configuration </span> </a> </li> <li class=md-nav__item> <a href=#2-preprocess-dataset class=md-nav__link> <span class=md-ellipsis> 2. Preprocess Dataset </span> </a> </li> <li class=md-nav__item> <a href=#3-test-training-small-sanity-check class=md-nav__link> <span class=md-ellipsis> 3. Test Training (Small Sanity Check) </span> </a> </li> <li class=md-nav__item> <a href=#4-full-training class=md-nav__link> <span class=md-ellipsis> 4. Full Training </span> </a> </li> <li class=md-nav__item> <a href=#5-merge-lora class=md-nav__link> <span class=md-ellipsis> 5. Merge LoRA </span> </a> </li> <li class=md-nav__item> <a href=#6-test-inference class=md-nav__link> <span class=md-ellipsis> 6. Test Inference </span> </a> </li> <li class=md-nav__item> <a href=#7-push-to-hub-optional class=md-nav__link> <span class=md-ellipsis> 7. Push to Hub (Optional) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-tips-and-tricks class=md-nav__link> <span class=md-ellipsis> Advanced Tips and Tricks </span> </a> <nav class=md-nav aria-label="Advanced Tips and Tricks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#multi-dataset-training class=md-nav__link> <span class=md-ellipsis> Multi-Dataset Training </span> </a> </li> <li class=md-nav__item> <a href=#checkpoint-management class=md-nav__link> <span class=md-ellipsis> Checkpoint Management </span> </a> </li> <li class=md-nav__item> <a href=#custom-evaluation class=md-nav__link> <span class=md-ellipsis> Custom Evaluation </span> </a> </li> <li class=md-nav__item> <a href=#optimizer-tuning class=md-nav__link> <span class=md-ellipsis> Optimizer Tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#hyperparameter-tuning-guide class=md-nav__link> <span class=md-ellipsis> Hyperparameter Tuning Guide </span> </a> <nav class=md-nav aria-label="Hyperparameter Tuning Guide"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#for-llama-3-8b-single-gpu class=md-nav__link> <span class=md-ellipsis> For Llama 3-8B (Single GPU) </span> </a> </li> <li class=md-nav__item> <a href=#for-llama-3-8-70b-multi-gpu class=md-nav__link> <span class=md-ellipsis> For Llama 3-8-70B (Multi-GPU) </span> </a> </li> <li class=md-nav__item> <a href=#for-maximum-speed class=md-nav__link> <span class=md-ellipsis> For Maximum Speed </span> </a> </li> <li class=md-nav__item> <a href=#for-maximum-quality class=md-nav__link> <span class=md-ellipsis> For Maximum Quality </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#common-issues-and-solutions class=md-nav__link> <span class=md-ellipsis> Common Issues and Solutions </span> </a> <nav class=md-nav aria-label="Common Issues and Solutions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#out-of-memory-during-training class=md-nav__link> <span class=md-ellipsis> "Out of Memory" During Training </span> </a> </li> <li class=md-nav__item> <a href=#training-loss-not-decreasing class=md-nav__link> <span class=md-ellipsis> Training Loss Not Decreasing </span> </a> </li> <li class=md-nav__item> <a href=#preprocessing-hangs-or-fails class=md-nav__link> <span class=md-ellipsis> Preprocessing Hangs or Fails </span> </a> </li> <li class=md-nav__item> <a href=#multi-gpu-not-working class=md-nav__link> <span class=md-ellipsis> Multi-GPU Not Working </span> </a> </li> <li class=md-nav__item> <a href=#secret-not-found class=md-nav__link> <span class=md-ellipsis> "Secret not found" </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cost-optimization-strategies class=md-nav__link> <span class=md-ellipsis> Cost Optimization Strategies </span> </a> <nav class=md-nav aria-label="Cost Optimization Strategies"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-always-preprocess-separately class=md-nav__link> <span class=md-ellipsis> 1. Always Preprocess Separately </span> </a> </li> <li class=md-nav__item> <a href=#2-test-with-smaller-models-first class=md-nav__link> <span class=md-ellipsis> 2. Test with Smaller Models First </span> </a> </li> <li class=md-nav__item> <a href=#3-use-smaller-gpus-for-testing class=md-nav__link> <span class=md-ellipsis> 3. Use Smaller GPUs for Testing </span> </a> </li> <li class=md-nav__item> <a href=#4-limit-test-datasets class=md-nav__link> <span class=md-ellipsis> 4. Limit Test Datasets </span> </a> </li> <li class=md-nav__item> <a href=#5-smart-checkpointing class=md-nav__link> <span class=md-ellipsis> 5. Smart Checkpointing </span> </a> </li> <li class=md-nav__item> <a href=#6-resume-from-checkpoint class=md-nav__link> <span class=md-ellipsis> 6. Resume from Checkpoint </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#monitoring-and-debugging class=md-nav__link> <span class=md-ellipsis> Monitoring and Debugging </span> </a> <nav class=md-nav aria-label="Monitoring and Debugging"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#real-time-logs class=md-nav__link> <span class=md-ellipsis> Real-time Logs </span> </a> </li> <li class=md-nav__item> <a href=#weights-biases class=md-nav__link> <span class=md-ellipsis> Weights &amp; Biases </span> </a> </li> <li class=md-nav__item> <a href=#check-preprocessed-data class=md-nav__link> <span class=md-ellipsis> Check Preprocessed Data </span> </a> </li> <li class=md-nav__item> <a href=#download-checkpoints class=md-nav__link> <span class=md-ellipsis> Download Checkpoints </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#scaling-to-8-gpus-for-the-brave class=md-nav__link> <span class=md-ellipsis> Scaling to 8 GPUs (For the Brave) </span> </a> <nav class=md-nav aria-label="Scaling to 8 GPUs (For the Brave)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#update-configuration class=md-nav__link> <span class=md-ellipsis> Update Configuration </span> </a> </li> <li class=md-nav__item> <a href=#update-yaml-for-memory class=md-nav__link> <span class=md-ellipsis> Update YAML for Memory </span> </a> </li> <li class=md-nav__item> <a href=#enable-deepspeed-zero-stage-3-optional class=md-nav__link> <span class=md-ellipsis> Enable DeepSpeed ZeRO Stage 3 (Optional) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#whats-next class=md-nav__link> <span class=md-ellipsis> What's Next? </span> </a> </li> <li class=md-nav__item> <a href=#resources class=md-nav__link> <span class=md-ellipsis> Resources </span> </a> </li> <li class=md-nav__item> <a href=#wrapping-up class=md-nav__link> <span class=md-ellipsis> Wrapping Up </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_6> <div class="md-nav__link md-nav__container"> <a href=../../LLMArchitecture/ParameterCount/ class="md-nav__link "> <span class=md-ellipsis> LLM Architecture </span> </a> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_6> <span class="md-nav__icon md-icon"></span> LLM Architecture </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#why-axolotl class=md-nav__link> <span class=md-ellipsis> Why Axolotl? </span> </a> </li> <li class=md-nav__item> <a href=#what-were-building class=md-nav__link> <span class=md-ellipsis> What We're Building </span> </a> </li> <li class=md-nav__item> <a href=#getting-started class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> <nav class=md-nav aria-label="Getting Started"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#install-modal class=md-nav__link> <span class=md-ellipsis> Install Modal </span> </a> </li> <li class=md-nav__item> <a href=#authenticate class=md-nav__link> <span class=md-ellipsis> Authenticate </span> </a> </li> <li class=md-nav__item> <a href=#set-up-your-secrets class=md-nav__link> <span class=md-ellipsis> Set Up Your Secrets </span> </a> </li> <li class=md-nav__item> <a href=#project-structure class=md-nav__link> <span class=md-ellipsis> Project Structure </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#understanding-multi-gpu-training class=md-nav__link> <span class=md-ellipsis> Understanding Multi-GPU Training </span> </a> <nav class=md-nav aria-label="Understanding Multi-GPU Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#the-memory-problem class=md-nav__link> <span class=md-ellipsis> The Memory Problem </span> </a> </li> <li class=md-nav__item> <a href=#the-multi-gpu-solution class=md-nav__link> <span class=md-ellipsis> The Multi-GPU Solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#configuration-overview class=md-nav__link> <span class=md-ellipsis> Configuration Overview </span> </a> <nav class=md-nav aria-label="Configuration Overview"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#configuration-constants class=md-nav__link> <span class=md-ellipsis> Configuration Constants </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#building-the-axolotl-image class=md-nav__link> <span class=md-ellipsis> Building the Axolotl Image </span> </a> <nav class=md-nav aria-label="Building the Axolotl Image"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cuda-base-configuration class=md-nav__link> <span class=md-ellipsis> CUDA Base Configuration </span> </a> </li> <li class=md-nav__item> <a href=#complete-image-definition class=md-nav__link> <span class=md-ellipsis> Complete Image Definition </span> </a> </li> <li class=md-nav__item> <a href=#alternative-official-axolotl-image class=md-nav__link> <span class=md-ellipsis> Alternative: Official Axolotl Image </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#training-configuration-with-yaml class=md-nav__link> <span class=md-ellipsis> Training Configuration with YAML </span> </a> <nav class=md-nav aria-label="Training Configuration with YAML"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#complete-training-config class=md-nav__link> <span class=md-ellipsis> Complete Training Config </span> </a> </li> <li class=md-nav__item> <a href=#model-settings class=md-nav__link> <span class=md-ellipsis> Model Settings </span> </a> </li> <li class=md-nav__item> <a href=#quantization-settings class=md-nav__link> <span class=md-ellipsis> Quantization Settings </span> </a> </li> <li class=md-nav__item> <a href=#dataset-configuration class=md-nav__link> <span class=md-ellipsis> Dataset Configuration </span> </a> </li> <li class=md-nav__item> <a href=#lora-parameters class=md-nav__link> <span class=md-ellipsis> LoRA Parameters </span> </a> </li> <li class=md-nav__item> <a href=#training-hyperparameters class=md-nav__link> <span class=md-ellipsis> Training Hyperparameters </span> </a> </li> <li class=md-nav__item> <a href=#memory-optimizations class=md-nav__link> <span class=md-ellipsis> Memory Optimizations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#helper-function-write-config-to-volume class=md-nav__link> <span class=md-ellipsis> Helper Function: Write Config to Volume </span> </a> </li> <li class=md-nav__item> <a href=#stage-1-dataset-preprocessing class=md-nav__link> <span class=md-ellipsis> Stage 1: Dataset Preprocessing </span> </a> </li> <li class=md-nav__item> <a href=#stage-2-multi-gpu-training class=md-nav__link> <span class=md-ellipsis> Stage 2: Multi-GPU Training </span> </a> <nav class=md-nav aria-label="Stage 2: Multi-GPU Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gpu-configuration class=md-nav__link> <span class=md-ellipsis> GPU Configuration </span> </a> </li> <li class=md-nav__item> <a href=#training-function class=md-nav__link> <span class=md-ellipsis> Training Function </span> </a> </li> <li class=md-nav__item> <a href=#understanding-the-accelerate-command class=md-nav__link> <span class=md-ellipsis> Understanding the Accelerate Command </span> </a> </li> <li class=md-nav__item> <a href=#run-training class=md-nav__link> <span class=md-ellipsis> Run Training </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#stage-3-merge-lora-adapters class=md-nav__link> <span class=md-ellipsis> Stage 3: Merge LoRA Adapters </span> </a> </li> <li class=md-nav__item> <a href=#stage-4-inference class=md-nav__link> <span class=md-ellipsis> Stage 4: Inference </span> </a> </li> <li class=md-nav__item> <a href=#complete-workflow-example class=md-nav__link> <span class=md-ellipsis> Complete Workflow Example </span> </a> <nav class=md-nav aria-label="Complete Workflow Example"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-customize-configuration class=md-nav__link> <span class=md-ellipsis> 1. Customize Configuration </span> </a> </li> <li class=md-nav__item> <a href=#2-preprocess-dataset class=md-nav__link> <span class=md-ellipsis> 2. Preprocess Dataset </span> </a> </li> <li class=md-nav__item> <a href=#3-test-training-small-sanity-check class=md-nav__link> <span class=md-ellipsis> 3. Test Training (Small Sanity Check) </span> </a> </li> <li class=md-nav__item> <a href=#4-full-training class=md-nav__link> <span class=md-ellipsis> 4. Full Training </span> </a> </li> <li class=md-nav__item> <a href=#5-merge-lora class=md-nav__link> <span class=md-ellipsis> 5. Merge LoRA </span> </a> </li> <li class=md-nav__item> <a href=#6-test-inference class=md-nav__link> <span class=md-ellipsis> 6. Test Inference </span> </a> </li> <li class=md-nav__item> <a href=#7-push-to-hub-optional class=md-nav__link> <span class=md-ellipsis> 7. Push to Hub (Optional) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-tips-and-tricks class=md-nav__link> <span class=md-ellipsis> Advanced Tips and Tricks </span> </a> <nav class=md-nav aria-label="Advanced Tips and Tricks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#multi-dataset-training class=md-nav__link> <span class=md-ellipsis> Multi-Dataset Training </span> </a> </li> <li class=md-nav__item> <a href=#checkpoint-management class=md-nav__link> <span class=md-ellipsis> Checkpoint Management </span> </a> </li> <li class=md-nav__item> <a href=#custom-evaluation class=md-nav__link> <span class=md-ellipsis> Custom Evaluation </span> </a> </li> <li class=md-nav__item> <a href=#optimizer-tuning class=md-nav__link> <span class=md-ellipsis> Optimizer Tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#hyperparameter-tuning-guide class=md-nav__link> <span class=md-ellipsis> Hyperparameter Tuning Guide </span> </a> <nav class=md-nav aria-label="Hyperparameter Tuning Guide"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#for-llama-3-8b-single-gpu class=md-nav__link> <span class=md-ellipsis> For Llama 3-8B (Single GPU) </span> </a> </li> <li class=md-nav__item> <a href=#for-llama-3-8-70b-multi-gpu class=md-nav__link> <span class=md-ellipsis> For Llama 3-8-70B (Multi-GPU) </span> </a> </li> <li class=md-nav__item> <a href=#for-maximum-speed class=md-nav__link> <span class=md-ellipsis> For Maximum Speed </span> </a> </li> <li class=md-nav__item> <a href=#for-maximum-quality class=md-nav__link> <span class=md-ellipsis> For Maximum Quality </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#common-issues-and-solutions class=md-nav__link> <span class=md-ellipsis> Common Issues and Solutions </span> </a> <nav class=md-nav aria-label="Common Issues and Solutions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#out-of-memory-during-training class=md-nav__link> <span class=md-ellipsis> "Out of Memory" During Training </span> </a> </li> <li class=md-nav__item> <a href=#training-loss-not-decreasing class=md-nav__link> <span class=md-ellipsis> Training Loss Not Decreasing </span> </a> </li> <li class=md-nav__item> <a href=#preprocessing-hangs-or-fails class=md-nav__link> <span class=md-ellipsis> Preprocessing Hangs or Fails </span> </a> </li> <li class=md-nav__item> <a href=#multi-gpu-not-working class=md-nav__link> <span class=md-ellipsis> Multi-GPU Not Working </span> </a> </li> <li class=md-nav__item> <a href=#secret-not-found class=md-nav__link> <span class=md-ellipsis> "Secret not found" </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cost-optimization-strategies class=md-nav__link> <span class=md-ellipsis> Cost Optimization Strategies </span> </a> <nav class=md-nav aria-label="Cost Optimization Strategies"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-always-preprocess-separately class=md-nav__link> <span class=md-ellipsis> 1. Always Preprocess Separately </span> </a> </li> <li class=md-nav__item> <a href=#2-test-with-smaller-models-first class=md-nav__link> <span class=md-ellipsis> 2. Test with Smaller Models First </span> </a> </li> <li class=md-nav__item> <a href=#3-use-smaller-gpus-for-testing class=md-nav__link> <span class=md-ellipsis> 3. Use Smaller GPUs for Testing </span> </a> </li> <li class=md-nav__item> <a href=#4-limit-test-datasets class=md-nav__link> <span class=md-ellipsis> 4. Limit Test Datasets </span> </a> </li> <li class=md-nav__item> <a href=#5-smart-checkpointing class=md-nav__link> <span class=md-ellipsis> 5. Smart Checkpointing </span> </a> </li> <li class=md-nav__item> <a href=#6-resume-from-checkpoint class=md-nav__link> <span class=md-ellipsis> 6. Resume from Checkpoint </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#monitoring-and-debugging class=md-nav__link> <span class=md-ellipsis> Monitoring and Debugging </span> </a> <nav class=md-nav aria-label="Monitoring and Debugging"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#real-time-logs class=md-nav__link> <span class=md-ellipsis> Real-time Logs </span> </a> </li> <li class=md-nav__item> <a href=#weights-biases class=md-nav__link> <span class=md-ellipsis> Weights &amp; Biases </span> </a> </li> <li class=md-nav__item> <a href=#check-preprocessed-data class=md-nav__link> <span class=md-ellipsis> Check Preprocessed Data </span> </a> </li> <li class=md-nav__item> <a href=#download-checkpoints class=md-nav__link> <span class=md-ellipsis> Download Checkpoints </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#scaling-to-8-gpus-for-the-brave class=md-nav__link> <span class=md-ellipsis> Scaling to 8 GPUs (For the Brave) </span> </a> <nav class=md-nav aria-label="Scaling to 8 GPUs (For the Brave)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#update-configuration class=md-nav__link> <span class=md-ellipsis> Update Configuration </span> </a> </li> <li class=md-nav__item> <a href=#update-yaml-for-memory class=md-nav__link> <span class=md-ellipsis> Update YAML for Memory </span> </a> </li> <li class=md-nav__item> <a href=#enable-deepspeed-zero-stage-3-optional class=md-nav__link> <span class=md-ellipsis> Enable DeepSpeed ZeRO Stage 3 (Optional) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#whats-next class=md-nav__link> <span class=md-ellipsis> What's Next? </span> </a> </li> <li class=md-nav__item> <a href=#resources class=md-nav__link> <span class=md-ellipsis> Resources </span> </a> </li> <li class=md-nav__item> <a href=#wrapping-up class=md-nav__link> <span class=md-ellipsis> Wrapping Up </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <div><h1 id=fine-tuning-llama-8-70b-with-axolotl-on-modal-multi-gpu-training-made-simple>Fine-tuning Llama 8-70B with Axolotl on Modal: Multi-GPU Training Made Simple<a class=headerlink href=#fine-tuning-llama-8-70b-with-axolotl-on-modal-multi-gpu-training-made-simple title="Permanent link">¶</a></h1> <p>📄 <strong><a href=https://github.com/adithya-s-k/AI-Engineering.academy/blob/main/docs/LLM/ServerLessFinetuning/FinetuneLlamaAxolotlGPUModal.py>View Complete Python Script</a></strong></p> <p>So you've trained nanoGPT from scratch and fine-tuned Gemma with Unsloth. Now let's go full beast mode - we're training Llama 8-70B across multiple GPUs. We're talking real production ML infrastructure here</p> <p>And the crazy part? We're doing it all with Axolotl on Modal. No Kubernetes cluster to manage, no infrastructure nightmares. Just distributed training power.</p> <h2 id=why-axolotl>Why Axolotl?<a class=headerlink href=#why-axolotl title="Permanent link">¶</a></h2> <p>I discovered Axolotl when I needed to fine-tune a 8-70B model and realized Unsloth wasn't built for multi-GPU setups. That's where Axolotl shines.</p> <p><strong>What makes Axolotl special:</strong> - <strong>Production-grade multi-GPU support</strong> - Train across 2, 4, or even 8 GPUs without writing custom distributed code - <strong>YAML-based configs</strong> - All your hyperparameters in one readable file. No more scattered parameters across Python code - <strong>Built-in DeepSpeed and FSDP</strong> - The same tech Microsoft uses to train massive models, just works out of the box - <strong>Extensive model support</strong> - Llama, Mistral, Qwen, you name it. Pre-configured recipes for all major models - <strong>Battle-tested</strong> - Used by companies to train production models, not just a research toy</p> <p>The thing is, when you're training a 8-70B model, you physically can't fit it on a single GPU. Even an A100-80GB can barely hold the model weights, let alone gradients and optimizer states. You NEED multi-GPU training.</p> <p>Axolotl handles all the complexity: splitting the model across GPUs, synchronizing gradients, managing checkpoints. You just write a YAML file and hit run.</p> <h2 id=what-were-building>What We're Building<a class=headerlink href=#what-were-building title="Permanent link">¶</a></h2> <p>This is a complete multi-GPU training pipeline with four independent stages:</p> <ol> <li><strong>Preprocess datasets</strong> - Tokenize and cache (on 1 GPU, because why waste money?)</li> <li><strong>Multi-GPU training</strong> - Fine-tune Llama 8-70B across 4 GPUs with LoRA</li> <li><strong>Merge LoRA adapters</strong> - Combine adapters with base model for deployment</li> <li><strong>Inference</strong> - Test your fine-tuned model</li> </ol> <p>Here's the mental model:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>┌──────────────────────┐
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>│  Preprocess Data     │  (1 GPU - $3.50/hr, run once)
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>│  Tokenize &amp; Cache    │
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>└─────────┬────────────┘
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>          │
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>┌─────────▼────────────┐
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>│  Multi-GPU Training  │  (4× A100-80GB - $14/hr)
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>│  Llama 8-70B + LoRA    │  ← The beast mode part
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>└─────────┬────────────┘
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>          │
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>┌─────────▼────────────┐
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>│   Merge LoRA         │  (1 GPU - $3.50/hr, ~30 min)
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>│  Adapters → Model    │
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>└─────────┬────────────┘
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>          │
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>┌─────────▼────────────┐
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>│    Inference         │  (1 GPU - test your model)
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>└──────────────────────┘
</span></code></pre></div> <p>Each stage is independent. Screw up training? Just re-run that step. Want to test different hyperparameters? Preprocessing is already cached.</p> <h2 id=getting-started>Getting Started<a class=headerlink href=#getting-started title="Permanent link">¶</a></h2> <h3 id=install-modal>Install Modal<a class=headerlink href=#install-modal title="Permanent link">¶</a></h3> <p>You know the drill by now:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>pip<span class=w> </span>install<span class=w> </span>modal
</span></code></pre></div> <h3 id=authenticate>Authenticate<a class=headerlink href=#authenticate title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>modal<span class=w> </span>setup
</span></code></pre></div> <p>Or with API keys:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=nb>export</span><span class=w> </span><span class=nv>MODAL_TOKEN_ID</span><span class=o>=</span>&lt;your_token_id&gt;
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=nb>export</span><span class=w> </span><span class=nv>MODAL_TOKEN_SECRET</span><span class=o>=</span>&lt;your_token_secret&gt;
</span></code></pre></div> <h3 id=set-up-your-secrets>Set Up Your Secrets<a class=headerlink href=#set-up-your-secrets title="Permanent link">¶</a></h3> <p>For this, you'll need: - <strong>Hugging Face token</strong> - Required for Llama models (they're gated) - <strong>Weights &amp; Biases API key</strong> - Optional but highly recommended for tracking training</p> <p><strong>Create the Modal secret:</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>modal<span class=w> </span>secret<span class=w> </span>create<span class=w> </span>secrets-hf-wandb<span class=w> </span><span class=se>\</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=w>  </span><span class=nv>HUGGINGFACE_TOKEN</span><span class=o>=</span>hf_xxxxxxxxxxxxx<span class=w> </span><span class=se>\</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=w>  </span><span class=nv>WANDB_API_KEY</span><span class=o>=</span>xxxxxxxxxxxxx
</span></code></pre></div> <blockquote> <p><strong>Note:</strong> The script looks for a secret named <code>secrets-hf-wandb</code>. If you use a different name, update the code where it says <code>Secret.from_name("secrets-hf-wandb")</code>.</p> </blockquote> <p><strong>Get your tokens:</strong> - HF token: <a href=https://huggingface.co/settings/tokens>hf.co/settings/tokens</a> - W&amp;B key: <a href=https://wandb.ai/authorize>wandb.ai/authorize</a></p> <h3 id=project-structure>Project Structure<a class=headerlink href=#project-structure title="Permanent link">¶</a></h3> <p>This is even simpler than the previous tutorials:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>ServerLessFinetuning/
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>├── FinetuneLlamaAxolotlGPUModal.py    # Everything lives here
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>└── .env                                # Optional: local secrets
</span></code></pre></div> <p>One file. That's it. All your configuration, all your training stages, all your pipeline logic - in one clean Python file.</p> <h2 id=understanding-multi-gpu-training>Understanding Multi-GPU Training<a class=headerlink href=#understanding-multi-gpu-training title="Permanent link">¶</a></h2> <p>Before we dive into code, let's understand why we even need multiple GPUs.</p> <h3 id=the-memory-problem>The Memory Problem<a class=headerlink href=#the-memory-problem title="Permanent link">¶</a></h3> <p>Here's the brutal math:</p> <table> <thead> <tr> <th>Model Size</th> <th>Parameters</th> <th>FP16 Weights</th> <th>Training Memory</th> <th>Fits on Single GPU?</th> </tr> </thead> <tbody> <tr> <td>Llama 3-8B</td> <td>8 billion</td> <td>~16GB</td> <td>~40GB</td> <td>✓ (A100-80GB)</td> </tr> <tr> <td>Llama 3-8-70B</td> <td>70 billion</td> <td>~140GB</td> <td>~280GB</td> <td>✗ (Impossible!)</td> </tr> <tr> <td>Llama 3-405B</td> <td>405 billion</td> <td>~810GB</td> <td>~1.6TB</td> <td>✗ (Very impossible!)</td> </tr> </tbody> </table> <p>Training memory = model weights + gradients + optimizer states + activations. Roughly 2-3x the model size.</p> <p>A single A100-80GB has... 80GB. You literally cannot fit a 8-70B model for training, even with quantization.</p> <h3 id=the-multi-gpu-solution>The Multi-GPU Solution<a class=headerlink href=#the-multi-gpu-solution title="Permanent link">¶</a></h3> <p>There are several strategies to distribute a model across GPUs:</p> <ol> <li> <p><strong>Data Parallelism (DP):</strong> Copy the full model to each GPU, split the batch across them. Great for small models, useless for 8-70B.</p> </li> <li> <p><strong>Tensor Parallelism (TP):</strong> Split individual layers across GPUs. Each GPU has part of each attention head, part of each MLP. Complex but efficient.</p> </li> <li> <p><strong>Pipeline Parallelism (PP):</strong> Different GPUs process different layers. GPU 0 has layers 1-20, GPU 1 has 21-40, etc. Simple but can have bubble time.</p> </li> <li> <p><strong>FSDP (Fully Sharded Data Parallel):</strong> The modern approach. Shard everything - model parameters, gradients, optimizer states. Each GPU only keeps what it needs, fetches the rest when needed.</p> </li> </ol> <p><strong>The beauty of Axolotl?</strong> You don't have to think about this. It uses HuggingFace Accelerate under the hood, which figures out the best strategy automatically. You just say "use 4 GPUs" and it handles the rest.</p> <h2 id=configuration-overview>Configuration Overview<a class=headerlink href=#configuration-overview title="Permanent link">¶</a></h2> <p>Let's start with the basics:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=kn>from</span><span class=w> </span><span class=nn>modal</span><span class=w> </span><span class=kn>import</span> <span class=n>App</span><span class=p>,</span> <span class=n>Image</span> <span class=k>as</span> <span class=n>ModalImage</span><span class=p>,</span> <span class=n>Volume</span><span class=p>,</span> <span class=n>Secret</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=c1># Create the Modal app</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a><span class=n>app</span> <span class=o>=</span> <span class=n>App</span><span class=p>(</span><span class=s2>"Finetuned_Llama_70b_Axolotl_MultiGPU"</span><span class=p>)</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a><span class=c1># Create persistent storage</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a><span class=n>exp_volume</span> <span class=o>=</span> <span class=n>Volume</span><span class=o>.</span><span class=n>from_name</span><span class=p>(</span><span class=s2>"Finetuned_Llama_70b_Axolotl"</span><span class=p>,</span> <span class=n>create_if_missing</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a><span class=c1># Mount the volume at /data in all containers</span>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a><span class=n>VOLUME_CONFIG</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>    <span class=s2>"/data"</span><span class=p>:</span> <span class=n>exp_volume</span><span class=p>,</span>
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a><span class=p>}</span>
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>
</span><span id=__span-6-14><a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a><span class=c1># Load secrets</span>
</span><span id=__span-6-15><a id=__codelineno-6-15 name=__codelineno-6-15 href=#__codelineno-6-15></a><span class=n>huggingface_secret</span> <span class=o>=</span> <span class=n>Secret</span><span class=o>.</span><span class=n>from_name</span><span class=p>(</span><span class=s2>"secrets-hf-wandb"</span><span class=p>)</span>
</span></code></pre></div> <p><strong>What's happening:</strong> - <strong>Volume:</strong> All our data lives here - preprocessed datasets, checkpoints, final models. Persists across runs. - <strong>Secrets:</strong> Injected as environment variables in our containers. Clean and secure.</p> <h3 id=configuration-constants>Configuration Constants<a class=headerlink href=#configuration-constants title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Time constants</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>HOURS</span> <span class=o>=</span> <span class=mi>60</span> <span class=o>*</span> <span class=mi>60</span>  <span class=c1># Makes timeouts readable</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=c1># GPU Configuration</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=n>GPU_TYPE</span> <span class=o>=</span> <span class=s2>"a100-80gb"</span>  <span class=c1># Can be: a100-80gb, a100-40gb, l40s, etc.</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a><span class=c1># Training Configuration</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a><span class=n>WANDB_PROJECT_DEFAULT</span> <span class=o>=</span> <span class=s2>"Llama-70b-MultiGPU-finetune"</span>
</span></code></pre></div> <p><strong>GPU type considerations:</strong></p> <p>For <strong>8B models:</strong> - L40S works great (~<span class=arithmatex>\(1/hr) - A100-40GB for comfort (\)</span>2.50/hr)</p> <p>For <strong>8-70B models:</strong> - A100-80GB is required (<span class=arithmatex>\(3.50/hr per GPU) - You'll need 4-8 of them (\)</span>14-28/hr total)</p> <p>For <strong>405B models:</strong> - 8× A100-80GB minimum ($28/hr) - Or use H100s, B100s</p> <h2 id=building-the-axolotl-image>Building the Axolotl Image<a class=headerlink href=#building-the-axolotl-image title="Permanent link">¶</a></h2> <p>This is where things get interesting. We need a container with CUDA, PyTorch, Axolotl, DeepSpeed, Flash Attention... the works.</p> <h3 id=cuda-base-configuration>CUDA Base Configuration<a class=headerlink href=#cuda-base-configuration title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=n>CUDA_VERSION</span> <span class=o>=</span> <span class=s2>"12.8.1"</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=n>CUDA_FLAVOR</span> <span class=o>=</span> <span class=s2>"devel"</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=n>CUDA_OS</span> <span class=o>=</span> <span class=s2>"ubuntu24.04"</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=n>CUDA_TAG</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>CUDA_VERSION</span><span class=si>}</span><span class=s2>-</span><span class=si>{</span><span class=n>CUDA_FLAVOR</span><span class=si>}</span><span class=s2>-</span><span class=si>{</span><span class=n>CUDA_OS</span><span class=si>}</span><span class=s2>"</span>
</span></code></pre></div> <p><strong>Why "devel"?</strong></p> <p>Flash Attention (which makes training way faster) needs to compile CUDA code during installation. The <code>runtime</code> image doesn't include the CUDA compiler (<code>nvcc</code>), so you'll get cryptic errors.</p> <p>The <code>devel</code> image includes the full CUDA toolkit. It's bigger, but it Just Works™.</p> <h3 id=complete-image-definition>Complete Image Definition<a class=headerlink href=#complete-image-definition title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=n>AXOLOTL_IMAGE</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>    <span class=c1># Start with NVIDIA's official CUDA image</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>    <span class=n>ModalImage</span><span class=o>.</span><span class=n>from_registry</span><span class=p>(</span><span class=sa>f</span><span class=s2>"nvidia/cuda:</span><span class=si>{</span><span class=n>CUDA_TAG</span><span class=si>}</span><span class=s2>"</span><span class=p>,</span> <span class=n>add_python</span><span class=o>=</span><span class=s2>"3.12"</span><span class=p>)</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a>    <span class=c1># Install system dependencies</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>    <span class=o>.</span><span class=n>apt_install</span><span class=p>(</span><span class=s2>"git"</span><span class=p>,</span> <span class=s2>"build-essential"</span><span class=p>)</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a>
</span><span id=__span-9-8><a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a>    <span class=c1># Install PyTorch first</span>
</span><span id=__span-9-9><a id=__codelineno-9-9 name=__codelineno-9-9 href=#__codelineno-9-9></a>    <span class=o>.</span><span class=n>uv_pip_install</span><span class=p>([</span>
</span><span id=__span-9-10><a id=__codelineno-9-10 name=__codelineno-9-10 href=#__codelineno-9-10></a>        <span class=s2>"torch"</span><span class=p>,</span>
</span><span id=__span-9-11><a id=__codelineno-9-11 name=__codelineno-9-11 href=#__codelineno-9-11></a>        <span class=s2>"torchvision"</span><span class=p>,</span>
</span><span id=__span-9-12><a id=__codelineno-9-12 name=__codelineno-9-12 href=#__codelineno-9-12></a>        <span class=s2>"torchaudio"</span><span class=p>,</span>
</span><span id=__span-9-13><a id=__codelineno-9-13 name=__codelineno-9-13 href=#__codelineno-9-13></a>    <span class=p>])</span>
</span><span id=__span-9-14><a id=__codelineno-9-14 name=__codelineno-9-14 href=#__codelineno-9-14></a>
</span><span id=__span-9-15><a id=__codelineno-9-15 name=__codelineno-9-15 href=#__codelineno-9-15></a>    <span class=c1># Install base dependencies for Axolotl</span>
</span><span id=__span-9-16><a id=__codelineno-9-16 name=__codelineno-9-16 href=#__codelineno-9-16></a>    <span class=o>.</span><span class=n>run_commands</span><span class=p>(</span>
</span><span id=__span-9-17><a id=__codelineno-9-17 name=__codelineno-9-17 href=#__codelineno-9-17></a>        <span class=s2>"uv pip install --no-deps -U packaging setuptools wheel ninja --system"</span>
</span><span id=__span-9-18><a id=__codelineno-9-18 name=__codelineno-9-18 href=#__codelineno-9-18></a>    <span class=p>)</span>
</span><span id=__span-9-19><a id=__codelineno-9-19 name=__codelineno-9-19 href=#__codelineno-9-19></a>
</span><span id=__span-9-20><a id=__codelineno-9-20 name=__codelineno-9-20 href=#__codelineno-9-20></a>    <span class=c1># Install Axolotl with DeepSpeed support</span>
</span><span id=__span-9-21><a id=__codelineno-9-21 name=__codelineno-9-21 href=#__codelineno-9-21></a>    <span class=o>.</span><span class=n>run_commands</span><span class=p>(</span><span class=s2>"uv pip install --no-build-isolation axolotl[deepspeed] --system"</span><span class=p>)</span>
</span><span id=__span-9-22><a id=__codelineno-9-22 name=__codelineno-9-22 href=#__codelineno-9-22></a>
</span><span id=__span-9-23><a id=__codelineno-9-23 name=__codelineno-9-23 href=#__codelineno-9-23></a>    <span class=c1># Install Flash Attention (this takes a while to compile)</span>
</span><span id=__span-9-24><a id=__codelineno-9-24 name=__codelineno-9-24 href=#__codelineno-9-24></a>    <span class=o>.</span><span class=n>run_commands</span><span class=p>(</span>
</span><span id=__span-9-25><a id=__codelineno-9-25 name=__codelineno-9-25 href=#__codelineno-9-25></a>        <span class=s2>"UV_NO_BUILD_ISOLATION=1 uv pip install flash-attn --no-build-isolation --system"</span>
</span><span id=__span-9-26><a id=__codelineno-9-26 name=__codelineno-9-26 href=#__codelineno-9-26></a>    <span class=p>)</span>
</span><span id=__span-9-27><a id=__codelineno-9-27 name=__codelineno-9-27 href=#__codelineno-9-27></a>
</span><span id=__span-9-28><a id=__codelineno-9-28 name=__codelineno-9-28 href=#__codelineno-9-28></a>    <span class=c1># Set environment variables</span>
</span><span id=__span-9-29><a id=__codelineno-9-29 name=__codelineno-9-29 href=#__codelineno-9-29></a>    <span class=o>.</span><span class=n>env</span><span class=p>({</span>
</span><span id=__span-9-30><a id=__codelineno-9-30 name=__codelineno-9-30 href=#__codelineno-9-30></a>        <span class=s2>"HF_HUB_ENABLE_HF_TRANSFER"</span><span class=p>:</span> <span class=s2>"1"</span><span class=p>,</span>  <span class=c1># Fast model downloads</span>
</span><span id=__span-9-31><a id=__codelineno-9-31 name=__codelineno-9-31 href=#__codelineno-9-31></a>        <span class=s2>"HF_HOME"</span><span class=p>:</span> <span class=s2>"/data/.cache"</span><span class=p>,</span>          <span class=c1># Cache in volume</span>
</span><span id=__span-9-32><a id=__codelineno-9-32 name=__codelineno-9-32 href=#__codelineno-9-32></a>    <span class=p>})</span>
</span><span id=__span-9-33><a id=__codelineno-9-33 name=__codelineno-9-33 href=#__codelineno-9-33></a><span class=p>)</span>
</span></code></pre></div> <p><strong>Key points:</strong></p> <ol> <li> <p><strong>Installation order matters:</strong> Base deps → Axolotl → Flash Attention. Do it wrong and you'll have dependency hell.</p> </li> <li> <p><strong><code>--no-build-isolation</code>:</strong> Required for Flash Attention. Don't ask me why, it's just how flash-attn works.</p> </li> <li> <p><strong><code>HF_HUB_ENABLE_HF_TRANSFER</code>:</strong> Enables parallel downloads from HuggingFace. Can be 5-10x faster for large models.</p> </li> <li> <p><strong>Cache in volume:</strong> All HuggingFace downloads go to <code>/data/.cache</code>, which persists. Download once, use forever.</p> </li> </ol> <blockquote> <p><strong>⏰ Build time warning:</strong> The first build takes 20-30 minutes because Flash Attention compiles from source. It's compiling optimized CUDA kernels for your GPU architecture. Go grab lunch. But here's the magic - Modal caches this image. Every subsequent run? Instant startup.</p> </blockquote> <h3 id=alternative-official-axolotl-image>Alternative: Official Axolotl Image<a class=headerlink href=#alternative-official-axolotl-image title="Permanent link">¶</a></h3> <p>Axolotl provides an official Docker image, but I don't use it:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=c1># This works, but I don't recommend it:</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a><span class=c1># AXOLOTL_IMAGE = ModalImage.from_registry(</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a><span class=c1>#     "axolotlai/axolotl-cloud:main-latest", add_python="3.12"</span>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a><span class=c1># ).env({</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a><span class=c1>#     "JUPYTER_ENABLE_LAB": "no",</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a><span class=c1>#     "JUPYTER_TOKEN": "",</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a><span class=c1>#     "HF_HOME": "/data/.cache",</span>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a><span class=c1># })</span>
</span></code></pre></div> <p><strong>Why custom image?</strong> - Official image auto-starts JupyterLab, which we don't need on Modal - Custom image is lighter and more predictable - Full control over versions (important for reproducibility)</p> <h2 id=training-configuration-with-yaml>Training Configuration with YAML<a class=headerlink href=#training-configuration-with-yaml title="Permanent link">¶</a></h2> <p>Here's where Axolotl really shines. All your configuration goes in a YAML file. No scattered parameters, no magic constants buried in code. Everything in one place.</p> <h3 id=complete-training-config>Complete Training Config<a class=headerlink href=#complete-training-config title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=n>TRAIN_CONFIG_YAML</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"""</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=s2>base_model: NousResearch/Meta-Llama-3-8B-Instruct</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=s2>model_type: LlamaForCausalLM</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a><span class=s2>tokenizer_type: AutoTokenizer</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a><span class=s2># Optional: Automatically upload to HuggingFace Hub</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=s2># hub_model_id: username/custom_model_name</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a><span class=s2>load_in_8bit: true</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a><span class=s2>load_in_4bit: false</span>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a>
</span><span id=__span-11-12><a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a><span class=s2>chat_template: llama3</span>
</span><span id=__span-11-13><a id=__codelineno-11-13 name=__codelineno-11-13 href=#__codelineno-11-13></a><span class=s2>datasets:</span>
</span><span id=__span-11-14><a id=__codelineno-11-14 name=__codelineno-11-14 href=#__codelineno-11-14></a><span class=s2>  - path: fozziethebeat/alpaca_messages_2k_test</span>
</span><span id=__span-11-15><a id=__codelineno-11-15 name=__codelineno-11-15 href=#__codelineno-11-15></a><span class=s2>    type: chat_template</span>
</span><span id=__span-11-16><a id=__codelineno-11-16 name=__codelineno-11-16 href=#__codelineno-11-16></a>
</span><span id=__span-11-17><a id=__codelineno-11-17 name=__codelineno-11-17 href=#__codelineno-11-17></a><span class=s2>dataset_prepared_path: /data/prepared_datasets/alpaca_2k</span>
</span><span id=__span-11-18><a id=__codelineno-11-18 name=__codelineno-11-18 href=#__codelineno-11-18></a><span class=s2>val_set_size: 0.05</span>
</span><span id=__span-11-19><a id=__codelineno-11-19 name=__codelineno-11-19 href=#__codelineno-11-19></a><span class=s2>output_dir: /data/outputs/lora-out</span>
</span><span id=__span-11-20><a id=__codelineno-11-20 name=__codelineno-11-20 href=#__codelineno-11-20></a>
</span><span id=__span-11-21><a id=__codelineno-11-21 name=__codelineno-11-21 href=#__codelineno-11-21></a><span class=s2>sequence_len: 4096</span>
</span><span id=__span-11-22><a id=__codelineno-11-22 name=__codelineno-11-22 href=#__codelineno-11-22></a><span class=s2>sample_packing: false</span>
</span><span id=__span-11-23><a id=__codelineno-11-23 name=__codelineno-11-23 href=#__codelineno-11-23></a>
</span><span id=__span-11-24><a id=__codelineno-11-24 name=__codelineno-11-24 href=#__codelineno-11-24></a><span class=s2># LoRA Configuration</span>
</span><span id=__span-11-25><a id=__codelineno-11-25 name=__codelineno-11-25 href=#__codelineno-11-25></a><span class=s2>adapter: lora</span>
</span><span id=__span-11-26><a id=__codelineno-11-26 name=__codelineno-11-26 href=#__codelineno-11-26></a><span class=s2>lora_r: 32</span>
</span><span id=__span-11-27><a id=__codelineno-11-27 name=__codelineno-11-27 href=#__codelineno-11-27></a><span class=s2>lora_alpha: 16</span>
</span><span id=__span-11-28><a id=__codelineno-11-28 name=__codelineno-11-28 href=#__codelineno-11-28></a><span class=s2>lora_dropout: 0.05</span>
</span><span id=__span-11-29><a id=__codelineno-11-29 name=__codelineno-11-29 href=#__codelineno-11-29></a><span class=s2>lora_target_linear: true</span>
</span><span id=__span-11-30><a id=__codelineno-11-30 name=__codelineno-11-30 href=#__codelineno-11-30></a>
</span><span id=__span-11-31><a id=__codelineno-11-31 name=__codelineno-11-31 href=#__codelineno-11-31></a><span class=s2># Weights &amp; Biases</span>
</span><span id=__span-11-32><a id=__codelineno-11-32 name=__codelineno-11-32 href=#__codelineno-11-32></a><span class=s2>wandb_project: </span><span class=si>{</span><span class=n>WANDB_PROJECT_DEFAULT</span><span class=si>}</span>
</span><span id=__span-11-33><a id=__codelineno-11-33 name=__codelineno-11-33 href=#__codelineno-11-33></a><span class=s2>wandb_entity:</span>
</span><span id=__span-11-34><a id=__codelineno-11-34 name=__codelineno-11-34 href=#__codelineno-11-34></a><span class=s2>wandb_watch:</span>
</span><span id=__span-11-35><a id=__codelineno-11-35 name=__codelineno-11-35 href=#__codelineno-11-35></a><span class=s2>wandb_name:</span>
</span><span id=__span-11-36><a id=__codelineno-11-36 name=__codelineno-11-36 href=#__codelineno-11-36></a><span class=s2>wandb_log_model:</span>
</span><span id=__span-11-37><a id=__codelineno-11-37 name=__codelineno-11-37 href=#__codelineno-11-37></a>
</span><span id=__span-11-38><a id=__codelineno-11-38 name=__codelineno-11-38 href=#__codelineno-11-38></a><span class=s2># Training Hyperparameters</span>
</span><span id=__span-11-39><a id=__codelineno-11-39 name=__codelineno-11-39 href=#__codelineno-11-39></a><span class=s2>gradient_accumulation_steps: 4</span>
</span><span id=__span-11-40><a id=__codelineno-11-40 name=__codelineno-11-40 href=#__codelineno-11-40></a><span class=s2>micro_batch_size: 8</span>
</span><span id=__span-11-41><a id=__codelineno-11-41 name=__codelineno-11-41 href=#__codelineno-11-41></a><span class=s2>num_epochs: 4</span>
</span><span id=__span-11-42><a id=__codelineno-11-42 name=__codelineno-11-42 href=#__codelineno-11-42></a><span class=s2>optimizer: adamw_bnb_8bit</span>
</span><span id=__span-11-43><a id=__codelineno-11-43 name=__codelineno-11-43 href=#__codelineno-11-43></a><span class=s2>lr_scheduler: cosine</span>
</span><span id=__span-11-44><a id=__codelineno-11-44 name=__codelineno-11-44 href=#__codelineno-11-44></a><span class=s2>learning_rate: 0.0002</span>
</span><span id=__span-11-45><a id=__codelineno-11-45 name=__codelineno-11-45 href=#__codelineno-11-45></a>
</span><span id=__span-11-46><a id=__codelineno-11-46 name=__codelineno-11-46 href=#__codelineno-11-46></a><span class=s2>bf16: auto</span>
</span><span id=__span-11-47><a id=__codelineno-11-47 name=__codelineno-11-47 href=#__codelineno-11-47></a><span class=s2>tf32: false</span>
</span><span id=__span-11-48><a id=__codelineno-11-48 name=__codelineno-11-48 href=#__codelineno-11-48></a>
</span><span id=__span-11-49><a id=__codelineno-11-49 name=__codelineno-11-49 href=#__codelineno-11-49></a><span class=s2># Memory Optimizations</span>
</span><span id=__span-11-50><a id=__codelineno-11-50 name=__codelineno-11-50 href=#__codelineno-11-50></a><span class=s2>gradient_checkpointing: true</span>
</span><span id=__span-11-51><a id=__codelineno-11-51 name=__codelineno-11-51 href=#__codelineno-11-51></a><span class=s2>resume_from_checkpoint:</span>
</span><span id=__span-11-52><a id=__codelineno-11-52 name=__codelineno-11-52 href=#__codelineno-11-52></a><span class=s2>logging_steps: 1</span>
</span><span id=__span-11-53><a id=__codelineno-11-53 name=__codelineno-11-53 href=#__codelineno-11-53></a><span class=s2>flash_attention: true</span>
</span><span id=__span-11-54><a id=__codelineno-11-54 name=__codelineno-11-54 href=#__codelineno-11-54></a>
</span><span id=__span-11-55><a id=__codelineno-11-55 name=__codelineno-11-55 href=#__codelineno-11-55></a><span class=s2>warmup_ratio: 0.1</span>
</span><span id=__span-11-56><a id=__codelineno-11-56 name=__codelineno-11-56 href=#__codelineno-11-56></a><span class=s2>evals_per_epoch: 4</span>
</span><span id=__span-11-57><a id=__codelineno-11-57 name=__codelineno-11-57 href=#__codelineno-11-57></a><span class=s2>saves_per_epoch: 4</span>
</span><span id=__span-11-58><a id=__codelineno-11-58 name=__codelineno-11-58 href=#__codelineno-11-58></a><span class=s2>weight_decay: 0.0</span>
</span><span id=__span-11-59><a id=__codelineno-11-59 name=__codelineno-11-59 href=#__codelineno-11-59></a>
</span><span id=__span-11-60><a id=__codelineno-11-60 name=__codelineno-11-60 href=#__codelineno-11-60></a><span class=s2>special_tokens:</span>
</span><span id=__span-11-61><a id=__codelineno-11-61 name=__codelineno-11-61 href=#__codelineno-11-61></a><span class=s2>   pad_token: &lt;|end_of_text|&gt;</span>
</span><span id=__span-11-62><a id=__codelineno-11-62 name=__codelineno-11-62 href=#__codelineno-11-62></a><span class=s2>"""</span>
</span></code></pre></div> <p>Let me break down the important parts:</p> <h3 id=model-settings>Model Settings<a class=headerlink href=#model-settings title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=nt>base_model</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">NousResearch/Meta-Llama-3-8B-Instruct</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a><span class=nt>model_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LlamaForCausalLM</span>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=nt>tokenizer_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">AutoTokenizer</span>
</span></code></pre></div> <p>This is configured for Llama 3-8B (for testing). When you're ready for the real deal:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=nt>base_model</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">meta-llama/Meta-Llama-3-8-70B-Instruct</span>
</span></code></pre></div> <p><strong>For other models:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=c1># Mistral 7B</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=nt>base_model</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">mistralai/Mistral-7B-Instruct-v0.2</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a><span class=nt>model_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">MistralForCausalLM</span>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=c1># Qwen 72B</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a><span class=nt>base_model</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Qwen/Qwen2.5-72B-Instruct</span>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=nt>model_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Qwen2ForCausalLM</span>
</span></code></pre></div> <h3 id=quantization-settings>Quantization Settings<a class=headerlink href=#quantization-settings title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=nt>load_in_8bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>   </span><span class=c1># Reduces memory by ~50%</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=nt>load_in_4bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class=w>  </span><span class=c1># Reduces memory by ~75%</span>
</span></code></pre></div> <p><strong>My recommendations:</strong> - <strong>8B models:</strong> <code>load_in_8bit: false</code> (use full precision, you have the memory) - <strong>8-70B models:</strong> <code>load_in_8bit: true</code> (essential to fit on 4 GPUs) - <strong>405B models:</strong> <code>load_in_4bit: true</code> (required, even with 8 GPUs)</p> <p>The quality hit from 8-bit is minimal. The quality hit from 4-bit is noticeable but acceptable.</p> <h3 id=dataset-configuration>Dataset Configuration<a class=headerlink href=#dataset-configuration title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=nt>datasets</span><span class=p>:</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">fozziethebeat/alpaca_messages_2k_test</span>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">chat_template</span>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a><span class=nt>dataset_prepared_path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">/data/prepared_datasets/alpaca_2k</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a><span class=nt>val_set_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span><span class=w>  </span><span class=c1># 5% for validation</span>
</span></code></pre></div> <p><strong>Multiple datasets:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=nt>datasets</span><span class=p>:</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dataset1/name</span>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">chat_template</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dataset2/name</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">alpaca</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a><span class=w>    </span><span class=nt>split</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">train</span>
</span></code></pre></div> <p>Axolotl supports many formats: <code>chat_template</code>, <code>alpaca</code>, <code>sharegpt</code>, <code>completion</code>, etc. Check the <a href=https://docs.axolotl.ai/ >Axolotl docs</a> for the full list.</p> <h3 id=lora-parameters>LoRA Parameters<a class=headerlink href=#lora-parameters title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=nt>adapter</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">lora</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class=w>           </span><span class=c1># Rank (higher = more capacity, slower training)</span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">16</span><span class=w>       </span><span class=c1># Scaling factor (usually r/2 or r)</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a><span class=nt>lora_dropout</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span><span class=w>   </span><span class=c1># Regularization</span>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a><span class=nt>lora_target_linear</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>  </span><span class=c1># Apply to all linear layers</span>
</span></code></pre></div> <p><strong>LoRA rank guidelines:</strong></p> <p>For <strong>8B models:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
</span></code></pre></div> <p>For <strong>8-70B models (high quality):</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
</span></code></pre></div> <p>For <strong>faster training:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
</span></code></pre></div> <p>Higher rank = more capacity to learn, but slower training and larger adapters.</p> <h3 id=training-hyperparameters>Training Hyperparameters<a class=headerlink href=#training-hyperparameters title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class=w>              </span><span class=c1># Batch size per GPU</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a><span class=nt>gradient_accumulation_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class=w>   </span><span class=c1># Steps before updating weights</span>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a><span class=nt>num_epochs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span id=__span-22-4><a id=__codelineno-22-4 name=__codelineno-22-4 href=#__codelineno-22-4></a><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.0002</span>
</span><span id=__span-22-5><a id=__codelineno-22-5 name=__codelineno-22-5 href=#__codelineno-22-5></a><span class=nt>optimizer</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">adamw_bnb_8bit</span><span class=w>        </span><span class=c1># 8-bit Adam (saves memory!)</span>
</span><span id=__span-22-6><a id=__codelineno-22-6 name=__codelineno-22-6 href=#__codelineno-22-6></a><span class=nt>lr_scheduler</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">cosine</span>
</span></code></pre></div> <p><strong>Effective batch size calculation:</strong> </p><div class="language-text highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>Effective Batch = micro_batch_size × gradient_accumulation_steps × num_gpus
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a>                = 8 × 4 × 4
</span><span id=__span-23-3><a id=__codelineno-23-3 name=__codelineno-23-3 href=#__codelineno-23-3></a>                = 128
</span></code></pre></div> <p>With 4 GPUs, each does batch size 8, accumulates over 4 steps, so effectively you're training with batch size 128.</p> <h3 id=memory-optimizations>Memory Optimizations<a class=headerlink href=#memory-optimizations title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=nt>gradient_checkpointing</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>  </span><span class=c1># Trade compute for memory (essential!)</span>
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a><span class=nt>flash_attention</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>         </span><span class=c1># Faster, more memory-efficient attention</span>
</span><span id=__span-24-3><a id=__codelineno-24-3 name=__codelineno-24-3 href=#__codelineno-24-3></a><span class=nt>bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">auto</span><span class=w>                    </span><span class=c1># Use bfloat16 if GPU supports it</span>
</span></code></pre></div> <p><strong>Gradient checkpointing</strong> is critical for large models. It recomputes activations during backward pass instead of storing them. Uses ~40% less memory at the cost of ~20% slower training. Totally worth it.</p> <p><strong>Flash Attention</strong> is a must-have. It's an optimized attention implementation that's both faster AND uses less memory. Win-win.</p> <h2 id=helper-function-write-config-to-volume>Helper Function: Write Config to Volume<a class=headerlink href=#helper-function-write-config-to-volume title="Permanent link">¶</a></h2> <p>Before we get to the training stages, here's a helper function that all stages use:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=k>def</span><span class=w> </span><span class=nf>write_config_to_volume</span><span class=p>(</span>
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a>    <span class=n>train_config_yaml</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span><span id=__span-25-3><a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a>    <span class=n>config_path</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"/data/config.yml"</span><span class=p>,</span>
</span><span id=__span-25-4><a id=__codelineno-25-4 name=__codelineno-25-4 href=#__codelineno-25-4></a>    <span class=n>update_paths</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-25-5><a id=__codelineno-25-5 name=__codelineno-25-5 href=#__codelineno-25-5></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-25-6><a id=__codelineno-25-6 name=__codelineno-25-6 href=#__codelineno-25-6></a><span class=w>    </span><span class=sd>"""Write YAML configuration to volume with optional path updates."""</span>
</span><span id=__span-25-7><a id=__codelineno-25-7 name=__codelineno-25-7 href=#__codelineno-25-7></a>    <span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-25-8><a id=__codelineno-25-8 name=__codelineno-25-8 href=#__codelineno-25-8></a>    <span class=kn>import</span><span class=w> </span><span class=nn>yaml</span>
</span><span id=__span-25-9><a id=__codelineno-25-9 name=__codelineno-25-9 href=#__codelineno-25-9></a>
</span><span id=__span-25-10><a id=__codelineno-25-10 name=__codelineno-25-10 href=#__codelineno-25-10></a>    <span class=c1># Parse YAML string into dict</span>
</span><span id=__span-25-11><a id=__codelineno-25-11 name=__codelineno-25-11 href=#__codelineno-25-11></a>    <span class=n>config_dict</span> <span class=o>=</span> <span class=n>yaml</span><span class=o>.</span><span class=n>safe_load</span><span class=p>(</span><span class=n>train_config_yaml</span><span class=p>)</span>
</span><span id=__span-25-12><a id=__codelineno-25-12 name=__codelineno-25-12 href=#__codelineno-25-12></a>
</span><span id=__span-25-13><a id=__codelineno-25-13 name=__codelineno-25-13 href=#__codelineno-25-13></a>    <span class=c1># Update paths to use volume instead of local dirs</span>
</span><span id=__span-25-14><a id=__codelineno-25-14 name=__codelineno-25-14 href=#__codelineno-25-14></a>    <span class=k>if</span> <span class=n>update_paths</span> <span class=ow>and</span> <span class=s2>"output_dir"</span> <span class=ow>in</span> <span class=n>config_dict</span><span class=p>:</span>
</span><span id=__span-25-15><a id=__codelineno-25-15 name=__codelineno-25-15 href=#__codelineno-25-15></a>        <span class=n>config_dict</span><span class=p>[</span><span class=s2>"output_dir"</span><span class=p>]</span> <span class=o>=</span> <span class=n>config_dict</span><span class=p>[</span><span class=s2>"output_dir"</span><span class=p>]</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span>
</span><span id=__span-25-16><a id=__codelineno-25-16 name=__codelineno-25-16 href=#__codelineno-25-16></a>            <span class=s2>"./outputs"</span><span class=p>,</span> <span class=s2>"/data/outputs"</span>
</span><span id=__span-25-17><a id=__codelineno-25-17 name=__codelineno-25-17 href=#__codelineno-25-17></a>        <span class=p>)</span>
</span><span id=__span-25-18><a id=__codelineno-25-18 name=__codelineno-25-18 href=#__codelineno-25-18></a>
</span><span id=__span-25-19><a id=__codelineno-25-19 name=__codelineno-25-19 href=#__codelineno-25-19></a>    <span class=c1># Ensure directory exists</span>
</span><span id=__span-25-20><a id=__codelineno-25-20 name=__codelineno-25-20 href=#__codelineno-25-20></a>    <span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>dirname</span><span class=p>(</span><span class=n>config_path</span><span class=p>),</span> <span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-25-21><a id=__codelineno-25-21 name=__codelineno-25-21 href=#__codelineno-25-21></a>
</span><span id=__span-25-22><a id=__codelineno-25-22 name=__codelineno-25-22 href=#__codelineno-25-22></a>    <span class=c1># Write to volume</span>
</span><span id=__span-25-23><a id=__codelineno-25-23 name=__codelineno-25-23 href=#__codelineno-25-23></a>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>config_path</span><span class=p>,</span> <span class=s2>"w"</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-25-24><a id=__codelineno-25-24 name=__codelineno-25-24 href=#__codelineno-25-24></a>        <span class=n>yaml</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>config_dict</span><span class=p>,</span> <span class=n>f</span><span class=p>,</span> <span class=n>default_flow_style</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span><span id=__span-25-25><a id=__codelineno-25-25 name=__codelineno-25-25 href=#__codelineno-25-25></a>
</span><span id=__span-25-26><a id=__codelineno-25-26 name=__codelineno-25-26 href=#__codelineno-25-26></a>    <span class=c1># Commit so it persists</span>
</span><span id=__span-25-27><a id=__codelineno-25-27 name=__codelineno-25-27 href=#__codelineno-25-27></a>    <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-25-28><a id=__codelineno-25-28 name=__codelineno-25-28 href=#__codelineno-25-28></a>
</span><span id=__span-25-29><a id=__codelineno-25-29 name=__codelineno-25-29 href=#__codelineno-25-29></a>    <span class=k>return</span> <span class=n>config_dict</span>
</span></code></pre></div> <p><strong>What it does:</strong> 1. Converts YAML string to dict (for inspection) 2. Updates paths to use <code>/data</code> volume (so outputs persist) 3. Writes config to volume 4. Commits volume (critical!)</p> <p>This keeps our config in one place and ensures all stages use the same configuration.</p> <h2 id=stage-1-dataset-preprocessing>Stage 1: Dataset Preprocessing<a class=headerlink href=#stage-1-dataset-preprocessing title="Permanent link">¶</a></h2> <p>Alright, let's get to the actual pipeline. First stage: preprocessing.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=c1># GPU Configuration for preprocessing (single GPU is fine)</span>
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a><span class=n>PREPROCESS_NUM_GPUS</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a><span class=n>PREPROCESS_GPU_CONFIG</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>GPU_TYPE</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>PREPROCESS_NUM_GPUS</span><span class=si>}</span><span class=s2>"</span>
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a>
</span><span id=__span-26-5><a id=__codelineno-26-5 name=__codelineno-26-5 href=#__codelineno-26-5></a><span class=nd>@app</span><span class=o>.</span><span class=n>function</span><span class=p>(</span>
</span><span id=__span-26-6><a id=__codelineno-26-6 name=__codelineno-26-6 href=#__codelineno-26-6></a>    <span class=n>image</span><span class=o>=</span><span class=n>AXOLOTL_IMAGE</span><span class=p>,</span>
</span><span id=__span-26-7><a id=__codelineno-26-7 name=__codelineno-26-7 href=#__codelineno-26-7></a>    <span class=n>volumes</span><span class=o>=</span><span class=n>VOLUME_CONFIG</span><span class=p>,</span>
</span><span id=__span-26-8><a id=__codelineno-26-8 name=__codelineno-26-8 href=#__codelineno-26-8></a>    <span class=n>secrets</span><span class=o>=</span><span class=p>[</span><span class=n>huggingface_secret</span><span class=p>],</span>
</span><span id=__span-26-9><a id=__codelineno-26-9 name=__codelineno-26-9 href=#__codelineno-26-9></a>    <span class=n>timeout</span><span class=o>=</span><span class=mi>24</span> <span class=o>*</span> <span class=n>HOURS</span><span class=p>,</span>
</span><span id=__span-26-10><a id=__codelineno-26-10 name=__codelineno-26-10 href=#__codelineno-26-10></a>    <span class=n>gpu</span><span class=o>=</span><span class=n>PREPROCESS_GPU_CONFIG</span><span class=p>,</span>  <span class=c1># Just 1 GPU</span>
</span><span id=__span-26-11><a id=__codelineno-26-11 name=__codelineno-26-11 href=#__codelineno-26-11></a><span class=p>)</span>
</span><span id=__span-26-12><a id=__codelineno-26-12 name=__codelineno-26-12 href=#__codelineno-26-12></a><span class=k>def</span><span class=w> </span><span class=nf>process_datasets</span><span class=p>(</span>
</span><span id=__span-26-13><a id=__codelineno-26-13 name=__codelineno-26-13 href=#__codelineno-26-13></a>    <span class=n>train_config_yaml</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>TRAIN_CONFIG_YAML</span><span class=p>,</span>
</span><span id=__span-26-14><a id=__codelineno-26-14 name=__codelineno-26-14 href=#__codelineno-26-14></a>    <span class=n>config_path</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"/data/config.yml"</span><span class=p>,</span>
</span><span id=__span-26-15><a id=__codelineno-26-15 name=__codelineno-26-15 href=#__codelineno-26-15></a><span class=p>):</span>
</span><span id=__span-26-16><a id=__codelineno-26-16 name=__codelineno-26-16 href=#__codelineno-26-16></a><span class=w>    </span><span class=sd>"""Preprocess and tokenize dataset before training using Axolotl."""</span>
</span><span id=__span-26-17><a id=__codelineno-26-17 name=__codelineno-26-17 href=#__codelineno-26-17></a>    <span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-26-18><a id=__codelineno-26-18 name=__codelineno-26-18 href=#__codelineno-26-18></a>    <span class=kn>import</span><span class=w> </span><span class=nn>subprocess</span>
</span><span id=__span-26-19><a id=__codelineno-26-19 name=__codelineno-26-19 href=#__codelineno-26-19></a>
</span><span id=__span-26-20><a id=__codelineno-26-20 name=__codelineno-26-20 href=#__codelineno-26-20></a>    <span class=c1># Set HF token</span>
</span><span id=__span-26-21><a id=__codelineno-26-21 name=__codelineno-26-21 href=#__codelineno-26-21></a>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HF_TOKEN"</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HUGGINGFACE_TOKEN"</span><span class=p>]</span>
</span><span id=__span-26-22><a id=__codelineno-26-22 name=__codelineno-26-22 href=#__codelineno-26-22></a>
</span><span id=__span-26-23><a id=__codelineno-26-23 name=__codelineno-26-23 href=#__codelineno-26-23></a>    <span class=c1># Write config to volume</span>
</span><span id=__span-26-24><a id=__codelineno-26-24 name=__codelineno-26-24 href=#__codelineno-26-24></a>    <span class=n>config_dict</span> <span class=o>=</span> <span class=n>write_config_to_volume</span><span class=p>(</span><span class=n>train_config_yaml</span><span class=p>,</span> <span class=n>config_path</span><span class=p>,</span> <span class=kc>True</span><span class=p>)</span>
</span><span id=__span-26-25><a id=__codelineno-26-25 name=__codelineno-26-25 href=#__codelineno-26-25></a>    <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-26-26><a id=__codelineno-26-26 name=__codelineno-26-26 href=#__codelineno-26-26></a>
</span><span id=__span-26-27><a id=__codelineno-26-27 name=__codelineno-26-27 href=#__codelineno-26-27></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>"Starting dataset preprocessing..."</span><span class=p>)</span>
</span><span id=__span-26-28><a id=__codelineno-26-28 name=__codelineno-26-28 href=#__codelineno-26-28></a>
</span><span id=__span-26-29><a id=__codelineno-26-29 name=__codelineno-26-29 href=#__codelineno-26-29></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-26-30><a id=__codelineno-26-30 name=__codelineno-26-30 href=#__codelineno-26-30></a>        <span class=c1># Run Axolotl preprocessing</span>
</span><span id=__span-26-31><a id=__codelineno-26-31 name=__codelineno-26-31 href=#__codelineno-26-31></a>        <span class=n>subprocess</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=s2>"axolotl"</span><span class=p>,</span> <span class=s2>"preprocess"</span><span class=p>,</span> <span class=n>config_path</span><span class=p>],</span> <span class=n>check</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-26-32><a id=__codelineno-26-32 name=__codelineno-26-32 href=#__codelineno-26-32></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"✓ Preprocessing completed"</span><span class=p>)</span>
</span><span id=__span-26-33><a id=__codelineno-26-33 name=__codelineno-26-33 href=#__codelineno-26-33></a>
</span><span id=__span-26-34><a id=__codelineno-26-34 name=__codelineno-26-34 href=#__codelineno-26-34></a>        <span class=c1># Commit preprocessed data</span>
</span><span id=__span-26-35><a id=__codelineno-26-35 name=__codelineno-26-35 href=#__codelineno-26-35></a>        <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-26-36><a id=__codelineno-26-36 name=__codelineno-26-36 href=#__codelineno-26-36></a>
</span><span id=__span-26-37><a id=__codelineno-26-37 name=__codelineno-26-37 href=#__codelineno-26-37></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-26-38><a id=__codelineno-26-38 name=__codelineno-26-38 href=#__codelineno-26-38></a>            <span class=s2>"status"</span><span class=p>:</span> <span class=s2>"completed"</span><span class=p>,</span>
</span><span id=__span-26-39><a id=__codelineno-26-39 name=__codelineno-26-39 href=#__codelineno-26-39></a>            <span class=s2>"config_path"</span><span class=p>:</span> <span class=n>config_path</span><span class=p>,</span>
</span><span id=__span-26-40><a id=__codelineno-26-40 name=__codelineno-26-40 href=#__codelineno-26-40></a>            <span class=s2>"preprocessed_data_path"</span><span class=p>:</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"dataset_prepared_path"</span><span class=p>),</span>
</span><span id=__span-26-41><a id=__codelineno-26-41 name=__codelineno-26-41 href=#__codelineno-26-41></a>            <span class=s2>"output_dir"</span><span class=p>:</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"output_dir"</span><span class=p>),</span>
</span><span id=__span-26-42><a id=__codelineno-26-42 name=__codelineno-26-42 href=#__codelineno-26-42></a>        <span class=p>}</span>
</span><span id=__span-26-43><a id=__codelineno-26-43 name=__codelineno-26-43 href=#__codelineno-26-43></a>    <span class=k>except</span> <span class=n>subprocess</span><span class=o>.</span><span class=n>CalledProcessError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span><span id=__span-26-44><a id=__codelineno-26-44 name=__codelineno-26-44 href=#__codelineno-26-44></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Preprocessing failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span></code></pre></div> <p><strong>What happens during preprocessing:</strong></p> <ol> <li><strong>Download dataset</strong> from HuggingFace (or load from cache)</li> <li><strong>Apply chat template</strong> to format conversations correctly</li> <li><strong>Tokenize everything</strong> using the model's tokenizer</li> <li><strong>Save to disk</strong> at <code>dataset_prepared_path</code></li> <li><strong>Split train/val</strong> based on <code>val_set_size</code></li> </ol> <p><strong>Why preprocess separately?</strong></p> <p>You might think "why not just preprocess during training?" Here's why this is better:</p> <ol> <li> <p><strong>Cost savings:</strong> Preprocessing doesn't need 4 GPUs. Why pay $14/hr when you can pay $3.50/hr?</p> </li> <li> <p><strong>Reusability:</strong> Preprocess once, train multiple times with different hyperparameters. Huge time saver when experimenting.</p> </li> <li> <p><strong>Debugging:</strong> If preprocessing fails, you know immediately. Not after 3 hours of training setup.</p> </li> <li> <p><strong>Visibility:</strong> You can inspect the preprocessed data to make sure it looks right.</p> </li> </ol> <p><strong>Run it:</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::process_datasets
</span></code></pre></div> <p>First run downloads the dataset and tokenizes everything. Takes 10-30 minutes depending on dataset size. Subsequent runs? Instant, because it's cached.</p> <h2 id=stage-2-multi-gpu-training>Stage 2: Multi-GPU Training<a class=headerlink href=#stage-2-multi-gpu-training title="Permanent link">¶</a></h2> <p>Here's where the magic happens. We're training Llama across multiple GPUs with Accelerate.</p> <h3 id=gpu-configuration>GPU Configuration<a class=headerlink href=#gpu-configuration title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=c1># GPU Configuration for training (2-8 GPUs)</span>
</span><span id=__span-28-2><a id=__codelineno-28-2 name=__codelineno-28-2 href=#__codelineno-28-2></a><span class=n>TRAIN_NUM_GPUS</span> <span class=o>=</span> <span class=mi>4</span>  <span class=c1># Can be adjusted from 2 to 8</span>
</span><span id=__span-28-3><a id=__codelineno-28-3 name=__codelineno-28-3 href=#__codelineno-28-3></a><span class=n>TRAIN_GPU_CONFIG</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>GPU_TYPE</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>TRAIN_NUM_GPUS</span><span class=si>}</span><span class=s2>"</span>
</span></code></pre></div> <p><strong>Scaling guidelines:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Min GPUs</th> <th>Recommended</th> <th>GPU Type</th> <th>Cost/hr</th> </tr> </thead> <tbody> <tr> <td>Llama 3-8B</td> <td>1</td> <td>1</td> <td>A100-40GB</td> <td>$2.50</td> </tr> <tr> <td>Llama 3-13B</td> <td>1</td> <td>2</td> <td>A100-40GB</td> <td>$5.00</td> </tr> <tr> <td>Llama 3-8-70B</td> <td>2</td> <td>4</td> <td>A100-80GB</td> <td>$14.00</td> </tr> <tr> <td>Llama 3-405B</td> <td>8</td> <td>8</td> <td>A100-80GB</td> <td>$28.00</td> </tr> </tbody> </table> <h3 id=training-function>Training Function<a class=headerlink href=#training-function title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=nd>@app</span><span class=o>.</span><span class=n>function</span><span class=p>(</span>
</span><span id=__span-29-2><a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>    <span class=n>image</span><span class=o>=</span><span class=n>AXOLOTL_IMAGE</span><span class=p>,</span>
</span><span id=__span-29-3><a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a>    <span class=n>volumes</span><span class=o>=</span><span class=n>VOLUME_CONFIG</span><span class=p>,</span>
</span><span id=__span-29-4><a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a>    <span class=n>secrets</span><span class=o>=</span><span class=p>[</span><span class=n>huggingface_secret</span><span class=p>],</span>
</span><span id=__span-29-5><a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a>    <span class=n>timeout</span><span class=o>=</span><span class=mi>24</span> <span class=o>*</span> <span class=n>HOURS</span><span class=p>,</span>
</span><span id=__span-29-6><a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a>    <span class=n>gpu</span><span class=o>=</span><span class=n>TRAIN_GPU_CONFIG</span><span class=p>,</span>  <span class=c1># e.g., "a100-80gb:4"</span>
</span><span id=__span-29-7><a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a><span class=p>)</span>
</span><span id=__span-29-8><a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a><span class=k>def</span><span class=w> </span><span class=nf>train_model</span><span class=p>(</span>
</span><span id=__span-29-9><a id=__codelineno-29-9 name=__codelineno-29-9 href=#__codelineno-29-9></a>    <span class=n>train_config_yaml</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>TRAIN_CONFIG_YAML</span><span class=p>,</span>
</span><span id=__span-29-10><a id=__codelineno-29-10 name=__codelineno-29-10 href=#__codelineno-29-10></a>    <span class=n>config_path</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"/data/config.yml"</span><span class=p>,</span>
</span><span id=__span-29-11><a id=__codelineno-29-11 name=__codelineno-29-11 href=#__codelineno-29-11></a><span class=p>):</span>
</span><span id=__span-29-12><a id=__codelineno-29-12 name=__codelineno-29-12 href=#__codelineno-29-12></a><span class=w>    </span><span class=sd>"""</span>
</span><span id=__span-29-13><a id=__codelineno-29-13 name=__codelineno-29-13 href=#__codelineno-29-13></a><span class=sd>    Train or fine-tune a model using Axolotl with multi-GPU support.</span>
</span><span id=__span-29-14><a id=__codelineno-29-14 name=__codelineno-29-14 href=#__codelineno-29-14></a><span class=sd>    Uses accelerate for multi-GPU training.</span>
</span><span id=__span-29-15><a id=__codelineno-29-15 name=__codelineno-29-15 href=#__codelineno-29-15></a><span class=sd>    """</span>
</span><span id=__span-29-16><a id=__codelineno-29-16 name=__codelineno-29-16 href=#__codelineno-29-16></a>    <span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-29-17><a id=__codelineno-29-17 name=__codelineno-29-17 href=#__codelineno-29-17></a>    <span class=kn>import</span><span class=w> </span><span class=nn>subprocess</span>
</span><span id=__span-29-18><a id=__codelineno-29-18 name=__codelineno-29-18 href=#__codelineno-29-18></a>
</span><span id=__span-29-19><a id=__codelineno-29-19 name=__codelineno-29-19 href=#__codelineno-29-19></a>    <span class=c1># Set up environment</span>
</span><span id=__span-29-20><a id=__codelineno-29-20 name=__codelineno-29-20 href=#__codelineno-29-20></a>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HF_TOKEN"</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HUGGINGFACE_TOKEN"</span><span class=p>]</span>
</span><span id=__span-29-21><a id=__codelineno-29-21 name=__codelineno-29-21 href=#__codelineno-29-21></a>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"WANDB_API_KEY"</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"WANDB_API_KEY"</span><span class=p>]</span>
</span><span id=__span-29-22><a id=__codelineno-29-22 name=__codelineno-29-22 href=#__codelineno-29-22></a>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"WANDB_PROJECT"</span><span class=p>]</span> <span class=o>=</span> <span class=n>WANDB_PROJECT_DEFAULT</span>
</span><span id=__span-29-23><a id=__codelineno-29-23 name=__codelineno-29-23 href=#__codelineno-29-23></a>
</span><span id=__span-29-24><a id=__codelineno-29-24 name=__codelineno-29-24 href=#__codelineno-29-24></a>    <span class=c1># Write config to volume</span>
</span><span id=__span-29-25><a id=__codelineno-29-25 name=__codelineno-29-25 href=#__codelineno-29-25></a>    <span class=n>config_dict</span> <span class=o>=</span> <span class=n>write_config_to_volume</span><span class=p>(</span>
</span><span id=__span-29-26><a id=__codelineno-29-26 name=__codelineno-29-26 href=#__codelineno-29-26></a>        <span class=n>train_config_yaml</span><span class=o>=</span><span class=n>train_config_yaml</span><span class=p>,</span>
</span><span id=__span-29-27><a id=__codelineno-29-27 name=__codelineno-29-27 href=#__codelineno-29-27></a>        <span class=n>config_path</span><span class=o>=</span><span class=n>config_path</span><span class=p>,</span>
</span><span id=__span-29-28><a id=__codelineno-29-28 name=__codelineno-29-28 href=#__codelineno-29-28></a>        <span class=n>update_paths</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-29-29><a id=__codelineno-29-29 name=__codelineno-29-29 href=#__codelineno-29-29></a>    <span class=p>)</span>
</span><span id=__span-29-30><a id=__codelineno-29-30 name=__codelineno-29-30 href=#__codelineno-29-30></a>
</span><span id=__span-29-31><a id=__codelineno-29-31 name=__codelineno-29-31 href=#__codelineno-29-31></a>    <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-29-32><a id=__codelineno-29-32 name=__codelineno-29-32 href=#__codelineno-29-32></a>
</span><span id=__span-29-33><a id=__codelineno-29-33 name=__codelineno-29-33 href=#__codelineno-29-33></a>    <span class=c1># Build accelerate command for multi-GPU training</span>
</span><span id=__span-29-34><a id=__codelineno-29-34 name=__codelineno-29-34 href=#__codelineno-29-34></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Starting training with </span><span class=si>{</span><span class=n>TRAIN_NUM_GPUS</span><span class=si>}</span><span class=s2> GPUs..."</span><span class=p>)</span>
</span><span id=__span-29-35><a id=__codelineno-29-35 name=__codelineno-29-35 href=#__codelineno-29-35></a>
</span><span id=__span-29-36><a id=__codelineno-29-36 name=__codelineno-29-36 href=#__codelineno-29-36></a>    <span class=n>cmd</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-29-37><a id=__codelineno-29-37 name=__codelineno-29-37 href=#__codelineno-29-37></a>        <span class=s2>"accelerate"</span><span class=p>,</span>
</span><span id=__span-29-38><a id=__codelineno-29-38 name=__codelineno-29-38 href=#__codelineno-29-38></a>        <span class=s2>"launch"</span><span class=p>,</span>
</span><span id=__span-29-39><a id=__codelineno-29-39 name=__codelineno-29-39 href=#__codelineno-29-39></a>        <span class=s2>"--multi_gpu"</span><span class=p>,</span>                        <span class=c1># Enable multi-GPU mode</span>
</span><span id=__span-29-40><a id=__codelineno-29-40 name=__codelineno-29-40 href=#__codelineno-29-40></a>        <span class=s2>"--num_processes"</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>TRAIN_NUM_GPUS</span><span class=p>),</span>  <span class=c1># Number of GPUs</span>
</span><span id=__span-29-41><a id=__codelineno-29-41 name=__codelineno-29-41 href=#__codelineno-29-41></a>        <span class=s2>"--num_machines"</span><span class=p>,</span> <span class=s2>"1"</span><span class=p>,</span>                <span class=c1># Single machine (Modal handles this)</span>
</span><span id=__span-29-42><a id=__codelineno-29-42 name=__codelineno-29-42 href=#__codelineno-29-42></a>        <span class=s2>"--mixed_precision"</span><span class=p>,</span> <span class=s2>"bf16"</span><span class=p>,</span>          <span class=c1># Use bfloat16 for speed</span>
</span><span id=__span-29-43><a id=__codelineno-29-43 name=__codelineno-29-43 href=#__codelineno-29-43></a>        <span class=s2>"--dynamo_backend"</span><span class=p>,</span> <span class=s2>"no"</span><span class=p>,</span>             <span class=c1># Disable torch.compile (stability)</span>
</span><span id=__span-29-44><a id=__codelineno-29-44 name=__codelineno-29-44 href=#__codelineno-29-44></a>        <span class=s2>"-m"</span><span class=p>,</span> <span class=s2>"axolotl.cli.train"</span><span class=p>,</span>            <span class=c1># Run Axolotl training</span>
</span><span id=__span-29-45><a id=__codelineno-29-45 name=__codelineno-29-45 href=#__codelineno-29-45></a>        <span class=n>config_path</span><span class=p>,</span>                          <span class=c1># Path to our YAML config</span>
</span><span id=__span-29-46><a id=__codelineno-29-46 name=__codelineno-29-46 href=#__codelineno-29-46></a>    <span class=p>]</span>
</span><span id=__span-29-47><a id=__codelineno-29-47 name=__codelineno-29-47 href=#__codelineno-29-47></a>
</span><span id=__span-29-48><a id=__codelineno-29-48 name=__codelineno-29-48 href=#__codelineno-29-48></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-29-49><a id=__codelineno-29-49 name=__codelineno-29-49 href=#__codelineno-29-49></a>        <span class=n>subprocess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>cmd</span><span class=p>,</span> <span class=n>check</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-29-50><a id=__codelineno-29-50 name=__codelineno-29-50 href=#__codelineno-29-50></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"✓ Training completed"</span><span class=p>)</span>
</span><span id=__span-29-51><a id=__codelineno-29-51 name=__codelineno-29-51 href=#__codelineno-29-51></a>
</span><span id=__span-29-52><a id=__codelineno-29-52 name=__codelineno-29-52 href=#__codelineno-29-52></a>        <span class=c1># Commit trained model to volume</span>
</span><span id=__span-29-53><a id=__codelineno-29-53 name=__codelineno-29-53 href=#__codelineno-29-53></a>        <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-29-54><a id=__codelineno-29-54 name=__codelineno-29-54 href=#__codelineno-29-54></a>
</span><span id=__span-29-55><a id=__codelineno-29-55 name=__codelineno-29-55 href=#__codelineno-29-55></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-29-56><a id=__codelineno-29-56 name=__codelineno-29-56 href=#__codelineno-29-56></a>            <span class=s2>"status"</span><span class=p>:</span> <span class=s2>"completed"</span><span class=p>,</span>
</span><span id=__span-29-57><a id=__codelineno-29-57 name=__codelineno-29-57 href=#__codelineno-29-57></a>            <span class=s2>"config_path"</span><span class=p>:</span> <span class=n>config_path</span><span class=p>,</span>
</span><span id=__span-29-58><a id=__codelineno-29-58 name=__codelineno-29-58 href=#__codelineno-29-58></a>            <span class=s2>"output_dir"</span><span class=p>:</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"output_dir"</span><span class=p>),</span>
</span><span id=__span-29-59><a id=__codelineno-29-59 name=__codelineno-29-59 href=#__codelineno-29-59></a>            <span class=s2>"base_model"</span><span class=p>:</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"base_model"</span><span class=p>),</span>
</span><span id=__span-29-60><a id=__codelineno-29-60 name=__codelineno-29-60 href=#__codelineno-29-60></a>            <span class=s2>"num_gpus"</span><span class=p>:</span> <span class=n>TRAIN_NUM_GPUS</span><span class=p>,</span>
</span><span id=__span-29-61><a id=__codelineno-29-61 name=__codelineno-29-61 href=#__codelineno-29-61></a>        <span class=p>}</span>
</span><span id=__span-29-62><a id=__codelineno-29-62 name=__codelineno-29-62 href=#__codelineno-29-62></a>
</span><span id=__span-29-63><a id=__codelineno-29-63 name=__codelineno-29-63 href=#__codelineno-29-63></a>    <span class=k>except</span> <span class=n>subprocess</span><span class=o>.</span><span class=n>CalledProcessError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span><span id=__span-29-64><a id=__codelineno-29-64 name=__codelineno-29-64 href=#__codelineno-29-64></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Training failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span></code></pre></div> <h3 id=understanding-the-accelerate-command>Understanding the Accelerate Command<a class=headerlink href=#understanding-the-accelerate-command title="Permanent link">¶</a></h3> <p>Let me break down what this command does:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a>accelerate<span class=w> </span>launch<span class=w> </span><span class=se>\</span>
</span><span id=__span-30-2><a id=__codelineno-30-2 name=__codelineno-30-2 href=#__codelineno-30-2></a><span class=w>  </span>--multi_gpu<span class=w> </span><span class=se>\ </span><span class=w>                    </span><span class=c1># Enable multi-GPU distributed training</span>
</span><span id=__span-30-3><a id=__codelineno-30-3 name=__codelineno-30-3 href=#__codelineno-30-3></a><span class=w>  </span>--num_processes<span class=w> </span><span class=m>4</span><span class=w> </span><span class=se>\ </span><span class=w>              </span><span class=c1># Use 4 GPUs (one process per GPU)</span>
</span><span id=__span-30-4><a id=__codelineno-30-4 name=__codelineno-30-4 href=#__codelineno-30-4></a><span class=w>  </span>--num_machines<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\ </span><span class=w>               </span><span class=c1># Single machine (Modal provides this)</span>
</span><span id=__span-30-5><a id=__codelineno-30-5 name=__codelineno-30-5 href=#__codelineno-30-5></a><span class=w>  </span>--mixed_precision<span class=w> </span>bf16<span class=w> </span><span class=se>\ </span><span class=w>         </span><span class=c1># Use bfloat16 for memory and speed</span>
</span><span id=__span-30-6><a id=__codelineno-30-6 name=__codelineno-30-6 href=#__codelineno-30-6></a><span class=w>  </span>--dynamo_backend<span class=w> </span>no<span class=w> </span><span class=se>\ </span><span class=w>            </span><span class=c1># Disable torch.compile (causes issues)</span>
</span><span id=__span-30-7><a id=__codelineno-30-7 name=__codelineno-30-7 href=#__codelineno-30-7></a><span class=w>  </span>-m<span class=w> </span>axolotl.cli.train<span class=w> </span><span class=se>\ </span><span class=w>           </span><span class=c1># Run Axolotl's training module</span>
</span><span id=__span-30-8><a id=__codelineno-30-8 name=__codelineno-30-8 href=#__codelineno-30-8></a><span class=w>  </span>/data/config.yml<span class=w>                  </span><span class=c1># Our YAML configuration</span>
</span></code></pre></div> <p><strong>What Accelerate does for you:</strong></p> <ol> <li><strong>Spawns one process per GPU</strong> - Each GPU gets its own Python process</li> <li><strong>Initializes distributed backend</strong> - Sets up NCCL for GPU communication</li> <li><strong>Shards the model</strong> - Splits model parameters across GPUs based on available memory</li> <li><strong>Synchronizes gradients</strong> - All-reduce operation after backward pass</li> <li><strong>Manages checkpointing</strong> - Handles saving/loading distributed checkpoints</li> </ol> <p>You write normal PyTorch code (which Axolotl already did), Accelerate makes it distributed. It's magical.</p> <h3 id=run-training>Run Training<a class=headerlink href=#run-training title="Permanent link">¶</a></h3> <p><strong>Basic run (test with 8B model first):</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1 href=#__codelineno-31-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::train_model
</span></code></pre></div> <p><strong>For actual 8-70B training, edit the YAML:</strong></p> <div class="language-python highlight"><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1 href=#__codelineno-32-1></a><span class=n>TRAIN_CONFIG_YAML</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"""</span>
</span><span id=__span-32-2><a id=__codelineno-32-2 name=__codelineno-32-2 href=#__codelineno-32-2></a><span class=s2>base_model: meta-llama/Meta-Llama-3-8-70B-Instruct</span>
</span><span id=__span-32-3><a id=__codelineno-32-3 name=__codelineno-32-3 href=#__codelineno-32-3></a><span class=s2>load_in_8bit: true</span>
</span><span id=__span-32-4><a id=__codelineno-32-4 name=__codelineno-32-4 href=#__codelineno-32-4></a><span class=s2>lora_r: 64</span>
</span><span id=__span-32-5><a id=__codelineno-32-5 name=__codelineno-32-5 href=#__codelineno-32-5></a><span class=s2>lora_alpha: 128</span>
</span><span id=__span-32-6><a id=__codelineno-32-6 name=__codelineno-32-6 href=#__codelineno-32-6></a><span class=s2>micro_batch_size: 4</span>
</span><span id=__span-32-7><a id=__codelineno-32-7 name=__codelineno-32-7 href=#__codelineno-32-7></a><span class=s2>gradient_accumulation_steps: 4</span>
</span><span id=__span-32-8><a id=__codelineno-32-8 name=__codelineno-32-8 href=#__codelineno-32-8></a><span class=s2>num_epochs: 3</span>
</span><span id=__span-32-9><a id=__codelineno-32-9 name=__codelineno-32-9 href=#__codelineno-32-9></a><span class=s2># ... rest of config</span>
</span><span id=__span-32-10><a id=__codelineno-32-10 name=__codelineno-32-10 href=#__codelineno-32-10></a><span class=s2>"""</span>
</span></code></pre></div> <p>Then run:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1 href=#__codelineno-33-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::train_model
</span></code></pre></div> <p><strong>Monitor your training:</strong></p> <ol> <li><strong>Modal Dashboard:</strong> Click the URL in the terminal for real-time logs and GPU utilization</li> <li><strong>Weights &amp; Biases:</strong> Go to <code>wandb.ai/&lt;username&gt;/Llama-70b-MultiGPU-finetune</code> for beautiful charts</li> <li><strong>Check GPU usage:</strong> All 4 GPUs should be near 100% utilization</li> </ol> <blockquote> <p><strong>💰 Cost Alert:</strong> 4× A100-80GB costs ~$14/hour. A 3-hour training run = $42. A 10-hour run = $140. This is why we preprocess separately and test with small models first!</p> </blockquote> <p><strong>Training checkpoints:</strong></p> <p>Axolotl automatically saves checkpoints to <code>/data/outputs/lora-out/checkpoint-{step}</code>. If training crashes, resume with:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1 href=#__codelineno-34-1></a><span class=nt>resume_from_checkpoint</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">/data/outputs/lora-out/checkpoint-1000</span>
</span></code></pre></div> <h2 id=stage-3-merge-lora-adapters>Stage 3: Merge LoRA Adapters<a class=headerlink href=#stage-3-merge-lora-adapters title="Permanent link">¶</a></h2> <p>After training, you have LoRA adapters (~100MB) that work with the base model. For deployment, it's easier to merge them into a single model.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1 href=#__codelineno-35-1></a><span class=c1># GPU Configuration for merging (single GPU is fine)</span>
</span><span id=__span-35-2><a id=__codelineno-35-2 name=__codelineno-35-2 href=#__codelineno-35-2></a><span class=n>MERGE_NUM_GPUS</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-35-3><a id=__codelineno-35-3 name=__codelineno-35-3 href=#__codelineno-35-3></a><span class=n>MERGE_GPU_CONFIG</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>GPU_TYPE</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>MERGE_NUM_GPUS</span><span class=si>}</span><span class=s2>"</span>
</span><span id=__span-35-4><a id=__codelineno-35-4 name=__codelineno-35-4 href=#__codelineno-35-4></a>
</span><span id=__span-35-5><a id=__codelineno-35-5 name=__codelineno-35-5 href=#__codelineno-35-5></a><span class=nd>@app</span><span class=o>.</span><span class=n>function</span><span class=p>(</span>
</span><span id=__span-35-6><a id=__codelineno-35-6 name=__codelineno-35-6 href=#__codelineno-35-6></a>    <span class=n>image</span><span class=o>=</span><span class=n>AXOLOTL_IMAGE</span><span class=p>,</span>
</span><span id=__span-35-7><a id=__codelineno-35-7 name=__codelineno-35-7 href=#__codelineno-35-7></a>    <span class=n>volumes</span><span class=o>=</span><span class=n>VOLUME_CONFIG</span><span class=p>,</span>
</span><span id=__span-35-8><a id=__codelineno-35-8 name=__codelineno-35-8 href=#__codelineno-35-8></a>    <span class=n>secrets</span><span class=o>=</span><span class=p>[</span><span class=n>huggingface_secret</span><span class=p>],</span>
</span><span id=__span-35-9><a id=__codelineno-35-9 name=__codelineno-35-9 href=#__codelineno-35-9></a>    <span class=n>timeout</span><span class=o>=</span><span class=mi>4</span> <span class=o>*</span> <span class=n>HOURS</span><span class=p>,</span>
</span><span id=__span-35-10><a id=__codelineno-35-10 name=__codelineno-35-10 href=#__codelineno-35-10></a>    <span class=n>gpu</span><span class=o>=</span><span class=n>MERGE_GPU_CONFIG</span><span class=p>,</span>
</span><span id=__span-35-11><a id=__codelineno-35-11 name=__codelineno-35-11 href=#__codelineno-35-11></a><span class=p>)</span>
</span><span id=__span-35-12><a id=__codelineno-35-12 name=__codelineno-35-12 href=#__codelineno-35-12></a><span class=k>def</span><span class=w> </span><span class=nf>merge_lora</span><span class=p>(</span>
</span><span id=__span-35-13><a id=__codelineno-35-13 name=__codelineno-35-13 href=#__codelineno-35-13></a>    <span class=n>train_config_yaml</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>TRAIN_CONFIG_YAML</span><span class=p>,</span>
</span><span id=__span-35-14><a id=__codelineno-35-14 name=__codelineno-35-14 href=#__codelineno-35-14></a>    <span class=n>config_path</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"/data/config.yml"</span><span class=p>,</span>
</span><span id=__span-35-15><a id=__codelineno-35-15 name=__codelineno-35-15 href=#__codelineno-35-15></a>    <span class=n>lora_model_dir</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-35-16><a id=__codelineno-35-16 name=__codelineno-35-16 href=#__codelineno-35-16></a><span class=p>):</span>
</span><span id=__span-35-17><a id=__codelineno-35-17 name=__codelineno-35-17 href=#__codelineno-35-17></a><span class=w>    </span><span class=sd>"""Merge trained LoRA adapters into the base model."""</span>
</span><span id=__span-35-18><a id=__codelineno-35-18 name=__codelineno-35-18 href=#__codelineno-35-18></a>    <span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-35-19><a id=__codelineno-35-19 name=__codelineno-35-19 href=#__codelineno-35-19></a>    <span class=kn>import</span><span class=w> </span><span class=nn>subprocess</span>
</span><span id=__span-35-20><a id=__codelineno-35-20 name=__codelineno-35-20 href=#__codelineno-35-20></a>
</span><span id=__span-35-21><a id=__codelineno-35-21 name=__codelineno-35-21 href=#__codelineno-35-21></a>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HF_TOKEN"</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HUGGINGFACE_TOKEN"</span><span class=p>]</span>
</span><span id=__span-35-22><a id=__codelineno-35-22 name=__codelineno-35-22 href=#__codelineno-35-22></a>
</span><span id=__span-35-23><a id=__codelineno-35-23 name=__codelineno-35-23 href=#__codelineno-35-23></a>    <span class=c1># Write config</span>
</span><span id=__span-35-24><a id=__codelineno-35-24 name=__codelineno-35-24 href=#__codelineno-35-24></a>    <span class=n>config_dict</span> <span class=o>=</span> <span class=n>write_config_to_volume</span><span class=p>(</span>
</span><span id=__span-35-25><a id=__codelineno-35-25 name=__codelineno-35-25 href=#__codelineno-35-25></a>        <span class=n>train_config_yaml</span><span class=o>=</span><span class=n>train_config_yaml</span><span class=p>,</span>
</span><span id=__span-35-26><a id=__codelineno-35-26 name=__codelineno-35-26 href=#__codelineno-35-26></a>        <span class=n>config_path</span><span class=o>=</span><span class=n>config_path</span><span class=p>,</span>
</span><span id=__span-35-27><a id=__codelineno-35-27 name=__codelineno-35-27 href=#__codelineno-35-27></a>        <span class=n>update_paths</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-35-28><a id=__codelineno-35-28 name=__codelineno-35-28 href=#__codelineno-35-28></a>    <span class=p>)</span>
</span><span id=__span-35-29><a id=__codelineno-35-29 name=__codelineno-35-29 href=#__codelineno-35-29></a>
</span><span id=__span-35-30><a id=__codelineno-35-30 name=__codelineno-35-30 href=#__codelineno-35-30></a>    <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-35-31><a id=__codelineno-35-31 name=__codelineno-35-31 href=#__codelineno-35-31></a>
</span><span id=__span-35-32><a id=__codelineno-35-32 name=__codelineno-35-32 href=#__codelineno-35-32></a>    <span class=c1># Build merge command</span>
</span><span id=__span-35-33><a id=__codelineno-35-33 name=__codelineno-35-33 href=#__codelineno-35-33></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>"Starting LoRA merge..."</span><span class=p>)</span>
</span><span id=__span-35-34><a id=__codelineno-35-34 name=__codelineno-35-34 href=#__codelineno-35-34></a>    <span class=n>cmd</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"axolotl"</span><span class=p>,</span> <span class=s2>"merge-lora"</span><span class=p>,</span> <span class=n>config_path</span><span class=p>]</span>
</span><span id=__span-35-35><a id=__codelineno-35-35 name=__codelineno-35-35 href=#__codelineno-35-35></a>
</span><span id=__span-35-36><a id=__codelineno-35-36 name=__codelineno-35-36 href=#__codelineno-35-36></a>    <span class=k>if</span> <span class=n>lora_model_dir</span><span class=p>:</span>
</span><span id=__span-35-37><a id=__codelineno-35-37 name=__codelineno-35-37 href=#__codelineno-35-37></a>        <span class=n>cmd</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=s2>"--lora-model-dir"</span><span class=p>,</span> <span class=n>lora_model_dir</span><span class=p>])</span>
</span><span id=__span-35-38><a id=__codelineno-35-38 name=__codelineno-35-38 href=#__codelineno-35-38></a>
</span><span id=__span-35-39><a id=__codelineno-35-39 name=__codelineno-35-39 href=#__codelineno-35-39></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-35-40><a id=__codelineno-35-40 name=__codelineno-35-40 href=#__codelineno-35-40></a>        <span class=n>subprocess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>cmd</span><span class=p>,</span> <span class=n>check</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-35-41><a id=__codelineno-35-41 name=__codelineno-35-41 href=#__codelineno-35-41></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"✓ LoRA merge completed"</span><span class=p>)</span>
</span><span id=__span-35-42><a id=__codelineno-35-42 name=__codelineno-35-42 href=#__codelineno-35-42></a>
</span><span id=__span-35-43><a id=__codelineno-35-43 name=__codelineno-35-43 href=#__codelineno-35-43></a>        <span class=c1># Commit merged model</span>
</span><span id=__span-35-44><a id=__codelineno-35-44 name=__codelineno-35-44 href=#__codelineno-35-44></a>        <span class=n>exp_volume</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span><span id=__span-35-45><a id=__codelineno-35-45 name=__codelineno-35-45 href=#__codelineno-35-45></a>
</span><span id=__span-35-46><a id=__codelineno-35-46 name=__codelineno-35-46 href=#__codelineno-35-46></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-35-47><a id=__codelineno-35-47 name=__codelineno-35-47 href=#__codelineno-35-47></a>            <span class=s2>"status"</span><span class=p>:</span> <span class=s2>"completed"</span><span class=p>,</span>
</span><span id=__span-35-48><a id=__codelineno-35-48 name=__codelineno-35-48 href=#__codelineno-35-48></a>            <span class=s2>"config_path"</span><span class=p>:</span> <span class=n>config_path</span><span class=p>,</span>
</span><span id=__span-35-49><a id=__codelineno-35-49 name=__codelineno-35-49 href=#__codelineno-35-49></a>            <span class=s2>"output_dir"</span><span class=p>:</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"output_dir"</span><span class=p>),</span>
</span><span id=__span-35-50><a id=__codelineno-35-50 name=__codelineno-35-50 href=#__codelineno-35-50></a>            <span class=s2>"lora_model_dir"</span><span class=p>:</span> <span class=n>lora_model_dir</span> <span class=ow>or</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"lora_model_dir"</span><span class=p>),</span>
</span><span id=__span-35-51><a id=__codelineno-35-51 name=__codelineno-35-51 href=#__codelineno-35-51></a>        <span class=p>}</span>
</span><span id=__span-35-52><a id=__codelineno-35-52 name=__codelineno-35-52 href=#__codelineno-35-52></a>
</span><span id=__span-35-53><a id=__codelineno-35-53 name=__codelineno-35-53 href=#__codelineno-35-53></a>    <span class=k>except</span> <span class=n>subprocess</span><span class=o>.</span><span class=n>CalledProcessError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span><span id=__span-35-54><a id=__codelineno-35-54 name=__codelineno-35-54 href=#__codelineno-35-54></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=sa>f</span><span class=s2>"LoRA merge failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span></code></pre></div> <p><strong>What merging does:</strong></p> <ol> <li>Loads base model weights</li> <li>Loads LoRA adapter weights</li> <li>Applies the LoRA transformations to base model</li> <li>Saves the combined model</li> </ol> <p><strong>Why only 1 GPU?</strong></p> <p>Merging is sequential - you're just doing matrix operations to combine weights. Doesn't benefit from multiple GPUs. Save the money.</p> <p><strong>Run it:</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1 href=#__codelineno-36-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::merge_lora
</span></code></pre></div> <p><strong>Specify custom LoRA directory:</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1 href=#__codelineno-37-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::merge_lora<span class=w> </span><span class=se>\</span>
</span><span id=__span-37-2><a id=__codelineno-37-2 name=__codelineno-37-2 href=#__codelineno-37-2></a><span class=w>  </span>--lora-model-dir<span class=o>=</span><span class=s2>"/data/outputs/lora-out"</span>
</span></code></pre></div> <p>Merging takes 15-45 minutes depending on model size. For 8-70B, expect ~30 minutes.</p> <h2 id=stage-4-inference>Stage 4: Inference<a class=headerlink href=#stage-4-inference title="Permanent link">¶</a></h2> <p>Let's test your fine-tuned model!</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1 href=#__codelineno-38-1></a><span class=c1># GPU Configuration for inference (single GPU)</span>
</span><span id=__span-38-2><a id=__codelineno-38-2 name=__codelineno-38-2 href=#__codelineno-38-2></a><span class=n>INFERENCE_NUM_GPUS</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-38-3><a id=__codelineno-38-3 name=__codelineno-38-3 href=#__codelineno-38-3></a><span class=n>INFERENCE_GPU_CONFIG</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>GPU_TYPE</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>INFERENCE_NUM_GPUS</span><span class=si>}</span><span class=s2>"</span>
</span><span id=__span-38-4><a id=__codelineno-38-4 name=__codelineno-38-4 href=#__codelineno-38-4></a>
</span><span id=__span-38-5><a id=__codelineno-38-5 name=__codelineno-38-5 href=#__codelineno-38-5></a><span class=nd>@app</span><span class=o>.</span><span class=n>function</span><span class=p>(</span>
</span><span id=__span-38-6><a id=__codelineno-38-6 name=__codelineno-38-6 href=#__codelineno-38-6></a>    <span class=n>image</span><span class=o>=</span><span class=n>AXOLOTL_IMAGE</span><span class=p>,</span>
</span><span id=__span-38-7><a id=__codelineno-38-7 name=__codelineno-38-7 href=#__codelineno-38-7></a>    <span class=n>volumes</span><span class=o>=</span><span class=n>VOLUME_CONFIG</span><span class=p>,</span>
</span><span id=__span-38-8><a id=__codelineno-38-8 name=__codelineno-38-8 href=#__codelineno-38-8></a>    <span class=n>secrets</span><span class=o>=</span><span class=p>[</span><span class=n>huggingface_secret</span><span class=p>],</span>
</span><span id=__span-38-9><a id=__codelineno-38-9 name=__codelineno-38-9 href=#__codelineno-38-9></a>    <span class=n>timeout</span><span class=o>=</span><span class=mi>1</span> <span class=o>*</span> <span class=n>HOURS</span><span class=p>,</span>
</span><span id=__span-38-10><a id=__codelineno-38-10 name=__codelineno-38-10 href=#__codelineno-38-10></a>    <span class=n>gpu</span><span class=o>=</span><span class=n>INFERENCE_GPU_CONFIG</span><span class=p>,</span>
</span><span id=__span-38-11><a id=__codelineno-38-11 name=__codelineno-38-11 href=#__codelineno-38-11></a><span class=p>)</span>
</span><span id=__span-38-12><a id=__codelineno-38-12 name=__codelineno-38-12 href=#__codelineno-38-12></a><span class=k>def</span><span class=w> </span><span class=nf>run_inference</span><span class=p>(</span>
</span><span id=__span-38-13><a id=__codelineno-38-13 name=__codelineno-38-13 href=#__codelineno-38-13></a>    <span class=n>train_config_yaml</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>TRAIN_CONFIG_YAML</span><span class=p>,</span>
</span><span id=__span-38-14><a id=__codelineno-38-14 name=__codelineno-38-14 href=#__codelineno-38-14></a>    <span class=n>config_path</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"/data/config.yml"</span><span class=p>,</span>
</span><span id=__span-38-15><a id=__codelineno-38-15 name=__codelineno-38-15 href=#__codelineno-38-15></a>    <span class=n>prompt</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>"Hello, how are you?"</span><span class=p>,</span>
</span><span id=__span-38-16><a id=__codelineno-38-16 name=__codelineno-38-16 href=#__codelineno-38-16></a>    <span class=n>lora_model_dir</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-38-17><a id=__codelineno-38-17 name=__codelineno-38-17 href=#__codelineno-38-17></a>    <span class=n>base_model</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-38-18><a id=__codelineno-38-18 name=__codelineno-38-18 href=#__codelineno-38-18></a><span class=p>):</span>
</span><span id=__span-38-19><a id=__codelineno-38-19 name=__codelineno-38-19 href=#__codelineno-38-19></a><span class=w>    </span><span class=sd>"""Run inference using the trained model."""</span>
</span><span id=__span-38-20><a id=__codelineno-38-20 name=__codelineno-38-20 href=#__codelineno-38-20></a>    <span class=kn>import</span><span class=w> </span><span class=nn>os</span>
</span><span id=__span-38-21><a id=__codelineno-38-21 name=__codelineno-38-21 href=#__codelineno-38-21></a>    <span class=kn>import</span><span class=w> </span><span class=nn>subprocess</span>
</span><span id=__span-38-22><a id=__codelineno-38-22 name=__codelineno-38-22 href=#__codelineno-38-22></a>    <span class=kn>import</span><span class=w> </span><span class=nn>tempfile</span>
</span><span id=__span-38-23><a id=__codelineno-38-23 name=__codelineno-38-23 href=#__codelineno-38-23></a>
</span><span id=__span-38-24><a id=__codelineno-38-24 name=__codelineno-38-24 href=#__codelineno-38-24></a>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HF_TOKEN"</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>"HUGGINGFACE_TOKEN"</span><span class=p>]</span>
</span><span id=__span-38-25><a id=__codelineno-38-25 name=__codelineno-38-25 href=#__codelineno-38-25></a>
</span><span id=__span-38-26><a id=__codelineno-38-26 name=__codelineno-38-26 href=#__codelineno-38-26></a>    <span class=c1># Write config</span>
</span><span id=__span-38-27><a id=__codelineno-38-27 name=__codelineno-38-27 href=#__codelineno-38-27></a>    <span class=n>config_dict</span> <span class=o>=</span> <span class=n>write_config_to_volume</span><span class=p>(</span>
</span><span id=__span-38-28><a id=__codelineno-38-28 name=__codelineno-38-28 href=#__codelineno-38-28></a>        <span class=n>train_config_yaml</span><span class=o>=</span><span class=n>train_config_yaml</span><span class=p>,</span>
</span><span id=__span-38-29><a id=__codelineno-38-29 name=__codelineno-38-29 href=#__codelineno-38-29></a>        <span class=n>config_path</span><span class=o>=</span><span class=n>config_path</span><span class=p>,</span>
</span><span id=__span-38-30><a id=__codelineno-38-30 name=__codelineno-38-30 href=#__codelineno-38-30></a>        <span class=n>update_paths</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-38-31><a id=__codelineno-38-31 name=__codelineno-38-31 href=#__codelineno-38-31></a>    <span class=p>)</span>
</span><span id=__span-38-32><a id=__codelineno-38-32 name=__codelineno-38-32 href=#__codelineno-38-32></a>
</span><span id=__span-38-33><a id=__codelineno-38-33 name=__codelineno-38-33 href=#__codelineno-38-33></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>"Starting inference..."</span><span class=p>)</span>
</span><span id=__span-38-34><a id=__codelineno-38-34 name=__codelineno-38-34 href=#__codelineno-38-34></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Prompt: </span><span class=si>{</span><span class=n>prompt</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-38-35><a id=__codelineno-38-35 name=__codelineno-38-35 href=#__codelineno-38-35></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>"-"</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span><span id=__span-38-36><a id=__codelineno-38-36 name=__codelineno-38-36 href=#__codelineno-38-36></a>
</span><span id=__span-38-37><a id=__codelineno-38-37 name=__codelineno-38-37 href=#__codelineno-38-37></a>    <span class=c1># Build inference command</span>
</span><span id=__span-38-38><a id=__codelineno-38-38 name=__codelineno-38-38 href=#__codelineno-38-38></a>    <span class=n>cmd</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"axolotl"</span><span class=p>,</span> <span class=s2>"inference"</span><span class=p>,</span> <span class=n>config_path</span><span class=p>]</span>
</span><span id=__span-38-39><a id=__codelineno-38-39 name=__codelineno-38-39 href=#__codelineno-38-39></a>
</span><span id=__span-38-40><a id=__codelineno-38-40 name=__codelineno-38-40 href=#__codelineno-38-40></a>    <span class=k>if</span> <span class=n>lora_model_dir</span><span class=p>:</span>
</span><span id=__span-38-41><a id=__codelineno-38-41 name=__codelineno-38-41 href=#__codelineno-38-41></a>        <span class=n>cmd</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=s2>"--lora-model-dir"</span><span class=p>,</span> <span class=n>lora_model_dir</span><span class=p>])</span>
</span><span id=__span-38-42><a id=__codelineno-38-42 name=__codelineno-38-42 href=#__codelineno-38-42></a>    <span class=k>if</span> <span class=n>base_model</span><span class=p>:</span>
</span><span id=__span-38-43><a id=__codelineno-38-43 name=__codelineno-38-43 href=#__codelineno-38-43></a>        <span class=n>cmd</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=s2>"--base-model"</span><span class=p>,</span> <span class=n>base_model</span><span class=p>])</span>
</span><span id=__span-38-44><a id=__codelineno-38-44 name=__codelineno-38-44 href=#__codelineno-38-44></a>
</span><span id=__span-38-45><a id=__codelineno-38-45 name=__codelineno-38-45 href=#__codelineno-38-45></a>    <span class=c1># Write prompt to temp file and pipe it</span>
</span><span id=__span-38-46><a id=__codelineno-38-46 name=__codelineno-38-46 href=#__codelineno-38-46></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-38-47><a id=__codelineno-38-47 name=__codelineno-38-47 href=#__codelineno-38-47></a>        <span class=k>with</span> <span class=n>tempfile</span><span class=o>.</span><span class=n>NamedTemporaryFile</span><span class=p>(</span><span class=n>mode</span><span class=o>=</span><span class=s2>"w"</span><span class=p>,</span> <span class=n>delete</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>suffix</span><span class=o>=</span><span class=s2>".txt"</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-38-48><a id=__codelineno-38-48 name=__codelineno-38-48 href=#__codelineno-38-48></a>            <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span>
</span><span id=__span-38-49><a id=__codelineno-38-49 name=__codelineno-38-49 href=#__codelineno-38-49></a>            <span class=n>prompt_file</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>name</span>
</span><span id=__span-38-50><a id=__codelineno-38-50 name=__codelineno-38-50 href=#__codelineno-38-50></a>
</span><span id=__span-38-51><a id=__codelineno-38-51 name=__codelineno-38-51 href=#__codelineno-38-51></a>        <span class=c1># Run inference with prompt from file</span>
</span><span id=__span-38-52><a id=__codelineno-38-52 name=__codelineno-38-52 href=#__codelineno-38-52></a>        <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>prompt_file</span><span class=p>,</span> <span class=s2>"r"</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-38-53><a id=__codelineno-38-53 name=__codelineno-38-53 href=#__codelineno-38-53></a>            <span class=n>result</span> <span class=o>=</span> <span class=n>subprocess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span>
</span><span id=__span-38-54><a id=__codelineno-38-54 name=__codelineno-38-54 href=#__codelineno-38-54></a>                <span class=n>cmd</span><span class=p>,</span>
</span><span id=__span-38-55><a id=__codelineno-38-55 name=__codelineno-38-55 href=#__codelineno-38-55></a>                <span class=n>stdin</span><span class=o>=</span><span class=n>f</span><span class=p>,</span>
</span><span id=__span-38-56><a id=__codelineno-38-56 name=__codelineno-38-56 href=#__codelineno-38-56></a>                <span class=n>capture_output</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-38-57><a id=__codelineno-38-57 name=__codelineno-38-57 href=#__codelineno-38-57></a>                <span class=n>text</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-38-58><a id=__codelineno-38-58 name=__codelineno-38-58 href=#__codelineno-38-58></a>                <span class=n>check</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span><span id=__span-38-59><a id=__codelineno-38-59 name=__codelineno-38-59 href=#__codelineno-38-59></a>            <span class=p>)</span>
</span><span id=__span-38-60><a id=__codelineno-38-60 name=__codelineno-38-60 href=#__codelineno-38-60></a>
</span><span id=__span-38-61><a id=__codelineno-38-61 name=__codelineno-38-61 href=#__codelineno-38-61></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"✓ Inference completed"</span><span class=p>)</span>
</span><span id=__span-38-62><a id=__codelineno-38-62 name=__codelineno-38-62 href=#__codelineno-38-62></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"</span><span class=se>\n</span><span class=s2>"</span> <span class=o>+</span> <span class=s2>"="</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span><span id=__span-38-63><a id=__codelineno-38-63 name=__codelineno-38-63 href=#__codelineno-38-63></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"MODEL OUTPUT:"</span><span class=p>)</span>
</span><span id=__span-38-64><a id=__codelineno-38-64 name=__codelineno-38-64 href=#__codelineno-38-64></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"="</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span><span id=__span-38-65><a id=__codelineno-38-65 name=__codelineno-38-65 href=#__codelineno-38-65></a>        <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>stdout</span><span class=p>)</span>
</span><span id=__span-38-66><a id=__codelineno-38-66 name=__codelineno-38-66 href=#__codelineno-38-66></a>        <span class=nb>print</span><span class=p>(</span><span class=s2>"="</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span><span id=__span-38-67><a id=__codelineno-38-67 name=__codelineno-38-67 href=#__codelineno-38-67></a>
</span><span id=__span-38-68><a id=__codelineno-38-68 name=__codelineno-38-68 href=#__codelineno-38-68></a>        <span class=k>if</span> <span class=n>result</span><span class=o>.</span><span class=n>stderr</span><span class=p>:</span>
</span><span id=__span-38-69><a id=__codelineno-38-69 name=__codelineno-38-69 href=#__codelineno-38-69></a>            <span class=nb>print</span><span class=p>(</span><span class=s2>"</span><span class=se>\n</span><span class=s2>STDERR:"</span><span class=p>)</span>
</span><span id=__span-38-70><a id=__codelineno-38-70 name=__codelineno-38-70 href=#__codelineno-38-70></a>            <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>stderr</span><span class=p>)</span>
</span><span id=__span-38-71><a id=__codelineno-38-71 name=__codelineno-38-71 href=#__codelineno-38-71></a>
</span><span id=__span-38-72><a id=__codelineno-38-72 name=__codelineno-38-72 href=#__codelineno-38-72></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-38-73><a id=__codelineno-38-73 name=__codelineno-38-73 href=#__codelineno-38-73></a>            <span class=s2>"status"</span><span class=p>:</span> <span class=s2>"completed"</span><span class=p>,</span>
</span><span id=__span-38-74><a id=__codelineno-38-74 name=__codelineno-38-74 href=#__codelineno-38-74></a>            <span class=s2>"prompt"</span><span class=p>:</span> <span class=n>prompt</span><span class=p>,</span>
</span><span id=__span-38-75><a id=__codelineno-38-75 name=__codelineno-38-75 href=#__codelineno-38-75></a>            <span class=s2>"output"</span><span class=p>:</span> <span class=n>result</span><span class=o>.</span><span class=n>stdout</span><span class=p>,</span>
</span><span id=__span-38-76><a id=__codelineno-38-76 name=__codelineno-38-76 href=#__codelineno-38-76></a>            <span class=s2>"model"</span><span class=p>:</span> <span class=n>base_model</span> <span class=ow>or</span> <span class=n>config_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"base_model"</span><span class=p>),</span>
</span><span id=__span-38-77><a id=__codelineno-38-77 name=__codelineno-38-77 href=#__codelineno-38-77></a>        <span class=p>}</span>
</span><span id=__span-38-78><a id=__codelineno-38-78 name=__codelineno-38-78 href=#__codelineno-38-78></a>
</span><span id=__span-38-79><a id=__codelineno-38-79 name=__codelineno-38-79 href=#__codelineno-38-79></a>    <span class=k>except</span> <span class=n>subprocess</span><span class=o>.</span><span class=n>CalledProcessError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span><span id=__span-38-80><a id=__codelineno-38-80 name=__codelineno-38-80 href=#__codelineno-38-80></a>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Error output: </span><span class=si>{</span><span class=n>e</span><span class=o>.</span><span class=n>stderr</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-38-81><a id=__codelineno-38-81 name=__codelineno-38-81 href=#__codelineno-38-81></a>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Command output: </span><span class=si>{</span><span class=n>e</span><span class=o>.</span><span class=n>stdout</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-38-82><a id=__codelineno-38-82 name=__codelineno-38-82 href=#__codelineno-38-82></a>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Inference failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-38-83><a id=__codelineno-38-83 name=__codelineno-38-83 href=#__codelineno-38-83></a>    <span class=k>finally</span><span class=p>:</span>
</span><span id=__span-38-84><a id=__codelineno-38-84 name=__codelineno-38-84 href=#__codelineno-38-84></a>        <span class=c1># Clean up temp file</span>
</span><span id=__span-38-85><a id=__codelineno-38-85 name=__codelineno-38-85 href=#__codelineno-38-85></a>        <span class=kn>import</span><span class=w> </span><span class=nn>os</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>os_module</span>
</span><span id=__span-38-86><a id=__codelineno-38-86 name=__codelineno-38-86 href=#__codelineno-38-86></a>        <span class=k>if</span> <span class=s2>"prompt_file"</span> <span class=ow>in</span> <span class=nb>locals</span><span class=p>():</span>
</span><span id=__span-38-87><a id=__codelineno-38-87 name=__codelineno-38-87 href=#__codelineno-38-87></a>            <span class=n>os_module</span><span class=o>.</span><span class=n>unlink</span><span class=p>(</span><span class=n>prompt_file</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Run inference:</strong></p> <p><strong>With LoRA adapters (before merging):</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1 href=#__codelineno-39-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::run_inference<span class=w> </span><span class=se>\</span>
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2 href=#__codelineno-39-2></a><span class=w>  </span>--prompt<span class=o>=</span><span class=s2>"Explain quantum computing in simple terms."</span><span class=w> </span><span class=se>\</span>
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3 href=#__codelineno-39-3></a><span class=w>  </span>--lora-model-dir<span class=o>=</span><span class=s2>"/data/outputs/lora-out"</span>
</span></code></pre></div> <p><strong>With merged model:</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1 href=#__codelineno-40-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::run_inference<span class=w> </span><span class=se>\</span>
</span><span id=__span-40-2><a id=__codelineno-40-2 name=__codelineno-40-2 href=#__codelineno-40-2></a><span class=w>  </span>--prompt<span class=o>=</span><span class=s2>"Write a poem about machine learning."</span><span class=w> </span><span class=se>\</span>
</span><span id=__span-40-3><a id=__codelineno-40-3 name=__codelineno-40-3 href=#__codelineno-40-3></a><span class=w>  </span>--base-model<span class=o>=</span><span class=s2>"/data/outputs/lora-out-merged"</span>
</span></code></pre></div> <p><strong>Test multiple prompts:</strong></p> <p>Create a Python script to batch test:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1 href=#__codelineno-41-1></a><span class=kn>import</span><span class=w> </span><span class=nn>modal</span>
</span><span id=__span-41-2><a id=__codelineno-41-2 name=__codelineno-41-2 href=#__codelineno-41-2></a>
</span><span id=__span-41-3><a id=__codelineno-41-3 name=__codelineno-41-3 href=#__codelineno-41-3></a><span class=n>app</span> <span class=o>=</span> <span class=n>modal</span><span class=o>.</span><span class=n>App</span><span class=o>.</span><span class=n>lookup</span><span class=p>(</span><span class=s2>"Finetuned_Llama_70b_Axolotl_MultiGPU"</span><span class=p>)</span>
</span><span id=__span-41-4><a id=__codelineno-41-4 name=__codelineno-41-4 href=#__codelineno-41-4></a><span class=n>run_inference</span> <span class=o>=</span> <span class=n>modal</span><span class=o>.</span><span class=n>Function</span><span class=o>.</span><span class=n>lookup</span><span class=p>(</span><span class=n>app</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=s2>"run_inference"</span><span class=p>)</span>
</span><span id=__span-41-5><a id=__codelineno-41-5 name=__codelineno-41-5 href=#__codelineno-41-5></a>
</span><span id=__span-41-6><a id=__codelineno-41-6 name=__codelineno-41-6 href=#__codelineno-41-6></a><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-41-7><a id=__codelineno-41-7 name=__codelineno-41-7 href=#__codelineno-41-7></a>    <span class=s2>"Explain gradient descent."</span><span class=p>,</span>
</span><span id=__span-41-8><a id=__codelineno-41-8 name=__codelineno-41-8 href=#__codelineno-41-8></a>    <span class=s2>"Write Python code to implement binary search."</span><span class=p>,</span>
</span><span id=__span-41-9><a id=__codelineno-41-9 name=__codelineno-41-9 href=#__codelineno-41-9></a>    <span class=s2>"What are the benefits of transformer architecture?"</span><span class=p>,</span>
</span><span id=__span-41-10><a id=__codelineno-41-10 name=__codelineno-41-10 href=#__codelineno-41-10></a><span class=p>]</span>
</span><span id=__span-41-11><a id=__codelineno-41-11 name=__codelineno-41-11 href=#__codelineno-41-11></a>
</span><span id=__span-41-12><a id=__codelineno-41-12 name=__codelineno-41-12 href=#__codelineno-41-12></a><span class=k>for</span> <span class=n>prompt</span> <span class=ow>in</span> <span class=n>prompts</span><span class=p>:</span>
</span><span id=__span-41-13><a id=__codelineno-41-13 name=__codelineno-41-13 href=#__codelineno-41-13></a>    <span class=n>result</span> <span class=o>=</span> <span class=n>run_inference</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>)</span>
</span><span id=__span-41-14><a id=__codelineno-41-14 name=__codelineno-41-14 href=#__codelineno-41-14></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"</span><span class=se>\n</span><span class=s2>Prompt: </span><span class=si>{</span><span class=n>prompt</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-41-15><a id=__codelineno-41-15 name=__codelineno-41-15 href=#__codelineno-41-15></a>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Output: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>'output'</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-41-16><a id=__codelineno-41-16 name=__codelineno-41-16 href=#__codelineno-41-16></a>    <span class=nb>print</span><span class=p>(</span><span class=s2>"-"</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></code></pre></div> <h2 id=complete-workflow-example>Complete Workflow Example<a class=headerlink href=#complete-workflow-example title="Permanent link">¶</a></h2> <p>Let me walk you through how I actually use this for a real project.</p> <h3 id=1-customize-configuration>1. Customize Configuration<a class=headerlink href=#1-customize-configuration title="Permanent link">¶</a></h3> <p>First, I edit the YAML config in the script:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1 href=#__codelineno-42-1></a><span class=n>TRAIN_CONFIG_YAML</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"""</span>
</span><span id=__span-42-2><a id=__codelineno-42-2 name=__codelineno-42-2 href=#__codelineno-42-2></a><span class=s2>base_model: meta-llama/Meta-Llama-3-8-70B-Instruct</span>
</span><span id=__span-42-3><a id=__codelineno-42-3 name=__codelineno-42-3 href=#__codelineno-42-3></a><span class=s2>model_type: LlamaForCausalLM</span>
</span><span id=__span-42-4><a id=__codelineno-42-4 name=__codelineno-42-4 href=#__codelineno-42-4></a><span class=s2>tokenizer_type: AutoTokenizer</span>
</span><span id=__span-42-5><a id=__codelineno-42-5 name=__codelineno-42-5 href=#__codelineno-42-5></a>
</span><span id=__span-42-6><a id=__codelineno-42-6 name=__codelineno-42-6 href=#__codelineno-42-6></a><span class=s2>load_in_8bit: true</span>
</span><span id=__span-42-7><a id=__codelineno-42-7 name=__codelineno-42-7 href=#__codelineno-42-7></a>
</span><span id=__span-42-8><a id=__codelineno-42-8 name=__codelineno-42-8 href=#__codelineno-42-8></a><span class=s2>chat_template: llama3</span>
</span><span id=__span-42-9><a id=__codelineno-42-9 name=__codelineno-42-9 href=#__codelineno-42-9></a><span class=s2>datasets:</span>
</span><span id=__span-42-10><a id=__codelineno-42-10 name=__codelineno-42-10 href=#__codelineno-42-10></a><span class=s2>  - path: my-username/my-custom-dataset</span>
</span><span id=__span-42-11><a id=__codelineno-42-11 name=__codelineno-42-11 href=#__codelineno-42-11></a><span class=s2>    type: chat_template</span>
</span><span id=__span-42-12><a id=__codelineno-42-12 name=__codelineno-42-12 href=#__codelineno-42-12></a>
</span><span id=__span-42-13><a id=__codelineno-42-13 name=__codelineno-42-13 href=#__codelineno-42-13></a><span class=s2>dataset_prepared_path: /data/prepared_datasets/my_data</span>
</span><span id=__span-42-14><a id=__codelineno-42-14 name=__codelineno-42-14 href=#__codelineno-42-14></a><span class=s2>output_dir: /data/outputs/llama70b-custom</span>
</span><span id=__span-42-15><a id=__codelineno-42-15 name=__codelineno-42-15 href=#__codelineno-42-15></a>
</span><span id=__span-42-16><a id=__codelineno-42-16 name=__codelineno-42-16 href=#__codelineno-42-16></a><span class=s2>sequence_len: 8192  # Longer context</span>
</span><span id=__span-42-17><a id=__codelineno-42-17 name=__codelineno-42-17 href=#__codelineno-42-17></a><span class=s2>lora_r: 64</span>
</span><span id=__span-42-18><a id=__codelineno-42-18 name=__codelineno-42-18 href=#__codelineno-42-18></a><span class=s2>lora_alpha: 128</span>
</span><span id=__span-42-19><a id=__codelineno-42-19 name=__codelineno-42-19 href=#__codelineno-42-19></a>
</span><span id=__span-42-20><a id=__codelineno-42-20 name=__codelineno-42-20 href=#__codelineno-42-20></a><span class=s2>gradient_accumulation_steps: 2</span>
</span><span id=__span-42-21><a id=__codelineno-42-21 name=__codelineno-42-21 href=#__codelineno-42-21></a><span class=s2>micro_batch_size: 4</span>
</span><span id=__span-42-22><a id=__codelineno-42-22 name=__codelineno-42-22 href=#__codelineno-42-22></a><span class=s2>num_epochs: 3</span>
</span><span id=__span-42-23><a id=__codelineno-42-23 name=__codelineno-42-23 href=#__codelineno-42-23></a><span class=s2>learning_rate: 0.0001</span>
</span><span id=__span-42-24><a id=__codelineno-42-24 name=__codelineno-42-24 href=#__codelineno-42-24></a>
</span><span id=__span-42-25><a id=__codelineno-42-25 name=__codelineno-42-25 href=#__codelineno-42-25></a><span class=s2>wandb_project: </span><span class=si>{</span><span class=n>WANDB_PROJECT_DEFAULT</span><span class=si>}</span>
</span><span id=__span-42-26><a id=__codelineno-42-26 name=__codelineno-42-26 href=#__codelineno-42-26></a>
</span><span id=__span-42-27><a id=__codelineno-42-27 name=__codelineno-42-27 href=#__codelineno-42-27></a><span class=s2># Everything else stays the same</span>
</span><span id=__span-42-28><a id=__codelineno-42-28 name=__codelineno-42-28 href=#__codelineno-42-28></a><span class=s2>gradient_checkpointing: true</span>
</span><span id=__span-42-29><a id=__codelineno-42-29 name=__codelineno-42-29 href=#__codelineno-42-29></a><span class=s2>flash_attention: true</span>
</span><span id=__span-42-30><a id=__codelineno-42-30 name=__codelineno-42-30 href=#__codelineno-42-30></a><span class=s2>bf16: auto</span>
</span><span id=__span-42-31><a id=__codelineno-42-31 name=__codelineno-42-31 href=#__codelineno-42-31></a><span class=s2>"""</span>
</span></code></pre></div> <h3 id=2-preprocess-dataset>2. Preprocess Dataset<a class=headerlink href=#2-preprocess-dataset title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1 href=#__codelineno-43-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::process_datasets
</span></code></pre></div> <p><strong>Expected time:</strong> 30 min - 2 hours (depends on dataset size) <strong>Cost:</strong> ~$2-5 (1 GPU for preprocessing)</p> <p>Grab a coffee while this runs.</p> <h3 id=3-test-training-small-sanity-check>3. Test Training (Small Sanity Check)<a class=headerlink href=#3-test-training-small-sanity-check title="Permanent link">¶</a></h3> <p>Before burning $100 on a full training run, I test with 1 epoch:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1 href=#__codelineno-44-1></a><span class=c1># Temporarily edit YAML:</span>
</span><span id=__span-44-2><a id=__codelineno-44-2 name=__codelineno-44-2 href=#__codelineno-44-2></a><span class=n>num_epochs</span><span class=p>:</span> <span class=mi>1</span>
</span></code></pre></div> <p>Then:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1 href=#__codelineno-45-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::train_model
</span></code></pre></div> <p><strong>Expected time:</strong> 30-60 minutes <strong>Cost:</strong> ~$7-14</p> <p>This catches configuration errors, memory issues, etc. If this works, the full run will work.</p> <h3 id=4-full-training>4. Full Training<a class=headerlink href=#4-full-training title="Permanent link">¶</a></h3> <p>Restore the full config and run:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1 href=#__codelineno-46-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::train_model
</span></code></pre></div> <p><strong>Expected time:</strong> 3-10 hours (depends on dataset size) <strong>Cost:</strong> $40-150</p> <p>Now you wait. Monitor W&amp;B to make sure loss is going down. Check Modal dashboard to verify all GPUs are utilized.</p> <p>Go touch grass. This is running on Modal's infrastructure, not your machine.</p> <h3 id=5-merge-lora>5. Merge LoRA<a class=headerlink href=#5-merge-lora title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1 href=#__codelineno-47-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::merge_lora
</span></code></pre></div> <p><strong>Expected time:</strong> 30-45 minutes <strong>Cost:</strong> ~$2-3</p> <h3 id=6-test-inference>6. Test Inference<a class=headerlink href=#6-test-inference title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1 href=#__codelineno-48-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::run_inference<span class=w> </span><span class=se>\</span>
</span><span id=__span-48-2><a id=__codelineno-48-2 name=__codelineno-48-2 href=#__codelineno-48-2></a><span class=w>  </span>--prompt<span class=o>=</span><span class=s2>"Test my fine-tuned 8-70B model with this prompt."</span>
</span></code></pre></div> <p><strong>Expected time:</strong> 1-2 minutes <strong>Cost:</strong> ~$0.10</p> <h3 id=7-push-to-hub-optional>7. Push to Hub (Optional)<a class=headerlink href=#7-push-to-hub-optional title="Permanent link">¶</a></h3> <p>Want to share your model? Add this to the YAML config:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1 href=#__codelineno-49-1></a><span class=nt>hub_model_id</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">your-username/llama-70b-custom-finetuned</span>
</span></code></pre></div> <p>Axolotl automatically pushes to HuggingFace Hub during training.</p> <p><strong>Total cost for full pipeline:</strong> $50-200 depending on dataset size and number of epochs.</p> <p><strong>Total time:</strong> 1 day (mostly waiting for training)</p> <p>Compare this to managing your own 4× A100-80GB cluster... yeah, Modal wins.</p> <h2 id=advanced-tips-and-tricks>Advanced Tips and Tricks<a class=headerlink href=#advanced-tips-and-tricks title="Permanent link">¶</a></h2> <h3 id=multi-dataset-training>Multi-Dataset Training<a class=headerlink href=#multi-dataset-training title="Permanent link">¶</a></h3> <p>Train on multiple datasets simultaneously:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1 href=#__codelineno-50-1></a><span class=nt>datasets</span><span class=p>:</span>
</span><span id=__span-50-2><a id=__codelineno-50-2 name=__codelineno-50-2 href=#__codelineno-50-2></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dataset1/name</span>
</span><span id=__span-50-3><a id=__codelineno-50-3 name=__codelineno-50-3 href=#__codelineno-50-3></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">chat_template</span>
</span><span id=__span-50-4><a id=__codelineno-50-4 name=__codelineno-50-4 href=#__codelineno-50-4></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dataset2/name</span>
</span><span id=__span-50-5><a id=__codelineno-50-5 name=__codelineno-50-5 href=#__codelineno-50-5></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">alpaca</span>
</span><span id=__span-50-6><a id=__codelineno-50-6 name=__codelineno-50-6 href=#__codelineno-50-6></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dataset3/name</span>
</span><span id=__span-50-7><a id=__codelineno-50-7 name=__codelineno-50-7 href=#__codelineno-50-7></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">sharegpt</span>
</span></code></pre></div> <p>Axolotl combines them automatically.</p> <h3 id=checkpoint-management>Checkpoint Management<a class=headerlink href=#checkpoint-management title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1 href=#__codelineno-51-1></a><span class=nt>saves_per_epoch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class=w>              </span><span class=c1># Save 4 times per epoch</span>
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2 href=#__codelineno-51-2></a><span class=nt>save_total_limit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">10</span><span class=w>            </span><span class=c1># Keep only 10 most recent checkpoints</span>
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3 href=#__codelineno-51-3></a><span class=nt>resume_from_checkpoint</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">/data/outputs/lora-out/checkpoint-1000</span>
</span></code></pre></div> <p><strong>Pro tip:</strong> If training crashes or you want to tweak learning rate halfway through, just resume from a checkpoint.</p> <h3 id=custom-evaluation>Custom Evaluation<a class=headerlink href=#custom-evaluation title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1 href=#__codelineno-52-1></a><span class=nt>val_set_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class=w>           </span><span class=c1># 10% for validation</span>
</span><span id=__span-52-2><a id=__codelineno-52-2 name=__codelineno-52-2 href=#__codelineno-52-2></a><span class=nt>evals_per_epoch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">10</span><span class=w>         </span><span class=c1># Evaluate 10 times per epoch</span>
</span></code></pre></div> <p>Watch validation loss in W&amp;B to catch overfitting.</p> <h3 id=optimizer-tuning>Optimizer Tuning<a class=headerlink href=#optimizer-tuning title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1 href=#__codelineno-53-1></a><span class=nt>optimizer</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">adamw_bnb_8bit</span><span class=w>      </span><span class=c1># Options: adamw_torch, adamw_bnb_8bit, adafactor</span>
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2 href=#__codelineno-53-2></a><span class=nt>lr_scheduler</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">cosine</span><span class=w>           </span><span class=c1># Options: linear, cosine, constant</span>
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3 href=#__codelineno-53-3></a><span class=nt>warmup_ratio</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
</span><span id=__span-53-4><a id=__codelineno-53-4 name=__codelineno-53-4 href=#__codelineno-53-4></a><span class=nt>weight_decay</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.01</span>
</span><span id=__span-53-5><a id=__codelineno-53-5 name=__codelineno-53-5 href=#__codelineno-53-5></a><span class=nt>max_grad_norm</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</span></code></pre></div> <p>I usually stick with <code>adamw_bnb_8bit</code> (saves memory) and <code>cosine</code> scheduler (smooth learning rate decay).</p> <h2 id=hyperparameter-tuning-guide>Hyperparameter Tuning Guide<a class=headerlink href=#hyperparameter-tuning-guide title="Permanent link">¶</a></h2> <h3 id=for-llama-3-8b-single-gpu>For Llama 3-8B (Single GPU)<a class=headerlink href=#for-llama-3-8b-single-gpu title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1 href=#__codelineno-54-1></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
</span><span id=__span-54-2><a id=__codelineno-54-2 name=__codelineno-54-2 href=#__codelineno-54-2></a><span class=nt>gradient_accumulation_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</span><span id=__span-54-3><a id=__codelineno-54-3 name=__codelineno-54-3 href=#__codelineno-54-3></a><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.0003</span>
</span><span id=__span-54-4><a id=__codelineno-54-4 name=__codelineno-54-4 href=#__codelineno-54-4></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
</span><span id=__span-54-5><a id=__codelineno-54-5 name=__codelineno-54-5 href=#__codelineno-54-5></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
</span><span id=__span-54-6><a id=__codelineno-54-6 name=__codelineno-54-6 href=#__codelineno-54-6></a><span class=nt>num_epochs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
</span><span id=__span-54-7><a id=__codelineno-54-7 name=__codelineno-54-7 href=#__codelineno-54-7></a><span class=nt>load_in_8bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class=w>  </span><span class=c1># Can use full precision</span>
</span></code></pre></div> <p><strong>GPU:</strong> 1× A100-40GB <strong>Cost:</strong> ~$2.50/hr <strong>Training time:</strong> 2-4 hours</p> <h3 id=for-llama-3-8-70b-multi-gpu>For Llama 3-8-70B (Multi-GPU)<a class=headerlink href=#for-llama-3-8-70b-multi-gpu title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-55-1><a id=__codelineno-55-1 name=__codelineno-55-1 href=#__codelineno-55-1></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span id=__span-55-2><a id=__codelineno-55-2 name=__codelineno-55-2 href=#__codelineno-55-2></a><span class=nt>gradient_accumulation_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span id=__span-55-3><a id=__codelineno-55-3 name=__codelineno-55-3 href=#__codelineno-55-3></a><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.0001</span>
</span><span id=__span-55-4><a id=__codelineno-55-4 name=__codelineno-55-4 href=#__codelineno-55-4></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
</span><span id=__span-55-5><a id=__codelineno-55-5 name=__codelineno-55-5 href=#__codelineno-55-5></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
</span><span id=__span-55-6><a id=__codelineno-55-6 name=__codelineno-55-6 href=#__codelineno-55-6></a><span class=nt>num_epochs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2-3</span>
</span><span id=__span-55-7><a id=__codelineno-55-7 name=__codelineno-55-7 href=#__codelineno-55-7></a><span class=nt>load_in_8bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>  </span><span class=c1># Essential</span>
</span></code></pre></div> <p><strong>GPU:</strong> 4× A100-80GB <strong>Cost:</strong> ~$14/hr <strong>Training time:</strong> 4-10 hours</p> <h3 id=for-maximum-speed>For Maximum Speed<a class=headerlink href=#for-maximum-speed title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-56-1><a id=__codelineno-56-1 name=__codelineno-56-1 href=#__codelineno-56-1></a><span class=nt>flash_attention</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-56-2><a id=__codelineno-56-2 name=__codelineno-56-2 href=#__codelineno-56-2></a><span class=nt>bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
</span><span id=__span-56-3><a id=__codelineno-56-3 name=__codelineno-56-3 href=#__codelineno-56-3></a><span class=nt>gradient_checkpointing</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-56-4><a id=__codelineno-56-4 name=__codelineno-56-4 href=#__codelineno-56-4></a><span class=nt>sample_packing</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>              </span><span class=c1># Pack multiple samples per sequence</span>
</span><span id=__span-56-5><a id=__codelineno-56-5 name=__codelineno-56-5 href=#__codelineno-56-5></a><span class=nt>pad_to_sequence_len</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-56-6><a id=__codelineno-56-6 name=__codelineno-56-6 href=#__codelineno-56-6></a><span class=nt>optimizer</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">adamw_bnb_8bit</span>
</span></code></pre></div> <h3 id=for-maximum-quality>For Maximum Quality<a class=headerlink href=#for-maximum-quality title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.00005</span><span class=w>            </span><span class=c1># Lower LR = more stable</span>
</span><span id=__span-57-2><a id=__codelineno-57-2 name=__codelineno-57-2 href=#__codelineno-57-2></a><span class=nt>num_epochs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class=w>                     </span><span class=c1># More epochs</span>
</span><span id=__span-57-3><a id=__codelineno-57-3 name=__codelineno-57-3 href=#__codelineno-57-3></a><span class=nt>warmup_ratio</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span><span class=w>                 </span><span class=c1># Longer warmup</span>
</span><span id=__span-57-4><a id=__codelineno-57-4 name=__codelineno-57-4 href=#__codelineno-57-4></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">128</span><span class=w>                       </span><span class=c1># Higher capacity</span>
</span><span id=__span-57-5><a id=__codelineno-57-5 name=__codelineno-57-5 href=#__codelineno-57-5></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
</span><span id=__span-57-6><a id=__codelineno-57-6 name=__codelineno-57-6 href=#__codelineno-57-6></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class=w>               </span><span class=c1># Smaller batches if you have memory</span>
</span></code></pre></div> <p>Quality vs. Speed is always a tradeoff. Experiment!</p> <h2 id=common-issues-and-solutions>Common Issues and Solutions<a class=headerlink href=#common-issues-and-solutions title="Permanent link">¶</a></h2> <h3 id=out-of-memory-during-training>"Out of Memory" During Training<a class=headerlink href=#out-of-memory-during-training title="Permanent link">¶</a></h3> <p><strong>Error:</strong> <code>CUDA out of memory</code></p> <p><strong>Solutions (in order of preference):</strong></p> <ol> <li> <p><strong>Reduce batch size:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</span><span id=__span-58-2><a id=__codelineno-58-2 name=__codelineno-58-2 href=#__codelineno-58-2></a><span class=nt>gradient_accumulation_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class=w>  </span><span class=c1># Keep effective batch size the same</span>
</span></code></pre></div> </li> <li> <p><strong>Enable gradient checkpointing</strong> (if not already): </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a><span class=nt>gradient_checkpointing</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span></code></pre></div> </li> <li> <p><strong>Use quantization:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=nt>load_in_8bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span></code></pre></div> </li> <li> <p><strong>Add more GPUs:</strong> </p><div class="language-python highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a><span class=n>TRAIN_NUM_GPUS</span> <span class=o>=</span> <span class=mi>8</span>
</span></code></pre></div> </li> <li> <p><strong>Reduce sequence length:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=nt>sequence_len</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2048</span>
</span></code></pre></div> </li> </ol> <h3 id=training-loss-not-decreasing>Training Loss Not Decreasing<a class=headerlink href=#training-loss-not-decreasing title="Permanent link">¶</a></h3> <p><strong>Symptoms:</strong> Loss stays flat or increases</p> <p><strong>Solutions:</strong></p> <ol> <li> <p><strong>Check your learning rate</strong> - might be too low: </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.0003</span>
</span></code></pre></div> </li> <li> <p><strong>Verify data quality:</strong></p> </li> <li>Load a few samples from preprocessed data</li> <li> <p>Make sure they look correct</p> </li> <li> <p><strong>Increase LoRA rank:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
</span><span id=__span-64-2><a id=__codelineno-64-2 name=__codelineno-64-2 href=#__codelineno-64-2></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
</span></code></pre></div> </li> <li> <p><strong>Train longer:</strong> </p><div class="language-yaml highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a><span class=nt>num_epochs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</span></code></pre></div> </li> <li> <p><strong>Check for data leakage</strong> - validation set might be in training set</p> </li> </ol> <h3 id=preprocessing-hangs-or-fails>Preprocessing Hangs or Fails<a class=headerlink href=#preprocessing-hangs-or-fails title="Permanent link">¶</a></h3> <p><strong>Solution:</strong></p> <p>Test dataset access locally first:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a><span class=kn>from</span><span class=w> </span><span class=nn>datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_dataset</span>
</span><span id=__span-66-2><a id=__codelineno-66-2 name=__codelineno-66-2 href=#__codelineno-66-2></a><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>"your/dataset"</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>"train"</span><span class=p>)</span>
</span><span id=__span-66-3><a id=__codelineno-66-3 name=__codelineno-66-3 href=#__codelineno-66-3></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Loaded </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span><span class=si>}</span><span class=s2> samples"</span><span class=p>)</span>
</span><span id=__span-66-4><a id=__codelineno-66-4 name=__codelineno-66-4 href=#__codelineno-66-4></a><span class=nb>print</span><span class=p>(</span><span class=n>dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></code></pre></div> <p>If that works, ensure your HF token is in Modal secrets.</p> <h3 id=multi-gpu-not-working>Multi-GPU Not Working<a class=headerlink href=#multi-gpu-not-working title="Permanent link">¶</a></h3> <p><strong>Symptoms:</strong> Training only uses 1 GPU</p> <p><strong>Debug steps:</strong></p> <ol> <li> <p><strong>Verify GPU count:</strong> </p><div class="language-python highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a><span class=n>TRAIN_NUM_GPUS</span> <span class=o>=</span> <span class=mi>4</span>  <span class=c1># Check this!</span>
</span></code></pre></div> </li> <li> <p><strong>Check GPU utilization in Modal dashboard</strong> - should see all 4 GPUs active</p> </li> <li> <p><strong>Add debug prints:</strong> </p><div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-68-2><a id=__codelineno-68-2 name=__codelineno-68-2 href=#__codelineno-68-2></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"GPUs available: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>device_count</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span></code></pre></div> </li> </ol> <h3 id=secret-not-found>"Secret not found"<a class=headerlink href=#secret-not-found title="Permanent link">¶</a></h3> <p><strong>Error:</strong> <code>Modal Secret "secrets-hf-wandb" not found</code></p> <p><strong>Solution:</strong></p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a>modal<span class=w> </span>secret<span class=w> </span>create<span class=w> </span>secrets-hf-wandb<span class=w> </span><span class=se>\</span>
</span><span id=__span-69-2><a id=__codelineno-69-2 name=__codelineno-69-2 href=#__codelineno-69-2></a><span class=w>  </span><span class=nv>HUGGINGFACE_TOKEN</span><span class=o>=</span>hf_xxx<span class=w> </span><span class=se>\</span>
</span><span id=__span-69-3><a id=__codelineno-69-3 name=__codelineno-69-3 href=#__codelineno-69-3></a><span class=w>  </span><span class=nv>WANDB_API_KEY</span><span class=o>=</span>xxx
</span></code></pre></div> <p>Or update the script to use your secret name:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=n>huggingface_secret</span> <span class=o>=</span> <span class=n>Secret</span><span class=o>.</span><span class=n>from_name</span><span class=p>(</span><span class=s2>"my-secret-name"</span><span class=p>)</span>
</span></code></pre></div> <h2 id=cost-optimization-strategies>Cost Optimization Strategies<a class=headerlink href=#cost-optimization-strategies title="Permanent link">¶</a></h2> <h3 id=1-always-preprocess-separately>1. Always Preprocess Separately<a class=headerlink href=#1-always-preprocess-separately title="Permanent link">¶</a></h3> <p><strong>Bad (expensive):</strong> </p><div class="language-bash highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a><span class=c1># This runs preprocessing on 4 GPUs!</span>
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a>modal<span class=w> </span>run<span class=w> </span>script.py::train_model
</span></code></pre></div> <p><strong>Good (cheap):</strong> </p><div class="language-bash highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=c1># Preprocess on 1 GPU</span>
</span><span id=__span-72-2><a id=__codelineno-72-2 name=__codelineno-72-2 href=#__codelineno-72-2></a>modal<span class=w> </span>run<span class=w> </span>script.py::process_datasets<span class=w>  </span><span class=c1># $3.50/hr</span>
</span><span id=__span-72-3><a id=__codelineno-72-3 name=__codelineno-72-3 href=#__codelineno-72-3></a>
</span><span id=__span-72-4><a id=__codelineno-72-4 name=__codelineno-72-4 href=#__codelineno-72-4></a><span class=c1># Train on 4 GPUs</span>
</span><span id=__span-72-5><a id=__codelineno-72-5 name=__codelineno-72-5 href=#__codelineno-72-5></a>modal<span class=w> </span>run<span class=w> </span>script.py::train_model<span class=w>      </span><span class=c1># $14/hr</span>
</span></code></pre></div> <p><strong>Savings:</strong> ~$10/hr during preprocessing (which can take 1-2 hours)</p> <h3 id=2-test-with-smaller-models-first>2. Test with Smaller Models First<a class=headerlink href=#2-test-with-smaller-models-first title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a><span class=c1># Test with 8B first</span>
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a><span class=n>base_model</span><span class=p>:</span> <span class=n>NousResearch</span><span class=o>/</span><span class=n>Meta</span><span class=o>-</span><span class=n>Llama</span><span class=o>-</span><span class=mi>3</span><span class=o>-</span><span class=mi>8</span><span class=n>B</span><span class=o>-</span><span class=n>Instruct</span>
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a><span class=n>TRAIN_NUM_GPUS</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-73-4><a id=__codelineno-73-4 name=__codelineno-73-4 href=#__codelineno-73-4></a>
</span><span id=__span-73-5><a id=__codelineno-73-5 name=__codelineno-73-5 href=#__codelineno-73-5></a><span class=c1># Then scale to 8-70B</span>
</span><span id=__span-73-6><a id=__codelineno-73-6 name=__codelineno-73-6 href=#__codelineno-73-6></a><span class=n>base_model</span><span class=p>:</span> <span class=n>meta</span><span class=o>-</span><span class=n>llama</span><span class=o>/</span><span class=n>Meta</span><span class=o>-</span><span class=n>Llama</span><span class=o>-</span><span class=mi>3</span><span class=o>-</span><span class=mi>8</span><span class=o>-</span><span class=mi>70</span><span class=n>B</span><span class=o>-</span><span class=n>Instruct</span>
</span><span id=__span-73-7><a id=__codelineno-73-7 name=__codelineno-73-7 href=#__codelineno-73-7></a><span class=n>TRAIN_NUM_GPUS</span> <span class=o>=</span> <span class=mi>4</span>
</span></code></pre></div> <p>Catch bugs on the cheap model, then run the expensive one.</p> <h3 id=3-use-smaller-gpus-for-testing>3. Use Smaller GPUs for Testing<a class=headerlink href=#3-use-smaller-gpus-for-testing title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a><span class=c1># Testing phase</span>
</span><span id=__span-74-2><a id=__codelineno-74-2 name=__codelineno-74-2 href=#__codelineno-74-2></a><span class=n>GPU_TYPE</span> <span class=o>=</span> <span class=s2>"l40s"</span>  <span class=c1># ~$1/hr</span>
</span><span id=__span-74-3><a id=__codelineno-74-3 name=__codelineno-74-3 href=#__codelineno-74-3></a>
</span><span id=__span-74-4><a id=__codelineno-74-4 name=__codelineno-74-4 href=#__codelineno-74-4></a><span class=c1># Production phase</span>
</span><span id=__span-74-5><a id=__codelineno-74-5 name=__codelineno-74-5 href=#__codelineno-74-5></a><span class=n>GPU_TYPE</span> <span class=o>=</span> <span class=s2>"a100-80gb"</span>  <span class=c1># ~$3.50/hr</span>
</span></code></pre></div> <h3 id=4-limit-test-datasets>4. Limit Test Datasets<a class=headerlink href=#4-limit-test-datasets title="Permanent link">¶</a></h3> <p>For testing, use a small subset:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a><span class=nt>datasets</span><span class=p>:</span>
</span><span id=__span-75-2><a id=__codelineno-75-2 name=__codelineno-75-2 href=#__codelineno-75-2></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">dataset/name</span>
</span><span id=__span-75-3><a id=__codelineno-75-3 name=__codelineno-75-3 href=#__codelineno-75-3></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">chat_template</span>
</span><span id=__span-75-4><a id=__codelineno-75-4 name=__codelineno-75-4 href=#__codelineno-75-4></a><span class=w>    </span><span class=nt>num_samples</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">100</span><span class=w>  </span><span class=c1># Just 100 samples for testing</span>
</span></code></pre></div> <p>Or edit the dataset on HuggingFace to create a "mini" version.</p> <h3 id=5-smart-checkpointing>5. Smart Checkpointing<a class=headerlink href=#5-smart-checkpointing title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-76-1><a id=__codelineno-76-1 name=__codelineno-76-1 href=#__codelineno-76-1></a><span class=nt>saves_per_epoch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class=w>           </span><span class=c1># Don't checkpoint too often</span>
</span><span id=__span-76-2><a id=__codelineno-76-2 name=__codelineno-76-2 href=#__codelineno-76-2></a><span class=nt>save_total_limit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class=w>          </span><span class=c1># Delete old checkpoints</span>
</span></code></pre></div> <p>Volume storage is free up to 50GB, but good practice for huge models.</p> <h3 id=6-resume-from-checkpoint>6. Resume from Checkpoint<a class=headerlink href=#6-resume-from-checkpoint title="Permanent link">¶</a></h3> <p>If training fails or you want to try different hyperparameters:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-77-1><a id=__codelineno-77-1 name=__codelineno-77-1 href=#__codelineno-77-1></a><span class=nt>resume_from_checkpoint</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">/data/outputs/lora-out/checkpoint-1000</span>
</span></code></pre></div> <p><strong>Don't start from scratch!</strong> You've already paid for those first 1000 steps.</p> <h2 id=monitoring-and-debugging>Monitoring and Debugging<a class=headerlink href=#monitoring-and-debugging title="Permanent link">¶</a></h2> <h3 id=real-time-logs>Real-time Logs<a class=headerlink href=#real-time-logs title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-78-1><a id=__codelineno-78-1 name=__codelineno-78-1 href=#__codelineno-78-1></a>modal<span class=w> </span>run<span class=w> </span>FinetuneLlamaAxolotlGPUModal.py::train_model
</span><span id=__span-78-2><a id=__codelineno-78-2 name=__codelineno-78-2 href=#__codelineno-78-2></a><span class=c1># Click the URL in output to open Modal dashboard</span>
</span></code></pre></div> <p><strong>Dashboard shows:</strong> - Real-time logs (stdout/stderr) - GPU utilization per GPU (should be ~95-100%) - Memory usage per GPU - Cost accumulation ($$$ ticking up)</p> <h3 id=weights-biases>Weights &amp; Biases<a class=headerlink href=#weights-biases title="Permanent link">¶</a></h3> <p>Go to <code>wandb.ai/&lt;username&gt;/Llama-70b-MultiGPU-finetune</code></p> <p><strong>Charts to watch:</strong> - <strong>Training loss</strong> - should decrease smoothly - <strong>Validation loss</strong> - should decrease, but slower than training - <strong>Learning rate</strong> - should follow the schedule (cosine decay) - <strong>GPU utilization</strong> - should be high</p> <p><strong>If training loss decreases but validation loss increases:</strong> You're overfitting. Reduce epochs or add regularization.</p> <h3 id=check-preprocessed-data>Check Preprocessed Data<a class=headerlink href=#check-preprocessed-data title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-79-1><a id=__codelineno-79-1 name=__codelineno-79-1 href=#__codelineno-79-1></a>modal<span class=w> </span>volume<span class=w> </span>ls<span class=w> </span>Finetuned_Llama_70b_Axolotl<span class=w> </span>/data/prepared_datasets
</span></code></pre></div> <p>Lists preprocessed files. You can download and inspect:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-80-1><a id=__codelineno-80-1 name=__codelineno-80-1 href=#__codelineno-80-1></a>modal<span class=w> </span>volume<span class=w> </span>get<span class=w> </span>Finetuned_Llama_70b_Axolotl<span class=w> </span><span class=se>\</span>
</span><span id=__span-80-2><a id=__codelineno-80-2 name=__codelineno-80-2 href=#__codelineno-80-2></a><span class=w>  </span>/data/prepared_datasets/alpaca_2k<span class=w> </span><span class=se>\</span>
</span><span id=__span-80-3><a id=__codelineno-80-3 name=__codelineno-80-3 href=#__codelineno-80-3></a><span class=w>  </span>./local_data
</span></code></pre></div> <h3 id=download-checkpoints>Download Checkpoints<a class=headerlink href=#download-checkpoints title="Permanent link">¶</a></h3> <div class="language-bash highlight"><pre><span></span><code><span id=__span-81-1><a id=__codelineno-81-1 name=__codelineno-81-1 href=#__codelineno-81-1></a><span class=c1># List checkpoints</span>
</span><span id=__span-81-2><a id=__codelineno-81-2 name=__codelineno-81-2 href=#__codelineno-81-2></a>modal<span class=w> </span>volume<span class=w> </span>ls<span class=w> </span>Finetuned_Llama_70b_Axolotl<span class=w> </span>/data/outputs/lora-out
</span><span id=__span-81-3><a id=__codelineno-81-3 name=__codelineno-81-3 href=#__codelineno-81-3></a>
</span><span id=__span-81-4><a id=__codelineno-81-4 name=__codelineno-81-4 href=#__codelineno-81-4></a><span class=c1># Download specific checkpoint</span>
</span><span id=__span-81-5><a id=__codelineno-81-5 name=__codelineno-81-5 href=#__codelineno-81-5></a>modal<span class=w> </span>volume<span class=w> </span>get<span class=w> </span>Finetuned_Llama_70b_Axolotl<span class=w> </span><span class=se>\</span>
</span><span id=__span-81-6><a id=__codelineno-81-6 name=__codelineno-81-6 href=#__codelineno-81-6></a><span class=w>  </span>/data/outputs/lora-out/checkpoint-1000<span class=w> </span><span class=se>\</span>
</span><span id=__span-81-7><a id=__codelineno-81-7 name=__codelineno-81-7 href=#__codelineno-81-7></a><span class=w>  </span>./local_checkpoint
</span></code></pre></div> <p>Useful for local testing or pushing to HuggingFace manually.</p> <h2 id=scaling-to-8-gpus-for-the-brave>Scaling to 8 GPUs (For the Brave)<a class=headerlink href=#scaling-to-8-gpus-for-the-brave title="Permanent link">¶</a></h2> <p>For massive models like Llama 3-405B or Mixtral 8×22B, you need 8 GPUs.</p> <h3 id=update-configuration>Update Configuration<a class=headerlink href=#update-configuration title="Permanent link">¶</a></h3> <div class="language-python highlight"><pre><span></span><code><span id=__span-82-1><a id=__codelineno-82-1 name=__codelineno-82-1 href=#__codelineno-82-1></a><span class=n>TRAIN_NUM_GPUS</span> <span class=o>=</span> <span class=mi>8</span>
</span><span id=__span-82-2><a id=__codelineno-82-2 name=__codelineno-82-2 href=#__codelineno-82-2></a><span class=n>GPU_TYPE</span> <span class=o>=</span> <span class=s2>"a100-80gb"</span>
</span></code></pre></div> <h3 id=update-yaml-for-memory>Update YAML for Memory<a class=headerlink href=#update-yaml-for-memory title="Permanent link">¶</a></h3> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-83-1><a id=__codelineno-83-1 name=__codelineno-83-1 href=#__codelineno-83-1></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</span><span id=__span-83-2><a id=__codelineno-83-2 name=__codelineno-83-2 href=#__codelineno-83-2></a><span class=nt>gradient_accumulation_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</span><span id=__span-83-3><a id=__codelineno-83-3 name=__codelineno-83-3 href=#__codelineno-83-3></a><span class=nt>load_in_8bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class=w>  </span><span class=c1># Or even 4bit</span>
</span><span id=__span-83-4><a id=__codelineno-83-4 name=__codelineno-83-4 href=#__codelineno-83-4></a>
</span><span id=__span-83-5><a id=__codelineno-83-5 name=__codelineno-83-5 href=#__codelineno-83-5></a><span class=c1># Effective batch: 2 × 2 × 8 = 32</span>
</span></code></pre></div> <h3 id=enable-deepspeed-zero-stage-3-optional>Enable DeepSpeed ZeRO Stage 3 (Optional)<a class=headerlink href=#enable-deepspeed-zero-stage-3-optional title="Permanent link">¶</a></h3> <p>For maximum memory efficiency, create <code>deepspeed_zero3.json</code>:</p> <div class="language-json highlight"><pre><span></span><code><span id=__span-84-1><a id=__codelineno-84-1 name=__codelineno-84-1 href=#__codelineno-84-1></a><span class=p>{</span>
</span><span id=__span-84-2><a id=__codelineno-84-2 name=__codelineno-84-2 href=#__codelineno-84-2></a><span class=w>  </span><span class=nt>"zero_optimization"</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-84-3><a id=__codelineno-84-3 name=__codelineno-84-3 href=#__codelineno-84-3></a><span class=w>    </span><span class=nt>"stage"</span><span class=p>:</span><span class=w> </span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-84-4><a id=__codelineno-84-4 name=__codelineno-84-4 href=#__codelineno-84-4></a><span class=w>    </span><span class=nt>"offload_optimizer"</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>"device"</span><span class=p>:</span><span class=w> </span><span class=s2>"cpu"</span><span class=p>},</span>
</span><span id=__span-84-5><a id=__codelineno-84-5 name=__codelineno-84-5 href=#__codelineno-84-5></a><span class=w>    </span><span class=nt>"offload_param"</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>"device"</span><span class=p>:</span><span class=w> </span><span class=s2>"cpu"</span><span class=p>}</span>
</span><span id=__span-84-6><a id=__codelineno-84-6 name=__codelineno-84-6 href=#__codelineno-84-6></a><span class=w>  </span><span class=p>},</span>
</span><span id=__span-84-7><a id=__codelineno-84-7 name=__codelineno-84-7 href=#__codelineno-84-7></a><span class=w>  </span><span class=nt>"fp16"</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>"enabled"</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=p>},</span>
</span><span id=__span-84-8><a id=__codelineno-84-8 name=__codelineno-84-8 href=#__codelineno-84-8></a><span class=w>  </span><span class=nt>"train_micro_batch_size_per_gpu"</span><span class=p>:</span><span class=w> </span><span class=mi>2</span>
</span><span id=__span-84-9><a id=__codelineno-84-9 name=__codelineno-84-9 href=#__codelineno-84-9></a><span class=p>}</span>
</span></code></pre></div> <p>Add to YAML:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-85-1><a id=__codelineno-85-1 name=__codelineno-85-1 href=#__codelineno-85-1></a><span class=nt>deepspeed</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">/path/to/deepspeed_zero3.json</span>
</span></code></pre></div> <p>DeepSpeed ZeRO Stage 3 shards everything - parameters, gradients, optimizer states - across GPUs. Even offloads to CPU when needed. Insanely memory efficient but adds communication overhead.</p> <p><strong>Cost:</strong> 8× A100-80GB = ~$28/hour</p> <p>A 10-hour training run = $280. Make sure you really need 8 GPUs!</p> <h2 id=whats-next>What's Next?<a class=headerlink href=#whats-next title="Permanent link">¶</a></h2> <p>You've built a production-grade multi-GPU training pipeline. Here's what you can do next:</p> <ol> <li> <p><strong>Custom datasets:</strong> Format your own data for Axolotl (see <a href=https://docs.axolotl.ai/ >Axolotl dataset docs</a>)</p> </li> <li> <p><strong>Advanced LoRA variants:</strong> Try QLoRA (4-bit quantized LoRA) or DoRA (decomposed LoRA)</p> </li> <li> <p><strong>Full fine-tuning:</strong> Remove <code>adapter: lora</code> and train all parameters (requires way more memory)</p> </li> <li> <p><strong>Evaluation:</strong> Add custom evaluation metrics beyond just loss</p> </li> <li> <p><strong>Deployment:</strong> Serve your model with vLLM (see the Gemma tutorial for details)</p> </li> <li> <p><strong>Multi-node training:</strong> Scale beyond 8 GPUs using Modal's multi-node support (advanced!)</p> </li> </ol> <h2 id=resources>Resources<a class=headerlink href=#resources title="Permanent link">¶</a></h2> <ul> <li><strong><a href=https://docs.axolotl.ai/ >Axolotl Documentation</a></strong> - Official docs with dataset formats and advanced configs</li> <li><strong><a href=https://github.com/axolotl-ai-cloud/axolotl>Axolotl GitHub</a></strong> - Source code and issue tracker</li> <li><strong><a href=https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples>Axolotl Examples</a></strong> - Pre-built configs for tons of models</li> <li><strong><a href=https://modal.com/docs>Modal Documentation</a></strong> - Everything about Modal</li> <li><strong><a href=https://huggingface.co/docs/accelerate>HuggingFace Accelerate</a></strong> - Deep dive into distributed training</li> <li><strong><a href=https://www.deepspeed.ai/ >DeepSpeed</a></strong> - For ZeRO optimization details</li> </ul> <h2 id=wrapping-up>Wrapping Up<a class=headerlink href=#wrapping-up title="Permanent link">¶</a></h2> <p>You just built what most companies call "ML infrastructure": - Multi-GPU distributed training - Automatic checkpointing and resumption - Experiment tracking with W&amp;B - Model versioning on HuggingFace - Cost-optimized preprocessing pipeline</p> <p>All in one Python file, running on Modal. No Kubernetes, no Docker nightmares, no spending weeks setting up infrastructure.</p> <p>The Unsloth tutorial showed you highly optimized single-GPU training. This tutorial showed you how to scale beyond that - training models that literally don't fit on a single GPU.</p> <p>This is how real ML teams train models nowadays. Not on local machines, not on hand-managed clusters. On serverless infrastructure like Modal, where you pay by the second and scale infinitely.</p> <p>The YAML configuration approach is especially powerful. Need to try different learning rates? Just edit one line and re-run. Want to train on a different dataset? Change one parameter. Everything is reproducible, everything is versioned, everything is clean.</p> <p>Got questions? Hit me up on Twitter <a href=https://x.com/adithya_s_k>@adithya_s_k</a>!</p> <p>Now go train that 8-70B model and show the world what you built. 🚀</p></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 13, 2025 19:27:59 UTC">October 13, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 13, 2025 19:27:59 UTC">October 13, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../FinetuneGemmaUnslothModalTutorial/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Fine-tuning Gemma with Unsloth"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Fine-tuning Gemma with Unsloth </div> </div> </a> <a href=../../LLMArchitecture/ParameterCount/ class="md-footer__link md-footer__link--next" aria-label="Next: Parameter Count"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Parameter Count </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>