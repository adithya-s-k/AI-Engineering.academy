<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/LLM/Axolotl/ rel=canonical><link rel=icon href=../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.21"><title>Index - AI Engineering Academy</title><link rel=stylesheet href=../../assets/stylesheets/main.2a3383ac.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Index - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/LLM/Axolotl/README.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/LLM/Axolotl/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Index - AI Engineering Academy"><meta name=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta name=twitter:image content=https://aiengineering.academy/assets/images/social/LLM/Axolotl/README.png><link rel=stylesheet href=../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
  posthog.init('phc_OL7nUCVeKtVJe8eHSKGs8zPTQAyr0hm8opAPFdFlkBz', {
      api_host: 'https://us.i.posthog.com',
      person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
  })
</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#introduction class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Index </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../RAG/ class=md-tabs__link> RAG </a> </li> <li class=md-tabs__item> <a href=../ class=md-tabs__link> LLM </a> </li> <li class=md-tabs__item> <a href=../../Deployment/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../ class=md-nav__link> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#what-is-fine-tuning class=md-nav__link> <span class=md-ellipsis> What is Fine Tuning? </span> </a> </li> <li class=md-nav__item> <a href=#introduction-to-axolotl class=md-nav__link> <span class=md-ellipsis> Introduction to Axolotl </span> </a> <nav class=md-nav aria-label="Introduction to Axolotl"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-features class=md-nav__link> <span class=md-ellipsis> Key Features: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#axolotl-supports-the-following-models class=md-nav__link> <span class=md-ellipsis> Axolotl supports the following models </span> </a> </li> <li class=md-nav__item> <a href=#idea-1-youtube-cloner class=md-nav__link> <span class=md-ellipsis> Idea 1: YouTube Cloner </span> </a> <nav class=md-nav aria-label="Idea 1: YouTube Cloner"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#project-objective class=md-nav__link> <span class=md-ellipsis> Project Objective </span> </a> </li> <li class=md-nav__item> <a href=#dataset-preparation class=md-nav__link> <span class=md-ellipsis> Dataset Preparation </span> </a> </li> <li class=md-nav__item> <a href=#tips-on-dataset-curation class=md-nav__link> <span class=md-ellipsis> Tips on Dataset Curation </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning class=md-nav__link> <span class=md-ellipsis> Fine-tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#prerequisites-optional class=md-nav__link> <span class=md-ellipsis> Prerequisites (Optional) </span> </a> <nav class=md-nav aria-label="Prerequisites (Optional)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> <span class=md-ellipsis> Conclusion </span> </a> </li> <li class=md-nav__item> <a href=#closing-thoughts class=md-nav__link> <span class=md-ellipsis> Closing Thoughts </span> </a> <nav class=md-nav aria-label="Closing Thoughts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#resources class=md-nav__link> <span class=md-ellipsis> Resources </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/LLM/Axolotl/README.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/LLM/Axolotl/README.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <div><h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">¶</a></h2> <p>Welcome to an immersive journey into the intricate art of fine-tuning AI models, where precision intertwines with customization. In this blog, we will delve into the profound significance of fine-tuning, unravel its complexities, and unveil a remarkable tool that streamlines this process - Axolotl.</p> <p>Get ready to master the art of fine-tuning an LLM (Large Language Model) using Axolotl. Throughout this tutorial, we'll navigate through the intricacies and adjustable elements of this transformative process.</p> <p>Axolotl predominantly operates through YAML files, requiring you to engage with and customize these files as part of the workflow.</p> <p>The Axolotl repository boasts comprehensive documentation, ensuring a swift grasp of its functionalities to kickstart your journey seamlessly.</p> <p>Join me as we unravel the journey from ideation to fine-tuning an LLM, leveraging Axolotl as our medium of choice for this transformative endeavor. Let's dive in!</p> <p>Introducing the Axolotl UI Editor</p> <p>Before we proceed further, I'd like to introduce the Axolotl UI Editor—a powerful tool designed to facilitate the editing of Axolotl YAML files for fine-tuning. This intuitive editor is a handy creation from <a href=https://www.cognitivelab.in/ >CognitiveLab</a>, providing a seamless interface for customizing Axolotl configurations.</p> <p>Explore the Axolotl UI Editor here: <a href=https://axolotl-ui.vercel.app/ >Axolotl UI Editor</a></p> <p>!<a href=https://prod-files-secure.s3.us-west-2.amazonaws.com/97859362-02bd-4d74-abc1-b66ffcf4d0ad/6ecd3ef8-c16b-4e2f-bb05-58503b49e969/Untitled.png>https://prod-files-secure.s3.us-west-2.amazonaws.com/97859362-02bd-4d74-abc1-b66ffcf4d0ad/6ecd3ef8-c16b-4e2f-bb05-58503b49e969/Untitled.png</a></p> <p>You can also find the source code for this editor on GitHub: <a href=https://github.com/adithya-s-k/axolotl-ui>Axolotl UI Editor - GitHub</a></p> <p>This user-friendly interface simplifies the process of tweaking Axolotl YAML files, making it accessible and efficient for fine-tuning tasks. Now, let's dive deeper into leveraging this tool for our fine-tuning journey.</p> <h2 id=what-is-fine-tuning>What is Fine Tuning?<a class=headerlink href=#what-is-fine-tuning title="Permanent link">¶</a></h2> <p>Before we delve into Axolotl, let's understand the significance of fine-tuning in the world of artificial intelligence. Fine-tuning is the process of taking a pre-trained model and adapting it to a specific task or dataset. It's akin to honing a skill - refining an already proficient model for a specialized purpose.</p> <p>Some examples of fine-tuning tasks can include:</p> <ul> <li>Fine-tuning for generating structured output (e.g., function calling).</li> <li>Fine-tuning to emulate someone's style or behavior.</li> </ul> <h2 id=introduction-to-axolotl>Introduction to Axolotl<a class=headerlink href=#introduction-to-axolotl title="Permanent link">¶</a></h2> <p>Axolotl, a versatile tool designed for AI model fine-tuning, emerges as a game-changer in the field. It supports various Hugging Face models, including llama, pythia, falcon, and mpt, empowering users with a multitude of configurations and architectures.</p> <h3 id=key-features><strong>Key Features:</strong><a class=headerlink href=#key-features title="Permanent link">¶</a></h3> <ol> <li><strong>Model Support:</strong> Easily train models such as llama, pythia, falcon, and mpt.</li> <li><strong>Configurability:</strong> Effortlessly customize configurations using a simple YAML file or CLI overwrite.</li> <li><strong>Dataset Flexibility:</strong> Load datasets in various formats, use custom formats, or bring your own tokenized datasets.</li> <li><strong>Advanced Techniques:</strong> Benefit from integrated features like xformer, flash attention, rope scaling, and multipacking.</li> <li><strong>Scalability:</strong> Run on a single GPU or multiple GPUs using FSDP or Deepspeed.</li> <li><strong>Containerization:</strong> Seamlessly run with Docker, either locally or in the cloud.</li> <li><strong>Logging and Checkpoints:</strong> Log results and optionally save checkpoints to wandb for comprehensive tracking.</li> </ol> <p>Axolotl provides a comprehensive suite of tools and capabilities, making AI model fine-tuning accessible, efficient, and adaptable to diverse use cases. Let's explore how to harness these features for optimizing and refining our models effectively.</p> <h2 id=axolotl-supports-the-following-models>Axolotl supports the following models<a class=headerlink href=#axolotl-supports-the-following-models title="Permanent link">¶</a></h2> <table> <thead> <tr> <th></th> <th>fp16/fp32</th> <th>lora</th> <th>qlora</th> <th>gptq</th> <th>gptq w/flash attn</th> <th>flash attn</th> <th>xformers attn</th> </tr> </thead> <tbody> <tr> <td>llama</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>Mistral</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>Mixtral-MoE</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>❓</td> </tr> <tr> <td>Pythia</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❌</td> <td>❌</td> <td>❌</td> <td>❓</td> </tr> <tr> <td>cerebras</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❌</td> <td>❌</td> <td>❌</td> <td>❓</td> </tr> <tr> <td>btlm</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❌</td> <td>❌</td> <td>❌</td> <td>❓</td> </tr> <tr> <td>mpt</td> <td>✅</td> <td>❌</td> <td>❓</td> <td>❌</td> <td>❌</td> <td>❌</td> <td>❓</td> </tr> <tr> <td>falcon</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❌</td> <td>❌</td> <td>❌</td> <td>❓</td> </tr> <tr> <td>gpt-j</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❌</td> <td>❌</td> <td>❓</td> <td>❓</td> </tr> <tr> <td>XGen</td> <td>✅</td> <td>❓</td> <td>✅</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>✅</td> </tr> <tr> <td>phi</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>❓</td> </tr> <tr> <td>RWKV</td> <td>✅</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>❓</td> </tr> <tr> <td>Qwen</td> <td>✅</td> <td>✅</td> <td>✅</td> <td>❓</td> <td>❓</td> <td>❓</td> <td>❓</td> </tr> </tbody> </table> <h2 id=idea-1-youtube-cloner><strong>Idea 1: YouTube Cloner</strong><a class=headerlink href=#idea-1-youtube-cloner title="Permanent link">¶</a></h2> <p>Have you ever wondered if you could clone a YouTuber? I recently had this intriguing idea to explore whether a language model could mimic the style of popular YouTubers. The concept is simple yet fascinating: using fine-tuning techniques with LoRA, I aim to teach a language model how to emulate the tone, pacing, and content style of specific YouTube channels.</p> <h3 id=project-objective><strong>Project Objective</strong><a class=headerlink href=#project-objective title="Permanent link">¶</a></h3> <p>The aim of the YouTube Cloner project is to investigate the abilities of Language Models (LLMs) in mimicking the speaking style of popular YouTubers. Its main goal is to determine the effectiveness of LLMs in replicating the tone, pacing, and content style of specific YouTube channels through fine-tuning on selected datasets.</p> <p>Objectives</p> <ul> <li><strong>Explore Emulation:</strong> Assess the ability of LLMs to mirror the unique tone and pacing of popular YouTube channels.</li> <li><strong>Fine-Tuning Experiment:</strong> Implement fine-tuning methods with hand-picked datasets to train LLMs for channel-specific styles.</li> </ul> <p>Join me on this exciting journey as we unravel the potential of language models in mimicking the essence of favorite YouTube personalities. Let's dive into the details and see how we can bring this idea to life!</p> <h3 id=dataset-preparation><strong>Dataset Preparation</strong><a class=headerlink href=#dataset-preparation title="Permanent link">¶</a></h3> <p>Before diving into the fine-tuning process, we need to prepare the dataset that will enable us to achieve our goal.</p> <p>If you're interested in recreating this project, I've open-sourced the web scraping and dataset curation code <a href=https://github.com/adithya-s-k/LLM-Alchemy-Chamber/tree/main/Projects/YT_Clones>here</a>. Feel free to star the repository if you find it helpful!</p> <p>I've also made the dataset available <a href=https://huggingface.co/datasets/AdithyaSK/Fireship_transcript_summar_prompt>here</a> on Hugging Face Datasets. You can explore the notebooks containing the code or directly use the dataset to replicate the steps I followed.</p> <p><a href=https://github.com/adithya-s-k/LLM-Alchemy-Chamber/tree/main/Projects/YT_Clones></a></p> <p><a href=https://huggingface.co/datasets/AdithyaSK/Fireship_transcript_summar_prompt>AdithyaSK/Fireship_transcript_summar_prompt · Datasets at Hugging Face</a></p> <h3 id=tips-on-dataset-curation>Tips on Dataset Curation<a class=headerlink href=#tips-on-dataset-curation title="Permanent link">¶</a></h3> <p>Dataset curation is a critical step often overlooked in many guides or tutorials, but I'll delve into the details behind building a good fine-tuning dataset.</p> <p>Firstly, you need to decide your input prompt and what you expect from the model. In the case of the YouTube Cloner, I aimed to use the video title and a brief summary as input to generate the video script.</p> <p>During dataset creation, my primary focus was to gather the YouTube video titles and transcripts and generate concise summaries. Here's how I accomplished this:</p> <ol> <li>Extracted links from a YouTube channel through web scraping.</li> <li>Downloaded audio, titles, and other relevant information.</li> <li>Transcribed the audio into text using a deep-seed audio-to-text API and generated summaries.</li> </ol> <p>Now, we have all the necessary components: titles, summaries, and video transcripts. Next, it's time to format the prompt.</p> <p>When fine-tuning base models like llama or mistal that are not instructionally fine-tuned, you can use your own prompt format. However, if the model is instructionally fine-tuned, you must follow the instruction format.</p> <p>Axolotl supports various dataset types like alpaca, llama, etc., but I generally prefer <strong><code>completion</code></strong> as it offers the most control over dataset formatting.</p> <p>Here is the prompt format I chose for this particular project:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>&lt;s&gt;
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>[INST]
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>You are a YouTuber called Fireship, creating engaging, high-intensity coding tutorials and tech news.
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>You cover a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>Given the title of the video: {title}
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>and a small summary: {video_summary}
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>[/INST]
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>Generate the video: {video_transcipt}
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>&lt;/s&gt;
</span></code></pre></div> <p>Here, "<s>" is the start-of-sentence token and "</s>" is the end-of-sentence token. These are unique tokens.</p> <p>The "" token indicates to the generator when to stop generating.</p> <p>So, essentially, these are important aspects to consider while fine-tuning.</p> <p>Providing these as the input is crucial.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>&lt;s&gt;[INST]
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>You are youtuber called Fireship you make engaging high-intensity and entertaining coding tutorials and tech news.
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>you covers a wide range of topics relevant to programmers, aiming to help them learn and improve their skills quickly.
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>Given the title of the video : Rust in 100 seconds
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>and a small summary : A 100 second video on Rust not a code report
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a>[/INST]
</span></code></pre></div> <p>With the title and summary,</p> <p>And you are making the LLM familiar with giving a response like this.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>Generate the video : Rust. A fast and memory efficient language known for taking everything that's wrong with low level systems programming languages like C plus plus, c, and assembly, then making it worse by eliminating pointers entirely and providing an unpronounceable name that makes developers angry. It was created by Graydon Hoare inspired by ML functional languages and aimed at building safe reliable software while remaining extremely fast. In fact, its motto is secure concurrency without sacrificing speed, which sounds almost too good to be true. The main problem with unsafe multi threaded programming today is data races. Data races happen when multiple threads try to access the same mutable value at the exact same time, but there's no way to know which thread will win, making your programs unreliable. It can also cause hard to debug errors because you have race conditions where each one occurs randomly based on timing issues between different threads. There are many libraries out there designed to mitigate this issue or prevent it altogether. But, rust takes a very different approach by simply disallowing shared state, requiring all values, including variables and functions to either be immutable or only changed via a unique owner called a reference type or pointer. This means data races cannot occur automatically in a typical application, though they can still be introduced accidentally if someone tries to use the borrow checker to perform some fancy hackery. What that usually results in is clear error messages about double usage of references instead of cryptic seg faults and other run time errors, allowing you to solve problems earlier in development, and with less frustration. And it allows you to do so with patterns already proven to work reliably. For example, smart pointers eliminate the need for things like raw pointers and free store allocators found in C plus plus, ensuring proper garbage collection. Instead of using inheritance, interfaces, generics, traits provide object oriented features such as polymorphism in a statically typed way. As awesome as that sounds, learning curves don't come much steeper than rust, mostly because of its ownership system, which I would describe as more of a philosophy than anything else. If you want the full explanation, subscribe to my channel. Otherwise, just enjoy these 2 hello worlds from the same file. You might think the first line here declares a variable named hello with the string hello world assigned to it. However, you'd be wrong. That doesn't actually define a new variable. Rather, It defines a function with an explicit return type of a string literal. When used in conjunction with println, it prints the string literally twice. Or we could define a global variable with mut, which changes the meaning of the assignment operator to mutate existing memory. Now, let me read you something really scary. To get rid of pointers completely. We have references instead. These act exactly like the address of operators in other languages, except they implement safety checks through rust's highly sophisticated borrow checker. On top of that, you can clone objects into new locations, move values around, deep copy and shallow copy across types, weak references, arc, ref cell, interior, pin, once cell, and on and on. At this point, you should start seeing how rust got its name. If you wanna build a complex multi threaded system with performance requirements. Your best bet may well be learning this crazy language that seems so easy on the surface. This has been the rust programming language in 100 seconds. Hit the like button if you wanna see more short videos like this. Thanks for watching and I will see you in the next one.
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>&lt;/s&gt;
</span></code></pre></div> <p>so this will be the main dataset prep part</p> <p>After formatting the dataset, store all the formatted prompts in a field or column named <code>text</code>. This step is crucial before pushing it to Hugging Face.</p> <p><a href=https://huggingface.co/docs/datasets/v2.17.1/en/upload_dataset#upload-with-python>Share a dataset to the Hub</a></p> <h3 id=fine-tuning>Fine-tuning<a class=headerlink href=#fine-tuning title="Permanent link">¶</a></h3> <p>After you have uploaded the dataset, 90% of the work is complete. All that's left to do now is use Axolotl to fine-tune the model.</p> <p>Axolotl makes it really easy. All you have to do is determine which type of fine-tuning you want to do, be it Lora, FFT, or qLora.</p> <p>Here is a high-level abstraction to help you decide which one you want to choose:</p> <p>Lora - If you want to fine-tune the model to respond in a particular type - like JSON, like the above YouTube cloner example. Domain adaptation using Lora is tough. Either you have to have a very high rank value to change a lot of parameters, or you will have to overfit on the data which might degrade your model's performance.</p> <p>If you want to fine-tune a model to respond to you in a particular style, then choose Lora. Generally, I fine-tune most of my models using Lora.</p> <p>qLora - If you want to perform Lora but don't have enough compute, you can use qLora which lets you fine-tune models on a lower-end GPU like T4 with 16GB of VRAM, but the time for fine-tuning will increase.</p> <p>FFT - Full weight fine-tuning - in Axolotl you can FFT llama on an A100-80GB variant because it is well optimized. I generally go with FFT for domain or language adaptation, but FFT might decrease the original model's performance.</p> <p>Now, while performing Lora, there are some more things you have to note down.</p> <p>The Rank - R and alpha values.</p> <p>Here is a tweet I wrote about the considerations to make to pick an R and alpha value:</p> <p><a href="https://publish.twitter.com/?url=https://twitter.com/adithya_s_k/status/1744065797268656579#">https://publish.twitter.com/?url=https://twitter.com/adithya_s_k/status/1744065797268656579#</a></p> <p>Now that that's out of the way, let's get into fine-tuning the model.</p> <h1 id=prerequisites-optional><strong>Prerequisites (Optional)</strong><a class=headerlink href=#prerequisites-optional title="Permanent link">¶</a></h1> <p>There are two primary prerequisites: a server with an Ampere GPU, such as A100, and a functioning conda setup.</p> <p>For the A100 GPU, you can utilize any cloud platform such as Google Cloud, AWS, Lambda Labs, or E2E Networks. Your choice may depend on your region and budget.</p> <p>local install</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>git<span class=w> </span>clone<span class=w> </span>&lt;https://github.com/OpenAccess-AI-Collective/axolotl&gt;
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=nb>cd</span><span class=w> </span>axolotl
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>pip3<span class=w> </span>install<span class=w> </span>packaging
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>pip3<span class=w> </span>install<span class=w> </span>-e<span class=w> </span><span class=s1>'.[flash-attn,deepspeed]'</span>
</span></code></pre></div> <p>docker </p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=w>  </span>docker<span class=w> </span>run<span class=w> </span>--gpus<span class=w> </span><span class=s1>'"all"'</span><span class=w> </span>--rm<span class=w> </span>-it<span class=w> </span>winglian/axolotl:main-py3.10-cu118-2.0.1
</span></code></pre></div> <p>assuming you have set up axolotl properly and have everything properly configured </p> <p>Pick a model you want to finetuned </p> <p>in this case i wanted to using the <a href=https://huggingface.co/mistralai/Mistral-7B-v0.1>base mistral model</a> </p> <p>you can see a folder called <a href=https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples>examples</a> and you can see all the models axolotl supports and their finetuning scripts</p> <p>now lets create a copy of the qLora finetunign yml and call it lora.yml and make the necessary changes</p> <p><a href=https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/mistral/qlora.yml></a></p> <p>as we will be using Lora and a different dataset and different rank</p> <p>here is the <code>yml</code> script to finetune the model </p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=nt>base_model</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">mistralai/Mistral-7B-v0.1</span><span class=w> </span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=nt>model_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">MistralForCausalLM</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=nt>tokenizer_type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">LlamaTokenizer</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=nt>is_mistral_derived_model</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=nt>load_in_8bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a><span class=c1># change load_in_4bit to false as we will be doing lora finetuning</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=nt>load_in_4bit</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class=w> </span><span class=c1># change from true to false</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a><span class=nt>strict</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a><span class=nt>datasets</span><span class=p>:</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">AdithyaSK/Fireship_transcript_summar_prompt</span><span class=w> </span><span class=c1># change the dataset to the dataset you have pushed on HF</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">completion</span><span class=w> </span><span class=c1>#change it from alpace to completion</span>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a><span class=nt>dataset_prepared_path</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">last_run_prepared</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a><span class=nt>val_set_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a><span class=nt>output_dir</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">./lora-out</span><span class=w> </span><span class=c1># change the directory name - it will the folder in which the final model will be stored</span>
</span><span id=__span-5-17><a id=__codelineno-5-17 name=__codelineno-5-17 href=#__codelineno-5-17></a>
</span><span id=__span-5-18><a id=__codelineno-5-18 name=__codelineno-5-18 href=#__codelineno-5-18></a><span class=nt>adapter</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">lora</span>
</span><span id=__span-5-19><a id=__codelineno-5-19 name=__codelineno-5-19 href=#__codelineno-5-19></a><span class=nt>lora_model_dir</span><span class=p>:</span>
</span><span id=__span-5-20><a id=__codelineno-5-20 name=__codelineno-5-20 href=#__codelineno-5-20></a>
</span><span id=__span-5-21><a id=__codelineno-5-21 name=__codelineno-5-21 href=#__codelineno-5-21></a><span class=nt>sequence_len</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">8192</span>
</span><span id=__span-5-22><a id=__codelineno-5-22 name=__codelineno-5-22 href=#__codelineno-5-22></a><span class=nt>sample_packing</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-5-23><a id=__codelineno-5-23 name=__codelineno-5-23 href=#__codelineno-5-23></a><span class=nt>pad_to_sequence_len</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-5-24><a id=__codelineno-5-24 name=__codelineno-5-24 href=#__codelineno-5-24></a>
</span><span id=__span-5-25><a id=__codelineno-5-25 name=__codelineno-5-25 href=#__codelineno-5-25></a><span class=nt>lora_r</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">64</span><span class=w> </span><span class=c1># lets change the rank from 32 to 64 to change more parameters in the model</span>
</span><span id=__span-5-26><a id=__codelineno-5-26 name=__codelineno-5-26 href=#__codelineno-5-26></a><span class=nt>lora_alpha</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">32</span><span class=w> </span><span class=c1># lets change alpha from 16 to 31 general rule of thumb is r = 2*aplha</span>
</span><span id=__span-5-27><a id=__codelineno-5-27 name=__codelineno-5-27 href=#__codelineno-5-27></a><span class=nt>lora_dropout</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span>
</span><span id=__span-5-28><a id=__codelineno-5-28 name=__codelineno-5-28 href=#__codelineno-5-28></a><span class=nt>lora_target_linear</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-5-29><a id=__codelineno-5-29 name=__codelineno-5-29 href=#__codelineno-5-29></a><span class=nt>lora_fan_in_fan_out</span><span class=p>:</span>
</span><span id=__span-5-30><a id=__codelineno-5-30 name=__codelineno-5-30 href=#__codelineno-5-30></a><span class=nt>lora_target_modules</span><span class=p>:</span><span class=w> </span>
</span><span id=__span-5-31><a id=__codelineno-5-31 name=__codelineno-5-31 href=#__codelineno-5-31></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">gate_proj</span>
</span><span id=__span-5-32><a id=__codelineno-5-32 name=__codelineno-5-32 href=#__codelineno-5-32></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">down_proj</span>
</span><span id=__span-5-33><a id=__codelineno-5-33 name=__codelineno-5-33 href=#__codelineno-5-33></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">up_proj</span>
</span><span id=__span-5-34><a id=__codelineno-5-34 name=__codelineno-5-34 href=#__codelineno-5-34></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">q_proj</span>
</span><span id=__span-5-35><a id=__codelineno-5-35 name=__codelineno-5-35 href=#__codelineno-5-35></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">v_proj</span>
</span><span id=__span-5-36><a id=__codelineno-5-36 name=__codelineno-5-36 href=#__codelineno-5-36></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">k_proj</span>
</span><span id=__span-5-37><a id=__codelineno-5-37 name=__codelineno-5-37 href=#__codelineno-5-37></a><span class=w>  </span><span class="p p-Indicator">-</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">o_proj</span>
</span><span id=__span-5-38><a id=__codelineno-5-38 name=__codelineno-5-38 href=#__codelineno-5-38></a>
</span><span id=__span-5-39><a id=__codelineno-5-39 name=__codelineno-5-39 href=#__codelineno-5-39></a><span class=nt>wandb_project</span><span class=p>:</span><span class=w> </span><span class=c1># if you want to track the models </span>
</span><span id=__span-5-40><a id=__codelineno-5-40 name=__codelineno-5-40 href=#__codelineno-5-40></a><span class=nt>wandb_entity</span><span class=p>:</span>
</span><span id=__span-5-41><a id=__codelineno-5-41 name=__codelineno-5-41 href=#__codelineno-5-41></a><span class=nt>wandb_watch</span><span class=p>:</span>
</span><span id=__span-5-42><a id=__codelineno-5-42 name=__codelineno-5-42 href=#__codelineno-5-42></a><span class=nt>wandb_name</span><span class=p>:</span>
</span><span id=__span-5-43><a id=__codelineno-5-43 name=__codelineno-5-43 href=#__codelineno-5-43></a><span class=nt>wandb_log_model</span><span class=p>:</span>
</span><span id=__span-5-44><a id=__codelineno-5-44 name=__codelineno-5-44 href=#__codelineno-5-44></a>
</span><span id=__span-5-45><a id=__codelineno-5-45 name=__codelineno-5-45 href=#__codelineno-5-45></a><span class=nt>gradient_accumulation_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class=w> </span><span class=c1># gradient accumulation can be increase to 8 but lets it keep it at 4. if you are getting cuda_out_of_memoery you can reduce this number</span>
</span><span id=__span-5-46><a id=__codelineno-5-46 name=__codelineno-5-46 href=#__codelineno-5-46></a><span class=nt>micro_batch_size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class=w> </span>
</span><span id=__span-5-47><a id=__codelineno-5-47 name=__codelineno-5-47 href=#__codelineno-5-47></a><span class=nt>num_epochs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class=w> </span><span class=c1># increase the number of epochs to 5. 1 epoch = the model has seen all the data once</span>
</span><span id=__span-5-48><a id=__codelineno-5-48 name=__codelineno-5-48 href=#__codelineno-5-48></a><span class=nt>optimizer</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">adamw_bnb_8bit</span>
</span><span id=__span-5-49><a id=__codelineno-5-49 name=__codelineno-5-49 href=#__codelineno-5-49></a><span class=nt>lr_scheduler</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">cosine</span>
</span><span id=__span-5-50><a id=__codelineno-5-50 name=__codelineno-5-50 href=#__codelineno-5-50></a><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.0002</span>
</span><span id=__span-5-51><a id=__codelineno-5-51 name=__codelineno-5-51 href=#__codelineno-5-51></a>
</span><span id=__span-5-52><a id=__codelineno-5-52 name=__codelineno-5-52 href=#__codelineno-5-52></a><span class=nt>train_on_inputs</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span id=__span-5-53><a id=__codelineno-5-53 name=__codelineno-5-53 href=#__codelineno-5-53></a><span class=nt>group_by_length</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span id=__span-5-54><a id=__codelineno-5-54 name=__codelineno-5-54 href=#__codelineno-5-54></a><span class=nt>bf16</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
</span><span id=__span-5-55><a id=__codelineno-5-55 name=__codelineno-5-55 href=#__codelineno-5-55></a><span class=nt>fp16</span><span class=p>:</span>
</span><span id=__span-5-56><a id=__codelineno-5-56 name=__codelineno-5-56 href=#__codelineno-5-56></a><span class=nt>tf32</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span id=__span-5-57><a id=__codelineno-5-57 name=__codelineno-5-57 href=#__codelineno-5-57></a>
</span><span id=__span-5-58><a id=__codelineno-5-58 name=__codelineno-5-58 href=#__codelineno-5-58></a><span class=nt>gradient_checkpointing</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-5-59><a id=__codelineno-5-59 name=__codelineno-5-59 href=#__codelineno-5-59></a><span class=nt>early_stopping_patience</span><span class=p>:</span>
</span><span id=__span-5-60><a id=__codelineno-5-60 name=__codelineno-5-60 href=#__codelineno-5-60></a><span class=nt>resume_from_checkpoint</span><span class=p>:</span>
</span><span id=__span-5-61><a id=__codelineno-5-61 name=__codelineno-5-61 href=#__codelineno-5-61></a><span class=nt>local_rank</span><span class=p>:</span>
</span><span id=__span-5-62><a id=__codelineno-5-62 name=__codelineno-5-62 href=#__codelineno-5-62></a><span class=nt>logging_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span id=__span-5-63><a id=__codelineno-5-63 name=__codelineno-5-63 href=#__codelineno-5-63></a><span class=nt>xformers_attention</span><span class=p>:</span>
</span><span id=__span-5-64><a id=__codelineno-5-64 name=__codelineno-5-64 href=#__codelineno-5-64></a><span class=nt>flash_attention</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span id=__span-5-65><a id=__codelineno-5-65 name=__codelineno-5-65 href=#__codelineno-5-65></a>
</span><span id=__span-5-66><a id=__codelineno-5-66 name=__codelineno-5-66 href=#__codelineno-5-66></a><span class=nt>loss_watchdog_threshold</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">5.0</span>
</span><span id=__span-5-67><a id=__codelineno-5-67 name=__codelineno-5-67 href=#__codelineno-5-67></a><span class=nt>loss_watchdog_patience</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
</span><span id=__span-5-68><a id=__codelineno-5-68 name=__codelineno-5-68 href=#__codelineno-5-68></a>
</span><span id=__span-5-69><a id=__codelineno-5-69 name=__codelineno-5-69 href=#__codelineno-5-69></a><span class=nt>warmup_steps</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
</span><span id=__span-5-70><a id=__codelineno-5-70 name=__codelineno-5-70 href=#__codelineno-5-70></a><span class=nt>evals_per_epoch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span id=__span-5-71><a id=__codelineno-5-71 name=__codelineno-5-71 href=#__codelineno-5-71></a><span class=nt>eval_table_size</span><span class=p>:</span>
</span><span id=__span-5-72><a id=__codelineno-5-72 name=__codelineno-5-72 href=#__codelineno-5-72></a><span class=nt>eval_max_new_tokens</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
</span><span id=__span-5-73><a id=__codelineno-5-73 name=__codelineno-5-73 href=#__codelineno-5-73></a><span class=nt>saves_per_epoch</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class=w> </span><span class=c1># change it from 1 to 4 to to make multiple save during the Epoch</span>
</span><span id=__span-5-74><a id=__codelineno-5-74 name=__codelineno-5-74 href=#__codelineno-5-74></a><span class=nt>debug</span><span class=p>:</span>
</span><span id=__span-5-75><a id=__codelineno-5-75 name=__codelineno-5-75 href=#__codelineno-5-75></a><span class=nt>deepspeed</span><span class=p>:</span>
</span><span id=__span-5-76><a id=__codelineno-5-76 name=__codelineno-5-76 href=#__codelineno-5-76></a><span class=nt>weight_decay</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">0.0</span>
</span><span id=__span-5-77><a id=__codelineno-5-77 name=__codelineno-5-77 href=#__codelineno-5-77></a><span class=nt>fsdp</span><span class=p>:</span>
</span><span id=__span-5-78><a id=__codelineno-5-78 name=__codelineno-5-78 href=#__codelineno-5-78></a><span class=nt>fsdp_config</span><span class=p>:</span>
</span><span id=__span-5-79><a id=__codelineno-5-79 name=__codelineno-5-79 href=#__codelineno-5-79></a><span class=nt>special_tokens</span><span class=p>:</span><span class=w> </span><span class=c1># very important </span>
</span><span id=__span-5-80><a id=__codelineno-5-80 name=__codelineno-5-80 href=#__codelineno-5-80></a><span class=w>  </span><span class=nt>bos_token</span><span class=p>:</span><span class=w> </span><span class=s>"&lt;s&gt;"</span><span class=w> </span><span class=c1># these tokens will be added at the first of every prompt</span>
</span><span id=__span-5-81><a id=__codelineno-5-81 name=__codelineno-5-81 href=#__codelineno-5-81></a><span class=w>  </span><span class=nt>eos_token</span><span class=p>:</span><span class=w> </span><span class=s>"&lt;/s&gt;"</span><span class=w> </span><span class=c1># these tokens will be added at the end of every prompt</span>
</span><span id=__span-5-82><a id=__codelineno-5-82 name=__codelineno-5-82 href=#__codelineno-5-82></a><span class=w>  </span><span class=nt>unk_token</span><span class=p>:</span><span class=w> </span><span class=s>"&lt;unk&gt;"</span>
</span></code></pre></div> <p>after you have made the necessary changes to the finetuning script</p> <p>to run the finetuning </p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class="l l-Scalar l-Scalar-Plain">accelerate launch -m axolotl.cli.train examples/mistral/lora.yml</span>
</span></code></pre></div> <p>to run inference on the model and test out the model </p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class="l l-Scalar l-Scalar-Plain">python -m axolotl.cli.inference examples/mistral/lora.yml --lora_model_dir="./lora-out"</span>
</span></code></pre></div> <p>to merge the model</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class="l l-Scalar l-Scalar-Plain">python3 -m axolotl.cli.merge_lora examples/mistral/lora.yml --lora_model_dir="./lora-out"</span>
</span></code></pre></div> <p>push the model to huggingface </p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class="l l-Scalar l-Scalar-Plain">pip install -U "huggingface_hub[cli]"</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class="l l-Scalar l-Scalar-Plain">huggingface-cli upload {name of model} ./{output directory} --repo-type model</span>
</span></code></pre></div> <h3 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">¶</a></h3> <p>In this blog post, we've explored the exciting world of fine-tuning AI models using Axolotl, a powerful tool that enhances the customization and precision of language models. We've discussed the importance of dataset preparation, model selection, and configuration settings for successful fine-tuning projects.</p> <p>By leveraging Axolotl's capabilities, we've embarked on intriguing projects like the YouTube Cloner, aiming to emulate the speaking style of popular YouTubers using language models. Throughout this journey, we've emphasized the creative potential and experimental nature of fine-tuning, showcasing how AI can adapt to specific tasks and datasets.</p> <p>Now equipped with the necessary knowledge and tools, you're ready to embark on your own fine-tuning adventures. Whether it's replicating existing projects or exploring new use cases, Axolotl offers a robust framework to streamline and optimize the fine-tuning process.</p> <h2 id=closing-thoughts>Closing Thoughts<a class=headerlink href=#closing-thoughts title="Permanent link">¶</a></h2> <p>As we conclude this exploration into fine-tuning using Axolotl, it's evident that the tool not only simplifies the process but also enhances the capabilities of AI practitioners. The ability to fine-tune models with precision, coupled with the flexibility offered by Axolotl, opens doors to a new era of AI customization.</p> <p>Embark on your journey with Axolotl, where every fine-tuning endeavor transforms into a seamless and efficient experience. Stay tuned for more insights and updates as we navigate the evolving landscape of AI refinement.</p> <blockquote> <p><em>If you found this post valuable, make sure to follow me for more insightful content. I frequently write about the practical applications of Generative AI, LLMs, Stable Diffusion, and explore the broader impacts of AI on society.</em> </p> <p><em>Let's stay connected on <a href=https://twitter.com/adithya_s_k>Twitter</a>. I'd love to engage in discussions with you.</em> </p> <p><em>If you're not a Medium member yet and wish to support writers like me, consider signing up through my referral link: <a href=https://adithyask.medium.com/membership>Medium Membership</a>. Your support is greatly appreciated!</em> </p> </blockquote> <h3 id=resources>Resources<a class=headerlink href=#resources title="Permanent link">¶</a></h3> <p><a href=https://dzlab.github.io/dltips/en/pytorch/llama-2-finetuning-axolotl/ >Fine-tuning Llama 2 with axolotl</a></p> <p><a href=https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672>A Beginner’s Guide to LLM Fine-Tuning</a></p> <p><a href=https://medium.com/@rohanbalkondekar/finetuning-llms-for-text-to-task-c04374454ac0>Finetuning LLMs For Text-to-Task</a></p> <p><a href=https://duarteocarmo.com/blog/fine-tune-llama-2-telegram>A poor man's guide to fine-tuning Llama 2</a></p> <p><a href=https://www.animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/ >Fine Tuning Llama Models With Qlora and Axolotl | ANIMAL-MACHINE</a></p> <p><a href=https://github.com/OpenAccess-AI-Collective/axolotl>https://github.com/OpenAccess-AI-Collective/axolotl</a></p></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 13, 2025 19:27:59 UTC">October 13, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 13, 2025 19:27:59 UTC">October 13, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>