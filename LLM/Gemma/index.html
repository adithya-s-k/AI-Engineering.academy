<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Mastering Applied AI, One Concept at a Time"><meta name=author content="Adithya S Kolavi"><link href=https://aiengineering.academy/LLM/Gemma/ rel=canonical><link href=../HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/ rel=prev><link href=Gemma_finetuning_notebook/ rel=next><link rel=icon href=../../assets/logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.21"><title>A Beginner’s Guide to Fine-Tuning Gemma - Adithya S K - Medium - AI Engineering Academy</title><link rel=stylesheet href=../../assets/stylesheets/main.2a3383ac.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-JP3605WT7D"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-JP3605WT7D",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-JP3605WT7D",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="A Beginner’s Guide to Fine-Tuning Gemma - Adithya S K - Medium - AI Engineering Academy"><meta property=og:description content="Mastering Applied AI, One Concept at a Time"><meta property=og:image content=https://aiengineering.academy/assets/images/social/LLM/Gemma/README.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://aiengineering.academy/LLM/Gemma/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="A Beginner’s Guide to Fine-Tuning Gemma - Adithya S K - Medium - AI Engineering Academy"><meta name=twitter:description content="Mastering Applied AI, One Concept at a Time"><meta name=twitter:image content=https://aiengineering.academy/assets/images/social/LLM/Gemma/README.png><link rel=stylesheet href=../../assets/stylesheets/custom.7c86dd97.min.css><!-- PostHog Analytics --><script>
  !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
  posthog.init('phc_OL7nUCVeKtVJe8eHSKGs8zPTQAyr0hm8opAPFdFlkBz', {
      api_host: 'https://us.i.posthog.com',
      person_profiles: 'identified_only', // or 'always' to create profiles for anonymous users as well
  })
</script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#meet-gemma class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@adithya_s_k</strong> on <a href=https://x.com/adithya_s_k> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="AI Engineering Academy" class="md-header__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Engineering Academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> A Beginner’s Guide to Fine-Tuning Gemma - Adithya S K - Medium </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=cyan aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../PromptEngineering/ class=md-tabs__link> Prompt Engineering </a> </li> <li class=md-tabs__item> <a href=../../RAG/ class=md-tabs__link> RAG </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> LLM </a> </li> <li class=md-tabs__item> <a href=../../Deployment/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../Agents/ class=md-tabs__link> Agents </a> </li> <li class=md-tabs__item> <a href=../../Projects/ class=md-tabs__link> Projects </a> </li> <li class=md-tabs__item> <a href=../../blog/ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="AI Engineering Academy" class="md-nav__button md-logo" aria-label="AI Engineering Academy" data-md-component=logo> <img src=../../assets/logo.png alt=logo> </a> AI Engineering Academy </label> <div class=md-nav__source> <a href=https://github.com/adithya-s-k/AI-Engineering.academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> adithya-s-k/AI-Engineering.academy </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../PromptEngineering/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../RAG/ class=md-nav__link> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> LLM </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> Finetuning Techniques </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Finetuning Techniques </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/PreTrain/ class=md-nav__link> <span class=md-ellipsis> PreTraining LLMs </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../TheoryBehindFinetuning/SFT/ class=md-nav__link> <span class=md-ellipsis> SFT </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/PPO/ class=md-nav__link> <span class=md-ellipsis> PPO(Proximal Policy Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/DPO/ class=md-nav__link> <span class=md-ellipsis> DPO(Direct Preference Optimization) </span> </a> </li> <li class=md-nav__item> <a href=../TheoryBehindFinetuning/ORPO/ class=md-nav__link> <span class=md-ellipsis> ORPO(Odds Ratio Preference Optimization) </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../TheoryBehindFinetuning/GRPO/ class=md-nav__link> <span class=md-ellipsis> GRPO(Group Relative Policy Optimization) </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3 checked> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> LLM Finetuning Hands on </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=true> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> LLM Finetuning Hands on </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3_1 checked> <div class="md-nav__link md-nav__container"> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Gemma </span> </a> <label class="md-nav__link md-nav__link--active" for=__nav_4_3_1 id=__nav_4_3_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_3_1_label aria-expanded=true> <label class=md-nav__title for=__nav_4_3_1> <span class="md-nav__icon md-icon"></span> Gemma </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=Gemma_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Finetune Gemma </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../LLama2/Llama2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Llama2 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../Llama3_finetuning_notebook.ipynb class=md-nav__link> <span class=md-ellipsis> Llama3 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../Mistral-7b/ class=md-nav__link> <span class=md-ellipsis> Mistral </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> VLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> VLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../VLM/Florence2_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> Florence2 </span> </a> </li> <li class=md-nav__item> <a href=../VLM/PaliGemma_finetuning_notebook/ class=md-nav__link> <span class=md-ellipsis> PaliGemma </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_5> <div class="md-nav__link md-nav__container"> <a href=../ServerLessFinetuning/ class="md-nav__link "> <span class=md-ellipsis> Serverless Finetuning with Modal </span> </a> <label class="md-nav__link " for=__nav_4_5 id=__nav_4_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> Serverless Finetuning with Modal </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ServerLessFinetuning/TrainNanoGPTModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Training NanoGPT on Modal </span> </a> </li> <li class=md-nav__item> <a href=../ServerLessFinetuning/TrainNanochatModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Training Nanochat on Modal </span> </a> </li> <li class=md-nav__item> <a href=../ServerLessFinetuning/FinetuneGemmaUnslothModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Fine-tuning Gemma with Unsloth </span> </a> </li> <li class=md-nav__item> <a href=../ServerLessFinetuning/FinetuneLlamaAxolotlGPUModalTutorial/ class=md-nav__link> <span class=md-ellipsis> Multi-GPU Training with Axolotl </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4_6> <div class="md-nav__link md-nav__container"> <a href=../LLMArchitecture/ParameterCount/ class="md-nav__link "> <span class=md-ellipsis> LLM Architecture </span> </a> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_6_label aria-expanded=false> <label class=md-nav__title for=__nav_4_6> <span class="md-nav__icon md-icon"></span> LLM Architecture </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Deployment/ class=md-nav__link> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Projects/ class=md-nav__link> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../blog/ class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#meet-gemma class=md-nav__link> <span class=md-ellipsis> Meet Gemma </span> </a> </li> <li class=md-nav__item> <a href=#colab-notebook-to-finetuning class=md-nav__link> <span class=md-ellipsis> Colab Notebook to Finetuning </span> </a> </li> <li class=md-nav__item> <a href=#github-repository class=md-nav__link> <span class=md-ellipsis> Github Repository </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#getting-started class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> </li> <li class=md-nav__item> <a href=#checking-gpu class=md-nav__link> <span class=md-ellipsis> Checking GPU </span> </a> </li> <li class=md-nav__item> <a href=#loading-required-libraries class=md-nav__link> <span class=md-ellipsis> Loading Required Libraries </span> </a> </li> <li class=md-nav__item> <a href=#logging-into-hugging-face-hub class=md-nav__link> <span class=md-ellipsis> Logging into Hugging Face Hub </span> </a> </li> <li class=md-nav__item> <a href=#loading-the-model class=md-nav__link> <span class=md-ellipsis> Loading the Model </span> </a> </li> <li class=md-nav__item> <a href=#loading-the-dataset class=md-nav__link> <span class=md-ellipsis> Loading the Dataset </span> </a> </li> <li class=md-nav__item> <a href=#formatting-the-dataset class=md-nav__link> <span class=md-ellipsis> Formatting the Dataset </span> </a> </li> <li class=md-nav__item> <a href=#after-formatting-we-should-get-something-like-this class=md-nav__link> <span class=md-ellipsis> After Formatting, We should get something like this </span> </a> </li> <li class=md-nav__item> <a href=#setting-model-parameters-and-lora class=md-nav__link> <span class=md-ellipsis> Setting Model Parameters and Lora </span> </a> </li> <li class=md-nav__item> <a href=#apply-lora class=md-nav__link> <span class=md-ellipsis> Apply Lora </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning-with-qlora-and-supervised-finetuning class=md-nav__link> <span class=md-ellipsis> Fine-Tuning with qLora and Supervised Finetuning </span> </a> </li> <li class=md-nav__item> <a href=#new-code-using-sfttrainer class=md-nav__link> <span class=md-ellipsis> new code using SFTTrainer </span> </a> <nav class=md-nav aria-label="new code using SFTTrainer"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#lets-start-the-training-process class=md-nav__link> <span class=md-ellipsis> Lets start the training process </span> </a> </li> <li class=md-nav__item> <a href=#merge-and-share class=md-nav__link> <span class=md-ellipsis> Merge and Share </span> </a> </li> <li class=md-nav__item> <a href=#test-the-merged-model class=md-nav__link> <span class=md-ellipsis> Test the merged model </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/edit/master/docs/LLM/Gemma/README.md title="Edit this page" class="md-content__button md-icon" rel=edit> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy/raw/master/docs/LLM/Gemma/README.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <div><p>URL Source: <a href=https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-gemma-0444d46d821c>https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-gemma-0444d46d821c</a></p> <p>Published Time: 2024-02-21T20:13:11.688Z</p> <p>Markdown Content: <a href="https://adithyask.medium.com/?source=post_page-----0444d46d821c--------------------------------"><img alt="Image 1: Adithya S K" src=https://miro.medium.com/v2/resize:fill:88:88/1*w1_VSVDg5oqt19oTB4MAMg.jpeg></a></p> <p>A Comprehensive Guide to Fine-Tuning Gemma</p> <p>Fine-tuning a state-of-the-art language model like Gemma can be an exciting journey. This guide will walk you through the process step by step, from setting up your environment to fine-tuning the model for your specific task. Whether you’re a seasoned machine learning practitioner or a newcomer to the field, this beginner-friendly tutorial will help you harness the power of Gemma for your projects.</p> <h2 id=meet-gemma>Meet Gemma<a class=headerlink href=#meet-gemma title="Permanent link">¶</a></h2> <p>a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.</p> <h2 id=colab-notebook-to-finetuning>Colab Notebook to Finetuning<a class=headerlink href=#colab-notebook-to-finetuning title="Permanent link">¶</a></h2> <h2 id=github-repository>Github Repository<a class=headerlink href=#github-repository title="Permanent link">¶</a></h2> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">¶</a></h2> <p>Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:</p> <p>1. <strong>GPU</strong>: <a href=https://huggingface.co/google/gemma-2b>gemma-2b</a> — can be finetuned on T4(free google colab) while <a href=https://huggingface.co/google/gemma-7b>gemma-7b</a> requires an A100 GPU.</p> <p>2. <strong>Python Packages</strong>: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:</p> <p>Let’s begin by checking if your GPU is correctly detected:</p> <p>!pip3 install -q -U bitsandbytes==0.42.0<br> !pip3 install -q -U peft==0.8.2<br> !pip3 install -q -U trl==0.7.10<br> !pip3 install -q -U accelerate==0.27.1<br> !pip3 install -q -U datasets==2.17.0<br> !pip3 install -q -U transformers==4.38.0</p> <p>Hugging Face Hub Account: You’ll need an account on the Hugging Face Model Hub. You can sign up <a href=https://huggingface.co/join>here</a>.</p> <h2 id=getting-started>Getting Started<a class=headerlink href=#getting-started title="Permanent link">¶</a></h2> <h2 id=checking-gpu>Checking GPU<a class=headerlink href=#checking-gpu title="Permanent link">¶</a></h2> <p>Let’s start by checking if your GPU is correctly detected:</p> <p>!nvidia-smi</p> <p>If your GPU is not recognized or you encounter CUDA out-of-memory errors during fine-tuning, consider using a more powerful GPU.</p> <h2 id=loading-required-libraries>Loading Required Libraries<a class=headerlink href=#loading-required-libraries title="Permanent link">¶</a></h2> <p>We’ll load the necessary Python libraries for our fine-tuning process:</p> <p>import json<br> import pandas as pd<br> import torch<br> from datasets import Dataset, load_dataset<br> from huggingface_hub import notebook_login<br> from peft import LoraConfig, PeftModel<br> from transformers import (<br> AutoModelForCausalLM,<br> AutoTokenizer,<br> BitsAndBytesConfig,<br> TrainingArguments,<br> pipeline,<br> logging,<br> )<br> from trl import SFTTrainer</p> <h2 id=logging-into-hugging-face-hub>Logging into Hugging Face Hub<a class=headerlink href=#logging-into-hugging-face-hub title="Permanent link">¶</a></h2> <p>Log in to the Hugging Face Model Hub using your credentials:</p> <p>notebook_login()</p> <h2 id=loading-the-model>Loading the Model<a class=headerlink href=#loading-the-model title="Permanent link">¶</a></h2> <p>model_id = "google/gemma-7b-it"<br> # model_id = "google/gemma-7b"<br> # model_id = "google/gemma-2b-it"<br> # model_id = "google/gemma-2b"model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})<br> tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)</p> <h2 id=loading-the-dataset>Loading the Dataset<a class=headerlink href=#loading-the-dataset title="Permanent link">¶</a></h2> <p>For this tutorial, we will fine-tune Gemma Instruct for code generation.</p> <p>we will be using this <a href=https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style>dataset</a> which is curated by <a href=https://twitter.com/4evaBehindSOTA>TokenBender (e/xperiments)</a> which is a awesome data for finetuning model for code generation. It follows the alpaca style of instructions which is an excellent starting point for this task. The dataset structure should resemble the following:</p> <p>{<br> "instruction": "Create a function to calculate the sum of a sequence of integers.",<br> "input":"[1, 2, 3, 4, 5]",<br> "output": "# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum"<br> }</p> <p>now lets load the dataset using huggingfaces datasets library</p> <p># Load your dataset (replace 'your_dataset_name' and 'split_name' with your actual dataset information)<br> # dataset = load_dataset("your_dataset_name", split="split_name")<br> dataset = load_dataset("TokenBender/code_instructions_122k_alpaca_style", split="train")</p> <h2 id=formatting-the-dataset>Formatting the Dataset<a class=headerlink href=#formatting-the-dataset title="Permanent link">¶</a></h2> <p>Now, let’s format the dataset in the required <a href=https://huggingface.co/google/gemma-7b-it>gemma instruction formate</a>.</p> <blockquote> <p><em>Many tutorials and blogs skip over this part, but I feel this is a really important step.</em></p> </blockquote> <p>&lt;start_of_turn&gt;user What is your favorite condiment? &lt;end_of_turn&gt;<br> &lt;start_of_turn&gt;model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!&lt;end_of_turn&gt;</p> <p>You can use the following code to process your dataset and create a JSONL file in the correct format:</p> <p>def generate_prompt(data_point):<br> """Gen. input text based on a prompt, task instruction, (context info.), and answer:param data_point: dict: Data point<br> :return: dict: tokenzed prompt<br> """<br> prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\<br> 'appropriately completes the request.\\n\\n'<br> # Samples with additional context into.<br> if data_point['input']:<br> text = f"""&lt;start_of_turn&gt;user {prefix_text} {data_point["instruction"]} here are the inputs {data_point["input"]} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point["output"]} &lt;end_of_turn&gt;"""<br> # Without<br> else:<br> text = f"""&lt;start_of_turn&gt;user {prefix_text} {data_point["instruction"]} &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model{data_point["output"]} &lt;end_of_turn&gt;"""<br> return text</p> <p># add the "prompt" column in the dataset<br> text_column = [generate_prompt(data_point) for data_point in dataset]<br> dataset = dataset.add_column("prompt", text_column)</p> <p>We'll need to tokenize our data so the model can understand.</p> <p>dataset = dataset.shuffle(seed=1234) # Shuffle dataset here<br> dataset = dataset.map(lambda samples: tokenizer(samples["prompt"]), batched=True)</p> <p>Split dataset into 90% for training and 10% for testing</p> <p>dataset = dataset.train_test_split(test_size=0.2)<br> train_data = dataset["train"]<br> test_data = dataset["test"]</p> <h2 id=after-formatting-we-should-get-something-like-this>After Formatting, We should get something like this<a class=headerlink href=#after-formatting-we-should-get-something-like-this title="Permanent link">¶</a></h2> <p>{<br> "text":"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;<br> &lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;",<br> "instruction":"Create a function to calculate the sum of a sequence of integers",<br> "input":"[1, 2, 3, 4, 5]",<br> "output":"# Python code def sum_sequence(sequence): sum = 0 for num in,<br> sequence: sum += num return sum",<br> "prompt":"&lt;start_of_turn&gt;user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] &lt;end_of_turn&gt;<br> &lt;start_of_turn&gt;model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum &lt;end_of_turn&gt;"<br> } </p> <p>While using SFT (<a href=https://huggingface.co/docs/trl/main/en/sft_trainer><strong>Supervised Fine-tuning Trainer</strong></a>) for fine-tuning, we will be only passing in the “text” column of the dataset for fine-tuning.</p> <h2 id=setting-model-parameters-and-lora>Setting Model Parameters and Lora<a class=headerlink href=#setting-model-parameters-and-lora title="Permanent link">¶</a></h2> <p>We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:</p> <h2 id=apply-lora>Apply Lora<a class=headerlink href=#apply-lora title="Permanent link">¶</a></h2> <p>Here comes the magic with peft! Let’s load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and the prepare_model_for_kbit_training method from PEFT.</p> <p>Here is a tweet on how to pick the best Lora config</p> <p>from peft import LoraConfig, get_peft_model<br> lora_config = LoraConfig(<br> r=64,<br> lora_alpha=32,<br> target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'],<br> lora_dropout=0.05,<br> bias="none",<br> task_type="CAUSAL_LM"<br> ) <br> model = get_peft_model(model, lora_config)</p> <p>Calculating the number of trainable parameters</p> <p>trainable, total = model.get_nb_trainable_parameters()<br> print(f"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%")</p> <blockquote> <p>expected output → Trainable: 200015872 | total: 8737696768 | Percentage: 2.2891%</p> </blockquote> <h2 id=fine-tuning-with-qlora-and-supervised-finetuning>Fine-Tuning with qLora and Supervised Finetuning<a class=headerlink href=#fine-tuning-with-qlora-and-supervised-finetuning title="Permanent link">¶</a></h2> <p>We’re ready to fine-tune our model using qLora. For this tutorial, we’ll use the <code>SFTTrainer</code> from the <code>trl</code> library for supervised fine-tuning. Ensure that you've installed the <code>trl</code> library as mentioned in the prerequisites.</p> <h1 id=new-code-using-sfttrainer>new code using SFTTrainer<a class=headerlink href=#new-code-using-sfttrainer title="Permanent link">¶</a></h1> <p>import transformersfrom trl import SFTTrainer</p> <p>tokenizer.pad_token = tokenizer.eos_token<br> torch.cuda.empty_cache()<br> trainer = SFTTrainer(<br> model=model,<br> train_dataset=train_data,<br> eval_dataset=test_data,<br> dataset_text_field="prompt",<br> peft_config=lora_config,<br> args=transformers.TrainingArguments(<br> per_device_train_batch_size=1,<br> gradient_accumulation_steps=4,<br> warmup_steps=0.03,<br> max_steps=100,<br> learning_rate=2e-4,<br> logging_steps=1,<br> output_dir="outputs",<br> optim="paged_adamw_8bit",<br> save_strategy="epoch",<br> ),<br> data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),<br> )</p> <h2 id=lets-start-the-training-process>Lets start the training process<a class=headerlink href=#lets-start-the-training-process title="Permanent link">¶</a></h2> <p># Start the training process<br> trainer.train()new_model = "gemma-Code-Instruct-Finetune-test" #Name of the model you will be pushing to huggingface model hub<br> # Save the fine-tuned model<br> trainer.model.save_pretrained(new_model)</p> <h2 id=merge-and-share>Merge and Share<a class=headerlink href=#merge-and-share title="Permanent link">¶</a></h2> <p>After fine-tuning, if you want to merge the model with LoRA weights or share it with the Hugging Face Model Hub, you can do so. This step is optional and depends on your specific use case.</p> <p># Merge the model with LoRA weights<br> base_model = AutoModelForCausalLM.from_pretrained(<br> model_id,<br> low_cpu_mem_usage=True,<br> return_dict=True,<br> torch_dtype=torch.float16,<br> device_map={"": 0},<br> )<br> merged_model= PeftModel.from_pretrained(base_model, new_model)<br> merged_model= merged_model.merge_and_unload()# Save the merged model<br> merged_model.save_pretrained("merged_model",safe_serialization=True)<br> tokenizer.save_pretrained("merged_model")<br> tokenizer.pad_token = tokenizer.eos_token<br> tokenizer.padding_side = "right"</p> <p># Push the model and tokenizer to the Hugging Face Model Hub<br> merged_model.push_to_hub(new_model, use_temp_dir=False)<br> tokenizer.push_to_hub(new_model, use_temp_dir=False)</p> <h2 id=test-the-merged-model>Test the merged model<a class=headerlink href=#test-the-merged-model title="Permanent link">¶</a></h2> <p>def get_completion(query: str, model, tokenizer) -&gt; str:<br> device = "cuda:0"<br> prompt_template = """<br> &lt;start_of_turn&gt;user<br> Below is an instruction that describes a task. Write a response that appropriately completes the request.<br> {query}<br> &lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model"""<br> prompt = prompt_template.format(query=query)<br> encodeds = tokenizer(prompt, return_tensors="pt", add_special_tokens=True)<br> model_inputs = encodeds.to(device)<br> generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)<br> # decoded = tokenizer.batch_decode(generated_ids)<br> decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)<br> return (decoded)</p> <p>result = get_completion(query="code the fibonacci series in python using reccursion", model=merged_model, tokenizer=tokenizer)<br> print(result)</p> <p>And that’s it! You’ve successfully fine-tuned Gemma Instruct for code generation. You can adapt this process for various natural language understanding and generation tasks. Keep exploring and experimenting with Gemma to unlock its full potential for your projects.</p> <p>Happy Fine-Tuning!!</p> <blockquote> <p><em>If you found this post valuable, make sure to follow me for more insightful content. I frequently write about the practical applications of Generative AI, LLMs, Stable Diffusion, and explore the broader impacts of AI on society.</em></p> <p><em>Let’s stay connected on</em> <a href=https://twitter.com/adithya_s_k><em>Twitter</em></a><em>. I’d love to engage in discussions with you.</em></p> <p><em>If you’re not a Medium member yet and wish to support writers like me, consider signing up through my referral link:</em> <a href=https://adithyask.medium.com/membership><em>Medium Membership</em></a><em>. Your support is greatly appreciated!</em></p> </blockquote></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 13, 2025 19:27:59 UTC">October 13, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="October 13, 2025 19:27:59 UTC">October 13, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../HandsOnWithFinetuning/GRPO/hacker_guide_to_GRPO/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Hands on with GRPO"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Hands on with GRPO </div> </div> </a> <a href=Gemma_finetuning_notebook/ class="md-footer__link md-footer__link--next" aria-label="Next: Finetune Gemma"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Finetune Gemma </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Adithya S Kolavi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://github.com/adithya-s-k/AI-Engineering.academy target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 444.7a20.4 20.4 0 1 1 0-40.7 20.4 20.4 0 1 1 0 40.7M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.6-183.4a20.4 20.4 0 1 1 0 40.8 20.4 20.4 0 1 1 0-40.8"/></svg> </a> <a href=https://x.com/adithya_s_k target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../assets/javascripts/custom.9e5da760.min.js></script> <!-- Rich Snippets / Structured Data --> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "EducationalOrganization",
    "name": "AI Engineering Academy",
    "url": "https://aiengineering.academy",
    "logo": "https://aiengineering.academy/assets/logo.png",
    "description": "A structured learning platform for AI engineers with clear paths in prompt engineering, RAG, fine-tuning, deployment, and agent development.",
    "sameAs": [
      "https://github.com/adithya-s-k/AI-Engineering.academy",
      "https://x.com/adithya_s_k"
    ],
    "founder": {
      "@type": "Person",
      "name": "Adithya S Kolavi"
    },
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    }
  }
</script> </body> </html>